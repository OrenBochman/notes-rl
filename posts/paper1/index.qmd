---
title: Dynamic collaborative filtering Thompson Sampling for cross-domain advertisements recommendation
subtitle: Paper Review
date: 2025-01-18
categories:
    - Paper 
    - Review
    - Bandit
    - Advertising
    - Collaborative Filtering
keywords:
    - Thompson Sampling
    - Recomender System
    - Collaborative Filtering
    - Doubly Robust Estimator
bibliography: bibliography.bib
image: cover.jpg
---

![litrature review](cover.jpg){.column-margin}


{{< video https://youtu.be/DAL7cpE3K7E >}}


So I don't have much time for this today so here is a quick note on:  [@ishikawa2022dynamiccollaborativefilteringthompson]

::: {.callout-tip }

## TL;DR

1. The authors are using **Thompson Sampling**. This is a Bayesian method in RL.
2. Thier problem is an advert recommendation system. So they are integrating Thompson sampling into making recommendations.
3. They use a doubly robust estimation method. This is something I learned about from Emma Brunskill's guest lecture in the Alberta Coursera course. And ever since I've been looking on how to do this in RL. Unforetunatly all I could find was work that used it in offline RL settings. So I was stoked to see it used as a central part of this paper. Using a doubly robust estimator is a sound technique for reducing variance without introducing a bias. And variance is the gratest impediment to learning quickly in RL. Also unlike some other ideas I've come accross it seems to align very well with Causal Inference.
4. The talk mentions a dataset the authors used for doing this work. Is this dataset available? I would like to try this out


One line on Thompson sampling,  one of the oldest technique in the RL playbook which uses the following rule: pick an action at random from the posterior distribution of the action values and then use the outcome to update the posterior distribution for the next step.


## The Paper

![paper](paper.pdf){.col-page width="800px" height="1000px"}


::: {.callout-caution }
## My ideas

- Find what data set was used.
- Is this dataset available?
- Can we make a minimal version to quickly test this kind of agent?
- Figure out a framework that extends tompson sampling to other RL problems.
    - need to add P(action|state) i.e. add conditioning of the bernulli on the state.
    - prehaps do simple counts of steps since starts or last reward.
    - prehaps using a succeror representation can help


- Marketing are the worst POMDPs. Testing real stuff is very hard so a good environment might help.
- I want to make an petting zoo env to support single & multiagent:
    1. auctions / non autions
    2. advertising (rec sys) with costs
    3. pricing with policies.
    - It should also allow incorperating real data from a dataset. Diretly or via sampling
    - It would be even neater to do this using a heirarchiacal model.
    - It would be even better if we can also incorportate the product, user hierecies.
    - It would be great if we have a 

:::