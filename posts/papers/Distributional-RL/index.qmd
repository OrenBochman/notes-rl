---
date: 2025-01-18
title: A Distributional Perspective on Reinforcement Learning
subtitle: Paper Review
description: Distributional RL -- Same but Different
categories:
    - Paper 
    - Review
    - Bandit
    - Advertising
    - Collaborative Filtering
keywords:
    - Thompson Sampling
    - Recomender System
    - Collaborative Filtering
bibliography: ./bibliography.bib
image: cover.jpg
draft: true
---

![litrature review](cover.jpg){.column-margin}

This is a more Bayesian version of RL. I have my own ideas on this but this is connected to Normalizing Flows and the like. 
The other idea is that we can view the bellman equation as a contraction mapping (aka $\gamma$).
I will get to this later.


::: {#fig-vid01}
{{< video https://youtu.be/ba_l8IKoMvU >}}

So I don't have much time for this today so here is a quick note on:  
[@bellemare2017distributionalperspectivereinforcementlearning]
:::



::: {.callout-tip }

## TL;DR - A Distributional Perspective on Reinforcement Learning {.unnumbered}

![Distributional RL in a nutshell](../img/in_a_nutshell.jpg)

1. The authors are using **Thompson Sampling**. This is a Bayesian method in RL.
1. Marc Bellemare is smooth Operator - and he defines a distributional Bellman operator.
1. 
    
:::

## Abstract

> In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting. -- @bellemare2017distributionalperspectivereinforcementlearning

::: {.callout-caution }
## My ideas

:::


## The Paper

![paper](paper.pdf){.col-page width="800px" height="1000px"}
