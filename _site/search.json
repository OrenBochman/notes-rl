[
  {
    "objectID": "t/welcome/index.html",
    "href": "t/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/c1-w2.html",
    "href": "posts/c1-w2.html",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/c1-w2.html#sec-Markov-Process",
    "href": "posts/c1-w2.html#sec-Markov-Process",
    "title": "Markov Decision Processes",
    "section": "Markov Process",
    "text": "Markov Process\nSilver goes into some detail on what we mean by state in RL:\n\nIn th abstract state can be any function of the history.\nThe state should summarize the information on the previous actions, and rewards.\nHe points out that the history in RL can be very long, for Atari games it can include actions plus all the pixels for every screen in many plays of the game. In contrast the state tries to capture the bare essentials of the history for decision making at each time step. For Atari games they used the last 4 screens as the state.\nA second point is that there is always a state. The full history is also a state, but not a very useful one. The internal representation of the ram in the Atari game is also a state, much smaller but this is the representation used by the environment and contains more information than is available to the agent. Ideally the agent would want to model this state, but again it contains lots more information than is available would need to male a decision.\nAnother useful property of the state is that it should have the Markov Property for a state space which is when the future is independent of the past given the present.\n\nA state S\\(_t\\) is Markov if and only if:\n\\[\n\\mathbb{P}[S_{t+1}  \\vert  S_{t}] =  \\mathbb{P}[S_{t+1}  \\vert  S_1,...,S_t] \\qquad \\text{(Markov Property)}\n\\tag{1}\\]\nThe state captures all relevant information from the history Once the state is known, the history may be thrown away i.e. The state is a sufficient statistic of the future\nRecall:\n\na statistic satisfies the criterion for sufficency when no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter”. — [@doi:10.1098/rsta.1922.0009]\n\nFor a Markov state \\(s\\) and successor state \\(s'\\), the state transition probability is defined by:\n\\[\n\\mathbb{P}_{ss'} = \\mathbb{P}[S_{t+1}=s'  \\vert  S_t=s]\n\\]\n\n\n\n\n\n\nMarkov Process or Chain Definion\n\n\n\n\nA Markov Process is\n\na tuple \\(⟨S,P⟩\\)\n\n\nwhere:\n\n\\(S\\) is a (finite) set of states\n\\(P\\) is a state transition probability matrix, \\(P_{ss'} = P[S_{t+1} = s'  \\vert S_t=s]\\) State transition matrix \\(P_{ss'}\\) defines transition probabilities from all states \\(s\\) to all successor states \\(s'\\),\n\n\\[\n\\begin{align*}\n  P=\\left( \\begin{array}{cc}\n      p_{11} & \\cdots & p_{1n} \\newline\n      \\vdots & \\ddots & \\vdots \\newline\n      p_{n1} & \\cdots & p_{nn} \\end{array} \\right)\n\\end{align*}\n\\tag{2}\\]"
  },
  {
    "objectID": "posts/c1-w2.html#sec-MRP",
    "href": "posts/c1-w2.html#sec-MRP",
    "title": "Markov Decision Processes",
    "section": "Markov Reward Process",
    "text": "Markov Reward Process\nA Markov Reward Process (MRP) is a Markov chain with values.\n\n\n\n\n\n\nMarkov Reward Process Definition\n\n\n\n\nA Markov Reward Process is\n\na tuple \\(⟨S, P, R, \\gamma⟩\\)\n\n\nwhere:\n\n\\(S\\) is a finite set of states\n\\(P\\) is a state transition probability matrix, where \\(P_{ss'} =  \\mathbb{P}[S_{t+1} = s' \\vert  S_t = s]\\)\n\\(R\\) is a reward function, \\(R_s = \\mathbb{E}[R_{t+1} \\vert S_t = s]\\)\n\\(\\gamma\\) is a discount factor, \\(\\gamma \\in [0, 1]\\)\n\n\n\n\n\n\n\n\n\nthe return definition\n\n\n\n\nThe return \\(G_t\\)\n\nis the total discounted reward from time-step t.\n\n\n\\[\nG_t =R_{t+1}+\\gamma R_{t+2}+...=\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\tag{3}\\]\nwhere:\n\n\\(R_t\\) is the reward at time-step \\(t\\)\n\\(\\gamma\\) the discount factor \\(\\gamma \\in [0, 1]\\) is the present value of future rewards.\nThe value of receiving reward \\(R\\) after \\(k+1\\) time-steps is \\(\\gamma^k R\\)\nThis values immediate reward above delayed reward.\n\n\\(\\gamma = 0\\) makes the agent short-sighted.\n\\(\\gamma = 1\\) makes the agent far-sighted.\n\n\n\n\n\n\n\n\n\n\nThe value function\n\n\n\n\nThe value function:\n\nThe state value function \\(v(s)\\) of an MRP is the expected return starting from state \\(s\\)\n\n\n\\[\nv(s) =\\mathbb{E} [G_t  \\vert  S_t = s]\n\\]\n\n\n\nBellman equations for MRP\nThe value function can be decomposed into two parts:\n\nan immediate reward \\(R_{t+1}\\) and\na discounted value of successor state \\(\\gamma v(S_{t+1})\\)\n\nThe Bellman equation for MRPs expresses this relationship:\n\\[\n\\begin{align*}\nv(s) &= \\mathbb{E}[G_t  \\vert  S_t=s] \\newline\n& = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2}+\\gamma^2 R_{t+3} + ...  \\vert S_t = s]   \\newline\n& = \\mathbb{E}[R_{t+1} + \\gamma( R_{t+2}+\\gamma^2 R_{t+3} + ... )  \\vert S_t = s]   \\newline\n& = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1}  \\vert  S_t = s]   \\newline\n& = \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1})  \\vert S_t = s]\n\\end{align*} \\qquad \\text{(Bellman Equation)}\n\\tag{4}\\]\nThe Bellman equation can also be expressed in terms of the dynamics matrix for state transitions:\n\\[\nv(s) = R_s + γ \\sum_{s'\\in S} P_{ss'}v(s) \\qquad \\text{value with dynamics}\n\\tag{5}\\]\nwhere we use the dynamics matrix \\(P\\) to express the expected value of the successor state."
  },
  {
    "objectID": "posts/c1-w2.html#sec-MDP",
    "href": "posts/c1-w2.html#sec-MDP",
    "title": "Markov Decision Processes",
    "section": "Markov Decision Processes",
    "text": "Markov Decision Processes\n\nThe k-Armed Bandit problem does not account for the fact that different situations call for different actions.\nBecause it the problem is limited to a single state agents can only make decisions based on immediate reward so they fail to consider the long-term impact of their decisions - this is an inability to make plan.\n\n\n\n\n\nThe agent–environment interaction in a Markov decision process.\n\n\n\n\n\n\n\nMDP definition\n\n\n\n\nA Markov Decision Process is a Markov Reward Process with decisions.\n\na tuple \\(⟨S, A, P, R, \\gamma⟩\\)\n\n\nwhere:\n\n\\(S\\) is a finite set of states\n\\(A\\) is a finite set of actions\n\\(P\\) is a state transition probability matrix, \\(P_{ss'}^a = \\mathbb{P}[S_{t+1} = s' \\vert S_t = s, A_t = a]\\)\n\\(R\\) is a reward function, \\(R_s^a = \\mathbb{E}[R_{t+1} \\vert S_t = s, A_t = a]\\)\n\\(\\gamma\\) is a discount factor, \\(\\gamma \\in [0, 1]\\)\n\n\n\n\nThe dynamics of an MDP\nIn a finite MDP, the sets of states, actions, and rewards (S, A and R) all have a finite number of elements. In this case, the random variables \\(S_t\\) and \\(R_t\\) have well defined discrete probability distributions dependent only on the preceding state and action.\nThe dynamics of an MDP are defined by the four argument dynamics function:\n\\[\np(s',r \\vert s,a)\\ \\dot =\\ Pr\\{S_t = s', R_t = r \\vert S_{t-1} = s, A_{t-1} = a\\}\\qquad  \\forall s',s \\in S\\ \\forall r\\in R\\ \\forall a\\in A\n\\tag{6}\\]\nwhere the sum of the probabilities over fixed set of s,a is 1:\n\\[\n\\sum_{s' \\in S} \\sum_{r \\in R} p(s',r \\vert s,a) = 1 \\qquad \\forall s \\in S, \\forall a \\in A \\qquad \\text{(Dynamics function)}\n\\tag{7}\\]\nThis is just a regular function that takes a state and action and returns a probability distribution over the next state and reward.\nIn a tabular setting, we can also express this function as a table. Here is my solution for ex 3.4, a table for the recycling robot\n\n\n\nTable 1: Dynamics function for a recycling robot MDP\n\n\n\n\n\n\\(s\\)\n\\(a\\)\n\\(s'\\)\n\\(r\\)\n\\(p(s',r \\mid s,a)\\)\n\n\n\n\nhigh\nsearch\nhigh\n\\(r_{search}\\)\n\\(\\alpha\\)\n\n\nhigh\nsearch\nlow\n\\(r_{search}\\)\n\\(1-\\alpha\\)\n\n\nlow\nsearch\nhigh\n-3\n\\(1-\\beta\\)\n\n\nlow\nsearch\nlow\n\\(r_{search}\\)\n\\(\\beta\\)\n\n\nhigh\nwait\nhigh\n\\(r_{wait}\\)\n1\n\n\nlow\nwait\nlow\n\\(r_{wait}\\)\n1\n\n\nlow\nrecharge\nhigh\n0\n1\n\n\n\n\n\n\na couple of takeaways from this exercise are:\n\nrewards are uniquely assigned for action at s resulting in s’ so we don’t need to make r another factor (i.e. list all possible rewards for (s,a,s’) tuple.\nthere are (s,a,s’,r) tuples for which we don’t have a non-zero probabilities - since there are no transition possible.\ne.g. the robot wont charge when high, so there isn’t a transition from that state, nor a reward, nor a probability.\n\n\n\nGraphical representation of an MDP\nThe graphical representation of an MDP is a directed graph where the nodes represent states and the edges represent actions. The graph is labeled with the probabilities of transitioning from one state to another given an action.\n\n\n\n\ngraphical MDP for cleaning robot\n\nIn an MDP, the probabilities given by four part dynamics function completely characterize the environment’s dynamics.\nThat is, the probability of each possible value for \\(S_{t+1}\\) and \\(R_{t+1}\\) depends only on the immediately preceding state \\(S_t\\) and action \\(A_t\\).\nThis is best viewed a restriction not on the decision process, but on the state.\nThe state must include information about all aspects of the past agent–environment interaction that make a difference for the future.\nIf it does, then the state is said to have the Markov property.\nWe can use the four-part dynamics function to compute the state transition probabilities for a given state and action:\n\\[\n\\begin{align}\n  p(s' \\vert s,a) &= \\mathbb{P}[S_t = s'\\vert S_{t-1} = s, A_{t-1} = a] \\newline\n  &= \\sum_{r \\in R} p(s',r \\vert s,a) \\qquad \\text{(state transition p)}\n\\end{align}\n\\tag{8}\\]\nwhere we summed over all possible rewards to get the state transition probability.\nWe can use the four-part dynamics function to compute the expected rewards for a given state and action:\n\\[\n\\begin{align}\n  r(s,a) &= \\mathbb{E}[R_t \\vert S_{t-1} = s, A_{t-1} = a] \\newline\n  &= \\sum_{r \\in R} r  \\times \\sum_{s' \\in S}  p(s', r, \\vert s, a)\n\\end{align}\n\\tag{9}\\]\nwhere we summed over all possible rewards and all successor state to get the expected reward.\nWe can also get the expected reward for a given state, action, and successor state using the four-part dynamics function:\n\\[\n\\begin{align}\n  r(s,a,s') &= \\mathbb{E}[R_t \\vert S_{t-1} = s, A_{t-1} = a, S_t = s'] \\newline\n  &= \\sum_{r \\in R} r \\times\n  \\frac {  p(s', r \\vert s, a) } {  p(s' \\vert s, a) }\n\\end{align}\n\\tag{10}\\]\nwhere we summed over all possible rewards to get the expected reward.\nNote: perhaps implementing these in python might be further clarify the math for a given state transition graph."
  },
  {
    "objectID": "posts/c1-w2.html#l2g1",
    "href": "posts/c1-w2.html#l2g1",
    "title": "Markov Decision Processes",
    "section": "Rewards and the Goal of an Agent",
    "text": "Rewards and the Goal of an Agent\nThe agent interacts with the environment by taking actions and receiving rewards.\nIn the bandit setting, it is enough to maximize immediate rewards. In the MDP setting, the agent must consider the long-term consequences of its actions.\nreturn \\(G_t\\) is the total future reward from time-step \\(t\\).\n\\[\nG_t \\dot= R_{t+1} + R_{t+2} +  R_{t+3} + \\ldots\n\\]\n\n\\(G_t\\) is a random variable because both transition and the rewards can be stochastic.\n\nThe goal of an agent is to maximize the expected cumulative reward.\n\n\n\n\n\n\nThe reward hypothesis\n\n\n\nThat all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\nsee [@SILVER2021103535]\n\n\nsome basic formulations of the goal of an agent:\n\nMaze runner: -1 for each time step until the goal is reached, then 0.\nRecycling robot: +1 per can, 0 otherwise.\nChess: 1 for a win, 0 for a draw, -1 for a loss.\n\nNote there is more material on this subject in the guest lecture by Michael Littman."
  },
  {
    "objectID": "posts/c1-w2.html#l2g2",
    "href": "posts/c1-w2.html#l2g2",
    "title": "Markov Decision Processes",
    "section": "Episodes and Episodic Tasks",
    "text": "Episodes and Episodic Tasks\nAn episode is a sequence of states, actions, and rewards that ends in a terminal state. An episodic task is a task with a well-defined terminal state.\nAn example of an episodic task is a game of chess, where the game ends when one player wins or the game is a draw. The opposite setting of an episodic task is a continuing task, where the agent interacts with the environment indefinitely."
  },
  {
    "objectID": "posts/c1-w2.html#guest-lecture-michael-littman-the-reward-hypothesis",
    "href": "posts/c1-w2.html#guest-lecture-michael-littman-the-reward-hypothesis",
    "title": "Markov Decision Processes",
    "section": "Guest Lecture: Michael Littman: The Reward Hypothesis",
    "text": "Guest Lecture: Michael Littman: The Reward Hypothesis\nLittman is a professor at Brown University and a leading researcher in reinforcement learning. He is known for his work on the reward hypothesis and the exploration-exploitation trade-off. He motivates the reward hypothesis with a humorous take on the old adage:\n\nGive a man a fish and he’ll eat for a day - traditional programming\nTeach a man to fish and he’ll eat for a lifetime - supervised learning\nGive a man a need for fish and he’ll figure it out - reinforcement learning\n\nI felt that the guest lecture was a bit of a let down. I was expecting more from a leading researcher in the field. The reward hypothesis is a fundamental concept and the lecture seemed all over the place. It raised many questions but didn’t answer them.\nIf we accept the hypothesis, then there are two areas need to be addressed:\n\nWhat rewards should agents optimize?\nDesigning algorithms to maximize them.\n\nSome rewards are easy to define, like winning a game, but others are more complex, like driving a car. His example was of air conditioning in a car, where the reward is not just the temperature but also the comfort of the passengers. Running the air conditioning has a cost in terms of fuel, but the comfort of the passengers is much harder to quantify, particularly since each passenger may have different preferences.\nNext he covered the two main approaches to setting up rewards in RL:\n\nRewards can be expressed as a final goal, or no goal yet:\n\nThe goal based representation: Goal achieved = +1, and everything else is 0. This has a downside of not signaling, to the agent, the urgency of getting to the goal.\nThe action-penalty representation: a -1 could be awarded every step that the goal is not yet achieved. This can cause problems if there is a small probability of getting stuck and never reaching the goal.\nIt seems that there are many ways to set up rewards. If we take a lesson from game theory, we can see that the value of rewards might be important or the relative value or order of rewards might be important. In rl values are often more important than the ‘order’ of rewards. However it might be interesting to consider if we can encode preferences into the rewards and if this formulation would still make sense in the context of the reward hypothesis and the bellman equations.\n\n\nLittleman then asked “Where do rewards come from?” and answered that they can come from - Programming - Human feedback - Examples - Mimic the rewards a human would give - Inverse reinforcement learning - learn the reward function from examples - Optimization - Evolutionary optimization like population dynamics. - The reward is the objective function - The reward is the gradient of the objective function\nNext he discusses some challenges to the reward hypothesis:\n\nTarget is something other than cumulative reward:\n\ncannot capture risk averse behavior\ncannot capture diversity of behavior\n\nis it a good match for a high level human behavior?\n\nsingle minded pursuit of a goal isn’t characteristic of good people.\nThe goals we “should” be pursuing may not be immediately evident to us - we might need time to understand making good decisions.\n\n\nThe big elephant in the room is that we can reject the reward hypothesis and have agents that peruse multiple goals. The main challenge is that it becomes harder to decide when there is a conflict between goals. However, the field of multi-objective optimization has been around for a long time and there are many ways to deal with this problem. Some are more similar to the reward hypothesis but others can lead to more complex behavior based on preferences and pareto optimality."
  },
  {
    "objectID": "posts/c1-w2.html#l3g1",
    "href": "posts/c1-w2.html#l3g1",
    "title": "Markov Decision Processes",
    "section": "Returns for Continuing Tasks",
    "text": "Returns for Continuing Tasks\n\nIn continuing tasks, the agent interacts with the environment indefinitely.\nThe return at time \\(t\\) is the sum of the rewards from time \\(t\\) to the end of the episode.\nThe return can be formulated using discounting, where the rewards are discounted by a factor \\(\\gamma\\).\n\n\\[\n\\begin{align}\nG_t&=R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\newline\n&= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\end{align}\n\\tag{11}\\]\n\nreturns have a recursive structure, where the return at time \\(t\\) is related to the return at time \\(t+1\\) by the discount factor \\(\\gamma\\).\n\n\\[  \n\\begin{align}\nG_t &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\newline\n&= R_{t+1} + \\gamma ( R_{t+2} + \\gamma R_{t+3} + \\ldots ) \\newline\n&= R_{t+1} + \\gamma G_{t+1}\n\\end{align}\n\\tag{12}\\]\nthis form of the return is called the recursive form of the return and is usefull in developing algorithms for reinforcement learning."
  },
  {
    "objectID": "posts/c1-w2.html#l3g2",
    "href": "posts/c1-w2.html#l3g2",
    "title": "Markov Decision Processes",
    "section": "Returns at Successive Time Steps",
    "text": "Returns at Successive Time Steps\n\nThe return at time \\(t\\) is related to the return at time \\(t+1\\) by the discount factor \\(\\gamma\\).\nThe return at time \\(t\\) is the sum of the reward at time \\(t\\) and the discounted return at time $t+1."
  },
  {
    "objectID": "posts/c1-w2.html#l3g3",
    "href": "posts/c1-w2.html#l3g3",
    "title": "Markov Decision Processes",
    "section": "Episodic vs. Continuing Tasks",
    "text": "Episodic vs. Continuing Tasks\n\nAn episodic task has a well-defined terminal state, and the episode ends when the terminal state is reached.\nA continuing task does not have a terminal state, and the agent interacts with the environment indefinitely.\nTo avoid infinite returns in continuing tasks, we use discounting to ensure that the return is finite.\nThe discount factor \\(\\gamma\\in(0,1)\\) is the present value of future rewards.\n\n@sutton2018reinforcement emphasizes that we can use the discount factor of [0,1] to unify both episodic and continuing tasks Here = 0 corresponds to myopic view of optimizing immediate rewards like in the k-armed bandit problem. The discount factor = 1 corresponds to the long-term view of optimizing undiscounted expected cumulative reward. into a single framework. This is a powerful idea that allows us to use the same algorithms for both types of tasks.\nI think it is a good place to consider a couple of ideas raised by Littman in the guest lecture:\nThe first is hyperbolic discounting and the second risk aversion.\nBehavioral economics has considered a notion of hyperbolic discounting where the discount factor is not constant but changes over time. This is a more realistic model of human behavior but is harder to work with mathematically. This idea is not covered in the course perhaps because it is a departure from the more rational exponential discounting model which we use.\ntwo forms of hyperbolic discounting are:\n\\[\nG(D) = \\frac{1}{1 + \\gamma D}\n\\]\n\\[\nφh(τ) = (1+ ατ)−γ/α\n\\tag{13}\\] where:\n\n\\(φh(τ)\\) is the hyperbolic discount factor at time \\(τ\\)\n\\(α\\) is the rate of discounting\n\\(γ\\) is the delay parameter\n\nthere is also quasi-hyperbolic discounting which is a combination of exponential and hyperbolic discounting.\n\\[\nG(D) = \\begin{cases}\n  1 & \\text{if } t = 0  \\newline\n  \\beta^k \\delta^{D} & \\text{if } t &gt; 0\n  \\end{cases}\n\\]\nThe notation for both terminal and non-terminal states is \\(S^+\\)\nExercise 2.10 proof of Equation 3.10\n\\[\n\\begin{align*}\nG_t &= \\sum_{k=0}^\\infty \\gamma^k = lim_{n \\rightarrow \\infty} (1 + \\gamma + \\gamma^2 + ... + \\gamma^n) \\newline\n&= lim_{n \\rightarrow \\infty} \\frac{(1 + \\gamma + \\gamma^2 + ... + \\gamma^n) (1 - \\gamma)}{(1 - \\gamma)} \\newline\n&= lim_{n \\rightarrow \\infty} \\frac{1 - \\gamma^{n+1}}{1 - \\gamma} \\newline\n&= \\frac{1}{1 - \\gamma}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/c2-w4.html",
    "href": "posts/c2-w4.html",
    "title": "Sample-based Learning Methods",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/c2-w4.html#lesson-1-what-is-a-model",
    "href": "posts/c2-w4.html#lesson-1-what-is-a-model",
    "title": "Sample-based Learning Methods",
    "section": "Lesson 1: What is a model?",
    "text": "Lesson 1: What is a model?\n\nLesson Learning Goals\n\nDescribe what a model is and how they can be used #\nClassify models as distribution models or sample models #\nIdentify when to use a distribution model or sample model #\nDescribe the advantages and disadvantages of sample models and distribution models #\nExplain why sample models can be represented more compactly than distribution models #\n\n\n\nWhat is a model and how can it be used?\n\nA model is a simplified representation of the environment dynamics\nModels can be used to simulate the environment\nIn this course a Model is a function that predicts the next state and reward given the current state and action\na transition model \\(s_{t_+1} = f(s_t, a_t)\\) predicts the next state given the current state and action\na reward model \\(r_{t_+1} = f(s_t, a_t)\\) predicts the reward given the current state and action\n\nthree other model types are mentioned in the ICML Tutorial on Model-Based Reinforcement Learning\n\nInverse models predict the action given the current state and next state \\(a_{t+1} = f_s^{-1}(s_t, s_{t+1})\\)\nDistances models predict the distance between the current state and the goal state \\(d_{ij} =d(s, s')\\)\nFuture return models predict the future return given the current state and action \\(G_t=Q(s_t, a_t)\\) or \\(G_t=V(s_t)\\)\n\nWhy do we want to use models?\n\nmodel allow us to simulate the environment without interacting with it.\nthis can increase sample efficiency - e.g. by replaying past experiences to propergate learning from goal to all predecessor states we have visited\nthis can reduce risks - e.g. by simulating dangerous situations instead of actually experiencing them.\nthis can reduce costs - e.g. by simulating costly actions in a simulated environment instead of paying the cost in the real environment.\nthis could be much faster than real-time interaction with the environment. Often in robotics simulation is orders of magnitude faster than real-time interaction.\n\n\n\nTypes of models\n\n\n\n\nmodels\n\n\nDistribution models predict the probability distribution of the next state and reward\nSample models predict a single next state and reward\n\nAlso there are:Environment simulator\n\nChess Programs typicaly can simulate all possible movers and evaluate the board position (tacticaly and strategically). The difference between the current board position and the board position after a move is the reward.\n[@Silver2016MasteringTG] mentions an Environment simulator for the game of go\n[@Agostinelli2019SolvingTR] used a simulator of the rubik’s cube to train a reinforcement learning agent to solve the cube.\n[@Bellemare2012TheAL] used a simulator of the game of atari to train a reinforcement learning agent to play atari games.\n[@Todorov2012MuJoCoAP] used a simulator of the physics of the real world to train a reinforcement learning agent to control a robot.\n[@Shen2018MWalkLT] used a simulator to train agents to navigate a graph using MCTS.\n[@Ellis2019WriteEA] used a REPL environment to train a reinforcement learning agent to write code.\n\n\n\nWhen to use a distribution model or sample model\n\nDistribution models are useful when we need to know the probability of different outcomes\nSample models are useful when we need to simulate the environment\n\n\n\nAdvantages and disadvantages of sample models and distribution models\n\nSample models can be represented more compactly than distribution models\nDistribution models can be more accurate than sample models\nexact expectations can be computed from distribution models\nassessing risks and uncertainties is easier with distribution models\n\n\n\nWhy sample models can be represented more compactly than distribution models\n\nSample models can be represented more compactly than distribution models because they only need to store a single next state and reward\nDistribution models need to store the joint probability of each possible next state and reward pair\nSample models can be more efficient when we only need to simulate the environment"
  },
  {
    "objectID": "posts/c2-w4.html#lesson-2-planning",
    "href": "posts/c2-w4.html#lesson-2-planning",
    "title": "Sample-based Learning Methods",
    "section": "Lesson 2: Planning",
    "text": "Lesson 2: Planning\n\nLesson Learning Goals\n\nExplain how planning is used to improve policies #\nDescribe random-sample one-step tabular Q-planning #\n\n\n\nHow planning is used to improve policies\n\nPlanning is the process of using a model to improve a policy or value function\nPlanning can be used to improve a policy or value function without interacting with the environment\nPlanning can be used to improve a policy or value function more efficiently than direct RL updates\n\nRandom-sample one-step tabular Q-planning {#sec-l2g2}\n\n\n\n\nQ-planning alg overview\n\n\n\n\nrandom sample one step tabular Q-planning\n\n\n\n\nTabular Q-planning is a planning algorithm that uses a sample model to improve a policy or value function\nTabular Q-planning uses a sample model to simulate the environment\nTabular Q-planning uses the simulated experience to improve a policy or value function\n\nadvantages of planning\n\nPlanning can be more efficient than direct RL updates\nPlanning can be used to improve a policy or value function without interacting with the environment\nPlanning can be used to improve a policy or value function more efficiently than direct RL updates"
  },
  {
    "objectID": "posts/c2-w4.html#lesson-3-dyna-as-a-formalism-for-planning",
    "href": "posts/c2-w4.html#lesson-3-dyna-as-a-formalism-for-planning",
    "title": "Sample-based Learning Methods",
    "section": "Lesson 3: Dyna as a formalism for planning",
    "text": "Lesson 3: Dyna as a formalism for planning\n\nLesson Learning Goals\n\nRecognize that direct RL updates use experience from the environment to improve a policy or value function #\nRecognize that planning updates use experience from a model to improve a policy or value function #\nDescribe how both direct RL and planning updates can be combined through the Dyna architecture #\nDescribe the Tabular Dyna-Q algorithm #\nIdentify the direct-RL and planning updates in Tabular Dyna-Q #\nIdentify the model learning and search control components of Tabular Dyna-Q #\nDescribe how learning from both direct and simulated experience impacts performance #\nDescribe how simulated experience can be useful when the model is accurate #\n\n\n\nDirect RL updates use experience from the environment to improve a policy or value function\n\nDirect RL updates use experience from the environment to improve a policy or value function\nDirect RL updates can be used to improve a policy or value function by interacting with the environment\n\n\n\nPlanning updates use experience from a model to improve a policy or value function\n\nPlanning updates use experience from a model to improve a policy or value function\nPlanning updates can be used to improve a policy or value function without interacting with the environment\n\n\n\nBoth direct RL and planning updates can be combined through the Dyna architecture\n\nDyna architecture combines direct RL updates and planning updates to improve a policy or value function\nDyna architecture uses a model to simulate the environment\nDyna architecture uses the simulated experience to improve a policy or value function\n\n\n\nThe Tabular Dyna-Q algorithm\n\nTabular Dyna-Q is a planning algorithm that uses a sample model to improve a policy or value function\nTabular Dyna-Q uses a sample model to simulate the environment\nTabular Dyna-Q uses the simulated experience to improve a policy or value function\n\n\n\n\n\nThe Tabular Dyna-Q algorithm\n\nExercise 8.2 Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiments?\nDyna-Q+ is like a generalized UCB while Dyna-Q+ is like a generalized epsilon greedy alg. Dyna-Q+ is doing more efficent exploration. It will revisits will be more spread out more over time but it scheme also tends to increases in non independent way - probabilities for unvisited regions keep growing so if it starts exploring it may like doing an extended sequence till it gets to a dead end.\nDyna Q exploration is independent for each state,action combo so retrying sequences get asymptotically less likely with time.\nExercise 8.3 Careful inspection of Figure 8.5 reveals that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?\nDyna-Q+ is more efficient at exploring so it learned a better policy, but since the environment was static Dyna-Q got to catch up, but it never reached the same policy.\nExercise 8.5 How might the tabular Dyna-Q algorithm shown on page 164 be modified to handle stochastic environments? How might this modification perform poorly on changing environments such as considered in this section? How could the algorithm be modified to handle stochastic environments and changing environments?\nto hadle a stochastic environment one would need to to model probabilities of stochastic dynamics. One way to do this is to use Bayesian updating with a dericlet prior and a multinomial posterior.\nThis modification would likely fare much worse since learning low probability transitions would require many visits to discover.\nIn the case of changing environment it would also take much longer for new state to be reflected in the model (if a state was visited 10 with just one transition and then the transition changed to another state then it would take many more than 10 vistis to quash the old probability and get the new one correct to 10%\nThis means that we adding a forgetting rule might be better then the plain derichlet-multinomial model.\nTo handle both stochastic and changing updates we may want to\n1. track the recency of the last visit and reward this option like in dyna-q plus.\n2. decay old probabilities - would require storing the time for each visit - i.e. path dependent model.\n3. A better idea is to use a hirachial model with parial pooling representing short term and long term transitions - this could fix the problem of decay by simply giving greater weight to the smaller more recent model.\nThe short term would track the last k visits in each state and the long term all the visits. We could then do partial pooling between these two estimators with much greater emphasis on the recent one!\n\n\n\n\nDirect-RL and planning updates in Tabular Dyna-Q\n\nTabular Dyna-Q uses direct RL updates to improve a policy or value function\nTabular Dyna-Q uses planning updates to improve a policy or value function\n\n\n\nModel learning and search control components of Tabular Dyna-Q\n\nTabular Dyna-Q uses a sample model to simulate the environment\nTabular Dyna-Q uses the simulated experience to improve a policy or value function\n\n\n\nLearning from both direct and simulated experience impacts performance\n\nLearning from both direct and simulated experience can improve performance\nLearning from both direct and simulated experience can be more efficient than direct RL updates\n\n\n\nSimulated experience can be useful when the model is accurate\n\nSimulated experience can be useful when the model is accurate\nSimulated experience can be used to improve a policy or value function without interacting with the environment"
  },
  {
    "objectID": "posts/c2-w4.html#lesson-4-dealing-with-inaccurate-models",
    "href": "posts/c2-w4.html#lesson-4-dealing-with-inaccurate-models",
    "title": "Sample-based Learning Methods",
    "section": "Lesson 4: Dealing with inaccurate models",
    "text": "Lesson 4: Dealing with inaccurate models\n\nLesson Learning Goals\n\nIdentify ways in which models can be inaccurate #\nExplain the effects of planning with an inaccurate model #\nDescribe how Dyna can plan successfully with a partially inaccurate model #\nExplain how model inaccuracies produce another exploration-exploitation trade-off #\nDescribe how Dyna-Q+ proposes a way to address this trade-off #\n\n\n\nWays in which models can be inaccurate\n\nModels can be inaccurate for many reasons\nbecause they have not sampled all actions in all states\nbecause the environment is has changed since the model was learned\nif the environment is stochastic\n\n\n\nEffects of planning with an inaccurate model\n\nPlanning with an inaccurate model can cause the value function to become worse\nPlanning with an inaccurate model can lead to sub-optimal policies\n\n\n\nDyna can plan successfully with a partially inaccurate model\n\nDyna can plan successfully with a partially inaccurate model\nDyna can use direct RL updates to improve a policy or value function as well as the model\nDyna can use planning updates to improve a policy or value function\n\n\n\nModel inaccuracies produce another exploration-exploitation trade-off\n\nModel inaccuracies produce another exploration-exploitation trade-off\nexploit an inaccurate model to improve the policy\nrevisit states/actions with low value to update the model\nCan we use an inverse sort of planning to identify states for which the model is inaccurate?\nModel inaccuracies can lead to suboptimal policies\nModel inaccuracies can lead to poor performance\n\n\n\nDyna-Q+ proposes a way to address this trade-off\n\n\n\n\nDyna-Q+ solution\n\n\nDyna-Q+ proposes a way to address this trade-off\nDyna-Q+ uses a bonus reward to encourage exploration\nDyna-Q+ can improve performance when the model is inaccurate\n\n\n\nDrew Bagnell on self-driving cars robotics and model-based reinforcement learning\nDrew Bagnell is a professor at Carnegie Mellon University and the CTO at Aurora innovation.\nHe has worked on self-driving cars and robotics. He has also worked on model-based reinforcement learning. He point out a dirty little secret that model-based reinforcement learning is a key technology for robotics.\nHe points out that the real world is expensive and dangerous. Using model based reinforcement learning can reduce the number of interactions with the real world and along learning about risky actions in the simulated world to improve performance in the real world. Also as we pointer out before this can usually be done much faster than real-time interaction with the environment.\nSample complexity: how many real-world samples are required to achieve high performance? It takes exponentially fewer interactions with a model than without. Not really sure what exponentially fewer means here - but it’s a lot fewer.\nQuadratic value function approximation goes back to optimal control in the 1960s. It’s continuous in states and actions. This is a method that should be part of the next course but isn’t covered there either\nFor linear transition dynamics with quadratic costs/rewards, it’s exact. For local convex / concave points, it is a good approximation of the true action-value function.\nHere is the math from his slide:\nQuadratic value function approximation\n\\[\nQ_t(x,a) = \\begin{bmatrix}     x   \\\\ a   \\\\ \\end{bmatrix}^T\n\\begin{bmatrix}     Q_{xx}   &&  Q_{xa}   \\\\ Q_{xa}   &&  Q_{uu}   \\\\ \\end{bmatrix}\n\\begin{bmatrix} x   \\\\ a   \\\\ \\end{bmatrix}^T +\n\\begin{bmatrix}    q_x   \\\\ q_a \\\\ \\end{bmatrix}^T\n\\begin{bmatrix}     x   \\\\ a   \\\\ \\end{bmatrix} + const\n\\tag{1}\\]\nThe approximation allows for calculating the optimal action-value in closed form (finite number of standard operations) even with continuous actions.\nDifferential dynamic programming takes advantage of the technique above.\n\nSo this seems complicated - because of matrix maths. But intuitively this is something we like to do in physics - add the term for the second derivative\nin the taylor series approximation of our function.\nThe 2nd paper is particularly clear and easy to work through for the approach just described.\n\n\n\n\n\n\ntimeline\n\n    title Bandit Algorithms Timeline\n\n    \n        1952 : Thompson Sampling\n        1955 : Upper Confidence Bound (UCB)\n        1963 : Epsilon-Greedy\n        2002 : Bayesian UCB\n        2011 : Bayesian Bandits\n        2012 : Contextual Bandits\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'timeline': { 'nodeSpacing': 50, 'sectionSpacing': 100, 'verticalStartPosition': 50, 'verticalSectionStartPosition': 50 }}}}%%\ntimeline\n    direction TD\n    title Reinforcement Learning Algorithms Timeline\n\n\n    \n        1948 : Monte Carlo Methods\n        1950 : Bellman Optimality Equations\n        1957 : Dynamic Programming\n        1959 : Temporal Difference Learning (TD)\n        1960 : Policy Iteration\n        1963 : Value Iteration\n        1983 : Q-Learning\n        1984 : Expected SARSA\n        1990 : Dyna-Q : Dyna-Q+\n        1992 : SARSA\n        1994 : Monte Carlo with E-Soft\n        1995 : Monte Carlo with Exploring Starts\n             : Generalized Policy Iteration (GPI)\n        1998 : Semi-Gradient TD\n        2000 : Differential Semi-Gradient SARSA\n        2001 : Gradient Monte Carlo (Gradient MC)\n        2003 : Gaussian Actor-Critic\n             : Softmax Actor-Critic\n             : Deep Q-Network (DQN)\n\n\n\n\n\n\n\n\n\n\n\nReferences\nMaterials from ICML Tutorial on Model-Based Reinforcement Learning:\nthe page above contains the following materials as well as an extensive bibliography.\n\nSlides\nPart 1: Introduction and Learning Models\nPart 2: Model-Based Control\nPart 3: Model-Based Control in the Loop\nPart 4: Beyond Vanilla MBRL\n\nFrom Bagnell’s talk:\n\nModern Adaptive Control and Reinforcement Learning\nSynthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization\nOptimal Control, Trajectory Optimization, Learning Dynamics"
  },
  {
    "objectID": "posts/c1-w1.html",
    "href": "posts/c1-w1.html",
    "title": "The K-Armed Bandit Problem",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/c1-w1.html#sec-k-armed-bandit",
    "href": "posts/c1-w1.html#sec-k-armed-bandit",
    "title": "The K-Armed Bandit Problem",
    "section": "K-armed bandits 🐙",
    "text": "K-armed bandits 🐙\nIn the k-armed bandit problem there is an agent who is assigned a state \\(s\\) by the environment and must learn which action \\(a\\) from the possible set of actions \\(A\\) leads to the goal state through a signal based on the greatest expected reward.\nOne way this can be achieved is using a Bayesian updating scheme starting from a uniform prior."
  },
  {
    "objectID": "posts/c1-w1.html#sec-l1g1",
    "href": "posts/c1-w1.html#sec-l1g1",
    "title": "The K-Armed Bandit Problem",
    "section": "Temporal nature of the bandit problem",
    "text": "Temporal nature of the bandit problem\nThe bandit problem cam be static problem with a fixed reward distribution. However, more generally it is a temporal problem when the rewards distribution changes over time and agent must learn to adapt to these changes.\n\n\n\n\n\n\nDifference between bandits and RL\n\n\n\nIn the typical bandit setting there is only one state. So after we pull the arm nothing in the problem changes.\nBandits problems where agents can discriminate between states are called contextual bandits.\nHowever, bandits embody one of the main themes of RL - that of estimating an expected reward for different actions.\nIn the more general RL setting we will be interested in more general problems where actions will lead the agent to new states and the goal is some specific state we need to reach.\n\n\n\n\n\nVideo\nbandit\n\n\nExample 1 (Using Multi-armed bandit to randomize a medical trial)  \n\nagent is the doctor\nactions {blue, yellow, red} treatment\nk = 3\nthe rewards are the health of the patients’ blood pressure.\na random trial in which a doctor need to pick one of three treatments.\nq(a) is the mean of the blood pressure for the patient.\n\n\n\n\n\n\nclinical trial\n\n\nAction Values and Greedy Action Selection\nThe value of an action is its expected reward which can be expressed mathematically as:\n\\[\n\\begin{align}\nq_{\\star}(a) & \\doteq \\mathbb{E}[R_t  \\vert  A_t=a] \\space \\forall a \\in \\{a_1 ... a_k\\} \\newline\n             & = \\sum_r p(r|a)r \\qquad \\text{(action value)}\n\\end{align}\n\\tag{1}\\]\nwhere:\n\n\\(\\doteq\\) means definition\n\\(\\mathbb{E}[r \\vert a]\\) means expectation of a reward given some action a Since agents want to maximize rewards, recalling the definition of expectations we can write this as:\n\nThe goal of the agent is to maximize the expected reward which we can express mathematically as:\n\\[\n\\arg\\max_a q(a)=\\sum_r p(r \\vert a) \\times r \\qquad \\text{(Greedification)}\n\\tag{2}\\]\nwhere:\n\n\\(\\arg \\max_a\\) means the argument \\(a\\) maximizes - so the agent is looking for the action that maximizes the expected reward and the outcome is an action.\n\n\n\nReward, Return, and Value Functions\nThe reward \\(r\\) is the immediate feedback from the environment after the agent takes an action.\nThe return \\(G_t\\) is the total discounted reward from time-step \\(t\\).\nThe value function \\(v(s)\\) of an MRP is the expected return starting from state \\(s\\).\n\n\n\n\ndecisions\n\nexample of decisions under uncertainty:\n\nmovie recommendation.\nclinical trials.\nmusic recommendation.\nfood ordering at a restaurant.\n\n\n\n\n\nwhy discuss bandits\n\nIt best to consider issues and algorithms design choices in the simplest setting first. The bandit problem is the simplest setting for RL. More advanced algorithms will incorporate parts we use to solve this simple settings.\n\nmaximizing rewards.\nbalancing exploration and exploitation.\nestimating expected rewards for different actions.\n\nare all problems we will encounter in both the bandit and the more general RL setting."
  },
  {
    "objectID": "posts/c1-w1.html#sec-epsilon-greedy-policies",
    "href": "posts/c1-w1.html#sec-epsilon-greedy-policies",
    "title": "The K-Armed Bandit Problem",
    "section": "Ɛ-Greedy Policies",
    "text": "Ɛ-Greedy Policies\nThe Ɛ-greedy policy uses a simple heuristic to balance exploration with exploitation. The idea is to choose the best action with probability \\(1-\\epsilon\\) and to choose a random action with probability \\(\\epsilon\\).\n\n\n\\begin{algorithm} \\caption{EpsilonGreedy(K, α, β)}\\begin{algorithmic} \\For{$t = 1, 2, \\ldots $} \\State p = random() \\If {$p &lt; \\epsilon$} \\State select radom action $x_t \\qquad$ \\Comment{explore} \\Else \\State select $x_t = \\arg\\max_k \\hat{\\theta}_k \\qquad$ \\Comment{exploit} \\EndIf \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nThe problem with Ɛ-greedy policies\n\n\n\n\nA problem with Ɛ-greedy is that it is not optimal in the long run.\nEven after it has found the best course of action it will continue to explore with probability \\(\\epsilon\\).\nThis is because the policy is not adaptive.\nOne method is too reduce \\(\\epsilon\\) over time. However unless there is a feedback from the environment this will likely stop exploring too soon or too late thus providing sub-optimal returns.\n\n\n\nThe following is a simple implementation of the Ɛ-greedy algorithm in Python from geeksforgeeks.org\n\n# Import required libraries \nimport numpy as np \nimport matplotlib.pyplot as plt \n  \n# Define Action class \nclass Actions: \n  def __init__(self, m): \n    self.m = m \n    self.mean = 0\n    self.N = 0\n  \n  # Choose a random action \n  def choose(self):  \n    return np.random.randn() + self.m \n  \n  # Update the action-value estimate \n  def update(self, x): \n    self.N += 1\n    self.mean = (1 - 1.0 / self.N)*self.mean + 1.0 / self.N * x \n  \n  \ndef run_experiment(m1, m2, m3, eps, N): \n      \n  actions = [Actions(m1), Actions(m2), Actions(m3)] \n  \n  data = np.empty(N) \n    \n  for i in range(N): \n    # epsilon greedy \n    p = np.random.random() \n    if p &lt; eps: \n      j = np.random.choice(3) \n    else: \n      j = np.argmax([a.mean for a in actions]) \n    x = actions[j].choose() \n    actions[j].update(x) \n  \n    # for the plot \n    data[i] = x \n  cumulative_average = np.cumsum(data) / (np.arange(N) + 1) \n  \n  # plot moving average ctr \n  plt.plot(cumulative_average) \n  plt.plot(np.ones(N)*m1) \n  plt.plot(np.ones(N)*m2) \n  plt.plot(np.ones(N)*m3) \n  plt.xscale('log') \n  plt.show() \n  \n  for a in actions: \n    print(a.mean) \n  \n  return cumulative_average \n\n\nc_1 = run_experiment(1.0, 2.0, 3.0, 0.1, 100000) \n#print(c_1)\n\n\n\n\n\n\n\n\n1.0414435367087398\n1.9782965280817517\n2.998981662974774\n\n\n\nc_05 = run_experiment(1.0, 2.0, 3.0, 0.05, 100000) \n#print(c_05)\n\n\n\n\n\n\n\n\n1.001990635270758\n1.9932072931039164\n3.0011204672769916\n\n\n\nc_01 = run_experiment(1.0, 2.0, 3.0, 0.01, 100000) \n#print(c_01)\n\n\n\n\n\n\n\n\n0.9352930959569423\n1.9548852539171198\n2.999251308717684\n\n\n\n# log scale plot \nplt.plot(c_1, label ='eps = 0.1') \nplt.plot(c_05, label ='eps = 0.05') \nplt.plot(c_01, label ='eps = 0.01') \nplt.legend() \nplt.xscale('log') \nplt.show()"
  },
  {
    "objectID": "posts/c1-w1.html#sec-benefits-of-exploitation-and-exploration",
    "href": "posts/c1-w1.html#sec-benefits-of-exploitation-and-exploration",
    "title": "The K-Armed Bandit Problem",
    "section": "Benefits of Exploitation & Exploration",
    "text": "Benefits of Exploitation & Exploration\n\nIn the short term we may maximize rewards following the best known course of action. However this may represent a local maximum.\nIn the long term agents that explore different options and keep uncovering better options until they find the best course of action corresponding to the global maximum.\n\nTo get the best of both worlds we need to balance exploration and exploitation ideally using a policy that uses feedback to adapt to its environment."
  },
  {
    "objectID": "posts/c1-w1.html#sec-optimistic-initial-values",
    "href": "posts/c1-w1.html#sec-optimistic-initial-values",
    "title": "The K-Armed Bandit Problem",
    "section": "Optimistic initial values",
    "text": "Optimistic initial values\n\nOptimistic initial values\n\nSetting all initially action values greater than the algorithmically available values in [0,1]\n\n\nThe methods we have discussed are dependent on the initial action-value estimates, \\(Q_1(a)\\). In the language of statistics, we call these methods biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once. For methods with constant \\(\\alpha\\), the bias is permanent, though decreasing over time.\n\n\n\\begin{algorithm} \\caption{OptimisticBernGreedy(K, α, β)}\\begin{algorithmic} \\For{$t = 1, 2, . . .$} \\State \\State \\Comment{ estimate model} \\For{$k = 1, . . . , K$} \\State $\\hat\\theta_k \\leftarrow 1 \\qquad$ \\Comment{optimistic initial value} \\EndFor \\State \\Comment{ select and apply action:} \\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$ \\State Apply $x_t$ and observe $r_t$ \\State \\Comment{ update distribution:} \\State $(α_{x_t}, β_{x_t}) \\leftarrow (α_{x_t} + r_t, β_{x_t} + 1 − r_t)$ \\EndFor \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "posts/c1-w1.html#sec-benefits-of-optimistic-initial-values-for-early-exploration",
    "href": "posts/c1-w1.html#sec-benefits-of-optimistic-initial-values-for-early-exploration",
    "title": "The K-Armed Bandit Problem",
    "section": "Benefits of optimistic initial values for early exploration",
    "text": "Benefits of optimistic initial values for early exploration\nSetting the initial action values to be higher than the true values has the effect of causing various bandit algorithm to try to exploit them - only to find out that most values are not as rewarding as it was led to expect.\nWhat happens is that the algorithm will initially explore more than it would have otherwise. Possibly even trying all the actions at least once.\nIn the short-term it will perform worse than Ɛ- greedy which tend to exploit. But as more of the state space is explored at least once the algorithm will beat an Ɛ-greedy policy which can take far longer to explore the space and find the optimal options.\n\n\n\nThe effect of optimistic initial action-value estimates\n\n\n\n\n\n\n\n\nCriticisms of optimistic initial values\n\n\n\n\nOptimistic initial values only drive early exploration. The agent will stop exploring once this is done.\nFor a non-stationary problems - this is inadequate.\nIn a real world problems the maximum reward is an unknown quantity."
  },
  {
    "objectID": "posts/c1-w1.html#sec-the-ucb-action-selection-method",
    "href": "posts/c1-w1.html#sec-the-ucb-action-selection-method",
    "title": "The K-Armed Bandit Problem",
    "section": "The UCB action selection method",
    "text": "The UCB action selection method\nUCB is an acronym for Upper Confidence Bound. The idea behind it is to select the action that has the highest upper confidence bound. This has the advantage over epsilon greedy that it will explore more in the beginning and then exploit more as the algorithm progresses.\nthe upper confidence bound is defined as:\n\\[\nA_t = \\arg\\max\\_a \\Bigg[\n  \\underbrace{Q_t(a)}_{exploitation} +\n  \\underbrace{c \\sqrt{\\frac{\\ln t}{N_t(a)} }}_{exploration}\n\\Bigg] \\qquad\n\\tag{6}\\]\nwhere:\n\n\\(Q_t(a)\\) is the action value\n\\(c\\) is a constant that determines the degree of exploration\n\\(N_t(a)\\) is the number of times action \\(a\\) has been selected prior to time \\(t\\)\n\n\n\n\n\nUCB intuition\n\nThe idea is we the action for which the action value plus the highest possible uncertainty give the highest sum. We are being optimistic in assuming this choice will give the highest reward. In reality any value in the confidence interval could be the true value. Each time we select an action we reduce the uncertainty in the exploration term and we also temper our optimism of the upper confidence bound by the number of times we have selected the action. This means that we will prefer to visit the actions that have not been visited as often.\nThe main advantage of UCB is that it is more efficient than epsilon greedy in the long run. If we measure the cost of learning in terms of the regret - the difference between the expected reward of the optimal action and the expected reward of the action we choose. UCB has a lower regret than epsilon greedy. The downside is that it is more complex and requires more computation.\n\n\n\\begin{algorithm} \\caption{UCB(K, α, β)}\\begin{algorithmic} \\For{$t = 1, 2, . . .$} \\For { $k = 1, . . . , K$ } \\State \\Comment{ $\\textcolor{blue}{compute\\ UCBs}$} \\State $U_k = \\hat\\theta_k + c \\sqrt{\\frac{\\ln t}{N_k}}$ \\EndFor \\State \\Comment{ $\\textcolor{blue}{select\\ and\\ apply\\ action}$} \\State $x_t \\leftarrow \\arg\\max_k h(x,U_x)$ \\State Apply xt and observe $y_t$ and $r_t$ \\State \\Comment{ $\\textcolor{blue}{estimate\\ model}$} \\For{$k = 1, . . . , K$} \\State $\\hat\\theta_k \\leftarrow a_k / (α_k + β_k)$ \\EndFor \\State \\Comment{ select and apply action:} \\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$ \\State Apply $x_t$ and observe $r_t$ \\State \\Comment{ update distribution:} \\State $(α_{x_t}, β_{x_t}) \\leftarrow (α_{x_t} + r_t, β_{x_t} + 1 − r_t)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nNote we can model UCB using an urn model."
  },
  {
    "objectID": "posts/c1-w1.html#Sec-Thompson-Sampling",
    "href": "posts/c1-w1.html#Sec-Thompson-Sampling",
    "title": "The K-Armed Bandit Problem",
    "section": "Thompson Sampling",
    "text": "Thompson Sampling\nThompson sampling is basically like UCB but taking the Bayesian approach to the bandit problem. We start with a prior distribution over the action values and then update this distribution as we take actions. The action we choose is then sampled from the posterior distribution. This has the advantage that it is more robust to non-stationary problems than UCB. The downside is that it is more computationally expensive.\n\nThompson Sampling Algorithm\nThe algorithm is as follows:\n\n\n\\begin{algorithm} \\caption{BernTS(K, α, β)}\\begin{algorithmic} \\For{$t = 1, 2, . . .$} \\State \\State \\Comment{ sample model} \\For{$k = 1, . . . , K$} \\State Sample $\\hat\\theta_k \\sim beta(α_k, β_k)$ \\EndFor \\State \\Comment{ select and apply action:} \\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$ \\State Apply $x_t$ and observe $r_t$ \\State \\Comment{ update distribution:} \\State $(α_{x_t}, β_{x_t}) \\leftarrow (α_{x_t} + r_t, β_{x_t} + 1 − r_t)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\nthis is a tutorial on Thompson Sampling"
  },
  {
    "objectID": "posts/c1-w1.html#L3G7",
    "href": "posts/c1-w1.html#L3G7",
    "title": "The K-Armed Bandit Problem",
    "section": "Optimism in the face of uncertainty",
    "text": "Optimism in the face of uncertainty\n\nOptimism in the face of uncertainty\n\nThis is a heuristic to ensure initial exploration of all actions by assuming that untried actions have a high expected reward. We then try to exploit them but end up successively downgrading their expected reward when they do not match our initial optimistic assessment.\n\n\nThe downside to this approach is when the space of action is continuous so we can never get to the benefits of exploration."
  },
  {
    "objectID": "posts/c1-w1.html#coding-bandits-with-mesa",
    "href": "posts/c1-w1.html#coding-bandits-with-mesa",
    "title": "The K-Armed Bandit Problem",
    "section": "Coding Bandits with MESA",
    "text": "Coding Bandits with MESA\n\nfrom tqdm import tqdm\nfrom mesa import Model, Agent\nfrom mesa.time import RandomActivation\nimport numpy as np\n\n\n\nclass EpsilonGreedyAgent(Agent):\n    \"\"\"\n    This agent implements the epsilon-greedy \n    \"\"\"\n\n    def __init__(self, unique_id, model, num_arms, epsilon=0.1):\n        super().__init__(unique_id,model)\n        self.num_arms = num_arms\n        self.epsilon = epsilon\n        self.q_values = np.zeros(num_arms)  # Initialize Q-value estimates\n        self.action_counts = np.zeros(num_arms)  # Track action counts\n\n    def choose_action(self):\n        if np.random.rand() &lt; self.epsilon:\n            # Exploration: Choose random arm\n            return np.random.randint(0, self.num_arms)\n        else:\n            # Exploitation: Choose arm with highest Q-value\n            return np.argmax(self.q_values)\n\n    def step(self, model):\n        chosen_arm = self.choose_action()\n        reward = model.get_reward(chosen_arm)\n        assert reward is not None, \"Reward is not provided by the model\"\n        self.action_counts[chosen_arm] += 1\n        self.q_values[chosen_arm] = (self.q_values[chosen_arm] * self.action_counts[chosen_arm] + reward) / (self.action_counts[chosen_arm] + 1)\n\n\nclass TestbedModel(Model):\n    \"\"\"\n    This model represents the 10-armed bandit testbed environment.\n    \"\"\"\n\n    def __init__(self, num_arms, mean_reward, std_dev,num_agents=1):\n        super().__init__()\n        self.num_agents = num_agents\n        self.num_arms = num_arms\n        self.mean_reward = mean_reward\n        self.std_dev = std_dev\n        self.env_init()\n        #self.arms = [None] * num_arms  # List to store arm rewards\n        self.schedule = RandomActivation(self)\n        for i in range(self.num_agents):\n          self.create_agent(EpsilonGreedyAgent, i, 0.1) \n\n    def env_init(self,env_info={}):\n        self.arms = np.random.randn(self.num_arms)  # Initialize arm rewards\n\n    def create_agent(self, agent_class, agent_id, epsilon):\n        \"\"\"\n        Create an RL agent instance with the specified class and parameters.\n        \"\"\"\n        agent = agent_class(agent_id, self, self.num_arms, epsilon)\n        self.schedule.add(agent)\n        return agent\n\n    def step(self):\n        for agent in self.schedule.agents:\n            chosen_arm = agent.choose_action()\n            reward = np.random.normal(self.mean_reward, self.std_dev)\n            self.arms[chosen_arm] = reward  # Update arm reward in the model\n            agent.step(self)  # Pass the model instance to the agent for reward access\n\n    def get_reward(self, arm_id):\n        # Access reward from the stored list\n        return self.arms[arm_id]\n\n\n# Example usage\nmodel = TestbedModel(10, 0, 1)  # Create model with 10 arms\nnum_runs = 200                  # The number of times we run the experiment\nnum_steps = 1000                # The number of pulls of each arm the agent takes\n\n\n# Run simulation for multiple steps\nfor _ in tqdm(range(num_runs)):\n    for _ in range(num_steps):\n        model.step()\n    model.step()\n\n/home/oren/.local/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:\n\nThe AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.\nWe would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919\n\n  0%|          | 0/200 [00:00&lt;?, ?it/s]  4%|▍         | 9/200 [00:00&lt;00:02, 85.52it/s]  9%|▉         | 18/200 [00:00&lt;00:02, 84.53it/s] 14%|█▎        | 27/200 [00:00&lt;00:02, 83.65it/s] 18%|█▊        | 36/200 [00:00&lt;00:01, 83.19it/s] 22%|██▎       | 45/200 [00:00&lt;00:01, 82.74it/s] 27%|██▋       | 54/200 [00:00&lt;00:01, 82.49it/s] 32%|███▏      | 63/200 [00:00&lt;00:01, 82.43it/s] 36%|███▌      | 72/200 [00:00&lt;00:01, 82.76it/s] 40%|████      | 81/200 [00:00&lt;00:01, 81.25it/s] 45%|████▌     | 90/200 [00:01&lt;00:01, 80.29it/s] 50%|████▉     | 99/200 [00:01&lt;00:01, 79.67it/s] 54%|█████▎    | 107/200 [00:01&lt;00:01, 78.90it/s] 57%|█████▊    | 115/200 [00:01&lt;00:01, 77.96it/s] 62%|██████▏   | 123/200 [00:01&lt;00:01, 76.64it/s] 66%|██████▌   | 131/200 [00:01&lt;00:00, 76.87it/s] 70%|██████▉   | 139/200 [00:01&lt;00:00, 77.03it/s] 74%|███████▎  | 147/200 [00:01&lt;00:00, 77.43it/s] 78%|███████▊  | 155/200 [00:01&lt;00:00, 77.38it/s] 82%|████████▏ | 163/200 [00:02&lt;00:00, 77.77it/s] 86%|████████▌ | 171/200 [00:02&lt;00:00, 78.03it/s] 90%|████████▉ | 179/200 [00:02&lt;00:00, 78.10it/s] 94%|█████████▎| 187/200 [00:02&lt;00:00, 78.03it/s] 98%|█████████▊| 195/200 [00:02&lt;00:00, 78.32it/s]100%|██████████| 200/200 [00:02&lt;00:00, 79.49it/s]"
  },
  {
    "objectID": "posts/c3-w2.1.html",
    "href": "posts/c3-w2.1.html",
    "title": "Constructing Features for Prediction",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/c3-w2.1.html#the-1000-step-random-walk-environment",
    "href": "posts/c3-w2.1.html#the-1000-step-random-walk-environment",
    "title": "Constructing Features for Prediction",
    "section": "The 1000 Step Random Walk Environment",
    "text": "The 1000 Step Random Walk Environment\nIn this lesson we implement the 1000 Random Walk example as an environment. This is good to demonstrate how to construct features for linear methods. We will use tile coding and coarse coding to create features for the Random Walk 1000 environment.\n\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass RandomWalk1000(gym.Env):\n    def __init__(self, num_states=1000, neighborhood_size=100, seed=None):\n        super().__init__()\n        self.num_states = num_states\n        self.neighborhood_size = neighborhood_size\n        self.observation_space = spaces.Discrete(num_states + 2) # add two states 0 and num_states + 1 as terminal states\n        self.action_space = spaces.Discrete(2)  # 0 for left, 1 for right\n        self.current_state = 500 # start in the middle\n        self.np_random, seed = gym.utils.seeding.np_random(seed)\n        self.trajectory = [500]\n\n    def reset(self, *, seed=None, options=None):\n        super().reset(seed=seed)\n        self.current_state = 500\n        self.trajectory = [500]\n        return self.current_state, {}\n\n    def step(self, action):\n\n        if action == 0: # move left\n             # left neighbours\n            left_start = max(1, self.current_state - self.neighborhood_size)\n            left_end = self.current_state\n            num_left = left_end - left_start\n\n            if left_start == 1:\n                prob_terminate_left = (self.neighborhood_size - num_left) / self.neighborhood_size\n            else:\n                prob_terminate_left = 0\n            \n            if self.np_random.random() &lt; prob_terminate_left:\n               \n                return 0, -1, True, False, {} # terminate left\n\n            next_state = self.np_random.integers(low=left_start, high=left_end)\n\n\n        elif action == 1: # move right\n             # right neighbours\n            right_start = self.current_state + 1\n            right_end = min(self.num_states + 1, self.current_state + self.neighborhood_size + 1)\n            num_right = right_end - right_start\n            if right_end == self.num_states + 1:\n                 prob_terminate_right = (self.neighborhood_size - num_right) / self.neighborhood_size\n            else:\n                prob_terminate_right = 0\n            \n            if self.np_random.random() &lt; prob_terminate_right:\n\n                return self.num_states + 1, 1, True, False, {} # terminate right\n\n            next_state = self.np_random.integers(low=right_start, high=right_end)\n        else:\n            raise ValueError(\"Invalid action\")\n\n        self.current_state = next_state\n\n        self.trajectory.append(self.current_state)\n        return self.current_state, 0, False, False, {} # not terminated or truncated\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_trajectory(trajectory, num_states):\n    \"\"\"Plots the trajectory of the random walk.\"\"\"\n    x = np.arange(len(trajectory))\n    y = np.array(trajectory)\n    \n    plt.figure(figsize=(12, 4))\n    plt.plot(x, y, marker='o', linestyle='-', markersize=3)\n    plt.xlabel('Time Step')\n    plt.ylabel('State')\n    plt.title('Random Walk Trajectory')\n    plt.yticks(np.arange(0, num_states+2, 100))\n    plt.grid(axis='y')\n\n    plt.tight_layout()\n    plt.show()\n\n\n#import gymnasium as gym\n#from random_walk_gym import RandomWalk1000\n\nenv = RandomWalk1000()\n\n# Reset the env\nobs, info = env.reset()\nterminated = False\n\nwhile not terminated:\n    # For this environment, an action is not needed.\n    # Here we pass in a dummy value\n    obs, reward, terminated, truncated, info = env.step(0)\n    print(f\"State: {obs + 1}, Reward: {reward}, Terminated: {terminated}\")\n\nenv.close()\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n\nState: 491, Reward: 0, Terminated: False\nState: 478, Reward: 0, Terminated: False\nState: 420, Reward: 0, Terminated: False\nState: 337, Reward: 0, Terminated: False\nState: 296, Reward: 0, Terminated: False\nState: 259, Reward: 0, Terminated: False\nState: 257, Reward: 0, Terminated: False\nState: 253, Reward: 0, Terminated: False\nState: 168, Reward: 0, Terminated: False\nState: 95, Reward: 0, Terminated: False\nState: 34, Reward: 0, Terminated: False\nState: 10, Reward: 0, Terminated: False\nState: 1, Reward: -1, Terminated: True\n\n\n\n\n\n\n\n\n\n\nenv = RandomWalk1000(num_states=1000, neighborhood_size=100)\nobs, info = env.reset()\nterminated = False\ntruncated = False\ntrajectory = []\nwhile not terminated and not truncated:\n    action = env.action_space.sample()  # Replace with your policy\n    obs, reward, terminated, truncated, info = env.step(action)\n    print(f'{obs=}, {action=}, {reward=}, {terminated=}')\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n\nobs=490, action=0, reward=0, terminated=False\nobs=499, action=1, reward=0, terminated=False\nobs=506, action=1, reward=0, terminated=False\nobs=486, action=0, reward=0, terminated=False\nobs=550, action=1, reward=0, terminated=False\nobs=506, action=0, reward=0, terminated=False\nobs=444, action=0, reward=0, terminated=False\nobs=391, action=0, reward=0, terminated=False\nobs=418, action=1, reward=0, terminated=False\nobs=357, action=0, reward=0, terminated=False\nobs=443, action=1, reward=0, terminated=False\nobs=533, action=1, reward=0, terminated=False\nobs=474, action=0, reward=0, terminated=False\nobs=536, action=1, reward=0, terminated=False\nobs=464, action=0, reward=0, terminated=False\nobs=512, action=1, reward=0, terminated=False\nobs=435, action=0, reward=0, terminated=False\nobs=520, action=1, reward=0, terminated=False\nobs=422, action=0, reward=0, terminated=False\nobs=519, action=1, reward=0, terminated=False\nobs=546, action=1, reward=0, terminated=False\nobs=626, action=1, reward=0, terminated=False\nobs=716, action=1, reward=0, terminated=False\nobs=745, action=1, reward=0, terminated=False\nobs=683, action=0, reward=0, terminated=False\nobs=783, action=1, reward=0, terminated=False\nobs=770, action=0, reward=0, terminated=False\nobs=715, action=0, reward=0, terminated=False\nobs=687, action=0, reward=0, terminated=False\nobs=639, action=0, reward=0, terminated=False\nobs=658, action=1, reward=0, terminated=False\nobs=592, action=0, reward=0, terminated=False\nobs=658, action=1, reward=0, terminated=False\nobs=584, action=0, reward=0, terminated=False\nobs=577, action=0, reward=0, terminated=False\nobs=526, action=0, reward=0, terminated=False\nobs=509, action=0, reward=0, terminated=False\nobs=524, action=1, reward=0, terminated=False\nobs=477, action=0, reward=0, terminated=False\nobs=428, action=0, reward=0, terminated=False\nobs=350, action=0, reward=0, terminated=False\nobs=396, action=1, reward=0, terminated=False\nobs=339, action=0, reward=0, terminated=False\nobs=352, action=1, reward=0, terminated=False\nobs=449, action=1, reward=0, terminated=False\nobs=549, action=1, reward=0, terminated=False\nobs=452, action=0, reward=0, terminated=False\nobs=413, action=0, reward=0, terminated=False\nobs=370, action=0, reward=0, terminated=False\nobs=348, action=0, reward=0, terminated=False\nobs=305, action=0, reward=0, terminated=False\nobs=276, action=0, reward=0, terminated=False\nobs=227, action=0, reward=0, terminated=False\nobs=236, action=1, reward=0, terminated=False\nobs=245, action=1, reward=0, terminated=False\nobs=200, action=0, reward=0, terminated=False\nobs=283, action=1, reward=0, terminated=False\nobs=260, action=0, reward=0, terminated=False\nobs=218, action=0, reward=0, terminated=False\nobs=298, action=1, reward=0, terminated=False\nobs=294, action=0, reward=0, terminated=False\nobs=276, action=0, reward=0, terminated=False\nobs=191, action=0, reward=0, terminated=False\nobs=274, action=1, reward=0, terminated=False\nobs=246, action=0, reward=0, terminated=False\nobs=193, action=0, reward=0, terminated=False\nobs=197, action=1, reward=0, terminated=False\nobs=244, action=1, reward=0, terminated=False\nobs=192, action=0, reward=0, terminated=False\nobs=137, action=0, reward=0, terminated=False\nobs=103, action=0, reward=0, terminated=False\nobs=84, action=0, reward=0, terminated=False\nobs=69, action=0, reward=0, terminated=False\nobs=137, action=1, reward=0, terminated=False\nobs=121, action=0, reward=0, terminated=False\nobs=119, action=0, reward=0, terminated=False\nobs=112, action=0, reward=0, terminated=False\nobs=123, action=1, reward=0, terminated=False\nobs=177, action=1, reward=0, terminated=False\nobs=120, action=0, reward=0, terminated=False\nobs=108, action=0, reward=0, terminated=False\nobs=75, action=0, reward=0, terminated=False\nobs=63, action=0, reward=0, terminated=False\nobs=0, action=0, reward=-1, terminated=True\n\n\n\n\n\n\n\n\n\nLets simulate the random walk till success and plot its trajectory.\n\nenv = RandomWalk1000(num_states=1000, neighborhood_size=100)\nobs, info = env.reset()\nterminated = False\ntruncated = False\nwhile not terminated and not truncated:\n    action = env.action_space.sample()  # Replace with your policy\n    obs, reward, terminated, truncated, info = env.step(action)\n\n\nplot_trajectory(env.trajectory, num_states=env.num_states)"
  },
  {
    "objectID": "posts/c3-w2.1.html#a-short-digression-on-einstein-tiling-for-rl",
    "href": "posts/c3-w2.1.html#a-short-digression-on-einstein-tiling-for-rl",
    "title": "Constructing Features for Prediction",
    "section": "A short digression on Einstein Tiling for RL",
    "text": "A short digression on Einstein Tiling for RL\nOne (awful) Idea I keep returning to is to use Einstein Tiling for RL. I mentino that Einstein in this context is not the physicist but rather a pun on the word ‘Einstein’ which means ‘one stone’ in German.\nLet’s quickly review why it is a bad idea, and then why it is also a fascinating idea.\n\nUnlike A square tiling this is an aperiodic tiling so we need to generate it efficiently. Depending on the space it will may take some time to generate the tiling. We need to store the tiling in memory. For a square tiling we can generate the tiling in a few lines of code. We can access the tiling or tile using a simple formula.\nWe need a quick way to find which tile a point is in. This is not to hard for one tile. But as the number of tiles increases this becomes more difficult. It is trivial for a square tiling where again we have a formula to efficiently determine the tile a point belongs to.\n\nSome reasons why it is a fascinating idea.\n\nWe only need one tiling. If we have one we can map the first tile ton any other location it is in the same orientation and we will get a new tiling! This is due to the aperiodic nature of the tiling.\nThe hat tile is constructed by glueing together eight smaller kite tiles that are sixth of a hexagon. We can easily use larger kites that so we can use two such grids as coarse and coarser tilings.\nDifferent can be similar locally but will tend to diverge. This suggest that we will get a good generalization.\nThere may be variant einsteins that are easier to generate and use\nIn https://www.ams.org/journals/proc/1995-123-11/S0002-9939-1995-1277129-X/ the authors show that for d&gt;=3 aperiodic tilings can naturally avoid more symmetries than just translations. I.e. we can have a periodic tilings in higher dimensions.\n\nI may be wrong but It may be possible to generate the tiling using a simple formula. Ok so far tiling generation is insanely complicated. Though this is not a judgment on the complexity of the tiling but rather the complexity of the code and mathematics to generate the tiling.\nThe hat tile allows one to create many different tilings of the state space in two dimensions. Each tiling is going to have a different set of features.\nAs the hat tile is constructed by glueing together eight smaller tiles. Tilings are created in a hierarchical manner. This suggests that we can will get fine and course feature in this process and that we can just keep going to increase discrimination.\nSome issues - it is possible to get two tilings that are the same but for a ‘conway worm’ this is a curve in the tiling that is different. The problem here is that the features will be the same for every where except the worm. Not good for generalization."
  },
  {
    "objectID": "posts/c2-w3.html",
    "href": "posts/c2-w3.html",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/c2-w3.html#lesson-1-td-for-control",
    "href": "posts/c2-w3.html#lesson-1-td-for-control",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Lesson 1: TD for Control",
    "text": "Lesson 1: TD for Control\n\nLesson Learning Goals\n\nExplain how generalized policy iteration can be used with TD to find improved policies #\nDescribe the Sarsa Control algorithm #\nUnderstand how the Sarsa control algorithm operates in an example MDP #\nAnalyze the performance of a learning algorithm in an MDP #"
  },
  {
    "objectID": "posts/c2-w3.html#sec-l1g1",
    "href": "posts/c2-w3.html#sec-l1g1",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Generalized Policy Iteration with TD",
    "text": "Generalized Policy Iteration with TD\nwe would like now to combine TD with a planning algorithm to use TD for control. We This will be a GPI algorithm.\n\nGeneralized Policy Iteration - Recap\nlets recap the Generalized Policy Iteration (GPI) algorithm:\n\nPolicy Evaluation: Update the value function V to be closer to the true value function of the current policy\nPolicy Improvement: Improve the policy \\(\\pi\\) based on the current value function V\nGeneralized Policy Iteration: Repeated steps of policy evaluation and policy improvement\nGPI does not require full evaluation of the value function, just an improvement can be used to update the policy.\npolicy iteration\n\nrun policy evaluation to convergence\ngreedifing the policy\n\nGPI MC\n\neach episode:\n\npolicy evaluation (nota full evaluation)\nimprovement per episode\n\n\nGPI TD\n\neach step:\n\npolicy evaluation (for just one action)\nimprovement pi after the single time step.\n\n\n\n\nRecall how in the first course we saw DP methods for solving MDPs using the four part dynamic function and its variants. We used the Bellman equation to write down a system of linear equations for the value function and solve them exactly. We then used the value function to find the optimal policy. So in DP we don’t need to interact with the environment or to learn. We can compute the value function and the optimal policy exactly.\nIn these course we relaxed the assumption of knowing the transition dynamics or the expected returns. This creates a new challange of learning V or Q from experience.\nIn the first lesson we saw how MC methods can help us learn the value function but with the caveat that we need to wait until the end of the episode to update the value function.\nHowever we have now seen how the TD(0) algorithm uses recursive nature of the Bellman equation for the value function to make approximate updates to the value function. This allows us to learn Values of states directly from experience.\nOnce we are able to approximate the value function, we can use it to create new generalized policy iteration algorithms. This part of the GPI remains the same, we still evaluate the policy and improve it. But now we can do this in an online fashion, updating the value function after each step.\nIn SARSA we are making updates to the policy after a single step - this may lead to much faster convergence to the optimal policy. It also allows us to improve our plans during an episode or in a continuing task.\nThe advantage of TD methods is that they can be used in continuing tasks, where the agent interacts with the environment indefinitely. This is because the value function is updated after each step, and the agent can continue to learn and improve its policy as it interacts with the environment. But this advantage is better understood by considering the episodic tasks, where the agent can learn during an episode that consequences of its actions are sub optimal. This allows TD(0) based GPI to make more frequent updates to the policy within one episode."
  },
  {
    "objectID": "posts/c2-w3.html#sec-l1g2",
    "href": "posts/c2-w3.html#sec-l1g2",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Sarsa: On-policy TD Control",
    "text": "Sarsa: On-policy TD Control\nNext we consider how we can derive and use a similar approximate updating of the action-value function to learn the action-value function directly from experience.\nlets recap the Bellman equation for the action-value function:\n\\[\n\\begin{aligned}\nq_\\pi(s,a) & \\dot = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\qquad \\newline\n& = \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\sum_{a'}\\pi(a' \\mid s') q_\\pi(s', a')] \\qquad\n\\end{aligned}\n\\tag{1}\\]\nWe can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.\n\\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n\\tag{2}\\]\n\n\n\n\n\n\nSARSA\n\n\n\n#| label: alg-sarsa\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{SARSA($\\alpha,\\epsilon$)}\n\\begin{algorithmic}[1]\n\\State Initialize:\n  \\State $\\qquad Q(s,a) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S^+}, a\\in\\mathcal{A}(s)\\ except\\ for\\ Q(terminal,\\cdot)=0$\n  \n\\For {each episode e:}\n  \\State Initialize $S$\n  \\State Choose A from S using policy derived from Q (e.g., $\\epsilon$-greedy)\n  \\For {each step of e}\n    \\State Take action A, observe R, S'\n    \\State Choose A' from S' using policy derived from Q (e.g., $\\epsilon$-greedy)\n    \\State $Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma Q(S',A') - Q(S, A)]$\n    \\State $S \\leftarrow S'$; $A \\leftarrow A'$\n  \\EndFor\n  \\State until $S$ is terminal\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\n\nThe SARSA algorithm is due to Rummery, Gavin Adrian, and Mahesan Niranjan. The name Sarsa is due to Rich Sutton and comes from the fact that the algorithm uses the tuple \\((S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\\) to update the action-value function. [@Rummery1994OnlineQU]\nSARSA is a sample-based algorithm to solve the Bellman equation for action-values. - It picks an action based on the current policy and then - It policy evaluation by a TD updates of Q the action-value function based on the reward and the next action. - Then it does a policy improvement."
  },
  {
    "objectID": "posts/c2-w3.html#sec-l1g3",
    "href": "posts/c2-w3.html#sec-l1g3",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "SARSA in an Example MDP",
    "text": "SARSA in an Example MDP\n\n\n\n\nwindy gridworld\n\nIn this grid world isn’t a good fit for MC methods as most policies never terminate. This is because the agent is pushed up by the wind and has to learn to navigate to the goal. Anyhow if the episode never terminates MC wont be able to update the value function.\nBut Sarsa can learn the optimal policy in this environment by abandoning the epsilon greedy policy in mid episode and finding better policies. We can see that early episodes take longer to terminate after the e-greedy policy stops peaks."
  },
  {
    "objectID": "posts/c2-w3.html#sec-l1g4",
    "href": "posts/c2-w3.html#sec-l1g4",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Performance of Learning Algorithms in an MDP",
    "text": "Performance of Learning Algorithms in an MDP\nOn the right side of the figure we see the performance of the learning algorithms in the windy grid world. We see that in this chart the Sarsa algorithm learns the optimal policy at Around step 7000 where the gradient becomes constant.\nQ. why is SARSA called an on-policy algorithm?\nthis is because it learns by sampling from the policy induced by Q while following the same policy \\(\\pi\\)."
  },
  {
    "objectID": "posts/c2-w3.html#lesson-2-off-policy-td-control-q-learning",
    "href": "posts/c2-w3.html#lesson-2-off-policy-td-control-q-learning",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Lesson 2: Off-policy TD Control: Q-learning",
    "text": "Lesson 2: Off-policy TD Control: Q-learning\n\nLesson Learning Goals\n\nDescribe the Q-learning algorithm #\nExplain the relationship between q-learning and the Bellman optimality equations. #\nApply q-learning to an MDP to find the optimal policy #\nUnderstand how Q-learning performs in an example MDP #\nUnderstand the differences between Q-learning and Sarsa #\nUnderstand how Q-learning can be off-policy without using importance sampling #\nDescribe how the on-policy nature of SARSA and the off-policy nature of Q-learning affect their relative performance #\n\n\nlets recap the Bellman optimality equation for the action-value function:\n\\[\n\\begin{aligned}\nq_{\\star}(s,a) = \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\max_{a'} q_{\\star}(s', a')]\n\\end{aligned}\n\\tag{3}\\]\nThe following is an update rule for Q-learning:\nWe can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.\n\\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a') - Q(S_t, A_t)]\n\\tag{4}\\]\n\n\n\n\n\n\nQ-learning (off-policy TD control)\n\n\n\n#| label: alg-q-learning\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{Q-learning Off-policy TD control}\n\\begin{algorithmic}[1]\n\\State Initialize:\n  \\State $\\qquad Q(s,a) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S^+}, a\\in\\mathcal{A}(s)\\ except\\ for\\ Q(terminal,\\cdot)=0$\n  \n\\For {each episode e:}\n  \\State Initialize $S$\n  \\For {each step of e}\n    \\State Choose A from $A_B(S)$ using any ergodic Behavioural policy $B$ - perhaps the $\\epsilon$-greedy induced by Q.\n    \\State Take action A, observe R, S'\n    \\State $Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\max_a Q(S',a) - Q(S, A)]$\n    \\State $S \\leftarrow S'$\n  \\EndFor\n \\State until $S$ is terminal\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\nNote: I made some cosmetic changes to the psuedo code in the book to resolve the confusion I had about nature the behavioral policy.\n\n\n\n\n\n\n\n\nThe behavioral policy in Q-learning\n\n\n\nQ-learning has a subtle issue I found confusing at first.\nHere I first state the issue:\nWhat is the behavioral policy we follow in these three td-learning algorithms when we sample the next action to follow?\nWe are not writing here that Q function is \\(Q_\\pi\\) but the value functions are by definitions expectations under some policy. In these algorithms we keep updating the Q function using TD(0) updates. If we update the Q function in a sense that the best action changes at a given step then the updated function now uses a new policy. (In the case of Sarsa we can actually get a worse policy after the update.) I figured this out very quickly.\nA fully specified Q-functions isn’t just defined by following a policy. It also induces a policy. This in generaly is a stochastic policy. But if we take the greedy action with arbitrary tie breaks we get one or more deterministic policies. So it seems that off policy algorithms like Q-learning and Expected Sarsa are following a sequence of policies that are induced by the Q function that is being learned.\nIn general off policy learning may be using S,A,R sequences that have been sampled like we clearly did in MC. So the question which arises is can sample from any ergodic policy as our behavioral policy in these off-policy algorithms or are we supposed to learn from experience and sample using the policy induced by latest and greatest Q function that we are learning?\nLuckily Martha White is very clear about this:\n\nThe target policy is the easy part - we are targeting \\(Q_{\\pi_\\star}\\).\nThe behavior policy is the policy can be any policy so long as it is ergodic.\n\nUsing an \\(\\epsilon\\)-greedy policy derived from Q is very logical choice but we could use any other policy.\n\n\n\n\n\n\n\n\n\n\nDo these algorithms converge?\n\n\n\n\nIs Q-learning guaranteed to converge ?\nThe course glossed over this in lectures perhaps referencing the text book – I will have to go back and check this.\nHowever as this is introductory CS and not Mathematics I will try to suspend my disbelief that the algs are guarenteed to converge and return to the point.\n\n\n\n\nIt is an off-policy algorithm.\n\n\nThe target policy is the easy part - we are targeting \\(Q_{\\pi_\\star}\\).\nThe behavior policy is the policy that we are following but what is that ?\n\nit is clearly not \\(Q_{\\pi_\\star}\\) as we don’t know it yet.\nwe initialized Q(s,a) arbitrarily - so we may have a uniform random policy.\nbu we actual have any random policy.\n\nany action that is a legit transition from the current state is a valid action.\nso long as their probabilities add up to 1.\n\nlater Martha keeps saying that we need the ergodicity of the MDP to ensure that out policy will visit all states and actions with non-zero probability.\nthis anyhow is one source of confusion.\nhowever an epsilon greedy policy of the induced policy from Q seems like a very good choice. Can we do better ?\nanother point to consider here is that this is a value iteration algorithm.\n\nwhat can we say about the inermediate Q functions that we are learning ?\nare they even a valid action value function ?\nis the policy they induce a coherent probability distribution over actions ?\n\n\n\n\nWhen we select the action A’ what policy are we using ?\n\n\nwe are clearly not using the policy that we are learning \\(\\pi_\\star\\) - we dont know it yet.\nwe are could use an \\(\\epsilon\\)-greedy policy by greedifying Q. this seems the most logical\nbut we could pretty much use any other policy.\nthis is because Q-learning is an off-policy algorithm.\nthe confusion arises because it is not clear what “any policy derived from Q” means in the algorithm.\nq-learning\n\nis a value iteration algorithm\nuses the Bellman optimality equation to update the action-value function.\nselects the action based on greedyfing the current q-values and then\n\nit policy evaluation by a TD updates of Q the action-value function based on the reward and the next action.\n\n\n\n\n\n\n\nwhy can’t q-learning account for the consequences of exploration in its policy ?\n\n\n\nq-learning learns the optimal policy but follows a some other policy. Let suppose the optimal policy is deterministic. And let’s suppose that the behavior policy is epsilon greedy based on that.\nThe alg does not follow the optimal policy - it follows the behavior policy and this will perform much worse because of exploration.\nIf we need to account for the consequences of exploration in the policy we need to use a different algorithm!"
  },
  {
    "objectID": "posts/c2-w3.html#sec-l2g4",
    "href": "posts/c2-w3.html#sec-l2g4",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Q-learning in an Example MDP",
    "text": "Q-learning in an Example MDP\n\n\n\n\nwindy gridworld\n\nIn this grid world isn’t a good fit for MC methods as most policies never terminate.\nQ-learning can learn the optimal policy in this environment by abandoning the epsilon greedy policy in mid episode and finding better policies.\nWe can see that early episodes take longer to terminate after the e-greedy policy stops peaks.\nHowever Q-learning does not take seem to factor in the consequences of exploration in its policy.\nThis is because it is learning the optimal policy and not the policy that it follows.\nQ-learning does not need Importance sampling to learn off-policy. This is because it is learning action values."
  },
  {
    "objectID": "posts/c2-w3.html#sec-l2g5",
    "href": "posts/c2-w3.html#sec-l2g5",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Comparing Sarsa and Q-learning",
    "text": "Comparing Sarsa and Q-learning\nQ-learning is an off-policy algorithm:\n\nthe target policy is the optimal policy since the update rule approximates the Bellman optimality equation.\nthe behavior policy is initially given updated at each step from the inital get updated a bit towards the optimal policy at each step.\n\nbecause it is learning \\(\\pi_*\\) (the optimal policy) but it samples a different policy.\nThis is in contrast to Sarsa, which is an on-policy algorithm because it learns the policy that it follows."
  },
  {
    "objectID": "posts/c2-w3.html#lesson-3-expected-sarsa",
    "href": "posts/c2-w3.html#lesson-3-expected-sarsa",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Lesson 3: Expected SARSA",
    "text": "Lesson 3: Expected SARSA\n\nLesson Learning Goals\n\nDescribe the Expected SARSA algorithm #\nDescribe Expected SARSA’s behavior in an example MDP #\nUnderstand how Expected SARSA compares to SARSA control #\nUnderstand how Expected SARSA can do off-policy learning without using importance sampling #\nExplain how Expected SARSA generalizes Q-learning #\n\n\n\\[\n\\begin{aligned}\nQ(S_t, A_t) & \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\mathbb{E}[Q(S_{t+1}, A_{t+1})] - Q(S_t, A_t)] \\newline\n& \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\sum_a \\pi(a|S_{t+1}) \\cdot Q(S_{t+1}, a) - Q(S_t, A_t)]\n\\end{aligned}\n\\]\n\nUnderstanding Expected Sarsa\nlets recap the Bellman equation for the action-value function:\n\\[\n\\begin{aligned}\nq_\\pi(s,a) & \\dot = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\qquad \\newline\n& = \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\sum_{a'}\\pi(a' \\mid s') q_\\pi(s', a')] \\qquad\n\\end{aligned}\n\\tag{5}\\]\nWe can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.\nin the sarsa update rule:\n\\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n\\tag{6}\\]\nwe knows the policy \\(\\pi\\) so we can make a better update by replacing the sampled next action with the expected value of the next action under the policy \\(\\pi\\).\nThis is the basis of Expected Sarsa.\nwhich uses the update rule:\n\\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\sum_a \\pi(a|S_{t+1}) \\cdot Q(S_{t+1}, a) - Q(S_t, A_t)]\n\\tag{7}\\]\nOtherwise the algorithm is the same as Sarsa.\n\nThis target is more stable than the Sarsa target because it is less noisy.\nThis makes it converge faster than Sarsa.\n\nthis has a has a down side - it has more computation than Sarsa due to avaraging over many actions for every step.\n\n\n\n\n\n\nExpected Sarsa\n\n\n\n#| label: alg-q-learning\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{expected SARSA}\n\\begin{algorithmic}[1]\n\n\\State Initialize:\n  \\State $\\qquad Q(s,a) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S^+}, a\\in\\mathcal{A}(s)\\ except\\ for\\ Q(terminal,\\cdot)=0$\n  \n\\For {each episode e:}\n  \\State Initialize $S$\n  \\State Choose A from S using policy derived from Q (e.g., $\\epsilon$-greedy)\n  \\For {each step of e}\n    \\State Take action A, observe R, S'\n    \\State Choose A' from S' using policy derived from Q (e.g., $\\epsilon$-greedy)\n    \\State $Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\sum_a \\pi(a|S') Q(S',A') - Q(S, A)]$\n    \\State $S \\leftarrow S'$; $A \\leftarrow A'$\n    \\State $S$ is terminal\n  \\EndFor\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\n\n\nSummary - Connecting the dots\n\n\n\n\nExpected Sarsa is a generalization of Q-learning and Sarsa.\nThere are many RL algorithms in our specialization flowchart.\nWe would like to find a few or even one algorithm that may be widely applicable to many different settings, carrying over the insights we learned from each new algorithm.\nThe first step in this direction was introducing the \\(epsilon\\) parameter to the bandit algorithms, which allowed us to treat the exploration-exploitation trade-off. We have seen additional strategies for exploration but we have been using either an \\(\\epsilon\\)-greedy strategy or a epsilon-soft strategy in most algorithms.\n\nWe also got the powerful idea of using confidence intervals as tie breakers in the case of multiple actions with the same expected reward.\n\nAnother step in this direction was to introduce discounting of rewards which let us parameter discounting with \\(\\gamma\\) and thus treat episodic and continuing tasks in the same way.\nThe MC algorithms showed that this is not enough to fully generalize to episodic and continuing tasks. However we got a powerful new ideas of inverse sampling and doubly robust estimators. For use with off-policy learning.\nNext we introduced GPI in which we combined policy evaluation and policy improvement algorithms to iteratively approximate the optimal policy in a single algorithm.\nAnother lesson was to using the TD error to bootstrap the value function. This let us update value functions after each step, rather than waiting until the end of the episode, increasing the data efficiency of the algorithms.\n\nWe also saw that we can use this idea with action-value functions, which is more fine grained than the value function and can lead to more efficient learning.\n\nNext we saw that Expected Sarsa is one such algorithm that can be used in many different settings.\n\nIt can be used in episodic and continuing tasks,\nIt can be used for on-policy and off-policy learning.\nIt is a GPI algorithm that uses the TD error to update the action-value function. And it the \\(\\epsilon\\)-greedy strategy implicit in its action-value function."
  },
  {
    "objectID": "posts/c3-w3.html",
    "href": "posts/c3-w3.html",
    "title": "Control with Approximation",
    "section": "",
    "text": "RL logo\n\n\n\n\n\n\n\nRL algorithm decision tree\n\n\n\n\nFigure 1: The algorithms we will be discussing in this lesson are function approximation versions of SARSA, Expected SARSA, and Q-learning. These are all sample-based algorithms that solve the Bellman equation for action-values. They differ in how they estimate the action-values and how they update them."
  },
  {
    "objectID": "posts/c3-w3.html#episodic-sarsa-with-function-approximation-video",
    "href": "posts/c3-w3.html#episodic-sarsa-with-function-approximation-video",
    "title": "Control with Approximation",
    "section": "Episodic SARSA with function approximation (Video)",
    "text": "Episodic SARSA with function approximation (Video)\nIn this video, Adam White, discusses the algorithm for “Episodic SARSA with function approximation”. He explains how it can be used to solve reinforcement learning problems with large or continuous state-spaces. He also delineates the importance of feature choices in this algorithm and how they can impact the performance of the system."
  },
  {
    "objectID": "posts/c3-w3.html#sec-l1g2",
    "href": "posts/c3-w3.html#sec-l1g2",
    "title": "Control with Approximation",
    "section": "Two ways to construct action dependent features?",
    "text": "Two ways to construct action dependent features?\n\n\n\n\n\n\n\nStacking\n\n\n\n\nFigure 2: Stacking involves concatenating the state features with the action features\n\n\n\n\n\n\n\n\nPassing\n\n\n\n\nFigure 3: Passing Actions to Features involves passing the state features through a neural network that also takes the action as input\n\n\n\nWe see two techniques for constructing action dependent features. It is worthwhile noting that these two techniques are quite generally used in Machine Learning. For example Concatenating is used in CNN and with multi-head attention in Transformers.\n\nStacking involves concatenating the state features with the action features. This is a simple and effective way to construct action dependent features.\n\nStacking can used both for linear function approximation and for neural networks in much the same way. Stacking is very simple but it has a problem, it keeps the same state features used for different actions separate. This seems to be both an over-parameterization (i.e. overfitting) and a an impediment to learning a good representation of the state.\n\nPassing Actions to Features attempts to remedy this issue. In this technique, the state features are passed through a neural network that also takes the action as input. This allows the network to learn a better representation of the state that is dependent on both the state and the action. This technique is more complex but can lead to better performance."
  },
  {
    "objectID": "posts/c3-w3.html#how-to-use-sarsa-in-episodic-tasks-with-function-approximation",
    "href": "posts/c3-w3.html#how-to-use-sarsa-in-episodic-tasks-with-function-approximation",
    "title": "Control with Approximation",
    "section": "How to use SARSA in episodic tasks with function approximation",
    "text": "How to use SARSA in episodic tasks with function approximation\nNext we will see how to use SARSA in episodic tasks with function approximation. The main idea is to use a similar update rule as before, but with the action value function approximated by a function approximator, i.e. it will be parametrized by weights w. Also we will need to add a the gradient of the to the update rule. As we will want to move the weights in the direction of the gradient of the action value function.\n\n\n\n\n\n\nSARSA\n\n\n\n#| label: alg-sarsa\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{SARSA($\\alpha,\\epsilon$)}\n\\begin{algorithmic}[1]\n\\State Initialize:\n  \\State $\\qquad Q(s,a) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S^+}, a\\in\\mathcal{A}(s)\\ except\\ for\\ Q(terminal,\\cdot)=0$\n  \n\\For {each episode e:}\n  \\State Initialize $S$\n  \\State Choose A from S using policy derived from Q (e.g., $\\epsilon$-greedy)\n  \\For {each step of e}\n    \\State Take action A, observe R, S'\n    \\State Choose A' from S' using policy derived from Q (e.g., $\\epsilon$-greedy)\n    \\State $Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma Q(S',A') - Q(S, A)]$\n    \\State $S \\leftarrow S'$; $A \\leftarrow A'$\n  \\EndFor\n  \\State until $S$ is terminal\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\n\nSARSA is a sample-based algorithm to solve the Bellman equation for action-values.\n\nIt picks an action based on the current policy and then\nIt policy evaluation by a TD updates of Q the action-value function based on the reward and the next action.\nThen it does a policy improvement.\n\n\n\n\n\n\n\n\nEpisodic SARSA with function approximation\n\n\n\n\nFigure 4: This figure shows the algorithm for Episodic semi-gradient SARSA which uses function approximation. The update rule is modified from tabular case, with an approximate action value function \\(\\hat{q}(s,a,\\mathbf{w})\\) as well as the inclusion of the gradient of the action value function \\(\\nabla \\hat{q} (S_t, A_t, \\mathbf{w})\\) . These two modification allows us to update the weights of the function approximator The rest of the algorithm remains unchanged.\n\n\n[@Rummery1994OnlineQU] introduced SARSA, but the name is due to Rich Sutton.\n\n\n\n\n\n\nEpisodic Semi-gradient SARSA\n\n\n\n#| label: alg-td-sarsa\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\n\\begin{algorithm}\n  \\caption{Episodic Semi-gradient SARSA for estimating ($\\hat{q} \\approx q_*$)}\n\\begin{algorithmic}[1]\n\n\\State $\\textbf{Input:}$\n  \\State $\\qquad \\text{a differentiable action-value fn parameterization } \\hat{q}: \\mathcal{S} \\times A \\times \\mathbb{R}^d \\to \\mathbb{R}$\n\\State $\\textbf{Initialize:}$\n  \\State $\\qquad$ value function weights $\\mathbf{w} \\in \\mathbb{R}^d$ arbitrarily, (e.g., $\\mathbf{w} = 0$)  \n\\For {each episode e:}\n  \\State Initialize $S$\n  \\State Choose A from S using policy derived from $\\color{red}\\hat{q}(S'A,\\mathbf{w})$ (e.g., $\\epsilon$-greedy)\n  \\For {each step of e}\n    \\State Take action A, observe R, S'\n    \\If {$S'$ is terminal}\n      \\State $\\color{red}\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ R - \\hat{q}(S, A, \\mathbf{w}) \\right] \\nabla \\hat{q}(S, A, \\mathbf{w})$ \n      \\State Go to next episode\n    \\EndIf\n    \\State Choose A' as a function of $\\color{red}\\hat{q}(S'A,\\mathbf{w})$ (e.g., $\\epsilon$-greedy)\n    \\State $\\color{red}\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ R + \\gamma \\hat{q}(S', A', \\mathbf{w}) - \\hat{q}(S, A, \\mathbf{w}) \\right] \\nabla \\hat{q}(S, A, \\mathbf{w})$ \n    \\State $S \\leftarrow S'$; \n    \\State $A \\leftarrow A'$\n  \\EndFor\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\n\nThis allows us to learn a good policy for the task. We will also need a step to update the weights.\n\n\n\n\n\n\n\nFigure 5: Talk titled ‘Advances in Value Estimation in Reinforcement Learning’ at London Machine Learning Meetu by Martha White on Two Pieces of Research on Exploration in Reinforcement Learning. On work from [patterson2024generalizedprojectedbellmanerror]\n\n\n\n\n\n\n\n\nOver Thinking Semi-gradient SARSA {.unnumbered}:\n\n\n\nSo you think you understand SARSA with function approximation?\n\nWhy is this a semi-gradient method?\nHints:\n\nwhat is the MSE td error of the Q-value the next state?\n\n\nIf we define the MSE of the Q-value of the next state as the projected Bellman error, then we can see that the update rule is a projected gradient descent on the projected Bellman error. This is a semi-gradient method because we are not using the true value of the next state in the update rule.\n\\[\n\\overline{{QE}}(w) \\doteq \\sum_{s\\in \\mathcal{S},a\\in \\mathcal{A}} \\mu(s)\\left[  q^*(s,a) - \\hat{q}(s,a,\\mathbf{w})  \\right]^2\n\\]\nsince we dont know the action value function we can’t compute the true value of the next state. Instead we use the value of the next state.\n\\[\n\\begin{align*}\n  \\overline{{QE}}(w) &\\approx \\sum_{s\\in \\mathcal{S},a\\in \\mathcal{A}} \\mu(s)\\left[ r+\\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w})  \\right]^2 \\\\\n  &= \\mathbb{E}_\\mu \\left[ (r+\\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}))^2 \\right]\n\\end{align*}\n\\]\nthen we have the following objective function:\n\\[ J(w) = \\mathbb{E}_\\mu \\left[ \\sum_{t=0}^\\infty \\gamma^t  \\left( r_t + \\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}) \\right)^2 \\right]\n\\]\n\nwhere:\n\n\\(\\mu\\) is the state visitation distribution\n\\(\\hat{q}(s,a,\\mathbf{w})\\) is the approximated action value function\n\\(\\mathbf{w}\\) is the weight vector\n\\(r_t\\) is the reward at time step t\n\\(\\gamma\\) is the discount factor\n\n\n\\[\n\\begin{align*}\n\\nabla_\\mathbf{w} J(w) &= \\mathbb{E}_\\mu \\left[ \\sum_{t=0}^\\infty \\gamma^t \\nabla_\\mathbf{w} \\left( r_t + \\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}) \\right)^2 \\right] \\\\\n&= \\mathbb{E}_\\mu \\left[ \\sum_{t=0}^\\infty \\gamma^t 2 \\left( r_t + \\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}) \\right) \\nabla_\\mathbf{w} \\left( r_t + \\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}) \\right) \\right] \\newline\n&= \\mathbb{E}_\\mu \\left[ \\sum_{t=0}^\\infty \\gamma^t 2 \\delta  \\left( \\gamma \\nabla_\\mathbf{w} \\hat{q}(s',a',\\mathbf{w}) - \\nabla_\\mathbf{w} \\hat{q}(s,a,\\mathbf{w}) \\right) \\right] \\\\\n\\end{align*}\n\\]\nwhere \\(\\delta = r_t + \\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w})\\) is the TD error.\nthis is the projected gradient of the projected Bellman error.\nwhich would give us the following update rule:\n\\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\delta \\left( \\gamma \\nabla_\\mathbf{w} \\hat{q}(s',a',\\mathbf{w}) - \\nabla_\\mathbf{w} \\hat{q}(s,a,\\mathbf{w}) \\right)\n\\]\nrather than the update rule we have in the algorithm:\n\\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\delta \\nabla_\\mathbf{w} \\hat{q}(s,a,\\mathbf{w})\n\\]\n\\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R_{t+1} + \\gamma \\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}) - \\hat{q}(S_t,A_t,\\mathbf{w})] \\nabla \\hat{q}(S_t,A_t,\\mathbf{w})\n\\]\n\nCan we do better than this?\n\nHint: using a projected Bellman error, we may get a better algorithm than SARSA with function approximation.\n\nDo we have any convergence guarantees for SARSA with function approximation?\n\nIn the text book [@sutton2018reinforcement sec. 10.1] the authors state that for a constant policy 1, this method converges in the same way that TD(0) does, with the same kind of error bound. Then they go on to say that for a non-constant policy, the convergence is a matter of ongoing research.\nFor the tabular setting, [@singh2000convergence] the authors show that the algorithm has asymptotic converges to the optimal policy provided that the policies from the policy improvement operator is “greedy in the limit with infinite exploration”\nFor the function approximation setting, Empirical results show that the linear SARSA can chatter, i.e. the weight vector does not go to infinity (i.e., it does not diverge) but oscillates in a bounded region.\n\nSince it is semi-gradient, we are not using the true value of the next state in the update rule.\nAre we biased?\nDoes this algorithm have lower variance than the TD(0) algorithm?\nWhen we choose the next action, are we on policy or off policy?\nIn the non terminal rule we use two samples, but they are highly correlated. Why is this a problem\nHow can we reduce this correlation?\nWhat is the policy improvement operator?\nWhat is the Lipschitz constant of this operator and why don’t we see this in the SARSA algorithm?"
  },
  {
    "objectID": "posts/c3-w3.html#sec-l1g1",
    "href": "posts/c3-w3.html#sec-l1g1",
    "title": "Control with Approximation",
    "section": "The update for Episodic SARSA with function approximation",
    "text": "The update for Episodic SARSA with function approximation\nSo far we have only been using function approximation to parametrize state value function,\n\\[\nV_\\pi(s) ≈ \\hat{v}(s,w) \\doteq \\mathbf{w}^T \\cdot \\mathbf{x}(S) \\qquad\n\\tag{1}\\]\nFor SARSA we need a parametrized approximation \\(\\hat{q}\\) for the the action value function \\(q_pi\\),\n\\[\nq_\\pi(s,a) ≈ \\hat{q}(s,a,\\mathbf{w}) \\doteq \\mathbf{w}^T \\cdot \\mathbf{x}(s,a) \\qquad\n\\tag{2}\\]"
  },
  {
    "objectID": "posts/c3-w3.html#episodic-sarsa-in-mountain-car-video",
    "href": "posts/c3-w3.html#episodic-sarsa-in-mountain-car-video",
    "title": "Control with Approximation",
    "section": "Episodic SARSA in Mountain Car (Video)",
    "text": "Episodic SARSA in Mountain Car (Video)\n\n\n\n\n\n\n\nFigure 6: The mountain car environment"
  },
  {
    "objectID": "posts/c3-w3.html#feature-choices-in-episodic-sarsa-with-function-approximation",
    "href": "posts/c3-w3.html#feature-choices-in-episodic-sarsa-with-function-approximation",
    "title": "Control with Approximation",
    "section": "Feature Choices in Episodic SARSA with Function Approximation",
    "text": "Feature Choices in Episodic SARSA with Function Approximation\n\n\n\n\n\n\n\nFigure 7: The feature representations for the mountain car problem\n\n\nWhat features do we use for the mountain car problem?\nfor the state:\n\nposition\nvelocity\n\nfor the action:\n\naccelerate left\naccelerate right\ndo nothing"
  },
  {
    "objectID": "posts/c3-w3.html#sec-l1g3",
    "href": "posts/c3-w3.html#sec-l1g3",
    "title": "Control with Approximation",
    "section": "Visualizing Value Function and Learning Curves",
    "text": "Visualizing Value Function and Learning Curves\n\n\n\n\n\n\n\nFigure 8: The value function for the mountain car problem\n\n\n\n\n\n\n\n\nFigure 9: The trajectory through the state space for the mountain car problem\n\n\n\n\n\n\n\n\nFigure 10: The learning curve for the mountain car problem\n\n\n\n\nThe first two figures show the learned value function for the mountain car problem. The first figure shows the value function for each state. The second shows a possible trajectory through the state space.\nThen we look at the learning curve for the mountain car problem. This shows how the value function improves over time as the agent learns the optimal policy. We see the familiar exponential decay in the learning curves.\nIt worth noting that this is a very simple environment and that many more sophisticated deep learning techniques don’t do a very good job on this problem."
  },
  {
    "objectID": "posts/c3-w3.html#expected-sarsa-with-function-approximation-video",
    "href": "posts/c3-w3.html#expected-sarsa-with-function-approximation-video",
    "title": "Control with Approximation",
    "section": "Expected SARSA with Function Approximation (Video)",
    "text": "Expected SARSA with Function Approximation (Video)\nNow we extend SARSA with under function approximation into Expected SARSA .\nFirst, recall the update rule for Tabular SARSA:\n\\[\nQ(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha (R_{t+1} + \\gamma Q( \\textcolor{red}{ S_{t+1}, A_{t+1} }) - Q(S_t,A_t)) \\qquad\n\\tag{3}\\]\nSARSA’s update target includes the action value for the next state in action.\nNext, recall how Tabular Expected SARSA uses the expectation over its target policy instead.\n\\[\nQ(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha (R_{t+1} + \\gamma \\textcolor{red}{ \\sum_{a'} \\pi (a' \\mid S_{t+1}) Q(S_{t+1},a')} - Q(S_t,A_t)) \\qquad\n\\tag{4}\\]\nWe can compute the same expectation using function approximation.\nFirst, recall the update for SARSA with function approximation. It looks similar to the tabular setting except the action value estimates are parameterized by the weight factor, \\(\\mathbf{w}\\). i.e. \\(q_\\pi (s, a) \\approx \\hat{q} (s, a, \\mathbf{w})\\)\n\\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R_{t+1} + \\gamma \\hat{q}(\\textcolor{red} { S_{t+1} } , \\textcolor{red} { A_{t+1}}, \\mathbf{w}) - \\hat{q}(S_t, A_t, \\mathbf{w})] \\nabla \\hat{q} (S_t, A_t, \\mathbf{w}) \\qquad\n\\tag{5}\\]\nExpected Sarsa with function approximation follows a similar structure.\n\\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R_{t+1} + \\gamma   \\textcolor{red}{ \\sum_{a'} \\pi( a' \\mid S_{t+1}) \\hat{q} (S_{t+1}, a' , \\mathbf{w} )} - \\hat{q}(S_t , A_t , \\mathbf{w})] \\nabla \\hat{q}(S_t , A_t , \\mathbf{w}) \\qquad\n\\tag{6}\\]"
  },
  {
    "objectID": "posts/c3-w3.html#sec-l1g4",
    "href": "posts/c3-w3.html#sec-l1g4",
    "title": "Control with Approximation",
    "section": "How this extends to Q-learning easily, since it is a subset of Expected SARSA",
    "text": "How this extends to Q-learning easily, since it is a subset of Expected SARSA\nFinally, for Q-learning with function approximation as Q-learning is a special case of expected SARSA in which we take the greedy. So next we replace the expectation over the target policy by argmax action to derive the Q-learning update rule.\nThe Q-learning update rule is:\n\\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R_{t+1} + \\gamma  \\textcolor{red}{ \\max_{a'} \\hat{q}(S_{t+1},a',\\mathbf{w})} −\\hat{q}(S_t,A_t,\\mathbf{w})] \\nabla \\hat{q}(S_t,A_t,\\mathbf{w}) \\qquad\n\\tag{7}\\]"
  },
  {
    "objectID": "posts/c3-w3.html#exploration-under-function-approximation-video",
    "href": "posts/c3-w3.html#exploration-under-function-approximation-video",
    "title": "Control with Approximation",
    "section": "Exploration under Function Approximation (Video)",
    "text": "Exploration under Function Approximation (Video)"
  },
  {
    "objectID": "posts/c3-w3.html#sec-l2g1",
    "href": "posts/c3-w3.html#sec-l2g1",
    "title": "Control with Approximation",
    "section": "Optimistic Initialization as a Form of Exploration",
    "text": "Optimistic Initialization as a Form of Exploration\nIn the tabular setting, we saw that we could use optimistic initialization as a form of early exploration. The way this works is that we initialize the value function to be very high, this way the agent will be encouraged to try to exploit one of the states. It will discover that the value of the state is not as high as it thought and will try to exploit another and so on until it have visited all the states and learned more realistic values for them. Over time the effect of the initial values will diminish. This however assumes two things:\n\nthe number of states is finite\nthe values are independent of each other\n\nIn the function approximation setting, we can try to do the same thing. This time we will want to initialize the weights so as to make the value function high. We face a number of issues in this setting:\n\nthe values are not independent of each other so each weight may loose its optimistic value long before the agent has explored many of the states within the features neighborhood in the state space.\nUnlike values we can’t be certain that high weights will lead to high values. While this may work for linear function approximation, for a non-linear function approximation using Neural Networks with tanh activation functions, positive weights may lead to negative values.\n\nepsilon-greedy exploration is easy to implement even with non-linear function approximation. However it is not as effective in the function approximation setting. Because it relies on randomness to explore states near those followed by the current policy. This is not as systematic as the exploration due to optimistic initialization in the tabular setting.\nImproving exploration with function approximation is an open research question\nIn this course we will stick to epsilon-greedy exploration.\nQ. Why is epsilon-greedy exploration ineffective in the function approximation setting?\n\nlike in the bandit setting, the agent keeps exploring even after it has found the optimal policy.\nlike in environments with a changing maze multiple epsilon-greedy exploration steps may be required to explore states required by the optimal policy that are not near the current policy. The chance for such an exploration is \\(\\mu(s)\\epsilon^n\\) where \\(mu(s)\\) is importance of the nearest state and n is the number of steps required to reach the target state. This can be vanishingly small for large state spaces. Which means it can take too long to find the optimal policy using epsilon-greedy exploration. We need to think of better ways to organize exploration in the function approximation setting.\nPrioritizing using count based on a coarse coding may be more effective. Even better if we track the uncertainty in the value function for each if these features. This is a form of intrinsic motivation.\n\nHowever the last video by Satinder Singh discusses using intrinsic motivation to improve exploration in reinforcement learning systems. And in it he shows a different paradigm of exploration. Rather than getting agents to explore systematic one wants to explore in a way that is interesting to the agent."
  },
  {
    "objectID": "posts/c3-w3.html#average-reward-a-new-way-of-formulating-control-problems-video",
    "href": "posts/c3-w3.html#average-reward-a-new-way-of-formulating-control-problems-video",
    "title": "Control with Approximation",
    "section": "Average Reward: A New Way of Formulating Control Problems (Video)",
    "text": "Average Reward: A New Way of Formulating Control Problems (Video)\nYou probably never thought about it, since discounting is familiar like a geometric series, but it can really skew the value function.\n\n\n\n\n\n\n\nThe near sighted MDP\n\n\n\n\nFigure 11: The near sighted MDP. The lower discounting causes the agent to prefer the smaller immediate reward over the larger delayed reward.\n\n\nIn most states, there’s only one action, so there are no decisions to be made. There’s only one state were a decision can be made. In this state, the agent can decide which ring to traverse.\nThis means there are two deterministic policies, traversing the left ring or traversing the right ring. The reward is zero everywhere except for in one transition in each ring. In the left ring, the reward is +1 immediately after state S. In the right ring, the reward is +2 immediately before state S. Intuitively, you would pick the right action because you know you will get +2 reward. But what would the value function tell us to do?\nIf we use discounting, what are the values of state S under these two different policies?\nThe policy that chooses the left action has a value of \\(v_l(S) = \\frac{1}{1-\\gamma^5}\\). How do we figure this out? If you write out the infinite discounted return, you will see this is a fairly straightforward geometric series with a closed form solution.\nThe policy that chooses the right action has a value of \\(v_r(S) = \\frac{2 \\gamma^4}{1-\\gamma^5}\\). Let’s think of the value of state S under these two part policies for particular values of \\(\\gamma\\).\n\\(\\gamma= 0.5 \\implies V_L \\approx 1 \\qquad V_R \\approx 0.1\\)\nThis means the policy that takes the left action is preferable under this more myopic discount.\nLet’s try\n\\(\\gamma= 0.9 \\implies V_L \\approx 2.4 \\qquad V_R \\approx 3.2\\)\nSo now we prefer the other policy.\nIn fact, we can figure out the minimum value of \\(\\gamma\\) so that the agent prefers the policy that goes right. \\(\\gamma\\) needs to be at least 0.841. So the problem here is that the discount magnitude depends on the problem.\nFor this example, 0.85 is sufficiently large. But if the rings had 100 states each, this discount factor would need to be over 0.99.\nIn general, the only way to ensure that the agents actions maximize reward over time is to keep increasing the discount factor towards 1.\nDepending on the problem, we might need \\(\\gamma\\) to be quite large. And we can’t set it to 1 in a continuing setting as the return might be infinite.\nNow, what’s wrong with having larger \\(\\gamma\\)? Larger values of \\(\\gamma\\) can also result in larger and more variables sums, which might be difficult to learn. So is there an alternative?"
  },
  {
    "objectID": "posts/c3-w3.html#sec-l3g1",
    "href": "posts/c3-w3.html#sec-l3g1",
    "title": "Control with Approximation",
    "section": "The Average Reward Setting",
    "text": "The Average Reward Setting\n\\[\nr(\\pi) \\doteq \\lim _{h \\to \\infty} \\frac{1}{h} \\sum_{t=1}^{h} \\mathbb{E}[R_t S_0,A_{0:t−1} \\sim \\pi]  \\qquad\n\\tag{8}\\]\nLet’s discuss a new objective called the average reward. Imagine the agent has interacted with the world for H steps. This is the reward it has received on average across those H steps. In other words, it’s rate of reward. If the agents goal is to maximize this average reward, then it cares equally about nearby and distant rewards. We denote the average reward of a policy with \\(R_\\pi\\).\n\\[\nr(\\pi) = \\sum_{s} \\mu_\\pi (s) \\sum_{a} \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a) r\n\\tag{9}\\]\nMore generally, we can write the average reward using the state visitation, \\(\\mu\\). This inner term is the expected reward in a state under policy \\(\\pi\\). The outer sum takes the expectation over how frequently the policy is in that state. Together, we get the expected reward across states. In other words, the average reward for a policy.\nIn the nearsighted example, the two deterministic possible policies visit either the left loop or the right loop indefinitely. In both cases, the five states in each loop are visited equally many times. In the left loop, the immediate expected reward is +0 for all states except one, which gets +1. This results in an average reward of 1 every 5 steps or 0.2.\n\\(r(\\pi_L)=1/5=0.2 \\qquad r(\\pi_R)=2/5=0.4\\)\nMost states in the right loop also have +0 \\(\\mu\\) to expected reward. But this time, the last state gets +2. This gives an average reward of 2 every 5 steps or 0.4.\n\n\n\n\n\n\n\nNear sighted MDP return for Average rewards\n\n\n\n\nFigure 12: Return for the nearsighted MDP for average rewards. The average reward for the left policy is 0.4 and for the right policy is 1.4.\n\n\n\n\n\n\n\n\nNear sighted MDP return for Average rewards\n\n\n\n\nFigure 13: Return for the nearsighted MDP for average rewards. The average reward for the left policy is \\(G_t=-1.8\\) and for the right policy is \\(G_t=0.8\\).\n\n\n\nWe can see the average reward puts preference on the policy that receives more reward in total without having to consider larger and larger discounts.\nThe average reward definition is intuitive for saying if one policy is better than another, but how can we decide which actions from a state are better?\nWhat we need are action values for this new setting. The first step is to figure out what the return is. In the average reward setting, returns are defined in terms of differences between rewards and the average reward \\(R_\\pi\\). This is called the differential return.\n\\[\nG_t = R_{t+1} −r_\\pi + R_{t+2} −r_\\pi + R_{t+2} −r_\\pi \\ldots \\qquad\n\\tag{10}\\]\nLet’s look at what the differential returns are in our nearsighted MDP.\nThe differential return represents how much more reward the agent will receive from the current state in action compared to the average reward of the policy. Let’s look at the differential return starting in state \\(s\\), first choosing action L and then following \\(\\pi\\) L afterwards.\nThe average reward for this policy is 0.2.\nThe differential return is the sum of rewards into the future with the average reward subtracted from each one. This sum starts in state S with the action L. We can compute it by summing to some finite horizon H. Then taking the limit as H goes to infinity. We are simplifying things slightly with this limit notation. While notation provider works in many cases, we need to use a different technique when the environment is periodic. In this case, we compute the return using a more general limit called the Cesàro sum, but this technical detail is not critical. The main point here is the intuition. We find that the differential return is 0.4. Now, let’s look at the other action. This time, we can break the differential return into two parts. First the sum for a single trajectory through the right loop. We can write the sum explicitly and it’s equal to 1. Then the sum corresponding to taking the left action indefinitely. This sum is the same as the differential return we just computed, 0.4. Adding the two parts together, we find that the differential return is 1.4.\nWe can write the average reward using the state visitation, \\(\\pi\\). This inner term is the expected reward in a state under policy \\(\\pi\\). The outer sum takes the expectation over how frequently the policy is in that state. Together, we get the expected reward across states. In other words, the average reward for a policy\nIn the average reward setting, returns are defined in terms of differences between rewards and the average reward \\(r_\\pi\\), which is called the differential return. The differential return represents how much more reward the agent will receive from the current state in action compared to the average reward of the policy.\nThe differential return represents how much better it is to take an action in a state then on average under a certain policy. The differential return can only be used to compare actions if the same policy is followed on subsequent time steps. To compare policies, their average reward should be used instead\nThis quantity captures how much more reward the agent will get by starting in a particular state than it would get on average over all states if it followed a fixed policy.\n\\[\nq_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s,A_t = a] \\qquad\n\\tag{11}\\]\nLike in the discounted setting, differential value functions can be written as Bellman equations. They only differ in that they subtract r() from the immediate reward and there is no discounting. \\[\nq_\\pi(s,a) = \\sum_{s',r} p(s',r \\mid s,a)(r −r(\\pi)) + \\sum_{a'} \\pi(a' \\mid s')q_\\pi(s',a') \\qquad\n\\tag{12}\\]\nMany algorithms from the discounted settings can be rewritten to apply to the average reward case."
  },
  {
    "objectID": "posts/c3-w3.html#sec-l3g2",
    "href": "posts/c3-w3.html#sec-l3g2",
    "title": "Control with Approximation",
    "section": "When Average Reward Optimal Policies are Different from Discounted Solutions",
    "text": "When Average Reward Optimal Policies are Different from Discounted Solutions"
  },
  {
    "objectID": "posts/c3-w3.html#sec-l3g3",
    "href": "posts/c3-w3.html#sec-l3g3",
    "title": "Control with Approximation",
    "section": "Differential Value Functions v.s. Discounted Value Functions",
    "text": "Differential Value Functions v.s. Discounted Value Functions\n\n\n\n\n\n\nEpisodic Semi-gradient SARSA\n\n\n\n#| label: alg-differential-sg-sarsa\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\n\\begin{algorithm}\n  \\caption{Differential Semi-gradient SARSA for estimating ($\\hat{q} \\approx q_*$)}\n\\begin{algorithmic}[1]\n\\State $\\textbf{Input:}$\n  \\State $\\qquad \\text{a differentiable action-value fn parameterization } \\hat{q}: \\mathcal{S} \\times A \\times \\mathbb{R}^d \\to \\mathbb{R}$\n\\State $\\textbf{Algorithm parameters:}$\n  \\State $\\qquad \\alpha, \\beta &gt; 0$\n\\State $\\textbf{Initialize:}$\n  \\State $\\qquad$ value function weights $\\mathbf{w} \\in \\mathbb{R}^d$ arbitrarily, (e.g., $\\mathbf{w} = 0$)  \n  \\State $\\bar{R} source \\in \\mathbb{R}$ avg reward estimate\n\\State Initialize $S$ and action $A$\n\\For {each step}\n  \\State Take action A, observe R, S'    \n  \\State Choose A' as a function of $\\hat{q}(S',\\cdot,\\mathbf{w})$ (e.g., $\\epsilon$-greedy)\n  \\State $\\delta \\leftarrow R - \\bar{R} + \\hat{q}(S', A', \\mathbf{w}) - \\hat{q}(S, A, \\mathbf{w}) $\n  \\State $ \\bar{R}\\leftarrow  \\bar{R} + \\beta \\delta$\n  \\State $\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\delta \\nabla \\hat{q}(S, A, \\mathbf{w})$ \n  \\State $S \\leftarrow S'$; $A \\leftarrow A'$  \n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}"
  },
  {
    "objectID": "posts/c3-w3.html#satinder-singh-on-intrinsic-rewards-video",
    "href": "posts/c3-w3.html#satinder-singh-on-intrinsic-rewards-video",
    "title": "Control with Approximation",
    "section": "Satinder Singh on Intrinsic Rewards (Video)",
    "text": "Satinder Singh on Intrinsic Rewards (Video)\n\n\n\n\n\n\n\nFigure 14: This is a high level talk by Satinder Singh on AI and RL\n\n\n\n\n\n\n\n\nFigure 15: This talk titled ‘Steps Towards Continual Learning’ by Satinder Singh on Reinforcement Learning at DLSS & RLSS 2017 - Montreal\n\n\n\n\n\n\n\n\nFigure 16: Talk titled ‘Discovery in Reinforcement Learning’ at Beijing Academy of Artificial Intelligence by Satinder Singh on Two Pieces of Research on Exploration in Reinforcement Learning.\n\n\n\n\nSatinder Singh is a professor at the University of Michigan. He is a leading researcher in reinforcement learning and has made significant contributions to the field. In this video, he discusses intrinsic rewards and how they can be used to improve learning in reinforcement learning systems. It’s worth noting that he is one of the researchers who has worked on options with Doina Precup.\nNow Satinder Singh is a good speaker and he has lots of interesting research results to share. Unfortunately, this video is not his finest hour. I would definitely recommend watching some of his other talks linked above.\n\n\n\n\n\n\nDiscussion prompt\n\n\n\n\nWhat are the issues with extending some of the exploration methods we learned about bandits and Dyna to the full RL problem? How can we do visitation counts or UCB with function approximation?\n\n\nA control agent with function approximation has to explore to find the best policy, learn a good state representation, and try to get a lot of reward, all at the same time. How might an agent balance these potentially conflicting goals?"
  },
  {
    "objectID": "posts/c3-w3.html#footnotes",
    "href": "posts/c3-w3.html#footnotes",
    "title": "Control with Approximation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ne.g. epsilon greedy policy without decay↩︎"
  },
  {
    "objectID": "posts/c3-w1.html",
    "href": "posts/c3-w1.html",
    "title": "On-Policy Prediction with Approximation",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms\n\n\n\nWe now start the third course in the reinforcement learning specialization.\nIn terms of the ?@fig-rl-chart we are on the left branch of the tree.\nThis course is about prediction and control with function approximation.\nThe main difference in this course is that will start consider continuous state spaces and action spaces which cannot be represented as tables.\n\nHowever many of the methods we will develop will be useful in handling large scale tabular problems as well.\nWe will use methods from supervised learning but only online methods that can handle non-stationary data.\nThe main differences are the use of weights to parameterize the value functions.\nThe use of function approximation to estimate value functions and policies in reinforcement learning.\n\nWeights lead to using a loss function to estimate the value function.\nMinimizing the continuous loss function leads to Gradient descent and\nUsing sampling leads to Stochastic gradient descent.\n\nThe idea of learning weights rather than values is a key idea in this course.\nThe tradeoff between discrimination and generalization is also a key idea in this course.\n\nWe will learn how to use function approximation to estimate value functions and policies in reinforcement learning.\nWe will also learn how to use function approximation to solve large-scale reinforcement learning problems.\nWe will see some simple linear function approximation methods and later\nWe will see modern nonlinear approximation methods using deep neural networks.\n\nI did not find the derivation of the SGD alg particularly enlightening and I have seen it several times. However the online setting is the best motivation for the use of SGD and makes perfect sense in the context of reinforcement learning. Minibatches are then a natural extension of this idea.\n\n\n\n\n\n\nReadings\n\n\n\n\n\n\n[@sutton2018reinforcement§9.1-9.4, pp. 194-209] book"
  },
  {
    "objectID": "posts/c3-w1.html#moving-to-parameterized-policies-video",
    "href": "posts/c3-w1.html#moving-to-parameterized-policies-video",
    "title": "On-Policy Prediction with Approximation",
    "section": "Moving to Parameterized Policies (video)",
    "text": "Moving to Parameterized Policies (video)\nThis video covers the first four learning objectives.\nThis video covers parameterized policies and how they can be used to approximate value functions. The idea is that using a table has some limitations/ The first is that the tables can be very large. For continuous states they can become infinite.\nThe second is that the tables can be very sparse and in a table we don’t generalize between states.\nWe see that we don’t really want functions that directly approximate the value function. We want functions that have some structure that we can learn.\nThis is called a parameterized function. The ideas is to use a weighted sum of the features of the state. This allows us to learn the weights and use them to approximate the value function. This is called linear function approximation. The way to get around this which is two fold. We first represent the salient properties of a states into features.\nThen we use weights to combine these features to approximate the value function. This is called linear function approximation.\nMore generally we can use non linear parameterized functions to approximate value functions.\nAdam shows that is the features are not picked wisely we may not be able to discriminate between states - out function for one state will be the same as for another dissimilar state. Learning about one will make us forget what we learned about the other. This is called bias. On the other hand if we have too many features we may not be able to generalize between states. This is called variance. The goal is to balance between bias and variance."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l1g1",
    "href": "posts/c3-w1.html#sec-l1g1",
    "title": "On-Policy Prediction with Approximation",
    "section": "Understanding parameterized functions",
    "text": "Understanding parameterized functions\n\nIn the previous courses we represented value functions as tables or arrays:\n\nFor \\(V(s)\\) we had an array of size \\(|S|\\),\nFor \\(Q(s,a)\\) we had an array of size \\(|S| \\times |A|\\). This becomes impractical as \\(|S| \\rightarrow \\infty\\). We can use parameterized functions to approximate value functions. This is called function approximation.\n\nLinear value function approximation is a simple and popular method.\n\nWe represent the value function as a linear combination of features:\n\n\n\\[\n\\hat{v}(s, \\mathbb{w}) \\approx v_\\pi(s) \\qquad\n\\tag{1}\\]\n\nwhere:\n\n\\(\\hat{v}()\\) is the approximate value function\n\\(\\mathbf{w}\\) is a weight vector\n\nfor example:"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l1g2",
    "href": "posts/c3-w1.html#sec-l1g2",
    "title": "On-Policy Prediction with Approximation",
    "section": "Linear value function approximation",
    "text": "Linear value function approximation\n\nWe can write the approximate value function as a linear combination of features:\n\n\\[\n\\hat{v}(s, \\mathbb{w}) \\dot = w_1 X + w_2 + Y \\qquad\n\\tag{2}\\]\n\nwhere:\n\n\\(X\\) and \\(Y\\) are features of the state \\(s\\)\n\\(w_1\\) and \\(w_2\\) are the weights of the features\n\nnow learning becomes finding better weights that parameterize the value function.\n\nfinding the weights that minimize the error between the approximate value function and the true value function:"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l1g3",
    "href": "posts/c3-w1.html#sec-l1g3",
    "title": "On-Policy Prediction with Approximation",
    "section": "Tabular case is a special case of linear value function approximation",
    "text": "Tabular case is a special case of linear value function approximation\n \\[\n\\begin{align*}\n\\hat{v}(s, \\mathbb{w}) & \\dot = \\sum w_i x_i(s) \\newline\n                       & = &lt;\\mathbf{w}, \\mathbf{x}(s)&gt; \\qquad\n\\end{align*}\n\\]\n\nhere:\n\n\\(\\mathbf{w}\\) is a weight vector\n\\(\\mathbf{x}(s)\\) is a feature vector that is 1 in the \\(i\\)-th position and 0 elsewhere.\n\nlinear value function approximation is a generalization of the tabular case.\nlimitations of linear value function approximation:\n\nthe choice of features limits the expressiveness of the value function.\nit can only represent linear relationships between the features and the value function.\nit can only represent a limited number of features.\n\nso how are tabular functions a special case of linear value function approximation?\n\nwe can see from the figure that all we need is use one hot encoding for the features. Then the weighted vector will be the same as the value function in the table.\n\n\n\n\n\n\n\n\n\nFigure 1: Linear value function approximation failure"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l1g4",
    "href": "posts/c3-w1.html#sec-l1g4",
    "title": "On-Policy Prediction with Approximation",
    "section": "There are many ways to parameterize an approximate value function",
    "text": "There are many ways to parameterize an approximate value function\n\n\n\n\n\n\n\nFigure 2: neural networks are non-linear fn approximators\n\n\n\nWe can use different types of functions to approximate the value function:\n\none hot encoding\nlinear functions\ntile coding\nneural networks"
  },
  {
    "objectID": "posts/c3-w1.html#generalization-and-discrimination-video",
    "href": "posts/c3-w1.html#generalization-and-discrimination-video",
    "title": "On-Policy Prediction with Approximation",
    "section": "Generalization and Discrimination (video)",
    "text": "Generalization and Discrimination (video)\nIn this video Martha covers the next three learning objectives. The video is about:\n\nGeneralization - using knowledge (V,Q,Pi) about similar states.\nDiscrimination - being able to distinguish between different states."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l1g5",
    "href": "posts/c3-w1.html#sec-l1g5",
    "title": "On-Policy Prediction with Approximation",
    "section": "Understanding generalization and discrimination",
    "text": "Understanding generalization and discrimination\n\n\n\n\n\n\n\nFigure 3: generalization and discrimination\n\n\n\nGeneralization:\n\nthe ability to estimate the value of states that were not seen during training.\nin the case of policy evaluation, generalization is the ability of updates of value functions in one state to affect the value of other states.\nin the tabular case, generalization is not possible because we only update the value of the state we are in.\nin the case of function approximation, we can think of generalization as corresponding to an embedding of the state space into a lower-dimensional space.\n\nDiscrimination: the ability to distinguish between different states."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l1g6",
    "href": "posts/c3-w1.html#sec-l1g6",
    "title": "On-Policy Prediction with Approximation",
    "section": "How generalization can be beneficial",
    "text": "How generalization can be beneficial\n\nGeneralization can be beneficial because:\n\nIt allows us to estimate the value of states that are similar to states seen during training.\nThis includes states that were not seen during training.\nIt allows us to estimate the value of states that are far from states seen during training. (So long as they are similar in terms of the features we are using to approximate the value function)"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l1g7",
    "href": "posts/c3-w1.html#sec-l1g7",
    "title": "On-Policy Prediction with Approximation",
    "section": "Why we want both generalization and discrimination from our function approximation",
    "text": "Why we want both generalization and discrimination from our function approximation\n\nWe want both generalization and discrimination from our function approximation because:\n\ngeneralization allows us to estimate the value of states that were not seen during training.\ndiscrimination allows us to distinguish between different states.\ngeneralization allows us to estimate the value of states that are similar to states seen during training.\ndiscrimination allows us to estimate the value of states that are far from states seen during training.\n\n\nWe hear a lot about function approximation and gradient methods having a bias or high variance. I tracked this from wikipedia and statistical learning. While it makes sense for a Bayesian regression I’m not sure that it is quite correct for RL. Unfortunately I don’t have a better explanation, though reviewing this policy gradient lecture might be helpful\n\n\n\n\n\n\nBias-variance tradeoff\n\n\n\n\nan important result called the bias-variance tradeoff:\n\nBias is the error introduced by approximating a real-world problem, which may be extremely complicated, by a much simpler model. This means that since we cannot discriminate between different states that share weights for the same feature vector we have errors we characterize as bias.\nHigh bias corresponds to underfitting in our model.\nVariance is the opposite issue arising from having more features than we need to discriminate between states. This means that updating certain weights will affect only some of these related states and not others. This type of error is called variance and is also undesirable.\nHigh variance corresponds to overfitting in our model which can be due to our model fitting the noise in the data rather than the underlying signal.\nIn general for a model there is some optimal point where the bias and variance are balanced. Going forward from that point we observe a trade off between bias and variance so we need to choose one or the other.\nThis choice is usually governed by business realities and the nature of the data or the problem we are trying to solve."
  },
  {
    "objectID": "posts/c3-w1.html#framing-value-estimation-as-supervised-learning-video",
    "href": "posts/c3-w1.html#framing-value-estimation-as-supervised-learning-video",
    "title": "On-Policy Prediction with Approximation",
    "section": "Framing Value Estimation as Supervised Learning (video)",
    "text": "Framing Value Estimation as Supervised Learning (video)\nIn this video we cover the next two learning objectives. Martha has a good background in supervised learning and she explains how many parts of RL can be framed as supervised learning problems.\nThings we like to learn in RL in this course:\n\nValue fn approximation (V)\nAction fn values (Q)\nPolicies (Pi)\n\nIn reality we may want to learn other things as well which can be framed as supervised learning problems:\n\nState representations - i.e. better features (CNNs, RNNs, etc)\nModels of Dynamics i.e. Transition probabilities (P)\nReward precesses (R) What is a good reward function? How do we learn it? This is an inverse reinforcement learning problem. It is ill posed because there are many reward functions that can explain the data. We need to find the simplest one that explains the data. This is intertwined with learning internal motivations and goals. c.f. Satinder Singh’s work on intrinsic motivation.\nGeneralized Value functions (Gvfs) c.f. Martha and Adam White’s work on GVFs.\nOptions (Spatial or Temporal Aggregations of actions) c.f. Doina Precup’s work on options and semi-markov decision processes.\n\nEven more things that we might want to learn in RL that might be framed as supervised learning problems:\n\nApproximate/Compressed policies for sub goals AKA heuristics\n\nFully Pooled policies (all states are the same) - Think random uniform policy\nPartial pooled policies, (some states are the same)\nUnpooled policies (all states are different - tabular setting)\nPriors for Policies\nHierarchies of options - Think Nethack\n\nBeliefs about policies\nBeliefs about other agents (theory of mind)\nBeliefs about the environment.\nCausal models of the environment\n\nwhat can we influence and what can’t we influence.\n\nCoordination and communication with other agents c.f. work by Jakob Foerster, Natasha Jacques, and Marco Baroni on emergent communication.\n\nWhat part of communication is cheap talk\nWhat part of communication is credible"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l1g8",
    "href": "posts/c3-w1.html#sec-l1g8",
    "title": "On-Policy Prediction with Approximation",
    "section": "How value estimation can be framed as a supervised learning problem",
    "text": "How value estimation can be framed as a supervised learning problem\n\nThe problem of policy evaluation in reinforcement learning can be framed as supervised learning problem\n\nIn the case of Monte Carlo methods,\n\nthe inputs are the states and\nthe outputs are the returns \\(G\\).\n\nIn the case of TD methods,\n\nthe inputs are the states and\nthe outputs are the one step bootstrapped returns. \\(U_t \\dot=R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})\\)"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l1g9",
    "href": "posts/c3-w1.html#sec-l1g9",
    "title": "On-Policy Prediction with Approximation",
    "section": "Not all function approximation methods are well suited for reinforcement learning",
    "text": "Not all function approximation methods are well suited for reinforcement learning\n\nIn principle, any function approximation technique from supervised learning can be applied to the policy evaluation task. However, not all are equally well-suited. – Martha White\n\n\nin RL the agent interacts with the environment and generates data, which corresponds to the online setting in supervised learning.\nWhen we want to use supervised learning we need to choose a method that is well suited for the online setting which can handle\n\nnon-stationary data.\nnon-stationary and correlated data (which is the case in RL).\n\n\nIn fact much of the learning in RL is about learning such correlations and quickly adapting to non-stationary in the environment.\nIn TD learning the target depends on \\(w\\) but in supervised learning the target is fixed and given."
  },
  {
    "objectID": "posts/c3-w1.html#the-value-error-objective-video",
    "href": "posts/c3-w1.html#the-value-error-objective-video",
    "title": "On-Policy Prediction with Approximation",
    "section": "The Value Error Objective (Video)",
    "text": "The Value Error Objective (Video)\nIn this video Adam White covers the first two learning objectives of the unit.\nThe main subject about using the mean squared error as a loss for the approximate value function.\nwe get a sequence of \\((S_1,v_{\\pi}(S_1) ),(S_2,v_{\\pi}(S_2) ),(S_3,v_{\\pi}(S_3) ), \\ldots\\) and we want to approximate the value function . We can track how well we are approximating \\(v_{\\pi}(s)\\) by using \\(\\hat{v}(s,\\mathbb{w})\\). The difference can be positive or negative so if we average it the sum will tend to cancel out. If we square the error we get a positive number we have a much better estimate of the error. And if normalize it by taking the mean we can get use it to compare runs of different lengths. This is called the mean squared error.\nIt turns out that this is not enough for RL and we need to take a weighted average using the state distribution \\(\\mu(s)\\). This is because we care more about some states than others. The state distribution is the long run probability of visiting the state \\(s\\) under the policy \\(\\pi\\). This weighted average is called the mean squared value error.\n\n\n\n\n\n\nYour Objective is My Loss\n\n\n\n\n\nIn the optimization literature the loss function is called the objective function. This is because we are trying to optimize the weights of the function to minimize the loss. So we will often hear the term objective function or just objective. Life is simpler if we recall that this is just a loss function for a supervised learning problem.\nA related point is that if we want to optimize our approximate value we can swap the with a different loss function or with a different approximation function and the outcome should remain the same, at least under certain conditions. This is how we can switch from the mean squared value error objective to the Monte Carlo objective and then to the TD learning objective."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l2g1",
    "href": "posts/c3-w1.html#sec-l2g1",
    "title": "On-Policy Prediction with Approximation",
    "section": "Understanding the mean-squared value error objective for policy evaluation",
    "text": "Understanding the mean-squared value error objective for policy evaluation\n\n\n\n\n\n\n\nFigure 4: mean-squared value error objective\n\n\n\nAn idealized Scenario:\n\ninput: \\(\\{(S_1, v_\\pi(S_1)), (S_2, v_\\pi(S_2)), \\ldots, (S_n, v_\\pi(S_n))\\}\\)\noutput: \\(\\hat v(s,w) \\approx v_\\pi(s)\\)\nhowever in reality we may get some some error in the approximation.\n\nthis could be due to our choice of the approximation.\nbut initially we just don’t have good weights - to fit the data.\n\nWhat we need is a way to measure the error in the approximation.\nAlso we may care more about some states than others and we can encode this using the state distribution \\(\\mu(s)\\).\n\nThe mean-squared value error objective for policy evaluation is to minimize the mean-squared error between the true value function and the approximate value function:\n\n\\[\n\\overline{VE} = \\sum_{s\\in S}\\mu(s)[v_\\pi(S) - \\hat{v}(S, \\mathbf{w})]^2\n\\tag{3}\\]\n\nwhere:\n\n\\(\\overline{VE}\\) is the mean-squared value error\n\\(\\mu(s)\\) is the state distribution\n\\(v_\\pi(s)\\) is the true value of state \\(s\\)\n\\(\\hat{v}(s, \\mathbf{w})\\) is the approximate value of state \\(s\\) with weights \\(\\mathbf{w}\\)\n\nthe goal is to find the weights that minimize the mean-squared value error."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l2g2",
    "href": "posts/c3-w1.html#sec-l2g2",
    "title": "On-Policy Prediction with Approximation",
    "section": "Explaining the role of the state distribution in the objective",
    "text": "Explaining the role of the state distribution in the objective\n\nThe state distribution \\(\\mu(s)\\) is the long run probability of visiting the state \\(s\\) under the policy \\(\\pi\\).\nThis makes more sense if our markov chain is ergodic - i.e. we can reach any state from any other state by following some transition trajectory.\nThe state distribution is important because it determines how much we care about the error in each state.\nThe state distribution is usually unknown, and hard to estimate as it has complex dependencies on the policy and the environment.\nWe will later see a result that shows how we can avoid the need to know the state distribution.\nIn the diagram we see that the state distribution is a probability distribution over the states of the MDP and that there is little probability mass of visiting states at the edges of the state space.\nThe mean square error has less impact in these low probability states."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l2g3",
    "href": "posts/c3-w1.html#sec-l2g3",
    "title": "On-Policy Prediction with Approximation",
    "section": "The idea behind gradient descent and stochastic gradient descent",
    "text": "The idea behind gradient descent and stochastic gradient descent\n\nGradient descent is an optimization algorithm that uses the gradient to find a local minimum of a function.\nThe gradients points in the direction of the steepest ascent of the function and our objective is to minimize the mean squared error we move in the opposite direction.\nHence the name gradient descent.\nThe gradient of the mean-squared value error with respect to the weights \\(\\mathbf{w}\\) is given by:\n\n\\[\n    w \\dot = \\left [ \\begin{matrix} w_1 \\\\ \\vdots \\\\ w_d  \\end{matrix} \\right ] \\qquad \\nabla f = \\left [ \\begin{matrix} \\frac{\\partial f}{\\partial w_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial w_d}  \\end{matrix} \\right ] \\qquad\n\\]\n\nfor a linear function:\n\n\\[\n\\hat{v}(s, \\mathbf{w}) = \\sum \\mathbf{w}^T \\mathbf{x}(s) \\\\\n\\frac{\\partial \\hat{v}(s, \\mathbf{w})}{\\partial w_i} = \\mathbf{x_i}(s) \\\\\n\\nabla \\hat{v}(s, \\mathbf{w}) = \\mathbf{x}(s) \\qquad\n\\tag{4}\\]\n\nwe can write the update rule for the weights as:\n\n\\[\nw_{t+1} \\dot= w_t - \\alpha \\nabla J(\\mathbf{w_t}) \\qquad\n\\tag{5}\\]\n\nStochastic gradient descent is a variant of gradient descent that uses a random sample of the data to estimate the gradient.\nStochastic gradient descent uses mini-batches of data to estimate the gradient, which makes it computationally efficient and reduces the variance of the gradient estimate.\nIn practice we will use variants like:\n\nAdam - which adapts the learning rate based on the gradient.\nRMSProp - which uses a moving average of the squared gradient.\nAdagrad - which uses a different learning rate for each parameter.\nSGD - which uses a fixed learning rate."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l2g7",
    "href": "posts/c3-w1.html#sec-l2g7",
    "title": "On-Policy Prediction with Approximation",
    "section": "Gradient descent converges to stationary points",
    "text": "Gradient descent converges to stationary points\n\n\n\n\n\n\n\nFigure 5: gradient descent\n\n\n\nGradient descent converges to stationary points because the gradient of the mean-squared value error is zero at the minimum.\nGradient descent can get stuck in a local minima, so it is important to use a good initialization and learning rate.\nStochastic gradient descent can escape a local minima because it uses a random sample of the data to estimate the gradient.\nIn general the optimizer is not guaranteed to find the global minimum of the function - just a local minima"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l2g8",
    "href": "posts/c3-w1.html#sec-l2g8",
    "title": "On-Policy Prediction with Approximation",
    "section": "How to use Gradient descent and Stochastic gradient descent to minimize the value error",
    "text": "How to use Gradient descent and Stochastic gradient descent to minimize the value error\n\\[\n\\begin{align*}\n\\nabla J(\\mathbf{w}) & = \\nabla \\sum_{s\\in S} \\mu(s)[v_\\pi(s) - \\hat{v}(s, \\mathbf{w})]^2 \\qquad  \\qquad  \\hat{v}(s, \\mathbf{w}) = &lt;\\mathbf{w},\\mathbf{x}(s)&gt;\\newline\n                    & =  \\sum_{s\\in S}  \\mu(s) \\nabla [v_\\pi(s) - \\hat{v}(s, \\mathbf{w})]^2 \\qquad  \\qquad \\nabla   \\hat{v}(s, \\mathbf{w}) = \\mathbf{x}(s)\\newline\n                    & =  \\sum_{s\\in S} \\mu(s) 2 [v_\\pi(s) - \\hat{v}(s, \\mathbf{w})]\\nabla \\hat{v}(s, \\mathbf{w})\n\\end{align*} \\qquad\n\\]\n\nStochastic Gradient Descent\nIf we have a sample of states \\(s_1, s_2, \\ldots, s_n\\) observed by following \\(pi\\)\nwe can write the update rule for a pair of weights as:\n\\[\nw_{t+1} \\dot= w_t + \\alpha [v_\\pi(S_1) - \\hat{v}(s_1, \\mathbf{w_1})]\\nabla \\hat{v}(s, \\mathbf{w}) \\qquad\n\\tag{6}\\]\nThis allows us to decrease the error in the value function by making updates for one state at a time and moving the weights in the direction of the negative gradient. By making this type of update we might increase the error occasionally but in the long run we will decrease the error.\n\nThis updating approach is called stochastic gradient descent, because it only uses a stochastic estimate of the gradient. In fact, the expectation of each stochastic gradient equals the gradient of the objective. You can think of this stochastic gradient as a noisy approximation to the gradient that is much cheaper to compute, but can nonetheless make steady progress to a minimum – Martha White\n\nwe have here one issue - we don’t know the true value of the policy \\(v_pi(s_1)\\), how do we get around this?\none option is to replace the true value with an estimate, one option is to use the return from the state \\(s_1\\).\nrecall that\n\\[\nv_\\pi(s) = \\mathbb{E}[G_t \\mid S_t = s] \\qquad\n\\]\nso we can substitute the true value with the return from the state \\(s_1\\).\n\\[\n\\begin{align*}\nw_{t+1} & \\dot= w_t + \\alpha [v_\\pi(S_1) - \\hat{v}(s_1, \\mathbf{w_1})]\\nabla \\hat{v}(s, \\mathbf{w}) \\qquad \\\\\n        & \\dot = w_t + \\alpha [G_1 - \\hat{v}(s_1, \\mathbf{w_1})]\\nabla \\hat{v}(s, \\mathbf{w}) \\qquad\n\\end{align*}\n\\tag{7}\\]"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l2g4",
    "href": "posts/c3-w1.html#sec-l2g4",
    "title": "On-Policy Prediction with Approximation",
    "section": "The gradient Monte Carlo algorithm for value estimation",
    "text": "The gradient Monte Carlo algorithm for value estimation\nWe now have a way to update the weights of the value function using the gradient of the mean-squared value error. Which allows us to present the gradient Monte Carlo algorithm for value estimation.\n\n\n\n\n\n\nNote 1: MC prediction fist visit for estimating \\(V \\approx v_\\pi\\)\n\n\n\n#| label: alg-gradient-mc\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\\begin{algorithm}\n\\caption{GradientMC($\\pi$)}\n\\begin{algorithmic}[1]\n\\State Input\n  \\State $\\qquad \\pi$ to be evaluated\n \\State  $\\qquad \\text{a differentiable function } \\hat{v}: \\mathcal{S} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$\n\\State Algorithm parameters:\n  \\State $\\qquad \\alpha \\in (0, 1]$ step size\n\\State Initialize:\n  \\State $\\qquad \\mathbf{w} \\leftarrow x \\in \\mathbb{R^d} \\text{ arbiterly}$ (e.g. w=0)\n\\For {each episode:}\n  \\State Generate an episode by following $\\pi: S_0, A_0, R_1, S_1, A_1, R_2,\\ldots, S_{T-1}, A_{T-1}, R_T$\n  \\State $G \\leftarrow 0$\n  \\For {each step of episode, $t = 0, 1, \\ldots, T-1$:}\n      \\State $\\mathbf{w} \\leftarrow w_t + \\alpha [G_t - \\hat{v}(S_t , \\mathbf{w_1})] \\nabla \\hat{v}(S_t , \\mathbf{w})$\n  \\EndFor\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\nThe Gradient Monte Carlo algorithm is a policy evaluation algorithm that uses stochastic gradient descent to minimize the mean-squared value error."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l2g5",
    "href": "posts/c3-w1.html#sec-l2g5",
    "title": "On-Policy Prediction with Approximation",
    "section": "How state aggregation can be used to approximate the value function",
    "text": "How state aggregation can be used to approximate the value function\n\n\n\n\n\n\n\nFigure 6: gradient mc with state aggregation\n\n\n\nState aggregation\nis a method for reducing the dimensionality of the state space by grouping similar states together.\ncan be used to approximate the value function by representing each group of states as a single state.\ncan be used to reduce the number of parameters in the value function and improve generalization.\nthe example used a 1000 state MDP with 10 groups of 100 states each.\nleft and right jump left 1-100 states and right 1-100 states.\nif they pass the terminal state they get to the terminal state.\nstate aggregation is a way to reduce the number of parameters in the value function by grouping similar states together.\nit is an example of linear function approximation.\nthere is one feature for each group of states.\nthe weights are updated using the gradient of the mean-squared value error.\n\n\\[\nw \\leftarrow w + \\alpha [G_t - \\hat{v}(S_t, \\mathbf{w})] \\nabla \\hat{v}(S_t, \\mathbf{w})\n\\]\nand the gradient of the approximate value function is given by:\n\\[\n\\nabla \\hat{v}(S_t, \\mathbf{w}) = \\mathbf{x}(S_t)\n\\]\nwhich is either 1 or 0 depending on the group of states."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l2g6",
    "href": "posts/c3-w1.html#sec-l2g6",
    "title": "On-Policy Prediction with Approximation",
    "section": "Applying Gradient Monte-Carlo with state aggregation",
    "text": "Applying Gradient Monte-Carlo with state aggregation\n\nGradient Monte Carlo with state aggregation is a policy evaluation algorithm that uses state aggregation to approximate the value function.\nThe algorithm works as follows:"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l3g1",
    "href": "posts/c3-w1.html#sec-l3g1",
    "title": "On-Policy Prediction with Approximation",
    "section": "The TD-update for function approximation",
    "text": "The TD-update for function approximation\n\nrecall the Monte Carlo update rule:\n\n\\[\nw \\leftarrow w + \\alpha [G_t - \\hat{v}(S_t, \\mathbf{w})] \\nabla \\hat{v}(S_t, \\mathbf{w})\n\\]\n\nwe can use other targets for the update rule than the return \\(G_t\\).\nwe can replace the return with any estimate of the value of the next state.\nwe can call this target \\(U_t\\) and if it is unbiased it converge to a local minimum of the mean squared value error.\nwe can use the one step bootstrapped return:\n\n\\[\nU_t \\dot=R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})\n\\tag{8}\\]\n\nbut this is not unbiased because it essential approximating the expected return using the current value of the state.\nthere is no guarantee that the the update will converge to a local minimum of the mean squared value error.\nhowever the update has many advantages over the Monte Carlo update:\n\nit has lower variance because it uses a single sample.\nit can update the value function after every step.\nit can learn online.\nit can learn from incomplete episodes.\nit can learn from non-episodic tasks.\n\nthe TD-update is nor a true gradient update because the target is not the true value of the state. we call it a semi-gradient update.\n\nlet’s estimate the gradient of the mean squared value error with respect to the weights \\(\\mathbf{w}\\):\n\n\n\\[\n\\begin{align*}\n\\nabla J(\\mathbf{w}) & = \\nabla \\frac{1}{2}[U_t - \\hat{v}(S_t, \\mathbf{w})]^2 \\newline\n                     & = (U_t - \\hat{v}(S_t, \\mathbf{w})) (\\nabla U_t - \\nabla \\hat{v}(S_t, \\mathbf{w})) \\\\\n                     & \\ne - (U_t - \\hat{v}(S_t, \\mathbf{w})) \\nabla \\hat{v}(S_t, \\mathbf{w}) \\quad \\text{unless} \\quad \\nabla  U_t = 0\n\\end{align*}\n\\tag{9}\\]\n\nbut for TD we have:\n\n\\[\n\\begin{align*}\n\\nabla U_t = & = \\nabla (R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})) \\newline\n            & = \\gamma \\nabla \\hat{v}(S_{t+1}, \\mathbf{w}) \\newline\n            & \\ne 0\n\\end{align*}\n\\tag{10}\\]\n\nSo the TD-update isn’t a true gradient update. However TD often converge in many cases we care updates."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l3g2",
    "href": "posts/c3-w1.html#sec-l3g2",
    "title": "On-Policy Prediction with Approximation",
    "section": "Advantages of TD compared to Monte-Carlo",
    "text": "Advantages of TD compared to Monte-Carlo\n\nAdam point out that in Gradient Monte Carlo we need to run the alg for a long time and decay the step size to get convergence. But that in practice we don’t decay the step size and we use a fixed step size.1\nTD has several advantages over Monte-Carlo:\n\nTD can update the value function after every step, while Monte-Carlo can only update the value function after the episode is complete.\nTD can learn online, while Monte-Carlo can only learn offline.\nTD can learn from incomplete episodes, while Monte-Carlo requires complete episodes.\nTD can learn from non-episodic tasks, while Monte-Carlo can only learn from episodic tasks."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l3g3",
    "href": "posts/c3-w1.html#sec-l3g3",
    "title": "On-Policy Prediction with Approximation",
    "section": "The Semi-gradient TD(0) algorithm for value estimation",
    "text": "The Semi-gradient TD(0) algorithm for value estimation\n\nThe Semi-gradient TD(0) algorithm is a policy evaluation algorithm that uses the TD-update for function approximation.\n\n\n\n\n\n\n\nThe Semi-gradient TD(0) algorithm for estimating \\(v_\\pi\\)\n\n\n\n#| label: alg-td-zero\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{Semi-gradient TD(0) for estimating $v_\\pi$}\n\\begin{algorithmic}[1]\n\\State Input:\n  \\State $\\qquad \\pi \\leftarrow \\text{policy to be evaluated}$\n   \\State  $\\qquad \\text{a differentiable function } \\hat{v}: \\mathcal{S} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$\n\\State Algorithm parameters:\n  \\State $\\qquad \\alpha \\in (0, 1]$ step size\n  \\State $\\qquad \\gamma \\in [0, 1]$ discount factor\n\\State Initialize:\n  \\State $\\qquad value function weights w \\leftarrow x \\in \\mathbb{R}^d \\quad \\forall s \\in \\mathcal{S}$ (e.g. w=0)\n\\FORALL {episode $e$:}\n  \\State $Initialize S$\n  \\FORALL {step $S \\in e$:}\n    \\State $\\text{Choose } A \\sim \\pi(\\cdot \\mid S)$\n    \\State Take action $A$, observe $R, S'$\n    \\State $w \\leftarrow w + \\alpha [R + \\gamma \\hat{v}(S', \\mathbf{w}) - \\hat{v}(S, \\mathbf{w})] \\nabla \\hat{v}(S, \\mathbf{w})$\n    \\State $S \\leftarrow S'$\n    \\State until $S$ is terminal\n  \\ENDFOR\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l3g4",
    "href": "posts/c3-w1.html#sec-l3g4",
    "title": "On-Policy Prediction with Approximation",
    "section": "TD converges to a biased value estimate",
    "text": "TD converges to a biased value estimate\n\nTD converges to a biased value estimate because it updates the value function using an estimate of the next state.\nThe bias of TD can be reduced by using a smaller step size or by using a more accurate estimate of the next state."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l3g5",
    "href": "posts/c3-w1.html#sec-l3g5",
    "title": "On-Policy Prediction with Approximation",
    "section": "TD converges much faster than Gradient Monte Carlo",
    "text": "TD converges much faster than Gradient Monte Carlo\n\n\n\n\n\n\n\nFigure 7: early learning experiment\n\n\n\nWe run the same random walk experiment for the 1000 episodes 1000 step random walk and we see that TD has a worse fit than MC on most of the range.\nWe run a second experiments with only 30 episodes to see early learning performance and we see that TD has a better fit than MC on most of the range. In this case we used the best alpha for each method. MC needed a much smaller alpha to get a good fit.\nTD converges much faster than Gradient Monte Carlo because it updates the value function after every step.\nGradient Monte Carlo can only update the value function after the episode is complete, which can be slow for long episodes.\nTD can learn online, while Gradient Monte Carlo can only learn offline.\nTD can learn from incomplete episodes, while Gradient Monte Carlo requires complete episodes.\nTD can learn from non-episodic tasks, while Gradient Monte Carlo can only learn from episodic tasks."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l3g6",
    "href": "posts/c3-w1.html#sec-l3g6",
    "title": "On-Policy Prediction with Approximation",
    "section": "Doina Precup’s talk on Building Knowledge for AI Agents with Reinforcement Learning",
    "text": "Doina Precup’s talk on Building Knowledge for AI Agents with Reinforcement Learning\n\n\n\n\n\n\n\nFigure 8: Dorina Precup\n\n\n\n\n\n\n\n\nFigure 9: Options are a temporal generalization\n\n\n\nIn this talk, Dorina Precup discusses the challenges of building knowledge for AI agents using reinforcement learning.\n\nDorina Precup is a professor at McGill University and a research team lead at DeepMind.\nShe is an expert in reinforcement learning and machine learning.\nHer interests are in the areas of abstractions.\nWhen I think about generalization in RL I think about:\n\nLearning a parameterized value function that can be used to estimate the value of any state.\nLearning a parameterized policy that can be used to select actions in any state.\nBeing able to transfer this policy to a similar task\nBeing able to learn using less interaction with the environment and more from replaying past experiences.\nBeing able to learn from a small number of examples.\n\nDorina talks about two other aspects of generalization:\n\nAction duration are one time step in an MDP, yet in reality some actions like traveling from one city to another require sticking to the action over an extended period of time.\n\nThis might be happen through planning but idealy, agents should be able to learn skills which are sequences of actions that are executed over an extended period of time.\nThis has been formalized in the literature as options.\nShe references two sources\n\n[@Sutton1999BetweenMA] a paper from 1999 on options in reinforcement learning.\n[@precup2000temporal] her doctoral thesis from 2000 on temporal abstraction in reinforcement learning.\n\nOptions consists of\n\nan initiation set \\(\\iota_\\omega(s)\\) the precondition which is a probability of starting the option in state \\(s\\).\na policy \\(\\pi_\\omega(a\\mid s)\\) that is executed in the option\na termination condition \\(\\beta_\\omega(s)\\). the termination condition is a probability of terminating the option in state \\(s\\).\n\nOptions are “chunks of behavior” that can be executed over an extended period of time.\nThe model will need to learn options and work with them.\nIT needs expected reward over the option.\nA transition model over the option.\nThese models are predictive models about outcomes conditioned on the model being executed.\nAdding options to the model weakens the MDP assumption, because the option duration is not fixed so state now have a longer dependence is a sequence of actions that are not Markovian 2.\nPrecup’s point out that combining temporal and spatial abstraction is an ongoing research challenge.\nShe also points out that the model needs to learn the options and the value function at the same time.\nAccording to her profile Precup has a number of students working on this problem. Some additional references are:\n\n[@Bacon2016TheOA] a paper from 2016 on option-critic architecture which extends actor-critic algorithms to work with options.\n\nEarlier work uses the term macro-actions to refer to options.\n\n[@bradtke1994reinforcement] a paper from 1994 on reinforcement learning with hierarchies of machines.\n\n\n\n\n\n\n\n\nOptions & CI\n\n\n\n\nThis type of formulation seems very similar to that used by Judea Pearl in his structureal graphical model of Causality. If we can express options as a graph of states we can use his algorithms to infer the best options to take in a given state.\noptions are like do operations (interventions)\nchoosing between options is like conterfactual reasoning."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l4g1",
    "href": "posts/c3-w1.html#sec-l4g1",
    "title": "On-Policy Prediction with Approximation",
    "section": "Deriving the TD-update with linear function approximation",
    "text": "Deriving the TD-update with linear function approximation\n\nLinear function is both:\nsimple enough to be understood, yet\npowerful enough that with TD to be useful to create agents that are stornger than human Atari games.\nThe TD-update with linear function approximation is a way to update the weights of the value function using the TD-error.\nThe TD-update with linear function approximation works as follows:\n\nCompute the TD-error \\(\\delta\\) as the difference between the one-step bootstrapped return and the approximate value of the next state.\nUpdate the weights \\(\\mathbf{w}\\) in the direction of the TD-error.\n\n\nrecall the TD-update rule:\n\\[\n\\delta \\dot= R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}) \\\\\nw \\leftarrow w + \\alpha \\delta_t \\nabla \\hat{v}(S_t, \\mathbf{w})\n\\] in the linear case we can write the value function as:\n\\[\n\\hat{v}(S_t, \\mathbf{w}) \\dot = \\sum \\mathbf{w}^T \\mathbf{x}(S_t) \\\\\n\\nabla \\hat{v}(S_t, \\mathbf{w}) = \\mathbf{x}(S_t)\n\\]\n\\[\nw \\leftarrow w + \\alpha \\delta_t \\mathbf{x}(S_t)\n\\]"
  },
  {
    "objectID": "posts/c3-w1.html#sec-l4g2",
    "href": "posts/c3-w1.html#sec-l4g2",
    "title": "On-Policy Prediction with Approximation",
    "section": "Tabular TD(0) is a special case of linear semi-gradient TD(0)",
    "text": "Tabular TD(0) is a special case of linear semi-gradient TD(0)\n\nTabular TD(0) is a special case of linear semi-gradient TD(0) where the features are one-hot encoded.\nIn the tabular case, the weights are the same as the value function in the table.\nIn the linear case, the weights are the parameters of the value function.\nTabular TD(0) can be seen as a special case of linear semi-gradient TD(0) where the features are one-hot encoded."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l4g4",
    "href": "posts/c3-w1.html#sec-l4g4",
    "title": "On-Policy Prediction with Approximation",
    "section": "Advantages of linear value function approximation over nonlinear",
    "text": "Advantages of linear value function approximation over nonlinear\n\nLinear value function approximation has several advantages over nonlinear value function approximation:\n\nLinear value function approximation is computationally efficient and easy to implement.\nLinear value function approximation is easy to interpret and understand.\nLinear value function approximation is less prone to overfitting than nonlinear value function approximation.\nLinear value function approximation can be used to approximate any function, while nonlinear value function approximation is limited by the choice of features.\n\nIf we have access to expert knowledge we can use it to define good features and use linear value function approximation to learn the value function quickly.\nMost of the theory of function approximation in reinforcement learning is based on linear value function approximation."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l4g5",
    "href": "posts/c3-w1.html#sec-l4g5",
    "title": "On-Policy Prediction with Approximation",
    "section": "The fixed point of linear TD learning",
    "text": "The fixed point of linear TD learning\n\\[\nw_{t+1} \\dot= w_t + \\alpha [R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w})] \\nabla \\hat{v}(S_t, \\mathbf{w})\n\\] recall that in the linear case we defined the approximate value function as:\n\\[\n\\hat{v}(S_{t+1}, \\mathbf{w}) \\cdot=  \\mathbf{w}^T \\mathbf{x}(S_{t+1})\n\\] with :\n\n\\(\\mathbf{w}\\) the weights of the value function\n\\(\\mathbf{x}(S_{t+1})\\) the features of the next state\n\nusing this definition we can write the update rule as:\n\\[\n\\begin{align*}\nw_{t+1} & = w_t + \\alpha [R_{t+1} + \\gamma \\mathbf{w}^T \\mathbf{x}_{t+1} - \\mathbf{w}^T \\mathbf{x}] \\mathbf{x}_t \\newline\n        &=  w_t + \\alpha [R_{t+1} \\mathbf{x}_t -  \\mathbf{x}_t( \\mathbf{x}_t - \\gamma \\mathbf{x_{t+1}})^T \\mathbf{w_t}] \\newline\n\\end{align*}\n\\]\nlet us now consider what this update looks like in expectation:\nwe can think about it as an expected update plus a noise term but the noise term is dominated by the behaviour of the expected update.\n\\[\n\\mathbb{E}[\\Delta w_{t}]  = \\alpha(b-Aw_t)\n\\] where:\n\n\\(b = \\mathbb{E}[R_{t+1} \\mathbf{x}_t]\\) - expectation over the features and the rewards\n\\(A = \\mathbb{E}[\\mathbf{x}_t( \\mathbf{x}_t - \\gamma \\mathbf{x_{t+1}})^T]\\) - an expectation over the rewards\n\nnote: this is a linear system of equations that looks like a linear regression problem.\nwhen the weights do not change we have a fixed point:\n\\[\n\\begin{align*}\n\\mathbb{E}[\\Delta w_{TD}] & = \\alpha(b-Aw_{TD}) = 0 \\newline\n\\implies & w_{TD} = A^{-1}b\n\\end{align*}\n\\tag{11}\\] more generally \\(w_{TD}\\) is the solution to this equation and we could show that it minimises\n\\[\n(b-Aw)^T(b-Aw)\n\\tag{12}\\]\nthis is related to Bellman equations via the projected Bellman error."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l4g6",
    "href": "posts/c3-w1.html#sec-l4g6",
    "title": "On-Policy Prediction with Approximation",
    "section": "Theoretical guarantee on the mean squared value error at the TD fixed point",
    "text": "Theoretical guarantee on the mean squared value error at the TD fixed point\n\\[\n\\overline{VE}(w_{TD}) \\leq \\frac{1}{1-\\gamma} \\min_{w} \\overline{VE}(w)\n\\tag{13}\\]\n\nif \\(\\gamma \\approx 1\\) then the mean squared value error at the TD fixed point can be large\nif \\(\\gamma \\approx 0\\) then the mean squared value error at the TD fixed point can be small\nif the features representation is good then the two will be equal regardless of \\(\\gamma\\). since both will be almost zero."
  },
  {
    "objectID": "posts/c3-w1.html#sec-l4g7",
    "href": "posts/c3-w1.html#sec-l4g7",
    "title": "On-Policy Prediction with Approximation",
    "section": "Semi-gradient TD(0) algorithm",
    "text": "Semi-gradient TD(0) algorithm\nIn the assignment I implemented the Semi-gradient TD(0) algorithm for value estimation."
  },
  {
    "objectID": "posts/c3-w1.html#footnotes",
    "href": "posts/c3-w1.html#footnotes",
    "title": "On-Policy Prediction with Approximation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwhy not?↩︎\nthe property that the future is independent of the past given the present↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rl-notes",
    "section": "",
    "text": "Policy Gradient\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\n\n\n\n\n\n\n\nApr 4, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nControl with Approximation\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nConstructing Features for Prediction\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nConstructing Features for Prediction\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nOn-Policy Prediction with Approximation\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nOn-Policy Prediction with Approximation\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSample-based Learning Methods\n\n\nPlanning, Learning & Acting\n\n\n\nCoursera\n\n\nNotes\n\n\nRL\n\n\nReinforcement learning\n\n\nPlanning\n\n\nTabular Q-planning\n\n\nDyna architecture\n\n\nTabular Dyna-Q algorithm\n\n\nDyna-Q+ algorithm\n\n\n\nIn these module we define cover model based RL sampling. We start with the Dyna architecture. Then we consider tabular Q-planning algorithm, the Tabular Dyna-Q and Dyna-Q+ algorithms\n\n\n\n\n\nMar 4, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal Difference Learning Methods for Control\n\n\nSample-based Learning Methods\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\nTemporal Difference Learning Methods\n\n\nSARSA algorithm\n\n\nQ-learning algorithm\n\n\nExpected Sarsa algorithm\n\n\n\nThis week, we will learn to using TD learning for control, as a generalized policy iteration strategy. We will see three different algorithms based on bootstrapping and Bellman equations for control: Sarsa, Q-learning and Expected Sarsa. We will see some of the differences between the methods for on-policy and off-policy control, and that Expected Sarsa is a unified algorithm for both.\n\n\n\n\n\nMar 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal Difference Learning Methods for Prediction\n\n\nSample-based Learning Methods\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\nTemporal Difference Learning Methods\n\n\nTD(0) algorithm\n\n\n\nIn these unit we define some key terms like rewards, states, action, value functions, action values functions. Then we consider at the the multi-armed bandit problem leading to exploration explotation dillema, the epsilon greedy algorithm.\n\n\n\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMonte-Carlo Methods for Prediction & Control\n\n\nSample-based Learning Methods\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\nMonte-Carlo methods\n\n\nAny visit Monte-Carlo prediction\n\n\nFirst visit Monte-Carlo prediction\n\n\nMonte-Carlo with Exploring Starts GPI\n\n\nExploring Starts\n\n\nMonte-Carlo with Ɛ-soft GPI\n\n\nƐ-soft policies\n\n\nOff-policy learning\n\n\nImportance sampling\n\n\n\nIn this module we learn about Sample based MC methods that allow learning from sampled episodes. We revise our initial algorithm to better handle exploration. In off policy RL we see methods to learn a policy using samples from another policy, corrected using importance sampleing.\n\n\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Programming\n\n\nRL Fundamentals\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\ndynamic programming\n\n\n\nIn week 4 we learn how to compute value functions and optimal policies, assuming you have the MDP model. You will implement dynamic programming to compute value functions and optimal policies and understand the utility of dynamic programming.\n\n\n\n\n\nMay 5, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nValue Functions & Bellman Equations\n\n\nRL Fundamentals\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\nValue Functions\n\n\nBellman Equations\n\n\nOptimality\n\n\nOptimal Policies\n\n\nOptimal Value Functions\n\n\n\nIn week 3 we learn about Value Functions and Bellman Equations, which are the key technology behind all the algorithms we will learn. We learn the definition of policies and value functions, as well as Bellman equations.\n\n\n\n\n\nMay 4, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Decision Processes\n\n\nRL Fundamentals\n\n\n\nCoursera\n\n\nrl\n\n\n\nIn week 2 we learn about Markov Decision Processes (MDP) and how to compute value functions and optimal policies, assuming you have the MDP model. We implement dynamic programming to compute value functions and optimal policies.\n\n\n\n\n\nMay 3, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nThe K-Armed Bandit Problem\n\n\nRL Fundamentals\n\n\n\nrl\n\n\nCoursera\n\n\n\nIn week 1 we define some key concepts like rewards, states, action, value functions, action values functions. We consider the the multi-armed bandit problem, leading to exploration explotation dillema, and the epsilon greedy algorithm.\n\n\n\n\n\nMay 2, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Introduction\n\n\nRL Fundamentals\n\n\n\nCoursera\n\n\nnotes\n\n\nrl\n\n\nreinforcement learning\n\n\n\nIn week 1 we define some key concepts like rewards, states, action, value functions, action values functions. We consider the the multi-armed bandit problem, leading to exploration explotation dillema, and the epsilon greedy algorithm.\n\n\n\n\n\nMay 1, 2022\n\n\nOren Bochman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/c3-w2.html",
    "href": "posts/c3-w2.html",
    "title": "Constructing Features for Prediction",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/c3-w2.html#coarse-coding-video",
    "href": "posts/c3-w2.html#coarse-coding-video",
    "title": "Constructing Features for Prediction",
    "section": "Coarse Coding (Video)",
    "text": "Coarse Coding (Video)\nIn this video, Adam White introduces the concept of coarse coding, covering the first learning objective of this lesson.\nCoarse coding are a way to represent states in a more general way than tabular representations. This allows for generalization across states. The trade-off is that the representation is less discriminative."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l1g1",
    "href": "posts/c3-w2.html#sec-l1g1",
    "title": "Constructing Features for Prediction",
    "section": "The difference between coarse coding and tabular representations",
    "text": "The difference between coarse coding and tabular representations\n\n\n\n\napproximation\n\nRecall that linear function approximation are paramertized by a weight vector \\(\\mathbf{w}\\) and a feature vector \\(\\mathbf{x}(s)\\).\nAs we saw in the previous unit tabular representations associates one feature per state, this is called a one-hot encoding of the state space.\n\n\n\n\none hot coding\n\nWe associate one hot encoding with an indicator function \\(\\delta_{ij}(s)\\). This is a very discriminative representation but it does generalize.\n\n\n\n\nstate aggregation\n\nWe also discussed using state aggregation for the 1000 state random walk example. In state aggregation we break the continuous state space into discrete regions and associate a feature with each region. This is a more general representation than tabular representations but less discriminative.\n\n\n\n\ncoarse coding\n\nCoarse coding uses multiple overlapping shapes to represent states. This is a more General representation than state aggregation but less discriminative. Features are the circles they are in. If the circles overlap, we can have items that are in multiple circles. I.e. they are characterized by multiple features. In the example shown there can be from one to three active features.\nSo the difference is that tabular representations are one hot encodings while coarse coding uses membership in multiple overlapping shapes to represent states.\nHow does coarse coding relates to state aggregation?\nCoarse coding is also a generalization of state aggregation. In state aggregation we break the state space into discrete regions and associate a feature with each region. But we don’t let these regions overlap. In coarse coding we allow the regions to overlap which can give greater generalization as regions can share features.\nIn this video the term Reception Field is used to describe the region of the state space that a feature is associated with. This is an idea that comes from CNNs."
  },
  {
    "objectID": "posts/c3-w2.html#generalization-properties-of-coarse-coding-video",
    "href": "posts/c3-w2.html#generalization-properties-of-coarse-coding-video",
    "title": "Constructing Features for Prediction",
    "section": "Generalization Properties of Coarse Coding (Video)",
    "text": "Generalization Properties of Coarse Coding (Video)\nIn this video Martha White discusses the generalization properties of coarse coding.\nShe looks at using small overlapping 1-d intervals to represent a 1-d function.\nWe see that changing shape size and number of effects the generalization properties of the representation.\n\n\n\n\nscale\n\n\n\n\nshape\n\n\n\n\ndiscrimination\n\n\n\nNext we looked at using short interval vs longer intervals to approximate a 1-d function. We see that the longer intervals give a smoother approximation."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l1g2",
    "href": "posts/c3-w2.html#sec-l1g2",
    "title": "Constructing Features for Prediction",
    "section": "The trade-off between discrimination and generalization",
    "text": "The trade-off between discrimination and generalization"
  },
  {
    "objectID": "posts/c3-w2.html#tile-coding-video",
    "href": "posts/c3-w2.html#tile-coding-video",
    "title": "Constructing Features for Prediction",
    "section": "Tile Coding (Video)",
    "text": "Tile Coding (Video)\nIn this video, Martha White introduces the concept of tile coding. This is simply a implementation of coarse coding using multiple overlapping grids."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l1g4",
    "href": "posts/c3-w2.html#sec-l1g4",
    "title": "Constructing Features for Prediction",
    "section": "Explain how tile coding is a (computationally?) convenient case of coarse coding",
    "text": "Explain how tile coding is a (computationally?) convenient case of coarse coding\nTile coding is a computationally efficient implementation of coarse coding. We use multiple overlapping tilings to represent states. Each tiling is a grid of tiles. Each tile is a feature.\nIf we use one tiling we get state aggregation. If we use multiple tilings we get coarse coding. One tiling means we don’t discriminate between states that are in the same tile. Multiple tilings means we can discriminate between states that are in the same tile in one tiling but not in another."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l1g5",
    "href": "posts/c3-w2.html#sec-l1g5",
    "title": "Constructing Features for Prediction",
    "section": "Describe how designing the tilings affects the resultant representation",
    "text": "Describe how designing the tilings affects the resultant representation\nThe textbook goes into some more details about how we can generalize using tile coding - using regular tilings generates in a diagonal pattern. Using random tilings generates more spherical regions.\nHowever we also saw that the number size and shape of the tiles affects the generalization properties of the representation. And that increasing the overlap between the tiles an increase the discrimination properties of the representation."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l1g6",
    "href": "posts/c3-w2.html#sec-l1g6",
    "title": "Constructing Features for Prediction",
    "section": "Understand that tile coding is a computationally efficient implementation of coarse coding",
    "text": "Understand that tile coding is a computationally efficient implementation of coarse coding\nTile coding is a computationally efficient implementation of coarse coding. Since grids are uniform it is easy to compute which cells a state is in. A second reason is that end up with a sparse representations thus the dot product is just the sum of the weights of the active features for each state.\nOne caveat is that in high dimensional spaces we end up an exponential number of features. This is called the curse of dimensionality."
  },
  {
    "objectID": "posts/c3-w2.html#using-tile-coding-in-td-video",
    "href": "posts/c3-w2.html#using-tile-coding-in-td-video",
    "title": "Constructing Features for Prediction",
    "section": "Using Tile Coding in TD (Video)",
    "text": "Using Tile Coding in TD (Video)\nIn this video, Adam White shows how to use tile coding in TD learning. He goes back to the 1000 state random walk example and shows how to use tile coding to approximate the value function. We end up needing six tiles.\n\n\n\n\ntile coding v.s. state aggregation"
  },
  {
    "objectID": "posts/c3-w2.html#feature-construction-for-linear-methods",
    "href": "posts/c3-w2.html#feature-construction-for-linear-methods",
    "title": "Constructing Features for Prediction",
    "section": "Feature Construction for Linear Methods",
    "text": "Feature Construction for Linear Methods\nIn the textbook we see two forms of features for linear methods that are not covered in the videos.\nThe first are polynomials. We might use polynomials features for the state to represent the state space. This seems to be a good for problems where RL is dealing to a greater extent with interpolation or regression.\nThe following is given as an example of a polynomial feature representation of the state space. It took a bit of time to understand what was going on here.\nThey explain about the different combination of two features \\(s_1\\) and \\(s_2\\) doesn’t cover some edge cases but using four \\((1,s_1,s_2,s_1s_2)\\) covers all the possible combinations of the two features. We might also want to include higher powers of the atoms and that is what the polynomial representation is doing.\n\\[\nx_i(s) = \\prod_{j=1}^k s_j^{c_{ij}}\n\\]\nIt important to point out that we are not using the polynomials as a function approximation basis function. What we are talking about is a formulation of multinomial from a set of fixed numbers \\(s_1 \\lsots s_k\\) I.e. we are talking about all the possible products product from powers of these atoms.\nThe second are Fourier bases.\n\\[\nx_i(s) = \\cos\\left(\\frac{2\\pi s^T a_i}{b}\\right)\n\\]\nThe book mentions that the Fourier basis is particularly useful for periodic functions.\nThere are many other orthogonal bases used as functnio expansions that could be used, as features for linear function approximation.\n\nWalsh functions and Haar wavelets have discrete support and are used in signal processing.\nLegendre polynomials are used in physics.\nChebyshev polynomials are used in numerical analysis."
  },
  {
    "objectID": "posts/c3-w2.html#other-forms-of-coarse-coding",
    "href": "posts/c3-w2.html#other-forms-of-coarse-coding",
    "title": "Constructing Features for Prediction",
    "section": "Other Forms of Coarse Coding",
    "text": "Other Forms of Coarse Coding\nIn the textbook we see that there are other forms of coarse coding.\nFor example in section 9.5.5 we see using radial basis functions.\n\nAn RBF\n\nis a real-valued function whose value depends only on the distance between the input and a fixed point (called the center).  \n\n\nVisualizing - Imagine a hill or bump centered at a specific point. The height of the hill at any other point depends solely on its distance from the center. The hill gets flatter as you move away from the cente\n \\[\nx_i(s) = \\exp\\left(-\\frac{\\|s-c_i\\|^2}{2\\sigma_i^2}\\right)\n\\]\n\nWhere\n\\(c_i\\) is the center of the radial basis function and\n\\(\\sigma_i\\) is the width.\n\nThis is a form of coarse coding where the features are the distance from a set of centers. This is a more general representation than tile coding but less discriminative. The advantage of RBFs over tiles is that they are approximate functions that vary smoothly and are differentiable. However it appears there is both a computational cost and no real advantage in having continuous/differential features according to the book.\n\n\n\n\n\n\n\n\n\nFigure 1: One-dimensional radial basis functions with centers at -2, 0, and 2.\n\n\n\n\nI find this a bit disappointing as it seems like a nice intermediate step between linear function approximation with its convergence guarantees and neural networks which have no such guarantees."
  },
  {
    "objectID": "posts/c3-w2.html#what-is-a-neural-network-video",
    "href": "posts/c3-w2.html#what-is-a-neural-network-video",
    "title": "Constructing Features for Prediction",
    "section": "What is a Neural Network? (Video)",
    "text": "What is a Neural Network? (Video)\nIn this video, Martha White introduces the concept of a neural network. We look at a simple one layer feed forward neural network. Where the \\(output=f(sW)\\) is a non-linear function of the input."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l2g1",
    "href": "posts/c3-w2.html#sec-l2g1",
    "title": "Constructing Features for Prediction",
    "section": "Define a neural network",
    "text": "Define a neural network\n\nA Neural network consists of a network of nodes which process and pass on information.\n\nThe circles are the noes\nThe lines are the connections\nThe nodes are organized in layers\n\nData starts at the input layer. It is passed through the connections to the hidden layer. The hidden layer is preforms some computation on the data and passes it to the output layer. This process repeats until the last layer produces the output of the network."
  },
  {
    "objectID": "posts/c3-w2.html#deep-neural-networks-video",
    "href": "posts/c3-w2.html#deep-neural-networks-video",
    "title": "Constructing Features for Prediction",
    "section": "Deep Neural Networks (Video)",
    "text": "Deep Neural Networks (Video)\nIn this video, Martha White introduces the concept of neural networks with multiple hidden layers and activation functions."
  },
  {
    "objectID": "posts/c3-w2.html#neural-networks-mechanics",
    "href": "posts/c3-w2.html#neural-networks-mechanics",
    "title": "Constructing Features for Prediction",
    "section": "Neural Networks Mechanics",
    "text": "Neural Networks Mechanics\nA node in the network is a function\n\\[\noutput = f[(w_1 \\times input_1) + (w_2 \\times input_2) + \\ldots + (w_n \\times input_n) + b]\n\\]\n\nwhere:\n\n\\(w_i\\) are the weights,\n\\(input_i\\) are the inputs, and\n\\(b\\) is the bias.\n\\(f\\) is the activation function.\n\n\nThe sum of the product of the weights and inputs is a linear operation. The activation function \\(f\\) is where a non-linearity is introduced into the network."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l2g2",
    "href": "posts/c3-w2.html#sec-l2g2",
    "title": "Constructing Features for Prediction",
    "section": "Define activation functions",
    "text": "Define activation functions\nActivation functions are non-linear functions that are applied to the output of a node. They introduce non-linearity into the network.\n\n\n\n\ntanh activation\n\n\n\n\nrectified linear activation function\n\n\nMartha White also mentions threshold activation functions. However these are not used in practice as they are not differentiable. There is some work since this course came out on compressing neural networks to use threshold activation functions which are easy to compute on a CPU as matrix multiplication becomes a series of comparisons. However these are trained with a differentiable approximation of the threshold function and then quantized to the threshold function."
  },
  {
    "objectID": "posts/c3-w2.html#the-neural-network-implementation",
    "href": "posts/c3-w2.html#the-neural-network-implementation",
    "title": "Constructing Features for Prediction",
    "section": "The Neural Network Implementation",
    "text": "The Neural Network Implementation\n\n\n\n\nNeural Network Implementation\n\nA neural network is a parameterized function that is a composition of linear and non-linear functions. It is a function of the state. The linear functions are the weights and the non-linear functions are the activation functions. The weights are learned from data."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l2g3",
    "href": "posts/c3-w2.html#sec-l2g3",
    "title": "Constructing Features for Prediction",
    "section": "Define a feed-forward architecture",
    "text": "Define a feed-forward architecture\nA feed forward architecture is a neural network where the connections between nodes do not form a cycle. The data flows from the input layer to the output layer.\nAn example of a non-feed forward architecture is a recurrent neural network where the connections between nodes form cycles."
  },
  {
    "objectID": "posts/c3-w2.html#non-linear-approximation-with-neural-networks-video",
    "href": "posts/c3-w2.html#non-linear-approximation-with-neural-networks-video",
    "title": "Constructing Features for Prediction",
    "section": "Non-linear Approximation with Neural Networks (video)",
    "text": "Non-linear Approximation with Neural Networks (video)"
  },
  {
    "objectID": "posts/c3-w2.html#sec-l2g4",
    "href": "posts/c3-w2.html#sec-l2g4",
    "title": "Constructing Features for Prediction",
    "section": "How Neural Networks are doing feature construction",
    "text": "How Neural Networks are doing feature construction\n\n\n\n\n\nneural feature 1\n\n\ndarker means greater activation for the feature\n\n\n\n\nneural feature 2\n\n\nthe one generalize differently\n\nWe construct a non-linear function of the state using a neural network.\nrecall A node takes the form\n\\[\noutput = f[(w_1 \\times input_1) + \\ldots + (w_n \\times input_n) + b]\n\\]\nWe call this output of the node a feature! We can see that these features are a non-linear function of the inputs. We repeat this process until we evaluate all the nodes of the final layer. And the output of this final layer is called the representation.\nNote: This is not very different from tile coding where we pass input to a tile coder and get back a new representation of the state.\nIn both cases we are constructing a non-linear mapping of the input of the features. And we take a nonlinear function of the representation to form the output - a nonlinear approximation of the state.\nRecall that in tile coding we had to set some hyper-parameters: size shape of tiles + number of tiling. These are fixed before training. In a neural network we also have hyperparameters for the size of the layers, the number of layers, the activation functions. These too are fixed before training.\nThe difference is that Neural networks have weights that get updated during training. But tile coding does not change during training."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l2g5",
    "href": "posts/c3-w2.html#sec-l2g5",
    "title": "Constructing Features for Prediction",
    "section": "How neural networks are a non-linear function of state",
    "text": "How neural networks are a non-linear function of state\n\n\n\n\n\nneural feature 3\n\n\nthere are no hard boundaries\n\n this shows how it generelises\n\nNeural networks are non linear functions of the because of the non-linear nature of the activation functions. These are applied recursively as we move to the final layer."
  },
  {
    "objectID": "posts/c3-w2.html#deep-neural-networks-video-1",
    "href": "posts/c3-w2.html#deep-neural-networks-video-1",
    "title": "Constructing Features for Prediction",
    "section": "Deep Neural Networks (Video)",
    "text": "Deep Neural Networks (Video)"
  },
  {
    "objectID": "posts/c3-w2.html#sec-l2g6",
    "href": "posts/c3-w2.html#sec-l2g6",
    "title": "Constructing Features for Prediction",
    "section": "How deep networks are a composition of layers",
    "text": "How deep networks are a composition of layers\nNeural networks are modular. We can add or remove layers. Each layer is a function of the previous layer. The output of the previous layer is the input to the next layer.\nDepth allows composition of features. Each layer can learn a different representation of the input. The final layer can learn a representation of the input that is a composition of the representations learned by the previous layers\nWe can design the network to remove undesirable features. For example we can design a network with a bottleneck that has less features than the input. This forces the network to learn a compressed representation of the input."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l2g7",
    "href": "posts/c3-w2.html#sec-l2g7",
    "title": "Constructing Features for Prediction",
    "section": "The tradeoff between learning capacity and challenges presented by deeper networks",
    "text": "The tradeoff between learning capacity and challenges presented by deeper networks\nDepth can increase the learning capacity of the network by allowing the network to learn complex compositions and abstractions. However, deeper networks are harder to train."
  },
  {
    "objectID": "posts/c3-w2.html#gradient-descent-for-training-neural-networks-video",
    "href": "posts/c3-w2.html#gradient-descent-for-training-neural-networks-video",
    "title": "Constructing Features for Prediction",
    "section": "Gradient Descent for Training Neural Networks (Video)",
    "text": "Gradient Descent for Training Neural Networks (Video)\n\n\n\nIf we use the square error loss then\n\\[\nL(\\hat y_k,y_k) = (\\hat y_k-y_k)^2 \\qquad\n\\tag{1}\\]\n\\[\nA = A −αδ^As\n\\]\n\\[\nB = B −αδ^Bx\n\\]\nLet’s start at the output of the network and work backwards. Recall: \\[\nx = f_A(sA)\n\\]\n\\[\n\\hat{y} = f_B(xB)\n\\]\nWe start by taking the partial derivative of the loss function with respect to the first set of weights B.\nWe use the chain rule given the derivative of L with respect to \\(\\hat{Y} \\times \\frac{∂\\hat{y}}{∂B}\\). The next step is again to use the chain rule for this derivative.\n\\[\n\\frac{∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂\\hat{y}_k}{∂B_{jk}}\n\\]\nlet’s introduce a new variable, θ where θ is the output of the hidden layer times the last set of weights.\n\\[\nθ \\dot = xB\n\\]\nThus\n\\[\n\\hat y \\dot = f_B(θ)\n\\]\nRewriting we have:\n\\[\n\\frac{∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂f_B(\\theta_k)}{∂\\theta} \\frac{∂\\theta_k}{∂B_{jk}}\n\\]\nand since\n\\[\n\\frac{∂\\theta_k}{∂B_{jk}} = x_j\n\\]\n\\[\n\\frac{∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂f_B(\\theta_k)}{∂\\theta} x_j\n\\]\nnow that we calculated the gradient for the last layer we can move to the previous layer.\nwe use\n\\[\n\\Psi \\dot =  sA\n\\]\nand\n\\[\nx \\dot = f_A(\\Psi)\n\\]\n\\[\n\\begin{aligned}\n\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}} &= \\delta_k^B \\frac {∂\\theta_k}{∂A_{ij}} \\newline\n& = \\delta_k^B B_{jk} \\frac {∂x_j}{∂A_{ij}} \\newline\n  & = \\delta_k^B B_{jk} \\frac {∂f_A(\\Psi_j)}{∂\\Psi_j} \\frac {∂\\Psi_j}{∂A_{ij}}\n\\end{aligned}\n\\]\nsince\n\\[\n\\frac {∂\\Psi_j}{∂A_{ij}} = s_{ij}\n\\]\nwe have\n\\[\n\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}} = \\delta_k^B B_{jk} \\frac {∂f_A(\\Psi_j)}{∂\\Psi_j} s_{ij}\n\\]\nWe can clean up this derivative by again, defining a term \\(δ_A\\).\n\\[\nδ^A_j = (B_{jk}δ^B_k ) \\frac{∂f_A(ψ_j)}{∂ψ_j}\n\\]\nThe final result will be:\n\\[\n\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}}= δ^A_j s_i\n\\]\nObtaining as a final result for both gradients the next expressions\n\\[\n\\frac {∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = δ^B_k x_j\n\\]"
  },
  {
    "objectID": "posts/c3-w2.html#sec-l3g1",
    "href": "posts/c3-w2.html#sec-l3g1",
    "title": "Constructing Features for Prediction",
    "section": "Computing the Gradient for a Single Hidden Layer Neural Network",
    "text": "Computing the Gradient for a Single Hidden Layer Neural Network\nLet’s summerize the results:\n\\[\n\\frac {∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = δ^B_k x_j \\qquad\n\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}}= δ^A_j s_i\n\\]\nwhere:\n\\[\nδ^B_k = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂f_B(\\theta_k)}{∂\\theta} \\qquad\nδ^A_j = (B_{jk}δ^B_k ) \\frac{∂f_A(ψ_j)}{∂ψ_j}\n\\]"
  },
  {
    "objectID": "posts/c3-w2.html#sec-l3g2",
    "href": "posts/c3-w2.html#sec-l3g2",
    "title": "Constructing Features for Prediction",
    "section": "Computing the Gradient for Arbitrarily Deep Networks",
    "text": "Computing the Gradient for Arbitrarily Deep Networks\n\n\n\n\n\n\n\nFigure 2: Gradient Descent Pseudo-code\n\n\n\n\n\n\n\n\nFigure 3: Gradient Descent Pseudo-code for RELU\n\n\n\nNow that we have estimated the gradient for a hidden layer neural network. We can use it to learn to optimize the weights of the network by updating the weights to minimize the error in the loss function in the direction of the negative gradient.\nThe pseudocode in the figure outlines how to implementing the backprop algorithm with Stochastic gradient descent.\nFor each data point s, y in our dataset, we first get our prediction \\(\\hat{y}\\) from the network. This is the forward pass. Then we can estimate the loss using the actual value \\(y\\)\nNext we compute the gradients starting from the output. We first compute \\(δ^B\\) and the gradient for \\(B\\), then we use this gradient to update the parameters \\(B\\), with the step size \\(α_B\\) for the last layer.\nNext, we update the parameters \\(A\\). We compute \\(δ^A\\) which reuses \\(δ^B\\).\nNotice, that by computing the gradients of the end of the network first, we avoid recomputing the same terms for A, that were already computed for \\(δB\\). We then compute the gradient for A and update A with this gradient using step size \\(α_A\\).\n\nNext we look at how we adapt the pseudocode to work with the ReLU activation on the hidden layer and a linear unit for the output.\nFirst, we compute the error for the output layer, then we compute the derivative of the ReLU units with respect to \\(\\Psi\\), and finally, we use the aerial signal from the output layer along with you to compute the air signal for the hidden layer, the rest remains the same"
  },
  {
    "objectID": "posts/c3-w2.html#optimization-strategies-for-nns-video",
    "href": "posts/c3-w2.html#optimization-strategies-for-nns-video",
    "title": "Constructing Features for Prediction",
    "section": "Optimization Strategies for NNs (Video)",
    "text": "Optimization Strategies for NNs (Video)"
  },
  {
    "objectID": "posts/c3-w2.html#sec-l3g3",
    "href": "posts/c3-w2.html#sec-l3g3",
    "title": "Constructing Features for Prediction",
    "section": "The Importance of Initialization for Neural Networks",
    "text": "The Importance of Initialization for Neural Networks\nOne simple yet effective initialization strategy for the weights, is to randomly sample the initial weights from a normal distribution with small variance Fig. 42. This way, each neuron has a different output from other neurons within its layer. This provides a more diverse set of potential features.\n\n\n\n\nWeights initialization\n\nBy keeping the variants small, we ensure that the output of each neuron is within the same range as its neighbors. One downside to this strategy is that, as we add more inputs to a neuron, the variance of the output grows."
  },
  {
    "objectID": "posts/c3-w2.html#sec-l3g4",
    "href": "posts/c3-w2.html#sec-l3g4",
    "title": "Constructing Features for Prediction",
    "section": "Strategies for Initializing Neural Networks",
    "text": "Strategies for Initializing Neural Networks\n\\[\nW_{init} ~ N(0,1)\n\\]\n\\[\nW_{init} ~ \\frac{N(0,1)}{\\sqrt{n_{in}}}\n\\]"
  },
  {
    "objectID": "posts/c3-w2.html#sec-l3g5",
    "href": "posts/c3-w2.html#sec-l3g5",
    "title": "Constructing Features for Prediction",
    "section": "Optimization Techniques for Training Neural Networks",
    "text": "Optimization Techniques for Training Neural Networks\n\nmomentum update AKA heavy ball method \\[\nW_{t+1} ← W_t −α∇_wL(W_t) + λM_t\n\\]\n\n\\[\nM_{t+1} = λM_t −α∇_wL\n\\]\nvector step size adaptation\n\nseparate step size for each weight"
  },
  {
    "objectID": "posts/c2-w2.html",
    "href": "posts/c2-w2.html",
    "title": "Temporal Difference Learning Methods for Prediction",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/c2-w2.html#lesson-1-introduction-to-temporal-difference-learning",
    "href": "posts/c2-w2.html#lesson-1-introduction-to-temporal-difference-learning",
    "title": "Temporal Difference Learning Methods for Prediction",
    "section": "Lesson 1: Introduction to Temporal Difference Learning",
    "text": "Lesson 1: Introduction to Temporal Difference Learning\n\nLesson Learning Goals\n\nDefine temporal-difference learning #\nDefine the temporal-difference error #\nUnderstand the TD(0) algorithm #\n\n\n\nTemporal Difference Learning definition\nNow we turn to a new class of methods called Temporal Difference (TD) learning. According to Bass, TD learning is one of the key innovations in RL. TD learning is a method that combines the sampling of Monte Carlo methods with the bootstrapping of Dynamic Programming methods. The term temporal in the name references learning from two subsequent time steps and the term difference refers to using the difference between the values of each state.\nLet us now derive the TD update rule for the value function:\nRecall the definition of the value function from the previous course, is the expected return when starting in a particular state and following a particular policy.\nWe write this as:\n\\[\nv_\\pi(s_t) \\dot = \\mathbb{E}_\\pi[G_t | S_t = s] \\qquad\n\\tag{1}\\]\nWe can motivate td update rule by considering the DP and MC update rules.\nIn The MC update rule a sample update based on the return for the entire episode. Which means that we can only update our value function at the end of the episode.\n\\[\nV(S_t) \\leftarrow V(S_t) + \\alpha [\\underbrace{G_t}_{\\text{MC target}} -V(S_t)]  \\qquad\n\\tag{2}\\]\nwhere:\n\\(G_t\\) is the return at time step \\(t\\) and \\(S_t\\) is the state at time step \\(t\\).\nThis is the actual return at time t.\nIn the DP update rule, the target is a full backup based on the entire distribution of possible next states which we can solve for exactly since we have the full dynamics of the environment.\n\\[\nV(S_t) \\leftarrow V(S_t) + \\sum_a\\pi(a|S_t) ([r(s,a) + \\gamma p(s'\\mid s,a) V(s')]) \\qquad\n\\tag{3}\\]\nwhere:\n\n\\(R_{t+1}\\) is the reward at time step \\(t+1\\)\n\\(V(S_{t+1})\\) is the approximate value of the state at time step \\(t+1\\)\n\\(\\gamma\\) is the discount factor\n\\(\\alpha\\) is the learning rate\n\nHow can we make updates at each time step?\n\\[\n\\begin{aligned}\nG_t & \\dot= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\qquad \\newline\n         &= R_{t+1} + \\gamma G_{t+1} \\qquad\n\\end{aligned}\n\\tag{4}\\]\n\\[\n\\begin{aligned}\nv_\\pi(s_t)  & = \\mathbb{E}_\\pi [G_t \\mid S_t = s] & \\text{definition}\\newline\n          & = \\mathbb{E}_\\pi [R_{t+1} + \\gamma G_{t+1} \\mid S_t = s] & \\text{(subst. Recursive return)}\\newline\n          & = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi (S_{t+1}) |S_t=s]    & \\text{(subst. value function)}\\newline\n          & = \\mathbb{E}_\\pi[R_{t+1}|S_t=s]  + \\gamma \\mathbb{E}_\\pi[v_\\pi (S_{t+1}) |S_t=s] & \\text{(by linearity of Expectation)} \\newline\n          & = R_{t+1} + \\gamma v_\\pi (S_{t+1}) & \\text{(by Expectation of constant RV)}\n\\end{aligned}\n\\]\nIn this formula we have replaced \\(G_t\\) with \\(R_{t+1} + \\gamma V(S_{t+1})\\). We now have a recursive formula for the value functions in terms of the next value function. The next value function is a stand in for the value of the Return \\(G_{t+1}\\) which we don’t know.\nwe can use this formula to make an online update to our value function using an MC estimate of the return, without saving the full list of rewards.\nsince \\(G_t\\), the MC update target is the return for the entire episode, we can only update our value function at the end of the episode. We will soon look at TD learning, which allows us to make updates at each time step.\nthe motivation for TD learning is based on the MC update rule. The MC update rule is a sample update based on the return for the entire episode. This means that we can only update our value function at the end of the episode. We will soon look at TD learning, which allows us to make updates at each time step.\n\\[\nV_\\pi(S_t) \\leftarrow V(S_t) + \\alpha [\n\\underbrace{R_{t+1} + \\gamma V(S_{t+1})}_{\\text{TD target}} - V(S_t)]\n\\]\nWe can use the following to make online TD updates like we were able to update our value function without saving the full list of rewards. here:\n\n\\(V(S_t)\\) is the value of the state at time \\(t\\)\n\\(V(S_{t+1})\\) is the value of the state at time \\(t+1\\)\n\\(R_{t+1}\\) is the reward at time \\(t+1\\)\n\\(\\alpha\\) is a constant the learning rate\nthe target is an estimate of the return\n\n\\[\n\\begin{aligned}\nG_t & \\dot = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\newline\n&= R_{t+1} + \\gamma G_{t+1} \\newline\n\\end{aligned}\n\\]\n\nThe MC target is an estimate of the expected value (average of sampled return) - The DP target is an estimate because the true value of the state is not known - The TD target is an estimate for both reasons: a sample of the expected value of the reward, and the current estimate of the value of the state rather than the true value.\n\nLike with MC, the target is a sample updates based on a single observed transition. This is in stark contrast to DP, where the target is a full backup based on the entire distribution of possible next states which we can solve for exactly since we have the full dynamics of the environment.\n\n\nTemporal Difference Error\n\nThe TD error is the difference between the estimated value of a state and the value of the state at the next time step.\n\n\\[\n\\delta_t \\dot = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n\\] this is one of the most important equations in reinforcement learning - we will see it again and again.\n\n\nTD(0) Algorithm\n\nThe TD(0) algorithm is a TD learning algorithm that uses a bootstrapping method to estimate the value of a state.\n\n\n\n\n\n\n\nThe TD(0) algorithm for estimating \\(v_\\pi\\)\n\n\n\n#| label: alg-td-zero\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{TD(0) for estimating $v_\\pi$}\n\\begin{algorithmic}[1]\n\\State Input:\n  \\State $\\qquad \\pi \\leftarrow \\text{policy to be evaluated}$\n\\State Initialize:\n  \\State $\\qquad V(s) \\leftarrow x \\in \\mathbb{R} \\quad \\forall s \\in \\mathcal{S}$\n\\FORALL {episode $e$:}\n  \\State $S \\leftarrow \\text{initial state of episode}$\n  \\State $\\text{Choose } A \\sim \\pi(\\cdot \\mid S)$\n  \\FORALL {step $S \\in e$:}\n    \\State Take action $A$, observe $R, S'$\n    \\State $V(S) \\leftarrow V(S) + \\alpha [R + \\gamma V(S') - V(S)]$\n    \\State $S \\leftarrow S'$\n    \\State $\\text{Choose} A' \\sim \\pi(\\cdot \\mid S)$\n    \\State $S$ is terminal\n  \\ENDFOR\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}"
  },
  {
    "objectID": "posts/c2-w2.html#lesson-2-advantages-of-td",
    "href": "posts/c2-w2.html#lesson-2-advantages-of-td",
    "title": "Temporal Difference Learning Methods for Prediction",
    "section": "Lesson 2: Advantages of TD",
    "text": "Lesson 2: Advantages of TD\n\nLesson Learning Goals\n\nUnderstand the benefits of learning online with TD #\nIdentify key advantages of TD methods over Dynamic Programming and Monte Carlo methods #\nIdentify the empirical benefits of TD learning #"
  },
  {
    "objectID": "posts/c3-w4.html",
    "href": "posts/c3-w4.html",
    "title": "Policy Gradient",
    "section": "",
    "text": "RL logo\n\n\n\n\n\n\n\nRL algorithm decision tree\n\n\n\n\nFigure 1: The algorithms discussed in this lesson are all part of the policy gradient family. These allow us to consider both discrete and continuous actions in the the average rewards settings. We will consider Softmax Actor-Critic, Gaussian Actor-Critic, and the REINFORCE algorithm. The last is missing from the chart.\nSometimes, the behavior codified in the policy is much simpler than the action value function. Thus, learning the policy directly can be more efficient. Learning policies is an end-to-end solution for solving many real-world RL problems. Coding such end-to-end solutions may be done under the umbrella of policy gradient methods. Once we cover the policy gradient theorem, we will see how we still need to use action value approximations to estimate the gradient of the average reward objective. A second way that we will use value function approximations is in the actor-critic algorithms. Here, the policy is called the actor, and the value function is called the critic. The critic evaluates the policy, and the actor is used to update the policy. The actor-critic algorithms are a hybrid of policy gradient and value function methods. They are more stable than policy gradient methods and can be used in more complex environments.\nAlso, I was disappointed that this course does not cover more modern algorithms, such as TRPO, PPO, or other Deep learning algorithms. I cannot stress this point enough.\nIn the last video in the previous lecture’s notes by Satinder Singh, all of the research on using Meta gradients to learn intrinsic rewards is also built on top of policy gradient methods - where he and his students looked at propagating these gradients through multiple the planing algorithms and later through the learning algorithm to learn a reward function and tackle the issues of exploration."
  },
  {
    "objectID": "posts/c3-w4.html#learning-policies-directly-video",
    "href": "posts/c3-w4.html#learning-policies-directly-video",
    "title": "Policy Gradient",
    "section": "Learning policies directly (Video)",
    "text": "Learning policies directly (Video)\n\n\n\n\n\n\n\nEnergy pumping policy\n\n\n\n\nFigure 6: In the mountain car environment, the parameterized value function is complex, but the parameterized policy is simple.\n\n\nIn this lesson course instructor Adam White introduces the idea of learning policies directly. He contrasts this with learning value functions and explains why learning policies directly can be more flexible and powerful.\n\n\n\n\n\n\nRethinking policies\n\n\n\nMoving on we will need to think very clearly about policies.\nTo this end it is worth spending a minute to quickly recap the definition properties and notation of a policy from the previous lessons:\n\nIntuitively a policy \\(\\pi\\) is just decision making rule.\nA deterministic policy is just a function that maps a state to an action. \\[\n\\pi : s\\in \\mathcal{S} \\to a \\in \\mathcal{A} \\qquad \\text{(deterministic policy)}\n\\]\nA stochastic policy is a function that maps a state to a probability distribution over actions. Stochastic policies are more general and include deterministic policies as a special case. So while we may talk of deterministic policies, we will use the mathematical form of a stochastic policy.\n\n\\[\n\\pi : s\\in \\mathcal{S} \\to \\mathbb{P}(\\mathcal{A}) \\qquad \\text{(stochastic policy)}\n\\]\n\nFormally, the policy is defined probabilistically as follows:\n\n\\[\n\\pi(a \\mid s) \\doteq Pr(A_t = a \\mid S_t = s) \\qquad \\text{(policy)}\n\\tag{1}\\]\n\nnote that this is a shorthand for the following:\n\n\\[\n\\pi(a \\mid s) = \\mathbb{E}[A_t \\mid S_t = s] \\qquad \\text{(policy)}\n\\]\nWhere \\(\\pi\\) is a probability distribution over actions given a state."
  },
  {
    "objectID": "posts/c3-w4.html#sec-l1g1",
    "href": "posts/c3-w4.html#sec-l1g1",
    "title": "Policy Gradient",
    "section": "How to Parametrize a Policies?",
    "text": "How to Parametrize a Policies?\n\n\n\n\n\n\n\npolicy parametrization\n\n\n\n\nFigure 7: When we parametrize a policy we will use the greek letter \\(\\theta \\in \\mathbb{R}^d\\) to denote the parameters of the policy.\n\n\n\n\n\n\n\n\npolicy parametrization constraints\n\n\n\n\nFigure 8: Constraints on the policy parameters can be used to ensure that the policy is valid.\n\n\n\nSo far we have been mostly looking at learning value functions. But when it comes to function approximation, it is often simpler to learn a policy directly.\nIn the mountain car environment we see the power pumping policy which accelerates the car in the direction it is moving. This is a near optimal policy for this environment. The policy is simple and can be learned directly and it makes no use of value functions. This may not always be the case.\nA visual summary of the policy parametrization is shown in the figure. Recall that the policy is a function that takes in a state and outputs a probability distribution over actions. We will use the greek letter \\(\\theta \\in \\mathbb{R}^d\\) to denote the parameters of the policy. This way we can reference the parameters of \\(\\hat{Q}(s,a,w)\\) the action value function are denoted by \\(\\mathbf{w}\\).\nThe parametrized policy is defined as follows:\n\\[\n\\pi(a \\mid s, \\theta) \\doteq Pr(A_t = a \\mid S_t = s, \\theta) \\qquad \\text{(parametrized policy)}\n\\tag{2}\\]\nis a probability distribution over actions given a state and the policy parameters.\nSince we are dealing with probabilities, the policy parameters must satisfy certain constraints. For example, the probabilities must sum to one. This is shown in the figure. These policy parameters constraints will ensure that the policy is valid.\nPolicy Gradient use gradient ascent:\n\\[\n\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta J(\\theta) \\qquad \\text{(gradient ascent)}\n\\tag{3}\\]\nwhere \\(\\alpha\\) is the step size and \\(\\nabla_\\theta J(\\theta)\\) is the gradient of the objective function \\(J(\\theta)\\) with respect to the policy parameters \\(\\theta\\).\n\nmethods that follow this update rule are called policy gradient methods.\nmethods that learn both a value function and a policy are called actor-critic methods."
  },
  {
    "objectID": "posts/c3-w4.html#sec-l1g2",
    "href": "posts/c3-w4.html#sec-l1g2",
    "title": "Policy Gradient",
    "section": "Define one class of parameterized policies based on the softmax function",
    "text": "Define one class of parameterized policies based on the softmax function\n\n\n\n\n\n\n\nsoftmax properties\n\n\n\n\nFigure 9\n\n\nThe Softmax policy based on the Boltzmann distribution is a probability distribution over actions given a state. It is parameterized by a vector of action preferences \\(h(s, a, \\theta)\\).\n\\[\n\\pi(a \\mid s, \\theta) \\doteq \\frac{e^{h(s, a, \\theta)}}{\\sum_{b\\in \\mathcal{A}} e^{h(s, b, \\theta)}} \\text{(softmax policy)} \\qquad\n\\tag{4}\\]\n\nthe numerator is the exponential of the action preference\nthe denominator is the sum of the exponentials of all action preferences\n\nSome properties of the softmax policy are that it can take in a vector of weights for different actions and output a probability distribution over actions. A second property is that the softmax policy generalizes the max function. A third property is that unlike the max function which is discontinuous the softmax policy is differentiable, making it amenable to gradient-based optimization.\n\nnegative values of h lead to positive action probabilities.\nequal values of h lead to equal action probabilities.\nthe softmax policy is a better option over than the \\(\\epsilon\\)-greedy policy over the action-value based methods."
  },
  {
    "objectID": "posts/c3-w4.html#advantages-of-policy-parameterization-video",
    "href": "posts/c3-w4.html#advantages-of-policy-parameterization-video",
    "title": "Policy Gradient",
    "section": "Advantages of Policy Parameterization (Video)",
    "text": "Advantages of Policy Parameterization (Video)\nIn this video we consider the advantages of using parameterized policies over action-value based methods. We will see that parameterized policies are more flexible than action-value based methods and can start off stochastic and then become deterministic."
  },
  {
    "objectID": "posts/c3-w4.html#sec-l1g3",
    "href": "posts/c3-w4.html#sec-l1g3",
    "title": "Policy Gradient",
    "section": "Advantages of using parameterized policies over action-value based methods",
    "text": "Advantages of using parameterized policies over action-value based methods\n\n\n\n\n\n\n\nsoftmax policy v.s. epsilon-greedy\n\n\n\n\nFigure 10: Softmax policy v.s. \\(\\epsilon\\)-greedy\n\n\n\n\n\n\n\n\nShort corridor with switched action\n\n\n\n\nFigure 11: In the Short corridor with switched action environment a deterministic policy fails to reach the goal. The only optimal policy is stochastic.\n\n\n\n\nOne advantage of parameterizing policies according to the softmax in action preferences is that the approximate policy can approach a deterministic policy, whereas with \\(\\epsilon\\)-greedy action selection over action values there is always an \\(\\epsilon\\) probability of selecting a random action.\n\n\nA second advantage of parameterizing policies according to the softmax in action preferences is that it enables the selection of actions with arbitrary probabilities. In problems with significant function approximation, the best approximate policy may be stochastic.\n\nFor example, in card games with imperfect information the optimal play is often a mixed strategy which means you should take two different actions each with a specific probability, such as when bluffing in Poker.\nAction-value methods have no natural way of finding stochastic optimal policies, whereas policy approximating methods can, as shown in The short Corridor environment"
  },
  {
    "objectID": "posts/c3-w4.html#the-objective-for-learning-policies-video",
    "href": "posts/c3-w4.html#the-objective-for-learning-policies-video",
    "title": "Policy Gradient",
    "section": "The Objective for Learning Policies (Video)",
    "text": "The Objective for Learning Policies (Video)\nIn this video Martha White dive deep into the objective for policy gradient algorithms. She then contrasts this with the gradient of the value fn objectives and lists some challenges for estimating the gradient of the average reward objective. We will learn that we can use the policy gradient theorem and to overcome these challenges and derive a gradient with an neat update rule ."
  },
  {
    "objectID": "posts/c3-w4.html#sec-l2g1",
    "href": "posts/c3-w4.html#sec-l2g1",
    "title": "Policy Gradient",
    "section": "The objective for policy gradient algorithms",
    "text": "The objective for policy gradient algorithms\nFormalizing the goal as an Objective\n\\[\n\\begin{align*}\nG_t &= \\sum_{t=0}^{T}  R_{t}  \\quad && \\text{(episodic)} \\newline\nG_t &= \\sum_{t=0}^{\\infty} \\gamma^t R_{t}   \\quad && \\text{(continuing - discounted reward)} \\newline\nG_t &= \\sum_{t=0}^{\\infty} R_{t} - r(\\pi)  \\quad && \\text{(continuing - avg. reward)}\n\\end{align*}\n\\tag{5}\\]\nThe average reward Objective for a policy is as follows: \\[\nr(\\pi) = \\sum_{t=0}^{T} \\mu(s) \\sum_{a} \\pi(a \\mid s, \\theta)  \\sum_{s',r} p(s',r \\mid s,a) r \\quad \\text{(avg. reward objective)}\n\\tag{6}\\]\nWhat does this mean?\n\nthe last sum is the expected reward for a state-action pair. \\(\\mathbb{E}[R_t \\mid S_t = s , A_t=a]\\)\nthe last two sums together are the expected reward for a state under weighted by the policy \\(\\pi\\). \\(\\mathbb{E}_\\pi[R_t \\mid S_t = s]\\)\nfull sum ads the time we spend in state \\(s\\) under \\(\\pi\\) therefore the expected reward for a state under the policy \\(\\pi\\) and the environment dynamics \\(p\\). \\(\\mathbb{E}_\\pi[R_t]\\)\n\nto optimize the average reward, we need to estimate the gradient of the avg. objective\n\\[\n\\nabla_\\theta r(\\pi) = \\nabla_\\theta \\sum_{t=0}^{T} \\textcolor{red}{\\underbrace{\\mu(s)}_{\\text{Depends on }\\theta}} \\sum_{a} \\pi(a \\mid s, \\theta) \\sum_{s',r} p(s',r \\mid s,a) r \\qquad\n\\tag{7}\\]\n\nMethods based on this are called policy gradient methods.\nWe are trying to maximize the average reward.\n\nThere are a few challenges with using the gradient in the above equation:\nAccording to the lesson \\(\\mu(s)\\) depends on \\(\\theta\\). Martha White point out that this state importance though parameterized only by s actually depends on the the policy \\(\\pi\\) which will evolve during its training based on the values of \\(\\theta\\). Which means out notation here is a bit misleading. She then contrasts it with the value function gradient is being evaluated using a fixed policy.\n\\[\n\\begin{align*}\n\\nabla_w \\bar{VE} &= \\nabla_w \\sum_{s}\\textcolor{red}{\\underbrace{\\mu(s)}_{\\text{Independent of }\\mathbf{w}}}  [V_{\\pi}(s)-\\bar{v}(s,w)]^2 \\newline\n&=\\sum_{s} \\textcolor{red}{\\mu(s)} \\nabla_w [V_{\\pi}(s)-\\bar{v}(s,w)]^2\n\\end{align*} \\text{(value function gradient)} \\qquad\n\\tag{8}\\]\nWe can avg reward as an objective for policy optimization and the its for the stochastic gradient ascent. Next we will consider how the policy gradient theorem can help us estimate the gradient of the average reward objective despite these setbacks."
  },
  {
    "objectID": "posts/c3-w4.html#the-policy-gradient-theorem-video",
    "href": "posts/c3-w4.html#the-policy-gradient-theorem-video",
    "title": "Policy Gradient",
    "section": "The Policy Gradient Theorem (Video)",
    "text": "The Policy Gradient Theorem (Video)\n\n\n\n\n\n\n\nUnderstanding the pg theorem up\n\n\n\n\nFigure 13\n\n\n\n\n\n\n\n\nUnderstanding the pg theorem left\n\n\n\n\nFigure 14\n\n\n\n\n\n\n\n\nUnderstanding the pg theorem all\n\n\n\n\nFigure 15\n\n\n\n\nIn this video course instructor Martha White explains the policy gradient theorem, a key result for optimizing policy in reinforcement learning. The goal is to maximize the average reward by adjusting policy parameters \\(\\theta\\) using gradient ascent. The challenge is estimating the gradient of the average reward, which initially involves a complex expression with the gradient of the stationary distribution over states (\\(\\mu(s)\\))."
  },
  {
    "objectID": "posts/c3-w4.html#sec-l2g2",
    "href": "posts/c3-w4.html#sec-l2g2",
    "title": "Policy Gradient",
    "section": "The results of the policy gradient theorem",
    "text": "The results of the policy gradient theorem\nThe policy gradient theorem simplifies this by providing a new expression for the gradient. This expression involves summing, over all states and actions, the product of the gradient of the policy and the action-value function (Q). The gradient of the policy indicates how to adjust parameters to increase the probability of an action, while the action-value function represents the expected return from taking that action.\nThe video illustrates this with a grid world example, showing how gradients for different actions point in different directions. By weighting these gradients with the corresponding action values, the theorem provides a direction to update the policy parameters that increases the probability of high-value actions and decreases the probability of low-value actions.\nThe product rule \\[\n\\nabla(f(x)g(x)) = \\nabla f(x)g(x) + f(x)\\nabla g(x) \\qquad \\text{(product rule)}\n\\tag{9}\\]\ntherefore:\n\\[\n\\begin{align*}\n\\nabla_\\theta r(\\pi) &= \\sum_{t=0}^{T} \\nabla \\mu(s) \\sum_{a} \\pi(a \\mid s,\\theta) \\sum_{s',r} p(s',r \\mid s,a) r \\newline &+  \\sum_{t=0}^{T} \\mu(s) \\nabla \\sum_{a} \\pi(a \\mid s, \\theta) \\sum_{s',r} p(s',r \\mid s,a) r\n\\end{align*}\n\\tag{10}\\]\nThe first term is the gradient of the stationary distribution and the second term is the gradient of the policy. The policy gradient theorem simplifies this expression by eliminating the need to estimate the gradient of the stationary distribution.\n\\[\n\\begin{align*}\n\\nabla_\\theta r(\\pi) &= \\sum_{s\\in \\mathcal{S}} \\mu(s) \\textcolor{red}{ \\sum_{a\\in{\\mathcal{A}}} \\nabla \\pi(a \\mid s,\\theta)  q_\\pi(s,a) }\n\\end{align*}\n\\tag{11}\\]\nThe policy gradient theorem provides a new expression for the gradient of the average reward objective. This expression involves summing, over all states and actions, the product of the gradient of the policy and the action-value function (Q). The gradient of the policy indicates how to adjust parameters to increase the probability of an action, while the action-value function represents the expected return from taking that action.\nMartha White points out that this expression is much easier to estimate.\nNow let’s try to understand how we use the theorem to estimate the gradient.\nWhat we will use it to approximate the gradient. Computing the sum over states is impractical.\nWhat we will do do is take a stochastic samples. This involves updating the policy parameters based on the gradient observed at the current state.\nTo simplify the update rule, the concept of expectations is introduced. By re-expressing the gradient as an expectation under the stationary distribution of the policy, the update can be further simplified to involve only a single action sampled from the current policy.\nThe final update rule resembles other learning rules seen in the course, where the policy parameters are adjusted proportionally to a stochastic gradient of the objective. The magnitude of the step is controlled by a step-size parameter\nThe actual computation of the stochastic gradient requires two components: the gradient of the policy and an estimate of the action-value function"
  },
  {
    "objectID": "posts/c3-w4.html#the-policy-gradient-theorem",
    "href": "posts/c3-w4.html#the-policy-gradient-theorem",
    "title": "Policy Gradient",
    "section": "The policy gradient theorem",
    "text": "The policy gradient theorem\nWe need some preliminary results and definitions.\n\nThe four part dynamics function from the [@sutton2018reinforcement pp. 48] book:\nNext we need the result from Exercise 3.18 in [@sutton2018reinforcement pp. 62]\nNext we need the result from Exercise 3.19 in [@sutton2018reinforcement pp. 62]\n\n\\[\np(s', r \\mid s, a) \\doteq Pr\\{S_t=s', R_t=r \\mid S_{t-1} = s , A_{t-1}= a\\} \\qquad \\text{(S.B. 3.2)}\n\\tag{12}\\]\n\n\n\n\n\n\nExercise 3.18\n\n\n\n\n\n\nThe value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:\n\n\n\nGive the equation corresponding to this intuition and diagram for the value at the root node, \\(v_\\pi(s)\\), in terms of the value at the expected leaf node, \\(q_\\pi(s, a)\\), given \\(S_t = s\\). This equation should include an expectation conditioned on following the policy, \\(\\pi\\). Then give a second equation in which the expected value is written out explicitly in terms of \\(\\pi(a \\mid s)\\) such that no expected value notation appears in the equation.\n\n\n\n\n\n\n\n\n\n\nbackup diagram from v() to q()\n\n\n\n\nFigure 16: backup diagram from \\(v_\\pi(s)\\) to \\(q_\\pi(s,a)\\)\n\n\n\nSolution\n\\[\n\\begin{align}\nV_\\pi(s) &= \\mathbb{E_\\pi}[q(s_t,a)  \\mid s_t = s, a_t=a ] && \\text{(def. of Value)} \\newline\n&= \\sum_a Pr(a \\mid s) q_\\pi(s,a) && \\text{(def. of Expectation)} \\newline\n&= \\textcolor{red}{\\sum_a \\pi(a \\mid s)} \\textcolor{green}{q_\\pi(s,a)} && \\text {(def. of policy)}\n\\end{align}\n\\tag{13}\\]\n\n\n\n\n\n\nExercise 3.19\n\n\n\n\n\n\nThe value of an action, \\(q_\\pi(s, a)\\), depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state—action pair) and branching to the possible next states:\n\n\n\nGive the equation corresponding to this intuition and diagram for the action value, \\(q_\\pi(s, a)\\), in terms of the expected next reward, \\(R_{t+1}\\), and the expected next state value, \\(v_\\pi(S_{t+1})\\), given that \\(S_t = s\\) and \\(A_t = a\\). This equation should include an expectation but not one conditioned on following the policy. Then give a second equation, writing out the expected value explicitly in terms of \\(p(s_0, r \\mid s, a)\\) defined by eq 3.2, such that no expected value notation appears in the equation.\n\n\n\n\n\n\n\n\n\n\nbackup diagram from q() to v()\n\n\n\n\nFigure 17: backup diagram from \\(q_\\pi(s,a)\\) to \\(v_\\pi(s')\\)\n\n\n\n\nSolution\n\\[\n\\begin{align*}\nq_\\pi(s, a) &= \\mathbb{E}[R_{t+1} v_\\pi (s_{t+1}) \\mid s_t = s, a_t = a] \\newline\n&= \\textcolor{blue}{\\sum_{s', r} p(s', r \\mid s, a)} \\textcolor{pink}{[r + v_\\pi(s')]}\n\\end{align*} \\qquad\n\\] {#ex-319-solution}\n\\[\n\\begin{align*}\nq_\\pi(s, a) &= \\sum_{s'} \\mathbb{E}_\\pi[G_{t} \\mid S_{t+1}=s'] Pr\\{S_{t+1} = s' \\mid S_t = s, A_t = a\\}\n\\newline &= \\sum_{s'} \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s, A_t = a, S_{t+1} = s'] Pr\\{S_{t+1} = s' \\mid S_t = s, A_t = a\\}\n\\newline &= \\sum_{s',r} \\left( r + \\gamma \\underbrace{\\mathbb{E}[G_{t+1} \\mid S_{t+1} = s']}_{v_\\pi|(s')} \\right) p(s', r \\mid s, a)\n\\newline &= \\sum_{s',r} [ r + \\gamma v_\\pi(s')] p(s', r \\mid s, a)\n\\end{align*}\n\\]\nHere is my version of the proof:\n\\[\n\\begin{align*}\n\\textcolor{cyan}{\\nabla_\\theta V_\\pi(s)} &= \\nabla_\\theta \\sum_a \\textcolor{red}{\\pi(a \\mid s)} \\textcolor{green}{ q_\\pi(s,a) } && \\text{backup } v_\\pi \\to q_\\pi \\text{ (Ex 3.18)}\n\\newline &= \\sum_a \\nabla_\\theta \\pi(a \\mid s) q_\\pi(s,a) + \\pi(a \\mid s) \\nabla_\\theta q_\\pi(s,a) && \\text{product rule}\n\\newline &= \\sum_a \\nabla_\\theta \\pi(a \\mid s) q_\\pi(s,a) + \\pi(a \\mid s) \\nabla_\\theta \\sum_{s'} \\textcolor{blue}{P(s',r \\mid s, a)} \\textcolor{pink}{[r + V_\\pi(s')]} && \\text{backup } q_\\pi \\to v_\\pi \\text{ (Ex 3.19)}\n\\newline &= \\sum_a \\nabla_\\theta \\pi(a \\mid s) q_\\pi(s,a) + \\pi(a \\mid s) \\sum_{s'} P(s',r \\mid s, a) \\nabla_\\theta V_\\pi(s') && P, r \\text{ are const w.r.t. } \\theta \\newline\n=& \\sum_{a \\in \\mathcal{A}} \\Big( \\nabla_\\theta \\pi(a \\mid s)Q_\\pi(s, a) + \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s,a)  \\textcolor{cyan}{\\nabla_\\theta V_\\pi(s')} \\Big) && \\text{total rule of probability on r for P }\n\\newline & \\blacksquare && \\qquad\n\\end{align*}\n\\tag{14}\\]"
  },
  {
    "objectID": "posts/c3-w4.html#sec-l2g3",
    "href": "posts/c3-w4.html#sec-l2g3",
    "title": "Policy Gradient",
    "section": "The importance of the policy gradient theorem",
    "text": "The importance of the policy gradient theorem\nCrucially, the policy gradient theorem eliminates the need to estimate the gradient of the stationary distribution (\\(\\mu\\)), making the gradient much easier to estimate from experience. This sets the stage for building incremental policy gradient algorithms, which will be discussed in the next lecture."
  },
  {
    "objectID": "posts/c3-w4.html#sec-l3g1",
    "href": "posts/c3-w4.html#sec-l3g1",
    "title": "Policy Gradient",
    "section": "Derive a sample-based estimate for the gradient of the average reward objective",
    "text": "Derive a sample-based estimate for the gradient of the average reward objective\n\\[\n\\theta_{t+1} \\doteq \\theta_t + \\alpha \\frac{ \\nabla_ \\pi (a_t \\mid s_t, \\theta)}{\\pi (a_t \\mid s_t, \\theta)} q_\\pi(s_t, a_t) \\qquad \\text{()}\n\\]\n\\[\n\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta ln \\pi(a_t \\mid s_t, \\theta) q_\\pi(s_t, a_t) \\qquad \\text{()}\n\\tag{15}\\]\nwhere \\(\\alpha\\) is the step size and \\(\\nabla_\\theta J(\\theta)\\) is the gradient of the objective function \\(J(\\theta)\\) with respect to the policy parameters \\(\\theta\\).\n\\[\n\\nabla \\ln (f(x)) = \\frac{\\nabla f(x)}{f(x)} \\qquad \\text{(log derivative)}\n\\tag{16}\\]"
  },
  {
    "objectID": "posts/c3-w4.html#reinforce-algorithm-extra",
    "href": "posts/c3-w4.html#reinforce-algorithm-extra",
    "title": "Policy Gradient",
    "section": "Reinforce Algorithm (Extra)",
    "text": "Reinforce Algorithm (Extra)\nThe reinforce algorithm isn’t covered in the course. However, it is in the readings. Also the reinforce algorithm is said to be the most direct implementation of the policy gradient theorem. Finaly the reinforce algorithm is used in one of my research projects and this seems to be a great opportunity to understand it better.\nSo without further ado, let’s dive into the reinforce algorithm.\n\n\n\nReinforce Algorithm\n\n\nReinforce reveals the main issues with the policy gradient theorem. While the policy gradient theorem provides an unbiased estimate of the gradient of the average reward objective, it is a high variance estimator. This means that the gradient is very noisy and can lead to slow learning.\nOne wat to reduce the variance of the policy gradient theorem is to use a baseline. A baseline is a function that is subtracted from the reward to reduce the variance of the policy gradient theorem. Subtracting the baseline does not change the expected value of the gradient3, but it can reduce the variance of the gradient estimate.\n\n\n\nReinforce Algorithm\n\n\nthe change is in the last three lines. The baseline is subtracted from the return G and the gradient is scaled by the baseline."
  },
  {
    "objectID": "posts/c3-w4.html#sec-l3g2",
    "href": "posts/c3-w4.html#sec-l3g2",
    "title": "Policy Gradient",
    "section": "Describe the actor-critic algorithm for control with function approximation, for continuing tasks",
    "text": "Describe the actor-critic algorithm for control with function approximation, for continuing tasks"
  },
  {
    "objectID": "posts/c3-w4.html#actor-critic-with-softmax-policies-video",
    "href": "posts/c3-w4.html#actor-critic-with-softmax-policies-video",
    "title": "Policy Gradient",
    "section": "Actor-Critic with Softmax Policies (video)",
    "text": "Actor-Critic with Softmax Policies (video)\nAdam White discusses one specific implementation of the actor-critic reinforcement learning algorithm using a linear function approximation of the action value with tile coding and a Softmax policy parameterization.\nActor-critic methods combine direct policy optimization (actor) with value estimation (critic) using temporal difference learning.\nThe critic evaluates the policy by updating state value estimates, while the actor updates policy parameters based on feedback from the critic. This implementation is designed for finite action sets and continuous states. It employs a Softmax policy that maps state-dependent action preferences to probabilities, ensuring these probabilities are positive and sum to one. Each state effectively has its own Softmax distribution, and actions are sampled proportionally to these probabilities.\nBoth the value function and action preferences are parameterized linearly. The critic uses a feature vector representing the current state to estimate the value function. For the actor, the action preferences depend on both state and action, necessitating a state-action feature vector. The parameterization requires duplicating state feature vectors for each action, resulting in a policy parameter vector (θ) larger than the critic’s weight vector (W).\nThe algorithm’s update equations include:\nCritic Update: A straightforward semi-gradient TD update using the feature vector scaled by the temporal difference residual (TDR). Actor Update: A more complex gradient that involves two components: State-action features for the selected action. A sum over all actions of state-action features scaled by the policy probabilities."
  },
  {
    "objectID": "posts/c3-w4.html#sec-l4g1",
    "href": "posts/c3-w4.html#sec-l4g1",
    "title": "Policy Gradient",
    "section": "Derive the actor-critic update for a softmax policy with linear action preferences",
    "text": "Derive the actor-critic update for a softmax policy with linear action preferences\nThe critic’s update rule is:\n\\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + α^\\mathbf{w} \\delta \\nabla \\hat{v}(S,w)\n\\]\nwhich uses semigradient TD(0) to update the value function.\nThe actor uses the tf-error from the critic to update the policy parameters: \\[\nθ \\leftarrow θ + α^θ δ ∇ \\ln \\pi (A \\mid S,\\theta)\n\\]\npolicy update with a softmax policy is:\n\\[\n\\pi(a \\mid s, \\theta) \\doteq \\frac{e^{h(s, a, \\theta)}}{\\sum_{b\\in \\mathcal{A}} e^{h(s, b, \\theta)}}\n\\]\nthis is like having a different softmax for each state\n  \nFeature of the action preferences function\nfor the critic \\[\n\\hat{v}(s,w) \\doteq w^T x(s)\n\\]\nfor the actor\n\\[\nh(s,a,θ) \\doteq θ^T x_h(s,a)\n\\]\nwe can do this by stacking\nSo with the softmax policy the critic’s update is:\n\\[\nw  \\leftarrow w + α^w \\delta x(s)\n\\]\nand the actor’s update to the preferences looks as follows.\n\\[\n\\nabla \\ln \\pi(a \\mid s, \\theta) = x_h(s,a) - \\sum_b \\pi(b \\mid s, \\theta) x_h(s,b)\n\\]\nThe gradient has two parts.\nThe first is the state action features for the selected action xh(s,a).\nThe second part is the state action features multiplied by the policy summed over all actions ∑ b π(b|s,θ)xh(s,b)."
  },
  {
    "objectID": "posts/c3-w4.html#sec-l4g2",
    "href": "posts/c3-w4.html#sec-l4g2",
    "title": "Policy Gradient",
    "section": "Implement this algorithm",
    "text": "Implement this algorithm"
  },
  {
    "objectID": "posts/c3-w4.html#sec-l4g3",
    "href": "posts/c3-w4.html#sec-l4g3",
    "title": "Policy Gradient",
    "section": "Design concrete function approximators for an average reward actor-critic algorithm",
    "text": "Design concrete function approximators for an average reward actor-critic algorithm"
  },
  {
    "objectID": "posts/c3-w4.html#sec-l4g4",
    "href": "posts/c3-w4.html#sec-l4g4",
    "title": "Policy Gradient",
    "section": "Analyze the performance of an average reward agent",
    "text": "Analyze the performance of an average reward agent"
  },
  {
    "objectID": "posts/c3-w4.html#sec-l4g5",
    "href": "posts/c3-w4.html#sec-l4g5",
    "title": "Policy Gradient",
    "section": "Derive the actor-critic update for a gaussian policy",
    "text": "Derive the actor-critic update for a gaussian policy\n\n\n\nActor-Critic\n\n\n\n\n\nActor-Critic Continuing\n\n\n\n\n\nActor-Critic Episodic"
  },
  {
    "objectID": "posts/c3-w4.html#sec-l4g6",
    "href": "posts/c3-w4.html#sec-l4g6",
    "title": "Policy Gradient",
    "section": "Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions",
    "text": "Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions\n\n\n\n\n\n\nDiscussion prompt\n\n\n\n\nAre tasks really ever continuing? Everything eventually breaks or dies. It’s clear that individual people do not learn from death, but we don’t live forever. Why might the continuing problem formulation be a reasonable model for long-lived agents?\n\n\n\n\n\n\n\n\n\nChapter 13\n\n\n\n\nFigure 18: Chapter 13 of [@sutton2018reinforcement] covering policy gradient methods."
  },
  {
    "objectID": "posts/c3-w4.html#footnotes",
    "href": "posts/c3-w4.html#footnotes",
    "title": "Policy Gradient",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwhich models a single roll of a die based on its historical performance. It generalizes the Bernoulli and a special case of the Multinomial for a single trial↩︎\nA step that does not logically follow from the previous one↩︎\nthe bias↩︎"
  },
  {
    "objectID": "posts/c1-w3.html",
    "href": "posts/c1-w3.html",
    "title": "Value Functions & Bellman Equations",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms\nDecision theory is the branch of Mathematics dealing with the analysis of decisions by a single agent. Game theory is the branch of Mathematics dealing with the analysis of decisions by multiple agents. The introduction of a second agent makes the problem more complex and introduces the notion of strategic behavior. Decision theory is in many ways a simplification of game theory. In [@silver2015], Dave Silver responded to a question that a simple way of viewing MARL is that each agents are an independent decision maker.\nOnce the problem is formulated as an MDP, finding the optimal policy is more efficient when using value functions.\nThis week, we learn the definition of policies and value functions, as well as Bellman equations, which are the key technology behind all the algorithms we will learn.\nFor someone with a background in game theory, the concept of a policy \\(\\pi\\) is not new in game theory, we call this a strategy and it is a mapping from states to actions. i.e. an assignment of some action to each state representing the best action that an agent should take in that state.\nA second familiar concept is the value function. In game theory, we call this the payoff for an action. The payoffs are typically assigned to the terminal states of the game and can be backpropagated to non-terminal states using the laws of probability. Here we are interested in the expected value of the rewards that an agent can expect to receive when following a policy \\(\\pi\\) from a given state \\(s\\).\nI found the Policy and values functions somewhat families due to some background in game theory and markov processes.\nI found the Bellman equations more of a challenge. I think the main issue is the unfamiliarity with the notation which make the material look like gibberish. However, the more I made myself more familiar with the notation, I came to see that these equations express a rather simple idea.\nWe describe a MDP as a linear process in time. However, it is really a tree of possible actions. What the Bellman equations express is that if we want to estimate the value \\(v_\\pi(s)\\) of a state or more specifically the value of an action \\(q_\\pi(s,a)\\) what we do is consider the immediate rewards and then we have have a copy of pretty much the same tree. As we move forward in time we will end up making ever smaller (discounted) corrections to our best assessment."
  },
  {
    "objectID": "posts/c1-w3.html#sec-policy-definition",
    "href": "posts/c1-w3.html#sec-policy-definition",
    "title": "Value Functions & Bellman Equations",
    "section": "Policy Definition",
    "text": "Policy Definition\n\nA policy \\(\\pi\\) is a distribution over actions for each possible state.\nIt is denoted by \\(\\pi(a|s)\\), which is the probability of taking action \\(a\\) in state \\(s\\) under policy \\(\\pi\\)."
  },
  {
    "objectID": "posts/c1-w3.html#sec-stochastic-vs-deterministic",
    "href": "posts/c1-w3.html#sec-stochastic-vs-deterministic",
    "title": "Value Functions & Bellman Equations",
    "section": "Stochastic vs Deterministic Policies",
    "text": "Stochastic vs Deterministic Policies\n\nA policy can be deterministic or stochastic.\nA deterministic policy is a policy that selects a single action in each state.\n\nFor example, the greedy policy selects the action with the highest value\n\nA stochastic policy is a policy that selects actions with some probability that can be conditioned on the state.\n\nFor example the uniform policy selects each action with equal probability."
  },
  {
    "objectID": "posts/c1-w3.html#sec-value-functions",
    "href": "posts/c1-w3.html#sec-value-functions",
    "title": "Value Functions & Bellman Equations",
    "section": "Value Functions",
    "text": "Value Functions\n\nWe generally want to evaluate the value of each state or better yet the value of each action in each state before we create the policy. To do this we define two types of value functions:\n\n\nState-value functions \\(V_\\pi\\)\nThe state-value function \\(v_{\\pi}(s)\\) is the expected return when starting in state \\(s\\) and following policy \\(\\pi\\) thereafter.\n\\[\nv_\\pi(s) \\dot = \\mathbb{E_\\pi}[G_t|S_t = s] \\quad \\text{for policy} \\quad \\pi \\qquad\n\\tag{1}\\]\n\n\nAction-value functions \\(Q_\\pi\\)\nThe action-value function \\(q_{\\pi}(s,a)\\) is the expected return when starting in state \\(s\\), taking action \\(a\\), and following policy \\(\\pi\\) thereafter.\n\\[\nq_\\pi(s,a) \\dot = \\mathbb{E_\\pi}[G_t|S_t = s, A_t = a] \\quad \\text{for policy} \\quad \\pi \\qquad\n\\tag{2}\\]\n\n\nRelationship between Value Functions and Policies\nIn the short term, the value functions are more useful than the return G\n\nThe return G is not immediately available\nThe return G can be non-deterministic.\n\nThe value functions are deterministic and can be computed from the MDP."
  },
  {
    "objectID": "posts/c1-w3.html#lesson-2-bellman-equations",
    "href": "posts/c1-w3.html#lesson-2-bellman-equations",
    "title": "Value Functions & Bellman Equations",
    "section": "Lesson 2: Bellman Equations",
    "text": "Lesson 2: Bellman Equations\n\n\n\n\n\n\nGoals\n\n\n\n\nDerive the Bellman equation for state-value functions #\nDerive the Bellman equation for action-value functions #\nUnderstand how Bellman equations relate current and future values #\nUse the Bellman equations to compute value functions the state value function is \\(v(s)\\) #"
  },
  {
    "objectID": "posts/c1-w3.html#sec-bellman-equation-state-value-functions",
    "href": "posts/c1-w3.html#sec-bellman-equation-state-value-functions",
    "title": "Value Functions & Bellman Equations",
    "section": "Bellman Equation for State-Value Functions",
    "text": "Bellman Equation for State-Value Functions\n\n\n\n\n\n\n\nFigure 3: backup diagram for \\(v_\\pi\\)\n\n\n\n\n\n\n\n\nBellman Equation intuition\n\n\n\nRichard Bellman was a uniquely gifted mathematician who worked on dynamic programming. The Bellman equations is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state. These equations form the basis of Dynamic Programming which is used in disparate problems including\n\nSchedule optimization\nString algorithms (e.g. sequence alignment)\nGraph algorithms (e.g. the shortest path problem)\nGraphical algorithms (e.g. the Vitrebi algorithm)\nBioinformatics (e.g. lattice models)\n\nAlthough Bellman was one of the greatest problem solvers of the 20th century, he was not a great communicator. He was known for his terse and cryptic writing. The Bellman equations are a case in point. They are simple to understand once you get the hang of them but they are not easy to read for the first time. However the key to understanding the Bellman equations is to understand that they are a recursive equation based on some physical process\n\nThe trick that one learns over time, a basic part of mathematical methodology, is to sidestep the equation and focus instead on the structure of the underlying physical process – Richard Bellman\n\nin the case of RL the recursive physical process is: \\[\n  S \\rightarrow A \\rightarrow R.\n\\tag{3}\\]\nand we can diagram it using a backup diagram as shown in the Figure 3 above.\nThe name backup diagram comes from the idea that we are backing up the value of the state \\(v(s)\\) from the successor state \\(v(s')\\). I.e. we are going back up the tree of possible effects of some action \\(a\\) starting from the state \\(s\\).\nWhile the Bellman equation are difficult to read, remember and to derive, the backup diagram are very easy to sketch even if you don’t remember the equations. Once you have sketch the backup diagram, you should be able to easily derive the Bellman equations.\nThis same intuition can be used for working through all the above dynamic programming algorithms!\n\n\nThe Bellman equation for state-value functions is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state.\n\\[\n\\begin{align}\n  v_\\pi(s) &= \\mathbb{E_\\pi}[G_t|S_t=s] \\newline\n           &= \\mathbb{E_\\pi}[R_{t+1} + \\gamma G_{t+1}|S_t=s] \\newline\n           &= \\mathbb{E_\\pi}[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t=s] \\newline\n           &= \\sum_a \\pi(a|s) \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma \\mathbb{E_\\pi}[G_{t+1}|S_{t+1}=s']) \\newline\n           &= \\sum_a \\pi(a|s) \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma v_\\pi(s'))\n\\end{align}\n\\tag{4}\\]"
  },
  {
    "objectID": "posts/c1-w3.html#sec-bellman-equation-action-value-functions",
    "href": "posts/c1-w3.html#sec-bellman-equation-action-value-functions",
    "title": "Value Functions & Bellman Equations",
    "section": "Bellman Equation for Action-Value Functions",
    "text": "Bellman Equation for Action-Value Functions\n\n\n\n\nbackup diagram for q(s,a) function\n\nThe Bellman equation for action-value functions is a recursive equation that decomposes the value of a state-action pair into the immediate reward and the discounted value of the successor state-action pair.\n\\[\n\\begin{align}\n  q_\\pi(s,a) & \\dot = \\mathbb{E_\\pi}[G_t|S_t=s, A_t=a] \\newline\n             &= \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma \\mathbb{E_\\pi}[G_{t+1}|S_{t+1}=s']) \\newline\n             & = \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma \\sum_{a'} \\pi(a'|s') \\mathbb{E_\\pi}[G_{t+1}|S_{t+1}=s', A_{t+1}=a']) \\newline\n             &= \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s',a'))\n\\end{align}\n\\tag{5}\\]"
  },
  {
    "objectID": "posts/c1-w3.html#bellman-equations",
    "href": "posts/c1-w3.html#bellman-equations",
    "title": "Value Functions & Bellman Equations",
    "section": "Bellman Equations",
    "text": "Bellman Equations\nthe bellman equations capture the relationship between the current value and the future value. The Bellman equations are a set of equations that express the relationship between the value of a state and the value of its successor states. The Bellman equations are used to compute the value functions of a Markov Decision Process (MDP)."
  },
  {
    "objectID": "posts/c1-w3.html#example-gridworld",
    "href": "posts/c1-w3.html#example-gridworld",
    "title": "Value Functions & Bellman Equations",
    "section": "Example: Gridworld",
    "text": "Example: Gridworld\n\n\n\nA\nB\n\n\nC\nD\n\n\n\nIn the 2x2 gridworld example, the agent can move up, down, left, or right. The agent receives a reward of 0 for each step taken unless it gets to location B for which it gets +5. The agent receives will return to the current cell if it bumping into the wall.\nWe will use the uniform random policy where the agent selects each action with equal probability.\ngamma = 0.7\nlets calculate the value of each state using the Bellman equation.\n\\[\n\\begin{align}\nv_\\pi(A) &= \\sum_a \\pi(a|A) \\sum_{s'} \\sum_r p(s',r|A,a) (r + \\gamma v(s')) \\newline\n     &= \\sum_a \\pi(a|A) (r + 0.7 v_\\pi(s')) \\newline\n     &= 0.25 \\times 0.7 \\times v(A) + 0.25 \\times (5 + 0.7 \\times v(B)) + 0.25 \\times 0.7 \\times v(C) + 0.25 \\times 0.7 \\times v(A) \\newline\nv_\\pi(B) &= 0.25 \\times 0.7 \\times v(A) + 0.5 \\times (5 + 0.7 \\times v(B)) + 0.25 \\times 0.7 \\times v(D) \\newline\nv_\\pi(C) &= 0.25 \\times 0.7 \\times v(A) + 0.5 \\times (0.7 \\times v(B)) + 0.25 \\times 0.7 \\times v(C) \\newline\nv_\\pi(D) &= 0.25 \\times 0.7 \\times v(B) + 0.5 \\times 0.7 \\times v(C) + 0.25 \\times 0.7 \\times v(D)\n\\end{align}\n\\] we can solve these equations to get the value of each state.\ntheses are\n\\[\n\\begin{align*}\nv_\\pi(A) &= 4.2 \\newline\nv_\\pi(B) &= 6.1 \\newline\nv_\\pi(C) &= 2.2 \\newline\nv_\\pi(D) &= 4.2 \\newline\n\\end{align*}\n\\]\nWe can use the Bellman equation to calculate the value of each state in the Gridworld. The value of each state is the expected return when starting in that state and following the policy \\(\\pi\\) thereafter. The value of each state is calculated by summing the immediate reward and the discounted value of the successor states.\nFor larger MDP the Bellman equations are not practical method to calculate the value of each state. Instead, we will use algorithms based on the Bellman equations to estimate the value of each state.\n\nLesson 3: Optimality (Optimal Policies & Value Functions)\n\n\n\n\n\n\nGoals\n\n\n\n\nDefine an optimal policy #\nUnderstand how a policy can be at least as good as every other policy in every state. #\nIdentify an optimal policy for given MDPs.\nDerive the Bellman optimality equation for state-value functions\nDerive the Bellman optimality equation for action-value functions\nUnderstand how the Bellman optimality equations relate to the previously introduced Bellman equations\nUnderstand the connection between the optimal value function and optimal policies\nVerify the optimal value function for given MDPs"
  },
  {
    "objectID": "posts/c1-w3.html#sec-optimal-policy",
    "href": "posts/c1-w3.html#sec-optimal-policy",
    "title": "Value Functions & Bellman Equations",
    "section": "Optimal Policy",
    "text": "Optimal Policy\n\n\n\n\nBellman Optimality Equation\n\n\nA policy \\(pi_1\\) is better than a policy \\(\\pi_2\\) if \\(v_{\\pi_1}(s) \\geq v_{\\pi_2}(s)\\) for all states \\(s\\).\nGiven any two policies \\(\\pi\\) and \\(\\pi'\\), if we pick the action that maximizes the value function, from either at every state, we will get a new policy that is at least as good as both \\(\\pi\\) and \\(\\pi'\\).\nThere is always at least one deterministic optimal policy for any MDP.\nAn optimal policy \\(\\pi^*\\) is a policy that is at least as good as every other policy in every state.\nThe optimal policy is denoted by \\(\\pi^*\\) and is defined as:\n\n\\[\n\\pi^* \\dot = \\arg \\max_{\\pi} v_{\\pi}(s) \\quad \\forall s\\in S\n\\tag{6}\\]"
  },
  {
    "objectID": "posts/c1-w3.html#sec-bellman-optimality-state-value-function",
    "href": "posts/c1-w3.html#sec-bellman-optimality-state-value-function",
    "title": "Value Functions & Bellman Equations",
    "section": "Bellman Optimality Equation for State-Value Functions",
    "text": "Bellman Optimality Equation for State-Value Functions\n\nThe Bellman optimality equation for state-value functions is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state under the optimal policy.\n\\[\n\\begin{align}\nv_*(s) & \\dot = \\max_{\\pi} v_{\\pi}(s) \\quad \\forall s \\in S\\newline\n       & = \\max_a \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1})|S_t=s, A_t=a] \\newline\n       & = \\max_a \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma v_*(s'))\n\\end{align}\n\\tag{7}\\]"
  },
  {
    "objectID": "posts/c1-w3.html#sec-bellman-optimality-action-value-function",
    "href": "posts/c1-w3.html#sec-bellman-optimality-action-value-function",
    "title": "Value Functions & Bellman Equations",
    "section": "Bellman Optimality Equation for Action-Value Functions",
    "text": "Bellman Optimality Equation for Action-Value Functions\n\nThe Bellman optimality equation for action-value functions is a recursive equation that decomposes the value of a state-action pair into the immediate reward and the discounted value of the successor state-action pair under the optimal policy.\n\\[\n\\begin{align}\nq_*(s,a) & \\dot = \\max_{\\pi} q_{\\pi}(s,a) \\quad \\forall s \\in S, \\forall a \\in A \\newline\n         & = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} q_*(S_{t+1}, a')|S_t=s, A_t=a] \\newline\n         & = \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma \\max_{a'} q_*(s',a'))\n\\end{align}\n\\tag{8}\\]\nMartha White asks the question: “How can \\(\\Pi_3\\) have better strictly better values than both \\(\\Pi_1\\) and \\(\\Pi_2\\) in all states if all we did is take the best action in each state from either \\(\\Pi_1\\) or \\(\\Pi_2\\)?”\nThis is because if for example we found a fast path through a bottleneck for any state that is before the bottleneck will have a higher value in the other policies which may have had longer paths through the bottleneck."
  },
  {
    "objectID": "posts/c1-w3.html#sec-optimal-value-functions",
    "href": "posts/c1-w3.html#sec-optimal-value-functions",
    "title": "Value Functions & Bellman Equations",
    "section": "Optimal Value Functions",
    "text": "Optimal Value Functions\n\nThe optimal value function \\(v_*(s)\\) is the expected return when starting in state \\(s\\) and following the optimal policy thereafter.\nThe optimal action-value function \\(q_*(s,a)\\) is the expected return when starting in state \\(s\\), taking action \\(a\\), and following the optimal policy thereafter.\nAn optimal policy can be obtained from the optimal action-value function by selecting the action with the highest value."
  },
  {
    "objectID": "posts/c1-w4.html",
    "href": "posts/c1-w4.html",
    "title": "Dynamic Programming",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/c1-w4.html#sec-policy-evaluation-control",
    "href": "posts/c1-w4.html#sec-policy-evaluation-control",
    "title": "Dynamic Programming",
    "section": "Policy Evaluation and Control",
    "text": "Policy Evaluation and Control\nThe distinction between policy evaluation and control:\n\npolicy evaluation (prediction)\n\nis the task of evaluating the future, i.e. the value function given some specific policy \\(\\pi\\).\n\ncontrol\n\nis the task of finding the optimal policy, given some specific value function \\(v\\).\n\nplanning\n\nis the task of finding the optimal policy \\(\\pi_{\\star}\\) and value function \\(v\\), given a model of the environment. this is typically done by dynamic programming methods.\n\n\nTypically we need to solve the prediction problem before we can solve the control problem. This is because we need to know the value of the states under the policy to be able to pick the best actions."
  },
  {
    "objectID": "posts/c1-w4.html#sec-dynamic-programming",
    "href": "posts/c1-w4.html#sec-dynamic-programming",
    "title": "Dynamic Programming",
    "section": "Dynamic Programming",
    "text": "Dynamic Programming\n\nDynamic programming is a method for solving complex problems by breaking them down into simpler sub-problems.\nIt is a general approach to solving problems that can be formulated as a sequence of decisions.\nDynamic programming can be applied to problems that have the following properties:\n\nOptimal substructure: The optimal solution to a problem can be obtained by combining the optimal solutions to its sub-problems.\nOverlapping sub-problems: The same sub-problems are solved multiple times."
  },
  {
    "objectID": "posts/c1-w4.html#sec-iterative-policy-evaluation",
    "href": "posts/c1-w4.html#sec-iterative-policy-evaluation",
    "title": "Dynamic Programming",
    "section": "Iterative Policy Evaluation Algorithm",
    "text": "Iterative Policy Evaluation Algorithm\nContinuing with our goal of finding the optimal policy, we now turn to the an algorithms that will allow us to predict the value all the state starting with even the most naive policy.\nThe iterative policy evaluation algorithm is a simple iterative algorithm that estimates the value function for a given policy \\(\\pi\\).\nWe start with no knowledge of the value function or the policy. We set all the values to zero and we may even assume all actions are equally likely and all states are equally good. This is the uniform random policy. Alternatively we can start with some other policy.\nThese two assumptions are implemented in the initialization step of the algorithm.\nThe crux of the algorithm is the update step which is based on the recursive bellman equation for the value function under a policy \\(\\pi\\):\n\\[\nv_{\\pi}(s) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')] \\sum_{a} \\pi(a|s)\n\\] I rearranged the terms to make it clear that we are iterating over the states we use this equation to update the value of each state using\n\nthe four part dynamics function \\(p(s',r|s,a)\\) to get the probability of receiving a reward \\(r\\) at a successor state \\(s'\\) given the current state \\(s\\) and action \\(a\\).\nthe value of the next state \\(V(s')\\). which we initially assumed is 0 and may have already updated\nthe policy \\(\\pi(a|s)\\) which we use to weigh the previous term\n\nAl this will give us the expected value of the state under the policy \\(\\pi\\).\nThe final part of the algorithm is the stopping condition. We stop when the change in the value function is less than a small threshold \\(\\theta\\).\nThe algorithm is guaranteed to converge to the value function for the policy \\(\\pi\\).\nHere is the concise statement of the algorithm with just one array in pseudo code:\n#| label: alg-Iterative-Policy-Evaluation\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{Iterative Policy Evaluation, for estimating $V \\approx v_{\\pi}$}\n\\begin{algorithmic}[1]\n\\State Input: $\\pi$, the policy to be evaluated, default to uniform random policy\n\\State Algorithm parameter: a small threshold $\\theta &gt; 0$ determining accuracy of estimation\n\\State $V(s) \\leftarrow \\leftarrow \\vec 0 \\forall s \\in S$\n\\REPEAT\n  \\STATE $\\Delta \\leftarrow 0$\n  \\FORALL { $s\\in S$}\n    \\STATE $v \\leftarrow  V(s)$\n    \\STATE $V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]\\quad$ \\comment{ Bellman equation}\n    \\STATE $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$\n  \\ENDFOR\n\\UNTIL{$\\Delta &lt; \\theta$}\n\\State Output: $V \\approx v_{\\pi}$\n\\end{algorithmic}\n\\end{algorithm}\nnote: the algorithm makes a couple of assumptions that are omitted in the pseudo code.\n\nthat we have access to the dynamics function \\(p(s',r|s,a)\\)\nthat we have access to the reward function \\(r(s,a,s')\\)"
  },
  {
    "objectID": "posts/c1-w4.html#sec-applying-iterative-policy-evaluation",
    "href": "posts/c1-w4.html#sec-applying-iterative-policy-evaluation",
    "title": "Dynamic Programming",
    "section": "Applying Iterative Policy Evaluation",
    "text": "Applying Iterative Policy Evaluation\nThe iterative policy evaluation algorithm can be applied to compute the value function for a given policy \\(\\pi\\)."
  },
  {
    "objectID": "posts/c1-w4.html#sec-l2g5",
    "href": "posts/c1-w4.html#sec-l2g5",
    "title": "Dynamic Programming",
    "section": "Value Iteration",
    "text": "Value Iteration\nValue iteration is an important example of Generalized Policy Iteration. It is an iterative algorithm that computes the optimal value function and the optimal policy for a given MDP but it does not directly referrence a particular policy.\nIn value iteration, the algorithm starts with an initial estimate of the value function and iteratively runs a single step of greedy polict evaluation per step, using the greedy value to update the state-value function.\nupdates the value function until it converges to the optimal value function.\n#| label: alg-Value-Iteration\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{Value Iteration, for estimating $\\pi \\approx \\pi_{\\star}$}\n\\begin{algorithmic}[1]\n\\State Input: $\\pi$, the policy to be evaluated\n\\State Algorithm parameter: a small threshold $\\theta &gt; 0$ determining accuracy of estimation\n\\State Initialize $V(s) \\leftarrow \\vec{0} \\forall s \\in \\mathbb{R}$\n\n\\REPEAT\n  \\STATE $\\Delta \\leftarrow 0$\n  \\FORALL { $s\\in S$}\n    \\STATE $v \\leftarrow  V(s)$\n    \\STATE $V(s) \\leftarrow max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$\n    \\STATE $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$\n  \\ENDFOR\n\\UNTIL{$\\Delta &lt; \\theta$}\n\\State Output: $V \\approx v_{\\pi}$ such that\n\\State $\\pi(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$\n\n\\end{algorithmic}\n\\end{algorithm}\n\nThe Dance of Policy and Value\n\n\n\n\nDance of Policy and Value\n\nThe policy iteration algorithm is called the dance of policy and value because it alternates between policy evaluation and policy improvement. The policy evaluation step computes the value function for the current policy, and the policy improvement step constructs a new better greedyfied policy based on the value function.\nThis is also true for other generalized policy iteration algorithms, such as value iteration, which alternates between policy evaluation and policy."
  },
  {
    "objectID": "posts/c3-w1.1.html",
    "href": "posts/c3-w1.1.html",
    "title": "On-Policy Prediction with Approximation",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms\nSome of the notes I made in this course became a bit too long. Rather than break the flow of the lesson I decided to move them to a separate file. This is one of those notes."
  },
  {
    "objectID": "posts/c3-w1.1.html#how-are-generalization-in-ml-is-closely-related-to-transfer-learning-in-rl.",
    "href": "posts/c3-w1.1.html#how-are-generalization-in-ml-is-closely-related-to-transfer-learning-in-rl.",
    "title": "On-Policy Prediction with Approximation",
    "section": "How are generalization in ML is closely related to transfer learning in RL.",
    "text": "How are generalization in ML is closely related to transfer learning in RL.\nIn ML we have a rather clear understanding of generalization. We have a training set and a test set. We train on the training set and then test on the test set. The goal is to do well on the test set. The test set is a sample from the same distribution as the training.\nGeometrically, for classification we want to a decision boundary that separates the classes with the least error and with a fewest number of parameters. This is the essence of the bias-variance tradeoff.\nIn RL we tend to think of generalization as the ability of an agent to perform well on a task that is different from the one it was trained on. The algorithms can take decades of CPU compute to solve a simple video game. But change a few pixels in the game and the agent can’t play at all. This suggest these agents are severely overfitting.\nWould we be able to learn much faster if we could avoid overfitting i.e. if we could generalize better?\nOne point worth considering here is that solving a general problem is harder than a specific one.\nE.g. To solve a maze we need a policy matrix. To solve all mazes we need an algorithm.\nSo in one sense it is harder to generalize than to solve a specific problem. However it also should allow us to discard most of the irrelevant information that take up most of the model’s capacity and end up slowing down learning.\n\nAgents learn a policy that is only suitable to a specific task. The policy doesn’t generalize to even small changes in the task, e.g. moving the start and goal in the same maze tasks.\nLearned representation for features are not abstract and thus can’t be mapped to a slightly different task (e.g. changing a few pixels in a game)\nWe definitely can’t map the representation to different tasks.\nIdeally, we would like to deal with challenging problems by reusing knowledge from agents trained on other problems.\nOne direction called options lays in decomposing learning a policy for a goal into reframing it into learning sub-goals, strategies and tactics and basic moves.\nAnother direction I call heuristics concerns finding minimal policies that are just strong enough to get the agent to the goal a high percentage of the time.\nLearning should be aggregational and compositional. However, these terms require reinterpretation for each problem and at many levels of abstraction."
  },
  {
    "objectID": "posts/c3-w1.1.html#human-like-to-use-heuristic-which-are-are",
    "href": "posts/c3-w1.1.html#human-like-to-use-heuristic-which-are-are",
    "title": "On-Policy Prediction with Approximation",
    "section": "Human like to use Heuristic, which are are:",
    "text": "Human like to use Heuristic, which are are:\n- A minimal sub-optimal policy that is suffiecnt to get the agent to its goal with high probability.\n- In an MDP with lots of sub-goals, we may have benefit in learning learning heuristic style policy for each sub-goal and then compose them into a policy for the goal. \n- Composing heuristics is vague so let try make it clear.\n    - We want to follow the heuristic policy until we reach a sub-goal.\n    - We then switch to the policy for the next sub-goal. \n    - If we have well established entry and exit points for each heuristic we can have two benefits one is generalization and the other is discrimination.\n        - Generalization is due to using the same heuristic from different starting points.\n        - Discrimination is due to having different heuristics for different sub-goals.\n        - A third advantage is that the heuristic policy is for a smaller state space and can be learned faster.\n        - Third advantage is may be that of mapping different sub-problem to the same heuristic may allow us to discard some of the features of the state space that are not required for the heuristic to work.\n    - Thus composing heuristics in this case is just about switching between heuristics at the right time.\n    - Another direction is to use the heuristics as a form of  priors for the policy we want to learn.  \n    - Simple models are often a good fit for more problems than complex models.\n    - If we are good at learning to decompose problems into simpler sub problems and then we might be able to leverage the power of heuristics.\n\n-   Heuristics don't always work but overall they capture the essence of the solution to the problem.\n-   Heuristics are usually more general than an optimal policy.\n-   A heuristic might be a very good behavior policy for off policy learning the optimal policy.\n-   I don't see RL algorithms for heuristics."
  },
  {
    "objectID": "posts/c3-w1.1.html#models-in-rl-try-to-approximate-mdp-dynamics-using-its-transition-and-rewards",
    "href": "posts/c3-w1.1.html#models-in-rl-try-to-approximate-mdp-dynamics-using-its-transition-and-rewards",
    "title": "On-Policy Prediction with Approximation",
    "section": "Models in RL try to approximate MDP dynamics using its transition and rewards",
    "text": "Models in RL try to approximate MDP dynamics using its transition and rewards\n-   In ML we often use boosting and bagging to aggregate very simple models.\n-   In RL we often replace the model by sampling from a replay buffer of the agent's past experiences."
  },
  {
    "objectID": "posts/c3-w1.1.html#the-problem-for-a-general-ai-is-very-much-the-problem-of-transfer-learning-in-rl.",
    "href": "posts/c3-w1.1.html#the-problem-for-a-general-ai-is-very-much-the-problem-of-transfer-learning-in-rl.",
    "title": "On-Policy Prediction with Approximation",
    "section": "The problem for a general ai is very much the problem of transfer learning in RL.",
    "text": "The problem for a general ai is very much the problem of transfer learning in RL.\n\nagents learn a very specific policy for a very specific task - the learned representation cannot be mapped to other tasks or even other states in the same task.\nif agents learning was decomposed into\n\nlearning very general policies that solved more abstract problems and then\nlearning a good composition of these policies to solve the specific problem.\nonly after getting to this point would the agent try to optimize the policy for the specific task.\ne.g. chess\n\nlearn the basic moves and average value of pieces\nlearning tactics - short term goals\nlearning about end game\n\nupdate the value of pieces based on the ending\n\nlearning about strategy\n\npositional play\n\nlearn about pawn formations and weak square\n\nvalue of pawn formations\nhow they can be used with learned tactics.\n\nthe center\n\nadd value to pieces based on their position on the board\n\nopen files and diagonals\n\nlong term plans\n\nminority attack, king side attack, central breakthrough\ncreating a passed pawn\nexchanging to win in the end game\nsacrificing material to get a better position\nattacking the king\n\ncastling\npiece development and the center\ntempo\n\nlocalize value of pieces in different positions on the board using the learned tactics and strategy."
  },
  {
    "objectID": "posts/c3-w1.1.html#bayesian-models-and-hierarchical-model-encode-knowledge-using-priors-which-can-pool-or-bias-beliefs-towards-a-certain-outcome.",
    "href": "posts/c3-w1.1.html#bayesian-models-and-hierarchical-model-encode-knowledge-using-priors-which-can-pool-or-bias-beliefs-towards-a-certain-outcome.",
    "title": "On-Policy Prediction with Approximation",
    "section": "Bayesian models and hierarchical model encode knowledge using priors which can pool or bias beliefs towards a certain outcome.",
    "text": "Bayesian models and hierarchical model encode knowledge using priors which can pool or bias beliefs towards a certain outcome.\n-   learning in Bayesian models is about updating the initial beliefs based on incoming evidence."
  },
  {
    "objectID": "posts/c3-w1.1.html#ci-may-be-useful-here",
    "href": "posts/c3-w1.1.html#ci-may-be-useful-here",
    "title": "On-Policy Prediction with Approximation",
    "section": "CI may be useful here",
    "text": "CI may be useful here\n\nIs in a big way about mapping knowledge into\n\nStatistical joint probabilities,\nCasual concepts that are not in the joint distributions like interventions and Contrafactuals, latent, missing, mediators, confounders, etc.\nHypothesizing a causal structural model, deriving a statistical model and Testing it against the data.\nInterventions in the form of actions and options -\n\nMany key ideas in RL are counterfactual reasoning\n\nOff-policy learning is about learning from data generated by a different policy.\nOptions are like do operations (interventions)\nChoosing between actions and options is like contrafactual reasoning.\n\nUsing and verifying CI models could be the way to unify the spatial and temporal abstraction in RL."
  },
  {
    "objectID": "posts/c2-w1.html",
    "href": "posts/c2-w1.html",
    "title": "Monte-Carlo Methods for Prediction & Control",
    "section": "",
    "text": "TLDR 🥜🥜🥜\n\n\n\n\n\n\nIn this module we will embrace the paradigm of “learning from experience”.\nThis is called Sample based Reinforcement Learning and it we will let us relax some strong of the requirements of dynamic programming, namely knowing the table of MDP dynamics.\nWe will first use efficient Monte-Carlo ⚅🃁 methods for 🔮 prediction problem of estimating \\(v_\\pi(S)\\) value functions and action–value functions \\(q_\\pi(a)\\) from sampled episodes.\nWe will revise our algorithm to better handle exploration using exploring starts and \\(\\epsilon\\)–soft policies.\nWe will adapt GPI algorithms for use with Mote-Carlo to solve the 🎮 control problem of policy improvement.\nWith off policy learning learn a policy using samples from another policy, by corrected using importance sampling."
  },
  {
    "objectID": "posts/c2-w1.html#sec-l2g1",
    "href": "posts/c2-w1.html#sec-l2g1",
    "title": "Monte-Carlo Methods for Prediction & Control",
    "section": "MC Action-Value Functions",
    "text": "MC Action-Value Functions\n\n\n\n\naction values\n\n\n\n\nback off\n\n\nThis back off diagram indicates that the value of a state S depends on the values of its actions.\n\nRecall that control is simply improving a policy using our action values estimate.\nPolicy improvement is done by Greedyfying a policy \\(\\pi\\) at a state \\(s\\) by selecting the action \\(a\\) with the highest action value.\nIf we are missing some action values we can make the policy worse!\nWe need to ensure that our RL algorithm engages the different actions of a state. There are two strategies:\n\nExploring starts\n\\(\\epsilon\\)-Soft strategies\n\n\n\n\n\n\nexploring starts\n\nThe following is the MC alg with exploring start for estimation.\n\n\n\n\nexploring starts pseudocode\n\nLet’s recap how GPI looks:\n\nKeeping \\(\\pi_0\\) fixed we do evaluation of \\(q_\\pi\\) using MC–ES\nWe improve \\(\\pi_0\\) by picking the actions with the highest values\nWe stop when we don’t improve \\(\\pi\\)\n\nHere, in the evaluation step, we estimate the action-values using MC prediction, with exploration driven by exploring Starts or an \\(\\epsilon\\)-soft policy"
  },
  {
    "objectID": "posts/c2-w1.html#lesson-4-off-policy-learning-for-prediction",
    "href": "posts/c2-w1.html#lesson-4-off-policy-learning-for-prediction",
    "title": "Monte-Carlo Methods for Prediction & Control",
    "section": "Lesson 4: Off-policy learning for prediction",
    "text": "Lesson 4: Off-policy learning for prediction\n\n\n\n\n\n\nLesson Learning Goals\n\n\n\n\nUnderstand how off-policy learning can help deal with the exploration problem #\nProduce examples of target policies and examples of behavior policies. #\nUnderstand importance sampling #\nUse importance sampling to estimate the expected value of a target distribution using samples from a different distribution. #\nUnderstand how to use importance sampling to correct returns #\nUnderstand how to modify the Monte Carlo prediction algorithm for off-policy learning. #\n\n\n\n\nOff-policy learning\n\nOff-policy learning is a way to learn a policy \\(\\pi\\) using samples from another policy \\(\\pi'\\).\nThis is useful when we have a policy that is easier to sample from than the policy we want to learn.\nA key idea is to correct the returns using importance sampling.\n\nFor example suppose we can use a rule based model to generate samples of agent state, action and rewards - but we don’t really have an MDP, value function or policy. We could start with a uniform random policy and then use the samples to learn a better policy. However this would require us to interact with the environment and our agents may not be able to do this. In the case of Sugarscape model the agents are not really making decisions, they are following rules.\nIf we wished to develop agent that learn using RL with different rules on or off and other settings and use those to learn a policy using many samples. One advantage of the Sugarscape model is that it is highly heterogeneous so we get a rich set of samples to work with. A second advantage is that the rule based model can be fast to sample from and we can generate many samples by running it using hyper-parameters optimized test-bed.\nSo if we have lots of samples we may not need to explore as much initially, but rather learn to exploit the samples we have. Once we learn a near optimal policy for the samples we can use our agent to explore new vistas in our environment.\n\n\nTarget and behavior policies\n\nThe target policy is the policy we want to learn.\nThe behavior policy is the policy we sample from.\n\n\n\nImportance sampling\n\nImportance sampling is a technique to estimate the expected value of a target distribution using samples from a different distribution.\nWhy cant we just use the samples from the behavior policy to estimate the target policy?\nThe answer is that the samples from the behavior policy are biased towards the behavior policy.\nIn the target policy we may have states that are never visited by the behavior policy.\nFor example we might want to learn a policy that focuses on trade rather than combat or Vica-versa. This extreme idea of introducing/eliminating some action would significantly change behavioral trajectories. Sample based methods could be able to handle these changes - if we can restrict them to each subset of actions but clearly the expected return of states will be diverge in the long run.\nSo what we want is someway to correct the returns from the behavior policy to the target policy.\nIt is used to correct returns from the behavior policy to the target policy.\n\nThe probability of a trajectory under \\(\\pi\\) is:\n\\[\n\\begin{align*}\n  P(A_t, S_{t+1}, & A_{t+1}, ... ,S_T | S_t, A_{t:T-1} \\sim \\pi) \\newline\n  & = \\pi(A_t|S_t)p(S_{t+1}|S_t, A_t)\\pi(A_{t+1}, S_{t+1}) \\cdot\\cdot\\cdot p(S_T|S_{T-1}, A_{T-1}) \\newline\n  & = \\prod_{k=t}^{T-1} \\pi(A_k|S_k)p(S_{k+1}|S_k, A_k)\n\\end{align*}\n\\tag{2}\\]\n\n\nImportance sampling ratio\nDefinition: The importance sampling ratio (rho, \\(\\rho\\)) is the relative probability of the trajectory under the target vs behavior policy:\n\\[\n\\begin{align}\n\\rho_{t:T-1} & \\doteq \\frac{\\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k) \\cancel{ p(S_{k+1} \\mid S_k, A_k)}}{\\prod_{k=t}^{T-1} b(A_k \\mid S_k) \\cancel{ p(S_{k+1} \\mid S_k, A_k)} } \\newline\n             & = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k \\mid S_k)}{b(A_k \\mid S_k)}\n\\end{align}\n\\tag{3}\\]\n\\[\nv_\\pi(s) = \\mathbb{E}_b[\\rho_{t:T-1} \\cdot G_t \\mid S_t = s] \\qquad\n\\tag{4}\\]\n\\[\nV(s) \\doteq \\frac{\\displaystyle \\sum_{t\\in \\mathscr T(s)}\\rho_{t:T(t) - 1} \\cdot G_t}{|\\mathscr T (s)|} \\qquad\n\\tag{5}\\]\n\\[\nV(s) \\doteq \\frac{\\displaystyle \\sum_{t\\in \\mathscr T(s)} \\Big(\\rho_{t:T(t) - 1} \\cdot G_t\\Big)}{\\displaystyle \\sum_{t\\in \\mathscr T(s)}\\rho_{t:T(t) - 1}} \\qquad\n\\tag{6}\\]\n\n\n\n\nimportance sampling example\n\n\n\n\noff policy trajectories\n\n\n\n\n\n\n\n\nOff-policy every visit MC prediction\n\n\n\n#| label: alg-mc-off-policy-prediction\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{OffPolicyMonteCarloPrediction()} \n\\begin{algorithmic}[1]\n\\State Input:\n  \\State $\\qquad \\pi \\leftarrow \\text{policy to be evaluated}$\n\\State Initialize:\n  \\State $\\qquad V(s) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S}$\n  \\State $\\qquad Returns(s) \\text{ an empty list,} \\quad \\forall s \\in \\mathcal{S}$\n\\For {each episode:}\n  \\State Generate an episode by following $\\pi: S_0, A_0, R_1,\\ldots, S_{T-1}, A_{T-1}, R_T$\n  \\State $G \\leftarrow 0, W \\leftarrow 1$\n  \\For {each step of episode, $t \\in T-1, T-2,..., 0$:}\n    \\State $G \\leftarrow \\gamma WG + R_{t+1}$\n    \\State Append $G$ to $Returns(S_t)$\n    \\State $V(S_t) \\leftarrow average(Returns(S_t))$\n    \\State $W \\leftarrow W \\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}$\n  \\EndFor\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\n\n\nOff-policy every visit MC control\n\n\n\n#| label: alg-mc-off-control-prediction\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{OffPolicyMonteCarloPrediction()} \n\\begin{algorithmic}[1]\n\\State Input:\n  \\State $\\qquad \\pi \\leftarrow \\text{policy to be evaluated}$\n\\State Initialize:\n  \\State $\\qquad V(s) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S}$\n  \\State $\\qquad Returns(s) \\text{ an empty list,} \\quad \\forall s \\in \\mathcal{S}$\n\\For {each episode:}\n  \\State Generate an episode by following $b: S_0, A_0, R_1,\\ldots, S_{T-1}, A_{T-1}, R_T$\n  \\State $G \\leftarrow 0, W \\leftarrow 1$\n  \\For {each step of episode, $t = T-1, T-2,\\ldots, 0$:}\n    \\State $G \\leftarrow \\gamma WG + R_{t+1}$\n    \\State Append $G$ to $Returns(S_t)$\n    \\State $V(S_t) \\leftarrow average(Returns(S_t))$\n    \\State $W \\leftarrow W \\frac{\\pi(A_t  \\mid S_t)}{b(A_t \\mid S_t)}$\n  \\EndFor\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\nEmma Brunskill: Batch Reinforcement Learning\nThese guest talks have a dual purpose:\n\nto let the speakers share their passion for the field and introduce us to their research. this can be a good start for reading more about our own interests or for looking how to solve real problems that we are facing.\nto show us how the concepts we are learning are being used in the real world.\n\n\nEmma Brunskill is a professor at Stanford University.\nBurnskill motivated her approach with an edutainment app in which the goal is to maximize student engagement in game based on historical data.\nIn batch RL we have a fixed dataset of samples and we want to learn a policy from this data.\nThis is useful when we have a fixed dataset of samples and we want to learn a policy from this data.\nThe key idea is to use importance sampling to correct the returns from the behavior policy to the target policy. We learned that the challenge this poses is primarily due to the bias of the behavior policy.\nImportance sampling provides us with an unbiased estimate of the value function yet can have high variance. These may can be exponentially large in the number of steps. So these results in very poor estimates for the value function if there are many steps in the trajectory.\nBrunskill suggest that the real challenge posed by batch RL is a sparsity of trajectories with actions leading to optimal next states under the target policy in the historical data.8\nOne point we learned about this is that we should seek algorithms that are more data efficient. However\nA send idea is to use parametric models which are biased by can learn the transition dynamics and the reward function more efficiently.\nBrunskill points out that since we have few samples we may need a better approach to get robust estimates of the value function.\nThis approach which comes from statistic is called doubly robust stimators and has been used in bandits and RL\nShe presents a chart from a 2019 paper with a comparison of different methods for RL in the cart-pole environment.\n\nOff policy policy gradient with state Distribution Correction - dominates the other methods. And has a significantly narrower confidence interval for the value, if I understand the figure correctly.\n\nShe also presents results from many papers on Generalization Guarantees for RL, which show that we can learn a policy that is close to the optimal policy with a small number of samples from another policy. However I cannot make much sense of the result in the slide.\nAn example of this is the Sugarscape model where we have a fixed dataset of samples from the rule-based model.\nMore generally, we can use batch RL to learn from historical data how to make better decisions in the future.\n\n\nCounterfactual\n\nYou don’t know what your life would be like if you weren’t reading this right now.\n\n\n\nCausal reasoning based on counterfactuals is a key idea to tackling this problem.\n\n\nCounterfactual or Batch Reinforcement Learning\n\nIn batch RL we have a fixed dataset of samples and we want to learn a new policy from this data.\n\n\n\n\n\n\n\n\nDoubly Robust Estimators\n\n\n\nDoubly robust estimators is a technique from statistics that and causal inference that allows us to combine to do importance sampling and model based learning and a propensity score to estimate the value function. combine the best of both worlds - they are robust to errors in the model and the policy.\n\\[\n\\hat V_{DR} =\\frac{1}{N} \\sum_{i=1}^N \\left[ \\rho_i (R_i + \\gamma Q(s_{i+1}, \\pi(s_{i+1}))) - \\rho_i \\hat Q_{\\pi}(s_i, a_i) + \\hat Q_{\\pi}(s_i, a_i) \\right]\n\\] where:\n\n\\(\\rho_i\\) is the importance sampling ratio for the \\(i\\)-th sample\n\\(R_i\\) is the reward - \\(Q(s_{i+1}, \\pi(s_{i+1}))\\) is the value of the next state under the target policy\n\\(\\hat Q_{\\pi}(s_i, a_i)\\) is the model based Q-function estimate\n\\(Q(s_{i+1}, \\pi(s_{i+1}))\\) is the value of the next state under the target policy\n\n\n\n\nand paper\n\nProvably Good Batch Reinforcement Learning Without Great Exploration\nData-Efficient Off-Policy Policy Evaluation for Reinforcement Learning"
  },
  {
    "objectID": "posts/c2-w1.html#footnotes",
    "href": "posts/c2-w1.html#footnotes",
    "title": "Monte-Carlo Methods for Prediction & Control",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrediction in the sense that we want to predict for \\(\\pi\\) how well it will preforms i.e. its expected returns for a state↩︎\nworth either 1 or 11↩︎\nface card are worth 10↩︎\nthis is a big simplifying assumption↩︎\nin DP we had to solve \\(n\\times n\\) - simultaneous equations↩︎\nthink of a medical trial↩︎\nthink of a self driving car↩︎\n\nCan we learn form one or two examples by sampling ?\nwhat if the good actions are never sampled by our algorithm?\n\n↩︎"
  },
  {
    "objectID": "posts/c1-w0.html",
    "href": "posts/c1-w0.html",
    "title": "Course Introduction",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/c1-w0.html#a-long-term-plan-to-learning-deep-rl",
    "href": "posts/c1-w0.html#a-long-term-plan-to-learning-deep-rl",
    "title": "Course Introduction",
    "section": "A Long term plan to learning Deep RL",
    "text": "A Long term plan to learning Deep RL\n\ndo this specialization\nread the rest of the book - there are a number of important subjects and algorithms that are not covered in the course.\nsolve more problems\nread more papers\n\n\nthere are a zillion algorithms and a gazillion environments out there\nreading a few papers will get you up to speed on the state of the art.\n\n\nimplement more algorithms\n\n\nthis is what people are going to pay you for\nstart with gradient bandit algorithms\nthen Thompson Sampling using different distributions\n\n\ntry and solve a real problem:\n\n\nI created a custom RL algorithm to solve the Lewis signaling game quickly and efficient - this allows agents to learn to communicate.\nI am now working on designing a multi-agent version of the game that should learn even faster.\n\n\ndo the deep rl course from Hugging Face\nfind RL challenges on Kaggle"
  },
  {
    "objectID": "posts/c1-w0.html#footnotes",
    "href": "posts/c1-w0.html#footnotes",
    "title": "Course Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe annotated book and flashcards will help here. This material is really logical - if you are surprised/confused you never assimilated some part of the material. Once you do it should become almost intuitive to reason about from scratch.↩︎"
  },
  {
    "objectID": "t/post-with-code/index.html",
    "href": "t/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/c1-w2.html#guest-lecture-with-michael-littman-on-the-reward-hypothesis",
    "href": "posts/c1-w2.html#guest-lecture-with-michael-littman-on-the-reward-hypothesis",
    "title": "Markov Decision Processes",
    "section": "Guest Lecture with Michael Littman on The Reward Hypothesis",
    "text": "Guest Lecture with Michael Littman on The Reward Hypothesis\n\n\n\n\n\n\nMichael Littman\n\n\n\n\nHis website\n\n\nLittman is a professor at Brown University and a leading researcher in reinforcement learning. He is known for his work on the reward hypothesis and the exploration-exploitation trade-off. He motivates the reward hypothesis with a humorous take on the old adage:\n\nGive a man a fish and he’ll eat for a day - traditional programming\nTeach a man to fish and he’ll eat for a lifetime - supervised learning\nGive a man a need for fish and he’ll figure it out - reinforcement learning\n\nI felt that the guest lecture was a bit of a let down. I was expecting more from a leading researcher in the field. The reward hypothesis is a fundamental concept and the lecture seemed all over the place. It raised many questions but didn’t answer them.\nIf we accept the hypothesis, then there are two areas need to be addressed:\n\nWhat rewards should agents optimize?\nDesigning algorithms to maximize them.\n\nSome rewards are easy to define, like winning a game, but others are more complex, like driving a car. His example was of air conditioning in a car, where the reward is not just the temperature but also the comfort of the passengers. Running the air conditioning has a cost in terms of fuel, but the comfort of the passengers is much harder to quantify, particularly since each passenger may have different preferences.\nNext he covered the two main approaches to setting up rewards in RL:\n\nRewards can be expressed as a final goal, or no goal yet:\n\nThe goal based representation: Goal achieved = +1, and everything else is 0. This has a downside of not signaling, to the agent, the urgency of getting to the goal.\nThe action-penalty representation: a -1 could be awarded every step that the goal is not yet achieved. This can cause problems if there is a small probability of getting stuck and never reaching the goal.\nIt seems that there are many ways to set up rewards. If we take a lesson from game theory, we can see that the value of rewards might be important or the relative value or order of rewards might be important. In rl values are often more important than the ‘order’ of rewards. However it might be interesting to consider if we can encode preferences into the rewards and if this formulation would still make sense in the context of the reward hypothesis and the bellman equations.\n\n\nLittleman then asked “Where do rewards come from?” and answered that they can come from - Programming - Human feedback - Examples - Mimic the rewards a human would give - Inverse reinforcement learning - learn the reward function from examples - Optimization - Evolutionary optimization like population dynamics. - The reward is the objective function - The reward is the gradient of the objective function\nNext he discusses some challenges to the reward hypothesis:\n\nTarget is something other than cumulative reward:\n\ncannot capture risk averse behavior\ncannot capture diversity of behavior\n\nis it a good match for a high level human behavior?\n\nsingle minded pursuit of a goal isn’t characteristic of good people.\nThe goals we “should” be pursuing may not be immediately evident to us - we might need time to understand making good decisions.\n\n\nThe big elephant in the room is that we can reject the reward hypothesis and have agents that peruse multiple goals. The main challenge is that it becomes harder to decide when there is a conflict between goals. However, the field of multi-objective optimization has been around for a long time and there are many ways to deal with this problem. Some are more similar to the reward hypothesis but others can lead to more complex behavior based on preferences and pareto optimality.\n\nLesson 3: Continuing Tasks\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nFormulate returns for continuing tasks using discounting. #\nDescribe how returns at successive time steps are related to each other. #\nUnderstand when to formalize a task as episodic or continuing. #\n\n\n\n\nReturns for Continuing Tasks\n\nIn continuing tasks, the agent interacts with the environment indefinitely.\nThe return at time \\(t\\) is the sum of the rewards from time \\(t\\) to the end of the episode.\nThe return can be formulated using discounting, where the rewards are discounted by a factor \\(\\gamma\\).\n\n\\[\n\\begin{align}\nG_t&=R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\newline\n&= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\end{align}\n\\tag{11}\\]\n\nreturns have a recursive structure, where the return at time \\(t\\) is related to the return at time \\(t+1\\) by the discount factor \\(\\gamma\\).\n\n\\[  \n\\begin{align}\nG_t &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\newline\n&= R_{t+1} + \\gamma ( R_{t+2} + \\gamma R_{t+3} + \\ldots ) \\newline\n&= R_{t+1} + \\gamma G_{t+1}\n\\end{align}\n\\tag{12}\\]\nthis form of the return is called the recursive form of the return and is usefull in developing algorithms for reinforcement learning.\n\n\nReturns at Successive Time Steps\n\nThe return at time \\(t\\) is related to the return at time \\(t+1\\) by the discount factor \\(\\gamma\\).\nThe return at time \\(t\\) is the sum of the reward at time \\(t\\) and the discounted return at time $t+1.\n\n\n\nEpisodic vs. Continuing Tasks\n\nAn episodic task has a well-defined terminal state, and the episode ends when the terminal state is reached.\nA continuing task does not have a terminal state, and the agent interacts with the environment indefinitely.\nTo avoid infinite returns in continuing tasks, we use discounting to ensure that the return is finite.\nThe discount factor \\(\\gamma\\in(0,1)\\) is the present value of future rewards.\n\n@sutton2018reinforcement emphasizes that we can use the discount factor of [0,1] to unify both episodic and continuing tasks Here = 0 corresponds to myopic view of optimizing immediate rewards like in the k-armed bandit problem. The discount factor = 1 corresponds to the long-term view of optimizing undiscounted expected cumulative reward. into a single framework. This is a powerful idea that allows us to use the same algorithms for both types of tasks.\nI think it is a good place to consider a couple of ideas raised by Littman in the guest lecture:\nThe first is hyperbolic discounting and the second risk aversion.\nBehavioral economics has considered a notion of hyperbolic discounting where the discount factor is not constant but changes over time. This is a more realistic model of human behavior but is harder to work with mathematically. This idea is not covered in the course perhaps because it is a departure from the more rational exponential discounting model which we use.\ntwo forms of hyperbolic discounting are:\n\\[\nG(D) = \\frac{1}{1 + \\gamma D}\n\\]\n\\[\nφh(τ) = (1+ ατ)−γ/α\n\\tag{13}\\] where:\n\n\\(φh(τ)\\) is the hyperbolic discount factor at time \\(τ\\)\n\\(α\\) is the rate of discounting\n\\(γ\\) is the delay parameter\n\nthere is also quasi-hyperbolic discounting which is a combination of exponential and hyperbolic discounting.\n\\[\nG(D) = \\begin{cases}\n  1 & \\text{if } t = 0  \\newline\n  \\beta^k \\delta^{D} & \\text{if } t &gt; 0\n  \\end{cases}\n\\]\nThe notation for both terminal and non-terminal states is \\(S^+\\)\nExercise 2.10 proof of Equation 3.10\n\\[\n\\begin{align*}\nG_t &= \\sum_{k=0}^\\infty \\gamma^k = lim_{n \\rightarrow \\infty} (1 + \\gamma + \\gamma^2 + ... + \\gamma^n) \\newline\n&= lim_{n \\rightarrow \\infty} \\frac{(1 + \\gamma + \\gamma^2 + ... + \\gamma^n) (1 - \\gamma)}{(1 - \\gamma)} \\newline\n&= lim_{n \\rightarrow \\infty} \\frac{1 - \\gamma^{n+1}}{1 - \\gamma} \\newline\n&= \\frac{1}{1 - \\gamma}\n\\end{align*}\n\\]"
  }
]