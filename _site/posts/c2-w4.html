<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="dcterms.date" content="2024-03-04">
<meta name="description" content="In these module we define cover model based RL sampling. We start with the Dyna architecture. Then we consider tabular Q-planning algorithm, the Tabular Dyna-Q and Dyna-Q+ algorithms">

<title>Sample-based Learning Methods â€“ rl-notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-d58cf0c17c84c672bbe6415fb2b2bd7c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background: /images/banner_black_3.jpg;
      }
</style>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">rl-notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Sample-based Learning Methods</h1>
            <p class="subtitle lead">Planning, Learning &amp; Acting</p>
                  <div>
        <div class="description">
          In these module we define cover model based RL sampling. We start with the Dyna architecture. Then we consider tabular Q-planning algorithm, the Tabular Dyna-Q and Dyna-Q+ algorithms
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Coursera</div>
                <div class="quarto-category">Notes</div>
                <div class="quarto-category">RL</div>
                <div class="quarto-category">Reinforcement learning</div>
                <div class="quarto-category">Planning</div>
                <div class="quarto-category">Tabular Q-planning</div>
                <div class="quarto-category">Dyna architecture</div>
                <div class="quarto-category">Tabular Dyna-Q algorithm</div>
                <div class="quarto-category">Dyna-Q+ algorithm</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 4, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="lesson-1-what-is-a-model" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-1-what-is-a-model">Lesson 1: What is a model?</h2>
<section id="lesson-learning-goals" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Describe what a model is and how they can be used <a href="#sec-l1g1">#</a></label></li>
<li><label><input type="checkbox">Classify models as <strong>distribution models</strong> or <strong>sample models</strong> <a href="#sec-l1g2">#</a></label></li>
<li><label><input type="checkbox">Identify when to use a distribution model or sample model <a href="#sec-l1g3">#</a></label></li>
<li><label><input type="checkbox">Describe the advantages and disadvantages of sample models and distribution models <a href="#sec-l1g4">#</a></label></li>
<li><label><input type="checkbox">Explain why sample models can be represented more compactly than distribution models <a href="#sec-l1g5">#</a></label></li>
</ul>
</section>
<section id="sec-l1g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g1">What is a model and how can it be used?</h3>
<ul>
<li><p>A model is a simplified representation of the environment dynamics</p></li>
<li><p>Models can be used to simulate the environment</p></li>
<li><p>In this course a Model is a function that predicts the next state and reward given the current state and action</p></li>
<li><p>a transition model <span class="math inline">\(s_{t_+1} = f(s_t, a_t)\)</span> predicts the next state given the current state and action</p></li>
<li><p>a reward model <span class="math inline">\(r_{t_+1} = f(s_t, a_t)\)</span> predicts the reward given the current state and action</p></li>
</ul>
<p>three other model types are mentioned in the <a href="https://sites.google.com/view/mbrl-tutorial">ICML Tutorial on Model-Based Reinforcement Learning</a></p>
<ul>
<li><strong>Inverse models</strong> predict the action given the current state and next state <span class="math inline">\(a_{t+1} = f_s^{-1}(s_t, s_{t+1})\)</span></li>
<li><strong>Distances models</strong> predict the distance between the current state and the goal state <span class="math inline">\(d_{ij} =d(s, s')\)</span></li>
<li><strong>Future return models</strong> predict the future return given the current state and action <span class="math inline">\(G_t=Q(s_t, a_t)\)</span> or <span class="math inline">\(G_t=V(s_t)\)</span></li>
</ul>
<p>Why do we want to use models?</p>
<ul>
<li>model allow us to simulate the environment without interacting with it.</li>
<li>this can increase sample efficiency - e.g.&nbsp;by replaying past experiences to propergate learning from goal to all predecessor states we have visited</li>
<li>this can reduce risks - e.g.&nbsp;by simulating dangerous situations instead of actually experiencing them.</li>
<li>this can reduce costs - e.g.&nbsp;by simulating costly actions in a simulated environment instead of paying the cost in the real environment.</li>
<li>this could be much faster than real-time interaction with the environment. Often in robotics simulation is orders of magnitude faster than real-time interaction.</li>
</ul>
</section>
<section id="sec-l1g2" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l1g2">Types of models</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/rl-models.png" class="img-fluid figure-img"></p>
<figcaption>models</figcaption>
</figure>
</div></div><ul>
<li><strong>Distribution models</strong> predict the probability distribution of the next state and reward</li>
<li><strong>Sample models</strong> predict a single next state and reward</li>
</ul>
<p>Also there are:Environment simulator</p>
<ul>
<li>Chess Programs typicaly can simulate all possible movers and evaluate the board position (tacticaly and strategically). The difference between the current board position and the board position after a move is the reward.</li>
<li><span class="citation" data-cites="Silver2016MasteringTG">[@Silver2016MasteringTG]</span> mentions an <strong>Environment simulator</strong> for the game of go</li>
<li><span class="citation" data-cites="Agostinelli2019SolvingTR">[@Agostinelli2019SolvingTR]</span> used a simulator of the rubikâ€™s cube to train a reinforcement learning agent to solve the cube.</li>
<li><span class="citation" data-cites="Bellemare2012TheAL">[@Bellemare2012TheAL]</span> used a simulator of the game of atari to train a reinforcement learning agent to play atari games.</li>
<li><span class="citation" data-cites="Todorov2012MuJoCoAP">[@Todorov2012MuJoCoAP]</span> used a simulator of the physics of the real world to train a reinforcement learning agent to control a robot.</li>
<li><span class="citation" data-cites="Shen2018MWalkLT">[@Shen2018MWalkLT]</span> used a simulator to train agents to navigate a graph using MCTS.</li>
<li><span class="citation" data-cites="Ellis2019WriteEA">[@Ellis2019WriteEA]</span> used a REPL environment to train a reinforcement learning agent to write code.</li>
</ul>
</section>
<section id="sec-l1g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g3">When to use a distribution model or sample model</h3>
<ul>
<li><strong>Distribution models</strong> are useful when we need to know the probability of different outcomes</li>
<li><strong>Sample models</strong> are useful when we need to simulate the environment</li>
</ul>
</section>
<section id="sec-l1g4" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g4">Advantages and disadvantages of sample models and distribution models</h3>
<ul>
<li><p><strong>Sample models</strong> can be represented more compactly than distribution models</p></li>
<li><p><strong>Distribution models</strong> can be more accurate than sample models</p></li>
<li><p>exact expectations can be computed from distribution models</p></li>
<li><p>assessing risks and uncertainties is easier with distribution models</p></li>
</ul>
</section>
<section id="sec-l1g5" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g5">Why sample models can be represented more compactly than distribution models</h3>
<ul>
<li><p><strong>Sample models</strong> can be represented more compactly than distribution models because they only need to store a single next state and reward</p></li>
<li><p><strong>Distribution models</strong> need to store the joint probability of each possible next state and reward pair</p></li>
<li><p><strong>Sample models</strong> can be more efficient when we only need to simulate the environment</p></li>
</ul>
</section>
</section>
<section id="lesson-2-planning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-2-planning">Lesson 2: Planning</h2>
<section id="lesson-learning-goals-1" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-1">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Explain how planning is used to improve policies <a href="#sec-l2g1">#</a></label></li>
<li><label><input type="checkbox">Describe random-sample one-step <strong>tabular Q-planning</strong> <a href="#sec-l2g2">#</a></label></li>
</ul>
</section>
<section id="sec-l2g1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l2g1">How planning is used to improve policies</h3>
<ul>
<li><strong>Planning</strong> is the process of using a model to improve a policy or value function</li>
<li><strong>Planning</strong> can be used to improve a policy or value function without interacting with the environment</li>
<li><strong>Planning</strong> can be used to improve a policy or value function more efficiently than direct RL updates</li>
</ul>
<p>Random-sample one-step <strong>tabular Q-planning</strong> {#sec-l2g2}</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/rl-wk5-random-sample-one-step-tabular-Q-learning.png" class="img-fluid figure-img"></p>
<figcaption>Q-planning alg overview</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/rl-wk5-Q-planning.png" class="img-fluid figure-img"></p>
<figcaption>random sample one step tabular Q-planning</figcaption>
</figure>
</div></div>
<!-- replace with latex version -->
<ul>
<li><strong>Tabular Q-planning</strong> is a planning algorithm that uses a sample model to improve a policy or value function</li>
<li><strong>Tabular Q-planning</strong> uses a sample model to simulate the environment</li>
<li><strong>Tabular Q-planning</strong> uses the simulated experience to improve a policy or value function</li>
</ul>
<p>advantages of planning</p>
<ul>
<li><strong>Planning</strong> can be more efficient than direct RL updates</li>
<li><strong>Planning</strong> can be used to improve a policy or value function without interacting with the environment</li>
<li><strong>Planning</strong> can be used to improve a policy or value function more efficiently than direct RL updates</li>
</ul>
</section>
</section>
<section id="lesson-3-dyna-as-a-formalism-for-planning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-3-dyna-as-a-formalism-for-planning">Lesson 3: Dyna as a formalism for planning</h2>
<section id="lesson-learning-goals-2" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-2">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Recognize that direct RL updates use experience from the environment to improve a policy or value function <a href="#sec-l3g1">#</a></label></li>
<li><label><input type="checkbox">Recognize that planning updates use experience from a model to improve a policy or value function <a href="#sec-l3g2">#</a></label></li>
<li><label><input type="checkbox">Describe how both direct RL and planning updates can be combined through the <strong>Dyna architecture</strong> <a href="#sec-l3g3">#</a></label></li>
<li><label><input type="checkbox">Describe the <strong>Tabular Dyna-Q algorithm</strong> <a href="#sec-l3g4">#</a></label></li>
<li><label><input type="checkbox">Identify the direct-RL and planning updates in <strong>Tabular Dyna-Q</strong> <a href="#sec-l3g5">#</a></label></li>
<li><label><input type="checkbox">Identify the model learning and search control components of <strong>Tabular Dyna-Q</strong> <a href="#sec-l3g6">#</a></label></li>
<li><label><input type="checkbox">Describe how learning from both direct and simulated experience impacts performance <a href="#sec-l3g7">#</a></label></li>
<li><label><input type="checkbox">Describe how simulated experience can be useful when the model is accurate <a href="#sec-l3g8">#</a></label></li>
</ul>
</section>
<section id="sec-l3g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g1">Direct RL updates use experience from the environment to improve a policy or value function</h3>
<ul>
<li><strong>Direct RL updates</strong> use experience from the environment to improve a policy or value function</li>
<li><strong>Direct RL updates</strong> can be used to improve a policy or value function by interacting with the environment</li>
</ul>
</section>
<section id="sec-l3g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g2">Planning updates use experience from a model to improve a policy or value function</h3>
<ul>
<li><strong>Planning updates</strong> use experience from a model to improve a policy or value function</li>
<li><strong>Planning updates</strong> can be used to improve a policy or value function without interacting with the environment</li>
</ul>
</section>
<section id="sec-l3g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g3">Both direct RL and planning updates can be combined through the <strong>Dyna architecture</strong></h3>
<ul>
<li><strong>Dyna architecture</strong> combines direct RL updates and planning updates to improve a policy or value function</li>
<li><strong>Dyna architecture</strong> uses a model to simulate the environment</li>
<li><strong>Dyna architecture</strong> uses the simulated experience to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l3g4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l3g4">The <strong>Tabular Dyna-Q algorithm</strong></h3>
<ul>
<li><strong>Tabular Dyna-Q</strong> is a planning algorithm that uses a sample model to improve a policy or value function</li>
<li><strong>Tabular Dyna-Q</strong> uses a sample model to simulate the environment</li>
<li><strong>Tabular Dyna-Q</strong> uses the simulated experience to improve a policy or value function</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/rl-wk5-tabular-dyna-Q.png" class="img-fluid figure-img"></p>
<figcaption>The Tabular Dyna-Q algorithm</figcaption>
</figure>
</div></div><p>Exercise 8.2 Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiments?</p>
<p>Dyna-Q+ is like a generalized UCB while Dyna-Q+ is like a generalized epsilon greedy alg. Dyna-Q+ is doing more efficent exploration. It will revisits will be more spread out more over time but it scheme also tends to increases in non independent way - probabilities for unvisited regions keep growing so if it starts exploring it may like doing an extended sequence till it gets to a dead end.<br>
Dyna Q exploration is independent for each state,action combo so retrying sequences get asymptotically less likely with time.</p>
<p>Exercise 8.3 Careful inspection of Figure 8.5 reveals that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?</p>
<p>Dyna-Q+ is more efficient at exploring so it learned a better policy, but since the environment was static Dyna-Q got to catch up, but it never reached the same policy.</p>
<p>Exercise 8.5 How might the tabular Dyna-Q algorithm shown on page 164 be modified to handle stochastic environments? How might this modification perform poorly on changing environments such as considered in this section? How could the algorithm be modified to handle stochastic environments and changing environments?</p>
<p>to hadle a stochastic environment one would need to to model probabilities of stochastic dynamics. One way to do this is to use Bayesian updating with a dericlet prior and a multinomial posterior.<br>
This modification would likely fare much worse since learning low probability transitions would require many visits to discover.<br>
In the case of changing environment it would also take much longer for new state to be reflected in the model (if a state was visited 10 with just one transition and then the transition changed to another state then it would take many more than 10 vistis to quash the old probability and get the new one correct to 10%<br>
This means that we adding a forgetting rule might be better then the plain derichlet-multinomial model.<br>
To handle both stochastic and changing updates we may want to<br>
1. track the recency of the last visit and reward this option like in dyna-q plus.<br>
2. decay old probabilities - would require storing the time for each visit - i.e.&nbsp;path dependent model.<br>
3. A better idea is to use a hirachial model with parial pooling representing short term and long term transitions - this could fix the problem of decay by simply giving greater weight to the smaller more recent model.<br>
The short term would track the last k visits in each state and the long term all the visits. We could then do partial pooling between these two estimators with much greater emphasis on the recent one!<br>
<br>
</p>
</section>
<section id="sec-l3g5" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g5">Direct-RL and planning updates in <strong>Tabular Dyna-Q</strong></h3>
<ul>
<li><strong>Tabular Dyna-Q</strong> uses direct RL updates to improve a policy or value function</li>
<li><strong>Tabular Dyna-Q</strong> uses planning updates to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l3g6" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g6">Model learning and search control components of <strong>Tabular Dyna-Q</strong></h3>
<ul>
<li><strong>Tabular Dyna-Q</strong> uses a sample model to simulate the environment</li>
<li><strong>Tabular Dyna-Q</strong> uses the simulated experience to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l3g7" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g7">Learning from both direct and simulated experience impacts performance</h3>
<ul>
<li>Learning from both direct and simulated experience can improve performance</li>
<li>Learning from both direct and simulated experience can be more efficient than direct RL updates</li>
</ul>
</section>
<section id="sec-l3g8" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g8">Simulated experience can be useful when the model is accurate</h3>
<ul>
<li>Simulated experience can be useful when the model is accurate</li>
<li>Simulated experience can be used to improve a policy or value function without interacting with the environment</li>
</ul>
</section>
</section>
<section id="lesson-4-dealing-with-inaccurate-models" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-4-dealing-with-inaccurate-models">Lesson 4: Dealing with inaccurate models</h2>
<section id="lesson-learning-goals-3" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-3">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Identify ways in which models can be inaccurate <a href="#sec-l4g1">#</a></label></li>
<li><label><input type="checkbox">Explain the effects of planning with an inaccurate model <a href="#sec-l4g2">#</a></label></li>
<li><label><input type="checkbox">Describe how <strong>Dyna</strong> can plan successfully with a partially inaccurate model <a href="#sec-l4g3">#</a></label></li>
<li><label><input type="checkbox">Explain how model inaccuracies produce another exploration-exploitation trade-off <a href="#sec-l4g4">#</a></label></li>
<li><label><input type="checkbox">Describe how <strong>Dyna-Q+</strong> proposes a way to address this trade-off <a href="#sec-l4g5">#</a></label></li>
</ul>
</section>
<section id="sec-l4g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g1">Ways in which models can be inaccurate</h3>
<ul>
<li>Models can be inaccurate for many reasons</li>
<li>because they have not sampled all actions in all states</li>
<li>because the environment is has changed since the model was learned</li>
<li>if the environment is stochastic</li>
</ul>
</section>
<section id="sec-l4g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g2">Effects of planning with an inaccurate model</h3>
<ul>
<li>Planning with an inaccurate model can cause the value function to become worse</li>
<li>Planning with an inaccurate model can lead to sub-optimal policies</li>
</ul>
</section>
<section id="sec-l4g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g3"><strong>Dyna</strong> can plan successfully with a partially inaccurate model</h3>
<ul>
<li><strong>Dyna</strong> can plan successfully with a partially inaccurate model</li>
<li><strong>Dyna</strong> can use direct RL updates to improve a policy or value function as well as the model</li>
<li><strong>Dyna</strong> can use planning updates to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l4g4" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g4">Model inaccuracies produce another exploration-exploitation trade-off</h3>
<ul>
<li><p>Model inaccuracies produce another exploration-exploitation trade-off</p></li>
<li><p>exploit an inaccurate model to improve the policy</p></li>
<li><p>revisit states/actions with low value to update the model</p></li>
<li><p>Can we use an inverse sort of planning to identify states for which the model is inaccurate?</p></li>
<li><p>Model inaccuracies can lead to suboptimal policies</p></li>
<li><p>Model inaccuracies can lead to poor performance</p></li>
</ul>
</section>
<section id="sec-l4g5" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l4g5"><strong>Dyna-Q+</strong> proposes a way to address this trade-off</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/rl-wk5-bonus-rewards-for-exploration.png" class="img-fluid figure-img"></p>
<figcaption>Dyna-Q+ solution</figcaption>
</figure>
</div></div><ul>
<li><strong>Dyna-Q+</strong> proposes a way to address this trade-off</li>
<li><strong>Dyna-Q+</strong> uses a bonus reward to encourage exploration</li>
<li><strong>Dyna-Q+</strong> can improve performance when the model is inaccurate</li>
</ul>
</section>
<section id="drew-bagnell-on-self-driving-cars-robotics-and-model-based-reinforcement-learning" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="drew-bagnell-on-self-driving-cars-robotics-and-model-based-reinforcement-learning">Drew Bagnell on self-driving cars robotics and model-based reinforcement learning</h3>
<p>Drew Bagnell is a professor at Carnegie Mellon University and the CTO at Aurora innovation.</p>
<p>He has worked on self-driving cars and robotics. He has also worked on model-based reinforcement learning. He point out a dirty little secret that model-based reinforcement learning is a key technology for robotics.</p>
<p>He points out that the real world is expensive and dangerous. Using model based reinforcement learning can reduce the number of interactions with the real world and along learning about risky actions in the simulated world to improve performance in the real world. Also as we pointer out before this can usually be done much faster than real-time interaction with the environment.</p>
<p>Sample complexity: how many real-world samples are required to achieve high performance? It takes exponentially fewer interactions with a model than without. Not really sure what exponentially fewer means here - but itâ€™s a lot fewer.</p>
<p>Quadratic value function approximation goes back to optimal control in the 1960s. Itâ€™s continuous in states and actions. This is a method that should be part of the next course but isnâ€™t covered there either</p>
<p>For linear transition dynamics with quadratic costs/rewards, itâ€™s exact. For local convex / concave points, it is a good approximation of the true action-value function.</p>
<p>Here is the math from his slide:</p>
<p>Quadratic value function approximation</p>
<p><span id="eq-quadratic-value-function-approximation"><span class="math display">\[
Q_t(x,a) = \begin{bmatrix}     x   \\ a   \\ \end{bmatrix}^T
\begin{bmatrix}     Q_{xx}   &amp;&amp;  Q_{xa}   \\ Q_{xa}   &amp;&amp;  Q_{uu}   \\ \end{bmatrix}
\begin{bmatrix} x   \\ a   \\ \end{bmatrix}^T +
\begin{bmatrix}    q_x   \\ q_a \\ \end{bmatrix}^T
\begin{bmatrix}     x   \\ a   \\ \end{bmatrix} + const
\tag{1}\]</span></span></p>
<p>The approximation allows for calculating the optimal action-value in closed form (finite number of standard operations) even with continuous actions.</p>
<p>Differential dynamic programming takes advantage of the technique above.<br>
<br>
So this seems complicated - because of matrix maths. But intuitively this is something we like to do in physics - add the term for the second derivative<br>
in the taylor series approximation of our function.</p>
<p>The 2nd paper is particularly clear and easy to work through for the approach just described.</p>
<div class="column-page">
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">timeline

    title Bandit Algorithms Timeline

    
        1952 : Thompson Sampling
        1955 : Upper Confidence Bound (UCB)
        1963 : Epsilon-Greedy
        2002 : Bayesian UCB
        2011 : Bayesian Bandits
        2012 : Contextual Bandits
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<div class="column-page">
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">%%{init: {'theme': 'base', 'themeVariables': { 'timeline': { 'nodeSpacing': 50, 'sectionSpacing': 100, 'verticalStartPosition': 50, 'verticalSectionStartPosition': 50 }}}}%%
timeline
    direction TD
    title Reinforcement Learning Algorithms Timeline


    
        1948 : Monte Carlo Methods
        1950 : Bellman Optimality Equations
        1957 : Dynamic Programming
        1959 : Temporal Difference Learning (TD)
        1960 : Policy Iteration
        1963 : Value Iteration
        1983 : Q-Learning
        1984 : Expected SARSA
        1990 : Dyna-Q : Dyna-Q+
        1992 : SARSA
        1994 : Monte Carlo with E-Soft
        1995 : Monte Carlo with Exploring Starts
             : Generalized Policy Iteration (GPI)
        1998 : Semi-Gradient TD
        2000 : Differential Semi-Gradient SARSA
        2001 : Gradient Monte Carlo (Gradient MC)
        2003 : Gaussian Actor-Critic
             : Softmax Actor-Critic
             : Deep Q-Network (DQN)


</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<p>Materials from <a href="https://sites.google.com/view/mbrl-tutorial">ICML Tutorial on Model-Based Reinforcement Learning</a>:</p>
<p>the page above contains the following materials as well as an extensive bibliography.</p>
<ul>
<li><a href="https://docs.google.com/presentation/d/1f-DIrIvh44-jmTIKdKcue0Hx2RqQSw52t4k8HEdn5-c/edit?usp=sharing">Slides</a></li>
<li><a href="https://slideslive.com/38930488/modelbased-methods-in-reinforcement-learning-part-1-introduction-learning-models">Part 1: Introduction and Learning Models</a></li>
<li><a href="https://slideslive.com/38930486/modelbased-methods-in-reinforcement-learning-part-2-modelbased-control">Part 2: Model-Based Control</a></li>
<li><a href="https://slideslive.com/38930487/modelbased-methods-in-reinforcement-learning-part-3-modelbased-control-in-the-loop">Part 3: Model-Based Control in the Loop</a></li>
<li><a href="https://slideslive.com/38930489/modelbased-methods-in-reinforcement-learning-part-4-beyond-vanilla-mbrl">Part 4: Beyond Vanilla MBRL</a></li>
</ul>
<p>From Bagnellâ€™s talk:</p>
<ul>
<li><a href="https://macrl-book.github.io/">Modern Adaptive Control and Reinforcement Learning</a></li>
<li><a href="https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf">Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization</a></li>
<li><a href="https://katefvision.github.io/katefSlides/trajectoryoptimization_katef.pdf">Optimal Control, Trajectory Optimization, Learning Dynamics</a></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>