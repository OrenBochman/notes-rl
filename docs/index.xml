<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Notes on Reinfocement Learning</title>
<link>https://orenbochman.github.io/</link>
<atom:link href="https://orenbochman.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Personal website, portfolio and blog</description>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Fri, 17 Jan 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>Dynamic collaborative filtering Thompson Sampling for cross-domain advertisements recommendation</title>
  <link>https://orenbochman.github.io/posts/paper1/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/paper1/cover.jpg" class="img-fluid figure-img"></p>
<figcaption>litrature review</figcaption>
</figure>
</div></div><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/DAL7cpE3K7E" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>So I don’t have much time for this today so here is a quick note on: <span class="citation" data-cites="ishikawa2022dynamiccollaborativefilteringthompson">(Ishikawa, Chung, and Hirate 2022)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>The authors are using <strong>Thompson Sampling</strong>. This is a Bayesian method in RL.</li>
<li>Thier problem is an advert recommendation system. So they are integrating Thompson sampling into making recommendations.</li>
<li>They use a doubly robust estimation method. This is something I learned about from Emma Brunskill’s guest lecture in the Alberta Coursera course. And ever since I’ve been looking on how to do this in RL. Unforetunatly all I could find was work that used it in offline RL settings. So I was stoked to see it used as a central part of this paper. Using a doubly robust estimator is a sound technique for reducing variance without introducing a bias. And variance is the gratest impediment to learning quickly in RL. Also unlike some other ideas I’ve come accross it seems to align very well with Causal Inference.</li>
<li>The talk mentions a dataset the authors used for doing this work. Is this dataset available? I would like to try this out</li>
</ol>
<p>One line on Thompson sampling, one of the oldest technique in the RL playbook which uses the following rule: pick an action at random from the posterior distribution of the action values and then use the outcome to update the posterior distribution for the next step.</p>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My ideas
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Find what data set was used.</li>
<li>Is this dataset available?</li>
<li>Can we make a minimal version to quickly test this kind of agent?</li>
<li>Figure out a framework that extends tompson sampling to other RL problems.
<ul>
<li>need to add P(action|state) i.e.&nbsp;add conditioning of the bernulli on the state.</li>
<li>prehaps do simple counts of steps since starts or last reward.</li>
<li>prehaps using a succeror representation can help</li>
</ul></li>
<li>Marketing are the worst POMDPs. Testing real stuff is very hard so a good environment might help.</li>
<li>I want to make an petting zoo env to support single &amp; multiagent:
<ol type="1">
<li>auctions / non autions</li>
<li>advertising (rec sys) with costs</li>
<li>pricing with policies.</li>
</ol>
<ul>
<li>It should also allow incorperating real data from a dataset. Diretly or via sampling</li>
<li>It would be even neater to do this using a heirarchiacal model.</li>
<li>It would be even better if we can also incorportate the product, user hierecies.</li>
<li>It would be great if we have a</li>
</ul></li>
</ul>
</div>
</div>


</section>
</div>
</div>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-ishikawa2022dynamiccollaborativefilteringthompson" class="csl-entry">
Ishikawa, Shion, Young-joo Chung, and Yu Hirate. 2022. <span>“Dynamic Collaborative Filtering Thompson Sampling for Cross-Domain Advertisements Recommendation.”</span> <a href="https://arxiv.org/abs/2208.11926">https://arxiv.org/abs/2208.11926</a>.
</div>
</div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Bandit</category>
  <category>Advertising</category>
  <category>Collaborative Filtering</category>
  <guid>https://orenbochman.github.io/posts/paper1/</guid>
  <pubDate>Fri, 17 Jan 2025 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/paper1/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Policy Gradient</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c3-w4.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div id="fig-episodic-semi-gradient-sarsa" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-episodic-semi-gradient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithm decision tree</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-episodic-semi-gradient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The algorithms discussed in this lesson are all part of the policy gradient family. These allow us to consider both discrete and continuous actions in the the average rewards settings. We will consider Softmax Actor-Critic, Gaussian Actor-Critic, and the REINFORCE algorithm. The last is missing from the chart.
</figcaption>
</figure>
</div></div>
<p>Sometimes, the behavior codified in the policy is much simpler than the action value function. Thus, learning the policy directly can be more efficient. Learning policies is an end-to-end solution for solving many real-world RL problems. Coding such end-to-end solutions may be done under the umbrella of policy gradient methods. Once we cover the policy gradient theorem, we will see how we still need to use action value approximations to estimate the gradient of the average reward objective. A second way that we will use value function approximations is in the actor-critic algorithms. Here, the policy is called the actor, and the value function is called the critic. The critic evaluates the policy, and the actor is used to update the policy. The actor-critic algorithms are a hybrid of policy gradient and value function methods. They are more stable than policy gradient methods and can be used in more complex environments.</p>

<div class="no-row-height column-margin column-container"><div id="fig-policy-gradient-theorem-proof" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-theorem-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-13-pg-theorem.png" class="img-fluid figure-img"></p>
<figcaption>the policy gradient theorem</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-gradient-theorem-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: The policy gradient theorem &amp; proof from <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement pp.325]</span> In it <img src="https://latex.codecogs.com/png.latex?%5Cnabla"> are with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the parameter of the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> but is omitted for brevity.
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR This lesson in a nutshell
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>in a nutshell</figcaption>
</figure>
</div>
<p>I found policy gradient methods to be rather complicated at first blush. Eventually, as the finer points sunk in, I developed bits and pieces of intuition that made me feel more comfortable.</p>
<p><strong>Parametrized policies</strong> are much easier to conceptualize than <em>parametrized value functions</em>. One intuition is that the parameters are weights for the action, and the policy’s actions are drawn in proportion to these weights. The <strong>softmax policy</strong> does precisely this type of weighting. My Bayesian-trained intuition for these weights comes from the <em>categorical distribution</em> <sup>1</sup>. For this distribution, we can define <strong>success</strong> as the action that gets my agent closest to its goal. This intuition is just my first-order mental model; we will develop more sophisticated machinery as we go along.</p>
<p>The obvious question that will arise as soon as you deal with some environment is:</p>
<blockquote class="blockquote">
<p>“How can we get some <em>arbitrary</em> feature <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to participate in the parametrized policy?”</p>
</blockquote>
<p>The answer that comes to mind is <em>use it to build the weights</em>.</p>
<p>The usual suspect is a (Bayesian) linear regression that includes the feature .</p>
<blockquote class="blockquote">
<p>How is my feature <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> going to participate in the decisions made by <img src="https://latex.codecogs.com/png.latex?%5Cpi">?</p>
</blockquote>
<p>If we tweak this feature, how will the policy give us better returns? Since we want to maximize returns, we should adjust the weights in the direction that provides us with the best returns.</p>
<p>That is also the intuition for a parametrized policy’s <strong>update rule</strong>. The direction is just the gradient of the policy. The big hurdle lies in estimating this gradient.</p>
<p>The course material is very concise and laser-focused on very specific learning goals. <strong>The policy gradient theorem</strong> is the key result that allows us to estimate policy gradients. Unfortunately, the course instructors did not cover the proof, which is silently relegated to the readings.</p>
<p>This proof is not as simple as represented in its preamble in the book. I saw both longer and shorter versions of proofs and felt that there is a bit of hand waving in one of the steps<sup>2</sup>. Many people who are seriously interested in RL will be compelled to go through the proof in detail. Also, more experienced students can make greater leaps.</p>
<p>One of my goals is to make satisfactory proofs for episodic and continuing cases that I can walk though at ease.</p>
</div>
</div>
<p>Also, I was disappointed that this course does not cover more modern algorithms, such as <a href="https://arxiv.org/abs/1502.05477">TRPO</a>, <a href="https://arxiv.org/abs/1707.06347">PPO</a>, or other Deep learning algorithms. I cannot stress this point enough.</p>
<p>In the last video in the previous lecture’s notes by <a href="https://scholar.google.com/citations?user=8RgDBoEAAAAJ&amp;hl=en">Satinder Singh</a>, all of the research on using Meta gradients to learn intrinsic rewards is also built on top of policy gradient methods - where he and his students looked at propagating these gradients through multiple the planing algorithms and later through the learning algorithm to learn a reward function and tackle the issues of exploration.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Course Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§13 pp. 321-336]</span> Figure&nbsp;18</label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Extra Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Extra resources I found useful to review though not required, nor part of the course material</p>
<ul>
<li><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html">Intro to Policy Optimization @ Spinning Up</a></li>
<li><a href="https://johnwlambert.github.io/policy-gradients/">Understanding Policy Gradients</a></li>
<li><a href="https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146">Policy Gradient Explained</a></li>
<li><a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/">Going Deeper Into Reinforcement Learning: Fundamentals of Policy Gradients</a></li>
<li><a href="https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/">Notes on the Generalized Advantage Estimation Paper</a></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Videos
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">



<p>Here are three extra videos by experts in the field that delve deeper into this topic. Each of these instructors have published papers with some of the most groundbreaking algorithms in the field and have a lot of insights to share.</p>
<ol type="1">
<li><p>In this lecture <a href="http://joschu.net/">John Schulman</a> covers Deep Reinforcement Learning Policy Gradients and Q-Learning. John Schulman is a research scientist at OpenAI and has published many papers on RL and Robotics. John Schulman who developed PPO and TRPO and Chat-GPT</p></li>
<li><p>In this lecture <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> covers policy gradients and advantage estimation. Pieter Abbeel is a professor at UC Berkeley and has published many papers on RL and Robotics.</p></li>
<li><p>In this lesson from a Deep Mind Course <a href="https://hadovanhasselt.com/about/">Hado van Hasselt</a> covers some advantages as well as challenges of policy gradient methods.</p></li>
</ol>
</div>
</div>
</div><div class="no-row-height column-margin column-container"><div id="fig-deep-reinforcement-learning" class="quarto-float quarto-figure quarto-figure-center anchored callout-4-contents callout-collapse collapse callout-margin-content">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deep-reinforcement-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/PtAIh9KSnjo" title="Deep Reinforcement Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-deep-reinforcement-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Talk titled ‘Deep Reinforcement Learning Policy Gradients and Q-Learning’ by John Schulman on Reinforcement Learning at at the Deep Learning School on September 24/25, 2016
</figcaption>
</figure>
</div><div id="fig-policy-gradients-and-advantage-estimation" class="quarto-float quarto-figure quarto-figure-center anchored callout-4-contents callout-collapse collapse callout-margin-content">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradients-and-advantage-estimation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/AKbX1Zvo7r8" title="Policy Gradients and Advantage Estimation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-gradients-and-advantage-estimation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Talk titled ‘Policy Gradients and Advantage Estimation’ by Pieter Abbeel. in his ‘Foundations of Deep RL Series’
</figcaption>
</figure>
</div><div id="fig-policy-grad-limitations" class="quarto-float quarto-figure quarto-figure-center anchored callout-4-contents callout-collapse collapse callout-margin-content">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-grad-limitations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/y3oqOjHilio" title="Policy-Gradient and Actor-Critic methods" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-grad-limitations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Research Scientist Hado van Hasselt from Deep Mind covers policy algorithms that can learn policies directly and actor critic algorithms that combine value predictions for more efficient learning. From DeepMind x UCL | Deep Learning Lecture Series 2021
</figcaption>
</figure>
</div></div>
<section id="lesson-1-learning-parameterized-policies" class="level1 page-columns page-full">
<h1>Lesson 1: Learning Parameterized Policies</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand how to define policies as parameterized functions #</label></li>
<li><label><input type="checkbox" checked="">Define one class of parameterized policies based on the softmax function #</label></li>
<li><label><input type="checkbox" checked="">Understand the advantages of using parameterized policies over action-value based methods #</label></li>
</ul>
</div>
</div>
<section id="learning-policies-directly-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="learning-policies-directly-video">Learning policies directly (Video)</h2>

<div class="no-row-height column-margin column-container"><div id="fig-policy-gradient" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-01.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Energy pumping policy</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: In the mountain car environment, the parameterized value function is complex, but the parameterized policy is simple.
</figcaption>
</figure>
</div></div><p>In this lesson course instructor Adam White introduces the idea of learning policies directly. He contrasts this with learning value functions and explains why learning policies directly can be more flexible and powerful.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Rethinking policies
</div>
</div>
<div class="callout-body-container callout-body">
<p>Moving on we will need to think very clearly about policies.</p>
<p>To this end it is worth spending a minute to quickly recap the definition properties and notation of a policy from the previous lessons:</p>
<ol type="1">
<li><p><em>Intuitively</em> a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is just decision making rule.</p></li>
<li><p>A <em>deterministic policy</em> is just a function that maps a state to an action. <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%20:%20s%5Cin%20%5Cmathcal%7BS%7D%20%5Cto%20a%20%5Cin%20%5Cmathcal%7BA%7D%20%5Cqquad%20%5Ctext%7B(deterministic%20policy)%7D%0A"></p></li>
<li><p>A stochastic policy is a function that maps a state to a probability distribution over actions. Stochastic policies are more general and include deterministic policies as a special case. So while we may talk of deterministic policies, we will use the mathematical form of a stochastic policy.</p></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%20:%20s%5Cin%20%5Cmathcal%7BS%7D%20%5Cto%20%5Cmathbb%7BP%7D(%5Cmathcal%7BA%7D)%20%5Cqquad%20%5Ctext%7B(stochastic%20policy)%7D%0A"></p>
<ol type="1">
<li>Formally, the policy is defined probabilistically as follows:</li>
</ol>
<p><span id="eq-policy-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%5Cmid%20s)%20%5Cdoteq%20Pr(A_t%20=%20a%20%5Cmid%20S_t%20=%20s)%20%5Cqquad%20%5Ctext%7B(policy)%7D%0A%5Ctag%7B1%7D"></span></p>
<ol type="1">
<li>note that this is a shorthand for the following:</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%5Cmid%20s)%20=%20%5Cmathbb%7BE%7D%5BA_t%20%5Cmid%20S_t%20=%20s%5D%20%5Cqquad%20%5Ctext%7B(policy)%7D%0A"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a probability distribution over actions given a state.</p>
</div>
</div>
</section>
<section id="sec-l1g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g1">How to Parametrize a Policies?</h2>

<div class="no-row-height column-margin column-container"><div id="fig-policy-gradient-param" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-param-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-02.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>policy parametrization</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-gradient-param-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: When we parametrize a policy we will use the greek letter <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> to denote the parameters of the policy.
</figcaption>
</figure>
</div><div id="fig-policy-gradient-constraint" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-constraint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-03.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>policy parametrization constraints</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-gradient-constraint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Constraints on the policy parameters can be used to ensure that the policy is valid.
</figcaption>
</figure>
</div></div>
<p>So far we have been mostly looking at learning value functions. But when it comes to function approximation, it is often simpler to learn a policy directly.</p>
<p>In the mountain car environment we see the power pumping policy which accelerates the car in the direction it is moving. This is a near optimal policy for this environment. The policy is simple and can be learned directly and it makes no use of value functions. This may not always be the case.</p>
<p>A visual summary of the policy parametrization is shown in the figure. Recall that the policy is a function that takes in a state and outputs a probability distribution over actions. We will use the greek letter <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> to denote the parameters of the policy. This way we can reference the parameters of <img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D(s,a,w)"> the action value function are denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D">.</p>
<p>The parametrized policy is defined as follows:</p>
<p><span id="eq-parametrized-policy-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%5Cdoteq%20Pr(A_t%20=%20a%20%5Cmid%20S_t%20=%20s,%20%5Ctheta)%20%5Cqquad%20%5Ctext%7B(parametrized%20policy)%7D%0A%5Ctag%7B2%7D"></span></p>
<p>is a probability distribution over actions given a state and the policy parameters.</p>
<p>Since we are dealing with probabilities, the policy parameters must satisfy certain constraints. For example, the probabilities must sum to one. This is shown in the figure. These policy parameters constraints will ensure that the policy is valid.</p>
<p>Policy Gradient use gradient ascent:</p>
<p><span id="eq-gradient-ascent"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7Bt+1%7D%20=%20%5Ctheta_t%20+%20%5Calpha%20%5Cnabla_%5Ctheta%20J(%5Ctheta)%20%5Cqquad%20%5Ctext%7B(gradient%20ascent)%7D%0A%5Ctag%7B3%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is the step size and <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta%20J(%5Ctheta)"> is the gradient of the objective function <img src="https://latex.codecogs.com/png.latex?J(%5Ctheta)"> with respect to the policy parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<ul>
<li>methods that follow this update rule are called <strong>policy gradient methods</strong>.</li>
<li>methods that learn both a value function and a policy are called <strong>actor-critic methods</strong>.</li>
</ul>
</section>
<section id="sec-l1g2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g2">Define one class of parameterized policies based on the softmax function</h2>

<div class="no-row-height column-margin column-container"><div id="fig-policy-softmax" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-softmax-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-04-softmax-policy.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>softmax properties</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-policy-softmax-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9
</figcaption>
</figure>
</div></div><p>The <strong>Softmax policy</strong> based on the Boltzmann distribution is a probability distribution over actions given a state. It is parameterized by a vector of action preferences <img src="https://latex.codecogs.com/png.latex?h(s,%20a,%20%5Ctheta)">.</p>
<p><span id="eq-softmax-policy"><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%5Cdoteq%20%5Cfrac%7Be%5E%7Bh(s,%20a,%20%5Ctheta)%7D%7D%7B%5Csum_%7Bb%5Cin%20%5Cmathcal%7BA%7D%7D%20e%5E%7Bh(s,%20b,%20%5Ctheta)%7D%7D%20%5Ctext%7B(softmax%20policy)%7D%20%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<ul>
<li>the numerator is the exponential of the action preference</li>
<li>the denominator is the sum of the exponentials of all action preferences</li>
</ul>
<p>Some properties of the softmax policy are that it can take in a vector of weights for different actions and output a probability distribution over actions. A second property is that the softmax policy generalizes the max function. A third property is that unlike the max function which is discontinuous the softmax policy is differentiable, making it amenable to gradient-based optimization.</p>
<ul>
<li>negative values of h lead to positive action probabilities.</li>
<li>equal values of h lead to equal action probabilities.</li>
<li>the softmax policy is a better option over than the <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy policy over the action-value based methods.</li>
</ul>
</section>
<section id="advantages-of-policy-parameterization-video" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-policy-parameterization-video">Advantages of Policy Parameterization (Video)</h2>
<p>In this video we consider the advantages of using parameterized policies over action-value based methods. We will see that parameterized policies are more flexible than action-value based methods and can start off stochastic and then become deterministic.</p>
</section>
<section id="sec-l1g3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g3">Advantages of using parameterized policies over action-value based methods</h2>

<div class="no-row-height column-margin column-container"><div id="fig-policy-action-preferences" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-action-preferences-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-05-action-preferences.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>softmax policy v.s. epsilon-greedy</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-action-preferences-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Softmax policy v.s. <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy
</figcaption>
</figure>
</div><div id="fig-short-corridor-env" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-short-corridor-env-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-06-stochastic-policy.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Short corridor with switched action</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-short-corridor-env-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: In the Short corridor with switched action environment a deterministic policy fails to reach the goal. The only optimal policy is stochastic.
</figcaption>
</figure>
</div></div>
<blockquote class="blockquote">
<p>One advantage of parameterizing policies according to the softmax in action preferences is <strong>that the approximate policy can approach a deterministic policy</strong>, whereas with <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy action selection over action values there is always an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> probability of selecting a random action.</p>
</blockquote>
<blockquote class="blockquote">
<p>A second advantage of parameterizing policies according to the softmax in action preferences is that <strong>it enables the selection of actions with arbitrary probabilities</strong>. In problems with significant function approximation, the best approximate policy may be stochastic.</p>
</blockquote>
<p>For example, in card games with imperfect information the optimal play is often a mixed strategy which means you should take two different actions each with a specific probability, such as when bluffing in Poker.</p>
<p>Action-value methods have no natural way of finding stochastic optimal policies, whereas policy approximating methods can, as shown in The short Corridor environment</p>
</section>
</section>
<section id="lesson-2-policy-gradient-for-continuing-tasks" class="level1 page-columns page-full">
<h1>Lesson 2: Policy Gradient for Continuing Tasks</h1>
<ul>
<li>parameterized policies are more flexible than action-value based methods</li>
<li>can start off stochastic and then become deterministic</li>
</ul>
<p>In function approximation, the optimal policy is not necessarily deterministic. Thus it is best to be able to learn stochastic policies.</p>
<p>Example where the optimal policy is stochastic:</p>
<ul>
<li>Sometimes it is just easier to learn a stochastic policy.</li>
<li>E.g. in mountain car, the parameterized value function is complex, but the parameterized policy is simple.</li>
</ul>

<div class="no-row-height column-margin column-container"><div id="fig-mountain-car-policy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-07-stochastic-simpler.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>mountain car environment values and policy</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: In mountain car, the parameterized value function is complex, but the parameterized policy is simple.
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Describe the objective for policy gradient algorithms #</label></li>
<li><label><input type="checkbox" checked="">Describe the results of the policy gradient theorem #</label></li>
<li><label><input type="checkbox" checked="">Understand the importance of the policy gradient theorem #</label></li>
</ul>
</div>
</div>
<section id="the-objective-for-learning-policies-video" class="level2">
<h2 class="anchored" data-anchor-id="the-objective-for-learning-policies-video">The Objective for Learning Policies (Video)</h2>
<p>In this video Martha White dive deep into the objective for policy gradient algorithms. She then contrasts this with the gradient of the value fn objectives and lists some challenges for estimating the gradient of the average reward objective. We will learn that we can use the policy gradient theorem and to overcome these challenges and derive a gradient with an neat update rule .</p>
</section>
<section id="sec-l2g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g1">The objective for policy gradient algorithms</h2>
<p>Formalizing the goal as an Objective</p>
<p><span id="eq-objectives"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AG_t%20&amp;=%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%20R_%7Bt%7D%20%20%5Cquad%20&amp;&amp;%20%5Ctext%7B(episodic)%7D%20%5Cnewline%0AG_t%20&amp;=%20%5Csum_%7Bt=0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Et%20R_%7Bt%7D%20%20%20%5Cquad%20&amp;&amp;%20%5Ctext%7B(continuing%20-%20discounted%20reward)%7D%20%5Cnewline%0AG_t%20&amp;=%20%5Csum_%7Bt=0%7D%5E%7B%5Cinfty%7D%20R_%7Bt%7D%20-%20r(%5Cpi)%20%20%5Cquad%20&amp;&amp;%20%5Ctext%7B(continuing%20-%20avg.%20reward)%7D%0A%5Cend%7Balign*%7D%0A%5Ctag%7B5%7D"></span></p>
<p>The average reward Objective for a policy is as follows: <span id="eq-average-reward-objective"><img src="https://latex.codecogs.com/png.latex?%0Ar(%5Cpi)%20=%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cmu(s)%20%5Csum_%7Ba%7D%20%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)%20r%20%5Cquad%20%5Ctext%7B(avg.%20reward%20objective)%7D%0A%5Ctag%7B6%7D"></span></p>
<p>What does this mean?</p>
<ul>
<li>the last sum is the expected reward for a state-action pair. <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BR_t%20%5Cmid%20S_t%20=%20s%20,%20A_t=a%5D"></li>
<li>the last two sums together are the expected reward for a state under weighted by the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">. <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%5Cpi%5BR_t%20%5Cmid%20S_t%20=%20s%5D"></li>
<li>full sum ads the time we spend in state <img src="https://latex.codecogs.com/png.latex?s"> under <img src="https://latex.codecogs.com/png.latex?%5Cpi"> therefore the expected reward for a state under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and the environment dynamics <img src="https://latex.codecogs.com/png.latex?p">. <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%5Cpi%5BR_t%5D"></li>
</ul>
<p>to optimize the average reward, we need to estimate the gradient of the avg. objective</p>
<p><span id="eq-average-reward-objective-gradient"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20r(%5Cpi)%20=%20%5Cnabla_%5Ctheta%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Ctextcolor%7Bred%7D%7B%5Cunderbrace%7B%5Cmu(s)%7D_%7B%5Ctext%7BDepends%20on%20%7D%5Ctheta%7D%7D%20%5Csum_%7Ba%7D%20%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)%20r%20%5Cqquad%0A%5Ctag%7B7%7D"></span></p>
<ol type="1">
<li>Methods based on this are called policy gradient methods.</li>
<li>We are trying to maximize the average reward.</li>
</ol>
<p>There are a few challenges with using the gradient in the above equation:</p>
<p>According to the lesson <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)"> depends on <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Martha White point out that this state importance though parameterized only by s actually depends on the the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> which will evolve during its training based on the values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Which means out notation here is a bit misleading. She then contrasts it with the value function gradient is being evaluated using a fixed policy.</p>
<p><span id="eq-value-function-gradient"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla_w%20%5Cbar%7BVE%7D%20&amp;=%20%5Cnabla_w%20%5Csum_%7Bs%7D%5Ctextcolor%7Bred%7D%7B%5Cunderbrace%7B%5Cmu(s)%7D_%7B%5Ctext%7BIndependent%20of%20%7D%5Cmathbf%7Bw%7D%7D%7D%20%20%5BV_%7B%5Cpi%7D(s)-%5Cbar%7Bv%7D(s,w)%5D%5E2%20%5Cnewline%0A&amp;=%5Csum_%7Bs%7D%20%5Ctextcolor%7Bred%7D%7B%5Cmu(s)%7D%20%5Cnabla_w%20%5BV_%7B%5Cpi%7D(s)-%5Cbar%7Bv%7D(s,w)%5D%5E2%0A%5Cend%7Balign*%7D%20%5Ctext%7B(value%20function%20gradient)%7D%20%5Cqquad%0A%5Ctag%7B8%7D"></span></p>
<p>We can avg reward as an objective for policy optimization and the its for the stochastic gradient ascent. Next we will consider how the policy gradient theorem can help us estimate the gradient of the average reward objective despite these setbacks.</p>
</section>
<section id="the-policy-gradient-theorem-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-policy-gradient-theorem-video">The Policy Gradient Theorem (Video)</h2>

<div class="no-row-height column-margin column-container"><div id="fig-policy-gradient-theorem" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-08-gridworld-policy.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Understanding the pg theorem up</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13
</figcaption>
</figure>
</div><div id="fig-policy-gradient-theorem" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-09-gridworld-policy.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Understanding the pg theorem left</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14
</figcaption>
</figure>
</div><div id="fig-policy-gradient-theorem" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-10-gridworld-policy.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Understanding the pg theorem all</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15
</figcaption>
</figure>
</div></div>

<p>In this video course instructor Martha White explains the policy gradient theorem, a key result for optimizing policy in reinforcement learning. The goal is to maximize the average reward by adjusting policy parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> using gradient ascent. The challenge is estimating the gradient of the average reward, which initially involves a complex expression with the gradient of the stationary distribution over states (<img src="https://latex.codecogs.com/png.latex?%5Cmu(s)">).</p>
</section>
<section id="sec-l2g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g2">The results of the policy gradient theorem</h2>
<p>The policy gradient theorem simplifies this by providing a new expression for the gradient. This expression involves summing, over all states and actions, the product of the gradient of the policy and the action-value function (Q). The gradient of the policy indicates how to adjust parameters to increase the probability of an action, while the action-value function represents the expected return from taking that action.</p>
<p>The video illustrates this with a grid world example, showing how gradients for different actions point in different directions. By weighting these gradients with the corresponding action values, the theorem provides a direction to update the policy parameters that increases the probability of high-value actions and decreases the probability of low-value actions.</p>
<p>The product rule <span id="eq-product-rule"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla(f(x)g(x))%20=%20%5Cnabla%20f(x)g(x)%20+%20f(x)%5Cnabla%20g(x)%20%5Cqquad%20%5Ctext%7B(product%20rule)%7D%0A%5Ctag%7B9%7D"></span></p>
<p>therefore:</p>
<p><span id="eq-policy-gradient-naive"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla_%5Ctheta%20r(%5Cpi)%20&amp;=%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla%20%5Cmu(s)%20%5Csum_%7Ba%7D%20%5Cpi(a%20%5Cmid%20s,%5Ctheta)%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)%20r%20%5Cnewline%20&amp;+%20%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cmu(s)%20%5Cnabla%20%5Csum_%7Ba%7D%20%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)%20r%0A%5Cend%7Balign*%7D%0A%5Ctag%7B10%7D"></span></p>
<p>The first term is the gradient of the stationary distribution and the second term is the gradient of the policy. The policy gradient theorem simplifies this expression by eliminating the need to estimate the gradient of the stationary distribution.</p>
<p><span id="eq-policy-gradient-theorem"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla_%5Ctheta%20r(%5Cpi)%20&amp;=%20%5Csum_%7Bs%5Cin%20%5Cmathcal%7BS%7D%7D%20%5Cmu(s)%20%5Ctextcolor%7Bred%7D%7B%20%5Csum_%7Ba%5Cin%7B%5Cmathcal%7BA%7D%7D%7D%20%5Cnabla%20%5Cpi(a%20%5Cmid%20s,%5Ctheta)%20%20q_%5Cpi(s,a)%20%7D%0A%5Cend%7Balign*%7D%0A%5Ctag%7B11%7D"></span></p>
<p>The policy gradient theorem provides a new expression for the gradient of the average reward objective. This expression involves summing, over all states and actions, the product of the gradient of the policy and the action-value function (Q). The gradient of the policy indicates how to adjust parameters to increase the probability of an action, while the action-value function represents the expected return from taking that action.</p>
<p>Martha White points out that this expression is much easier to estimate.</p>
<p>Now let’s try to understand how we use the theorem to estimate the gradient.</p>
<p>What we will use it to approximate the gradient. Computing the sum over states is impractical.</p>
<p>What we will do do is take a stochastic samples. This involves updating the policy parameters based on the gradient observed at the current state.</p>
<p>To simplify the update rule, the concept of expectations is introduced. By re-expressing the gradient as an expectation under the <strong>stationary distribution of the policy</strong>, the update can be further simplified to involve only a single action sampled from the current policy.</p>
<p>The final update rule resembles other learning rules seen in the course, where the policy parameters are adjusted proportionally to a stochastic gradient of the objective. The magnitude of the step is controlled by a step-size parameter</p>
<p>The actual computation of the stochastic gradient requires two components: the gradient of the policy and an estimate of the action-value function</p>
</section>
<section id="the-policy-gradient-theorem" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-policy-gradient-theorem">The policy gradient theorem</h2>
<p>We need some preliminary results and definitions.</p>
<ol type="1">
<li>The four part dynamics function from the <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement pp. 48]</span> book:</li>
<li>Next we need the result from Exercise 3.18 in <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement pp. 62]</span></li>
<li>Next we need the result from Exercise 3.19 in <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement pp. 62]</span></li>
</ol>
<p><span id="eq-dynamics-function"><img src="https://latex.codecogs.com/png.latex?%0Ap(s',%20r%20%5Cmid%20s,%20a)%20%5Cdoteq%20Pr%5C%7BS_t=s',%20R_t=r%20%5Cmid%20S_%7Bt-1%7D%20=%20s%20,%20A_%7Bt-1%7D=%20a%5C%7D%20%5Cqquad%20%5Ctext%7B(S.B.%203.2)%7D%0A%5Ctag%7B12%7D"></span></p>
<div id="note-318" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 3.18
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>The value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:</p>
</blockquote>

<blockquote class="blockquote">
<p>Give the equation corresponding to this intuition and diagram for the value at the root node, <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)">, in terms of the value at the expected leaf node, <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,%20a)">, given <img src="https://latex.codecogs.com/png.latex?S_t%20=%20s">. This equation should include an expectation conditioned on following the policy, <img src="https://latex.codecogs.com/png.latex?%5Cpi">. Then give a second equation in which the expected value is written out explicitly in terms of <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%20%5Cmid%20s)"> such that no expected value notation appears in the equation.</p>
</blockquote>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fig-ex-3-18" class="quarto-float quarto-figure quarto-figure-center anchored callout-8-contents callout-collapse collapse show callout-margin-content">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ex-3-18-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/ex-3-18.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>backup diagram from v() to q()</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ex-3-18-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: backup diagram from <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> to <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,a)">
</figcaption>
</figure>
</div></div><section id="solution" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="solution">Solution</h3>
<p><span id="eq-3-18-solution"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AV_%5Cpi(s)%20&amp;=%20%5Cmathbb%7BE_%5Cpi%7D%5Bq(s_t,a)%20%20%5Cmid%20s_t%20=%20s,%20a_t=a%20%5D%20&amp;&amp;%20%5Ctext%7B(def.%20of%20Value)%7D%20%5Cnewline%0A&amp;=%20%5Csum_a%20Pr(a%20%5Cmid%20s)%20q_%5Cpi(s,a)%20&amp;&amp;%20%5Ctext%7B(def.%20of%20Expectation)%7D%20%5Cnewline%0A&amp;=%20%5Ctextcolor%7Bred%7D%7B%5Csum_a%20%5Cpi(a%20%5Cmid%20s)%7D%20%5Ctextcolor%7Bgreen%7D%7Bq_%5Cpi(s,a)%7D%20&amp;&amp;%20%5Ctext%20%7B(def.%20of%20policy)%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B13%7D"></span></p>
<div id="note-319" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 3.19
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>The value of an action, <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,%20a)">, depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state—action pair) and branching to the possible next states:</p>
</blockquote>

<blockquote class="blockquote">
<p>Give the equation corresponding to this intuition and diagram for the action value, <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,%20a)">, in terms of the expected next reward, <img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D">, and the expected next state value, <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(S_%7Bt+1%7D)">, given that <img src="https://latex.codecogs.com/png.latex?S_t%20=%20s"> and <img src="https://latex.codecogs.com/png.latex?A_t%20=%20a">. This equation should include an expectation but not one conditioned on following the policy. Then give a second equation, writing out the expected value explicitly in terms of <img src="https://latex.codecogs.com/png.latex?p(s_0,%20r%20%5Cmid%20s,%20a)"> defined by eq 3.2, such that no expected value notation appears in the equation.</p>
</blockquote>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fig-ex-3-19" class="quarto-float quarto-figure quarto-figure-center anchored callout-9-contents callout-collapse collapse show callout-margin-content">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ex-3-19-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/ex-3-19.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>backup diagram from q() to v()</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ex-3-19-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: backup diagram from <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,a)"> to <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s')">
</figcaption>
</figure>
</div></div></section>
<section id="solution-1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="solution-1">Solution</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Aq_%5Cpi(s,%20a)%20&amp;=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20v_%5Cpi%20(s_%7Bt+1%7D)%20%5Cmid%20s_t%20=%20s,%20a_t%20=%20a%5D%20%5Cnewline%0A&amp;=%20%5Ctextcolor%7Bblue%7D%7B%5Csum_%7Bs',%20r%7D%20p(s',%20r%20%5Cmid%20s,%20a)%7D%20%5Ctextcolor%7Bpink%7D%7B%5Br%20+%20v_%5Cpi(s')%5D%7D%0A%5Cend%7Balign*%7D%20%5Cqquad%0A"> {#ex-319-solution}</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Aq_%5Cpi(s,%20a)%20&amp;=%20%5Csum_%7Bs'%7D%20%5Cmathbb%7BE%7D_%5Cpi%5BG_%7Bt%7D%20%5Cmid%20S_%7Bt+1%7D=s'%5D%20Pr%5C%7BS_%7Bt+1%7D%20=%20s'%20%5Cmid%20S_t%20=%20s,%20A_t%20=%20a%5C%7D%0A%5Cnewline%20&amp;=%20%5Csum_%7Bs'%7D%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%20%5Cmid%20S_t%20=%20s,%20A_t%20=%20a,%20S_%7Bt+1%7D%20=%20s'%5D%20Pr%5C%7BS_%7Bt+1%7D%20=%20s'%20%5Cmid%20S_t%20=%20s,%20A_t%20=%20a%5C%7D%0A%5Cnewline%20&amp;=%20%5Csum_%7Bs',r%7D%20%5Cleft(%20r%20+%20%5Cgamma%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BG_%7Bt+1%7D%20%5Cmid%20S_%7Bt+1%7D%20=%20s'%5D%7D_%7Bv_%5Cpi%7C(s')%7D%20%5Cright)%20p(s',%20r%20%5Cmid%20s,%20a)%0A%5Cnewline%20&amp;=%20%5Csum_%7Bs',r%7D%20%5B%20r%20+%20%5Cgamma%20v_%5Cpi(s')%5D%20p(s',%20r%20%5Cmid%20s,%20a)%0A%5Cend%7Balign*%7D%0A"></p>
<p>Here is my version of the proof:</p>
<p><span id="eq-policy-gradient-theorem-proof"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Ctextcolor%7Bcyan%7D%7B%5Cnabla_%5Ctheta%20V_%5Cpi(s)%7D%20&amp;=%20%5Cnabla_%5Ctheta%20%5Csum_a%20%5Ctextcolor%7Bred%7D%7B%5Cpi(a%20%5Cmid%20s)%7D%20%5Ctextcolor%7Bgreen%7D%7B%20q_%5Cpi(s,a)%20%7D%20&amp;&amp;%20%5Ctext%7Bbackup%20%7D%20v_%5Cpi%20%5Cto%20q_%5Cpi%20%5Ctext%7B%20(Ex%203.18)%7D%0A%5Cnewline%20&amp;=%20%5Csum_a%20%5Cnabla_%5Ctheta%20%5Cpi(a%20%5Cmid%20s)%20q_%5Cpi(s,a)%20+%20%5Cpi(a%20%5Cmid%20s)%20%5Cnabla_%5Ctheta%20q_%5Cpi(s,a)%20&amp;&amp;%20%5Ctext%7Bproduct%20rule%7D%0A%5Cnewline%20&amp;=%20%5Csum_a%20%5Cnabla_%5Ctheta%20%5Cpi(a%20%5Cmid%20s)%20q_%5Cpi(s,a)%20+%20%5Cpi(a%20%5Cmid%20s)%20%5Cnabla_%5Ctheta%20%5Csum_%7Bs'%7D%20%5Ctextcolor%7Bblue%7D%7BP(s',r%20%5Cmid%20s,%20a)%7D%20%5Ctextcolor%7Bpink%7D%7B%5Br%20+%20V_%5Cpi(s')%5D%7D%20&amp;&amp;%20%5Ctext%7Bbackup%20%7D%20q_%5Cpi%20%5Cto%20v_%5Cpi%20%5Ctext%7B%20(Ex%203.19)%7D%0A%5Cnewline%20&amp;=%20%5Csum_a%20%5Cnabla_%5Ctheta%20%5Cpi(a%20%5Cmid%20s)%20q_%5Cpi(s,a)%20+%20%5Cpi(a%20%5Cmid%20s)%20%5Csum_%7Bs'%7D%20P(s',r%20%5Cmid%20s,%20a)%20%5Cnabla_%5Ctheta%20V_%5Cpi(s')%20&amp;&amp;%20P,%20r%20%5Ctext%7B%20are%20const%20w.r.t.%20%7D%20%5Ctheta%20%5Cnewline%0A=&amp;%20%5Csum_%7Ba%20%5Cin%20%5Cmathcal%7BA%7D%7D%20%5CBig(%20%5Cnabla_%5Ctheta%20%5Cpi(a%20%5Cmid%20s)Q_%5Cpi(s,%20a)%20+%20%5Cpi(a%20%5Cmid%20s)%20%5Csum_%7Bs'%7D%20P(s'%20%5Cmid%20s,a)%20%20%5Ctextcolor%7Bcyan%7D%7B%5Cnabla_%5Ctheta%20V_%5Cpi(s')%7D%20%5CBig)%20&amp;&amp;%20%5Ctext%7Btotal%20rule%20of%20probability%20on%20r%20for%20P%20%7D%0A%5Cnewline%20&amp;%20%5Cblacksquare%20&amp;&amp;%20%5Cqquad%0A%5Cend%7Balign*%7D%0A%5Ctag%7B14%7D"></span></p>
</section>
</section>
<section id="sec-l2g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g3">The importance of the policy gradient theorem</h2>
<p>Crucially, the policy gradient theorem eliminates the need to estimate the gradient of the stationary distribution (<img src="https://latex.codecogs.com/png.latex?%5Cmu">), making the gradient much easier to estimate from experience. This sets the stage for building incremental policy gradient algorithms, which will be discussed in the next lecture.</p>
</section>
</section>
<section id="lesson-3-actor-critic-for-continuing-tasks" class="level1">
<h1>Lesson 3: Actor-Critic for Continuing Tasks</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Derive a sample-based estimate for the gradient of the average reward objective #</label></li>
<li><label><input type="checkbox" checked="">Describe the actor-critic algorithm for control with function approximation, for continuing tasks #</label></li>
</ul>
</div>
</div>
<section id="sec-l3g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g1">Derive a sample-based estimate for the gradient of the average reward objective</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7Bt+1%7D%20%5Cdoteq%20%5Ctheta_t%20+%20%5Calpha%20%5Cfrac%7B%20%5Cnabla_%20%5Cpi%20(a_t%20%5Cmid%20s_t,%20%5Ctheta)%7D%7B%5Cpi%20(a_t%20%5Cmid%20s_t,%20%5Ctheta)%7D%20q_%5Cpi(s_t,%20a_t)%20%5Cqquad%20%5Ctext%7B()%7D%0A"></p>
<p><span id="eq-gradient-ascent"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7Bt+1%7D%20=%20%5Ctheta_t%20+%20%5Calpha%20%5Cnabla_%5Ctheta%20ln%20%5Cpi(a_t%20%5Cmid%20s_t,%20%5Ctheta)%20q_%5Cpi(s_t,%20a_t)%20%5Cqquad%20%5Ctext%7B()%7D%0A%5Ctag%7B15%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is the step size and <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta%20J(%5Ctheta)"> is the gradient of the objective function <img src="https://latex.codecogs.com/png.latex?J(%5Ctheta)"> with respect to the policy parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<p><span id="eq-log-derivative"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla%20%5Cln%20(f(x))%20=%20%5Cfrac%7B%5Cnabla%20f(x)%7D%7Bf(x)%7D%20%5Cqquad%20%5Ctext%7B(log%20derivative)%7D%0A%5Ctag%7B16%7D"></span></p>
</section>
<section id="reinforce-algorithm-extra" class="level2">
<h2 class="anchored" data-anchor-id="reinforce-algorithm-extra">Reinforce Algorithm (Extra)</h2>
<p>The reinforce algorithm isn’t covered in the course. However, it is in the readings. Also the reinforce algorithm is said to be the most direct implementation of the policy gradient theorem. Finaly the reinforce algorithm is used in one of my research projects and this seems to be a great opportunity to understand it better.</p>
<p>So without further ado, let’s dive into the reinforce algorithm.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-30-reinfoce.png" class="img-fluid figure-img"></p>
<figcaption>Reinforce Algorithm</figcaption>
</figure>
</div>
<p><strong>Reinforce</strong> reveals the main issues with the policy gradient theorem. While the policy gradient theorem provides an unbiased estimate of the gradient of the average reward objective, it is a high variance estimator. This means that the gradient is very noisy and can lead to slow learning.</p>
<p>One wat to reduce the variance of the policy gradient theorem is to use a baseline. A baseline is a function that is subtracted from the reward to reduce the variance of the policy gradient theorem. Subtracting the baseline does not change the expected value of the gradient<sup>3</sup>, but it can reduce the variance of the gradient estimate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-31-reinforce-with-baselines.png" class="img-fluid figure-img"></p>
<figcaption>Reinforce Algorithm</figcaption>
</figure>
</div>
<p>the change is in the last three lines. The baseline is subtracted from the return G and the gradient is scaled by the baseline.</p>
</section>
<section id="sec-l3g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g2">Describe the actor-critic algorithm for control with function approximation, for continuing tasks</h2>
</section>
</section>
<section id="lesson-4-policy-parameterizations" class="level1">
<h1>Lesson 4: Policy Parameterizations</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Derive the actor-critic update for a softmax policy with linear action preferences #</label></li>
<li><label><input type="checkbox" checked="">Implement this algorithm #</label></li>
<li><label><input type="checkbox" checked="">Design concrete function approximators for an average reward actor-critic algorithm #</label></li>
<li><label><input type="checkbox" checked="">Analyze the performance of an average reward agent #</label></li>
<li><label><input type="checkbox" checked="">Derive the actor-critic update for a gaussian policy #</label></li>
<li><label><input type="checkbox" checked="">Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions #</label></li>
</ul>
</div>
</div>
<section id="actor-critic-with-softmax-policies-video" class="level2">
<h2 class="anchored" data-anchor-id="actor-critic-with-softmax-policies-video">Actor-Critic with Softmax Policies (video)</h2>
<p>Adam White discusses one specific implementation of the actor-critic reinforcement learning algorithm using a linear function approximation of the action value with tile coding and a Softmax policy parameterization.</p>
<p>Actor-critic methods combine direct policy optimization (actor) with value estimation (critic) using temporal difference learning.</p>
<p>The critic evaluates the policy by updating state value estimates, while the actor updates policy parameters based on feedback from the critic. This implementation is designed for finite action sets and continuous states. It employs a Softmax policy that maps state-dependent action preferences to probabilities, ensuring these probabilities are positive and sum to one. Each state effectively has its own Softmax distribution, and actions are sampled proportionally to these probabilities.</p>
<p>Both the value function and action preferences are parameterized linearly. The critic uses a feature vector representing the current state to estimate the value function. For the actor, the action preferences depend on both state and action, necessitating a state-action feature vector. The parameterization requires duplicating state feature vectors for each action, resulting in a policy parameter vector (θ) larger than the critic’s weight vector (W).</p>
<p>The algorithm’s update equations include:</p>
<p>Critic Update: A straightforward semi-gradient TD update using the feature vector scaled by the temporal difference residual (TDR). Actor Update: A more complex gradient that involves two components: State-action features for the selected action. A sum over all actions of state-action features scaled by the policy probabilities.</p>
</section>
<section id="sec-l4g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g1">Derive the actor-critic update for a softmax policy with linear action preferences</h2>
<p>The critic’s update rule is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%CE%B1%5E%5Cmathbf%7Bw%7D%20%5Cdelta%20%5Cnabla%20%5Chat%7Bv%7D(S,w)%0A"></p>
<p>which uses semigradient TD(0) to update the value function.</p>
<p>The actor uses the tf-error from the critic to update the policy parameters: <img src="https://latex.codecogs.com/png.latex?%0A%CE%B8%20%5Cleftarrow%20%CE%B8%20+%20%CE%B1%5E%CE%B8%20%CE%B4%20%E2%88%87%20%5Cln%20%5Cpi%20(A%20%5Cmid%20S,%5Ctheta)%0A"></p>
<p>policy update with a softmax policy is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%5Cdoteq%20%5Cfrac%7Be%5E%7Bh(s,%20a,%20%5Ctheta)%7D%7D%7B%5Csum_%7Bb%5Cin%20%5Cmathcal%7BA%7D%7D%20e%5E%7Bh(s,%20b,%20%5Ctheta)%7D%7D%0A"></p>
<p>this is like having a different softmax for each state</p>
<p><img src="https://orenbochman.github.io/posts/pg-21.png" class="img-fluid"> <img src="https://orenbochman.github.io/posts/pg-20.png" class="img-fluid"> <img src="https://orenbochman.github.io/posts/pg-22-features.png" class="img-fluid" alt="stacking"></p>
<p>Feature of the action preferences function</p>
<p>for the critic <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(s,w)%20%5Cdoteq%20w%5ET%20x(s)%0A"></p>
<p>for the actor</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah(s,a,%CE%B8)%20%5Cdoteq%20%CE%B8%5ET%20x_h(s,a)%0A"></p>
<p>we can do this by stacking</p>
<p>So with the softmax policy the critic’s update is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw%20%20%5Cleftarrow%20w%20+%20%CE%B1%5Ew%20%5Cdelta%20x(s)%0A"></p>
<p>and the actor’s update to the preferences looks as follows.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla%20%5Cln%20%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20=%20x_h(s,a)%20-%20%5Csum_b%20%5Cpi(b%20%5Cmid%20s,%20%5Ctheta)%20x_h(s,b)%0A"></p>
<p>The gradient has two parts.</p>
<p>The first is the state action features for the selected action xh(s,a).</p>
<p>The second part is the state action features multiplied by the policy summed over all actions ∑ b π(b|s,θ)xh(s,b).</p>
</section>
<section id="sec-l4g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g2">Implement this algorithm</h2>
</section>
<section id="sec-l4g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g3">Design concrete function approximators for an average reward actor-critic algorithm</h2>
</section>
<section id="sec-l4g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g4">Analyze the performance of an average reward agent</h2>
</section>
<section id="sec-l4g5" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g5">Derive the actor-critic update for a gaussian policy</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-11-actor-critic.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Actor-Critic</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-12-actor-critic-alg.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Actor-Critic Continuing</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/pg-32-actor-critic.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Actor-Critic Episodic</figcaption>
</figure>
</div>
</section>
<section id="sec-l4g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g6">Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discussion prompt
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Are tasks really ever continuing? Everything eventually breaks or dies. It’s clear that individual people do not learn from death, but we don’t live forever. Why might the continuing problem formulation be a reasonable model for long-lived agents?</p>
</blockquote>
</div>
</div>
<div id="fig-chapter-13-policy-gradient" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chapter-13-policy-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="ch13-policy-gradient.pdf" class="col-page" width="700" height="1000"></p>
<figcaption>Chapter 13</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chapter-13-policy-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Chapter 13 of <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement]</span> covering policy gradient methods.
</figcaption>
</figure>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>which models a single roll of a die based on its historical performance. It generalizes the Bernoulli and a special case of the Multinomial for a single trial↩︎</p></li>
<li id="fn2"><p>A step that does not logically follow from the previous one↩︎</p></li>
<li id="fn3"><p>the bias↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c3-w4.html</guid>
  <pubDate>Wed, 03 Apr 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Control with Approximation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c3-w3.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div id="fig-episodic-semi-gradient-sarsa" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-episodic-semi-gradient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithm decision tree</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-episodic-semi-gradient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The algorithms we will be discussing in this lesson are function approximation versions of SARSA, Expected SARSA, and Q-learning. These are all sample-based algorithms that solve the Bellman equation for action-values. They differ in how they estimate the action-values and how they update them.
</figcaption>
</figure>
</div></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§10 pp. 243-246]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=243">book</a></label></li>
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§10.3 pp. 249-252]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=249">book</a></label></li>
</ul>
</div>
</div>
</div>
<section id="lesson-1-episodic-sarsa-with-function-approximation" class="level1 page-columns page-full">
<h1>Lesson 1: Episodic SARSA with Function Approximation</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Explain</em> the update for Episodic SARSA with function approximation #</label></li>
<li><label><input type="checkbox" checked=""><em>Introduce</em> the feature choices, including passing actions to features or stacking state features #</label></li>
<li><label><input type="checkbox" checked=""><em>Visualize</em> value function and learning curves #</label></li>
<li><label><input type="checkbox" checked=""><em>Discuss</em> how this extends to Q-learning easily, since it is a subset of Expected SARSA #</label></li>
</ul>
</div>
</div>
<section id="episodic-sarsa-with-function-approximation-video" class="level2">
<h2 class="anchored" data-anchor-id="episodic-sarsa-with-function-approximation-video">Episodic SARSA with function approximation (Video)</h2>
<p>In this video, Adam White, discusses the algorithm for “Episodic SARSA with function approximation”. He explains how it can be used to solve reinforcement learning problems with large or continuous state-spaces. He also delineates the importance of feature choices in this algorithm and how they can impact the performance of the system.</p>
</section>
<section id="sec-l1g2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g2">Two ways to construct action dependent features?</h2>

<div class="no-row-height column-margin column-container"><div id="fig-feature-stacking" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-feature-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/nn-feature-stacking.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Stacking</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-feature-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Stacking involves concatenating the state features with the action features
</figcaption>
</figure>
</div><div id="fig-feature-inputs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-feature-inputs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/nn-feature-inputs.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Passing</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-feature-inputs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Passing Actions to Features involves passing the state features through a neural network that also takes the action as input
</figcaption>
</figure>
</div></div>
<p>We see two techniques for constructing action dependent features. It is worthwhile noting that these two techniques are quite generally used in Machine Learning. For example Concatenating is used in CNN and with multi-head attention in Transformers.</p>
<ol type="1">
<li><strong>Stacking</strong> involves concatenating the state features with the action features. This is a simple and effective way to construct action dependent features.</li>
</ol>
<p>Stacking can used both for linear function approximation and for neural networks in much the same way. Stacking is very simple but it has a problem, it keeps the same state features used for different actions separate. This seems to be both an over-parameterization (i.e.&nbsp;overfitting) and a an impediment to learning a good representation of the state.</p>
<ol start="2" type="1">
<li><strong>Passing Actions to Features</strong> attempts to remedy this issue. In this technique, the state features are passed through a neural network that also takes the action as input. This allows the network to learn a better representation of the state that is dependent on both the state and the action. This technique is more complex but can lead to better performance.</li>
</ol>
</section>
<section id="how-to-use-sarsa-in-episodic-tasks-with-function-approximation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="how-to-use-sarsa-in-episodic-tasks-with-function-approximation">How to use SARSA in episodic tasks with function approximation</h2>
<p>Next we will see how to use SARSA in episodic tasks with function approximation. The main idea is to use a similar update rule as before, but with the action value function approximated by a function approximator, i.e.&nbsp;it will be parametrized by weights <strong>w</strong>. Also we will need to add a the gradient of the to the update rule. As we will want to move the weights in the direction of the gradient of the action value function.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SARSA
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-sarsa" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-pseudocode-number="1" data-line-number="true" data-line-number-punc=":" data-no-end="false" data-indent-size="1.2em" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{SARSA($\alpha,\epsilon$)}\begin{algorithmic} \State Initialize: \State $\qquad Q(s,a) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S^+}, a\in\mathcal{A}(s)\ except\ for\ Q(terminal,\cdot)=0$ \For {each episode e:} \State Initialize $S$ \State Choose A from S using policy derived from Q (e.g., $\epsilon$-greedy) \For {each step of e} \State Take action A, observe R, S' \State Choose A' from S' using policy derived from Q (e.g., $\epsilon$-greedy) \State $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma Q(S',A') - Q(S, A)]$ \State $S \leftarrow S'$; $A \leftarrow A'$ \EndFor \State until $S$ is terminal \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<p>SARSA is a sample-based algorithm to solve the Bellman equation for action-values.</p>
<ul>
<li>It picks an action based on the current policy and then</li>
<li>It policy evaluation by a TD updates of Q the action-value function based on the reward and the next action.</li>
<li>Then it does a policy improvement.</li>
</ul>

<div class="no-row-height column-margin column-container"><div id="fig-episodic-semi-grafient-sarsa" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-episodic-semi-grafient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/episodic-semi-grafient-sarsa.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Episodic SARSA with function approximation</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-episodic-semi-grafient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: This figure shows the algorithm for Episodic semi-gradient SARSA which uses function approximation. The update rule is modified from tabular case, with an approximate action value function <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)"> as well as the inclusion of the gradient of the action value function <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Chat%7Bq%7D%20(S_t,%20A_t,%20%5Cmathbf%7Bw%7D)"> . These two modification allows us to update the weights of the function approximator The rest of the algorithm remains unchanged.
</figcaption>
</figure>
</div></div><p><span class="citation" data-cites="Rummery1994OnlineQU">[@Rummery1994OnlineQU]</span> introduced SARSA, but the name is due to Rich Sutton.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Episodic Semi-gradient SARSA
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-td-sarsa" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-pseudocode-number="2" data-line-number="true" data-line-number-punc=":" data-no-end="false" data-indent-size="1.2em" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{Episodic Semi-gradient SARSA for estimating ($\hat{q} \approx q_*$)}\begin{algorithmic} \State $\textbf{Input:}$ \State $\qquad \text{a differentiable action-value fn parameterization } \hat{q}: \mathcal{S} \times A \times \mathbb{R}^d \to \mathbb{R}$ \State $\textbf{Initialize:}$ \State $\qquad$ value function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily, (e.g., $\mathbf{w} = 0$) \For {each episode e:} \State Initialize $S$ \State Choose A from S using policy derived from $\color{red}\hat{q}(S'A,\mathbf{w})$ (e.g., $\epsilon$-greedy) \For {each step of e} \State Take action A, observe R, S' \If {$S'$ is terminal} \State $\color{red}\mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ R - \hat{q}(S, A, \mathbf{w}) \right] \nabla \hat{q}(S, A, \mathbf{w})$ \State Go to next episode \EndIf \State Choose A' as a function of $\color{red}\hat{q}(S'A,\mathbf{w})$ (e.g., $\epsilon$-greedy) \State $\color{red}\mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ R + \gamma \hat{q}(S', A', \mathbf{w}) - \hat{q}(S, A, \mathbf{w}) \right] \nabla \hat{q}(S, A, \mathbf{w})$ \State $S \leftarrow S'$; \State $A \leftarrow A'$ \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<p>This allows us to learn a good policy for the task. We will also need a step to update the weights.</p>

<div class="no-row-height column-margin column-container"><div id="fig-projected-bellman-error" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-projected-bellman-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/nFM5kLJl34c" title="Advances in Value Estimation in Reinforcement Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-projected-bellman-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Talk titled ‘Advances in Value Estimation in Reinforcement Learning’ at London Machine Learning Meetu by Martha White on Two Pieces of Research on Exploration in Reinforcement Learning. On work from [patterson2024generalizedprojectedbellmanerror]
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Over Thinking Semi-gradient SARSA {.unnumbered}:
</div>
</div>
<div class="callout-body-container callout-body">
<p>So you think you understand SARSA with function approximation?</p>
<ol type="1">
<li><p>Why is this a semi-gradient method?</p>
<p>Hints:</p>
<ul>
<li>what is the MSE td error of the Q-value the next state?</li>
</ul></li>
</ol>
<p>If we define the MSE of the Q-value of the next state as the projected Bellman error, then we can see that the update rule is a projected gradient descent on the projected Bellman error. This is a semi-gradient method because we are not using the true value of the next state in the update rule.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7B%7BQE%7D%7D(w)%20%5Cdoteq%20%5Csum_%7Bs%5Cin%20%5Cmathcal%7BS%7D,a%5Cin%20%5Cmathcal%7BA%7D%7D%20%5Cmu(s)%5Cleft%5B%20%20q%5E*(s,a)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%20%5Cright%5D%5E2%0A"></p>
<p>since we dont know the action value function we can’t compute the true value of the next state. Instead we use the value of the next state.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%5Coverline%7B%7BQE%7D%7D(w)%20&amp;%5Capprox%20%5Csum_%7Bs%5Cin%20%5Cmathcal%7BS%7D,a%5Cin%20%5Cmathcal%7BA%7D%7D%20%5Cmu(s)%5Cleft%5B%20r+%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%20%5Cright%5D%5E2%20%5C%5C%0A%20%20&amp;=%20%5Cmathbb%7BE%7D_%5Cmu%20%5Cleft%5B%20(r+%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D))%5E2%20%5Cright%5D%0A%5Cend%7Balign*%7D%0A"></p>
<p>then we have the following objective function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20J(w)%20=%20%5Cmathbb%7BE%7D_%5Cmu%20%5Cleft%5B%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%5Cgamma%5Et%20%20%5Cleft(%20r_t%20+%20%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%5E2%20%5Cright%5D%0A"></p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmu"> is the state visitation distribution</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)"> is the approximated action value function</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> is the weight vector</li>
<li><img src="https://latex.codecogs.com/png.latex?r_t"> is the reward at time step t</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the discount factor</li>
</ul></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla_%5Cmathbf%7Bw%7D%20J(w)%20&amp;=%20%5Cmathbb%7BE%7D_%5Cmu%20%5Cleft%5B%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%5Cgamma%5Et%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Cleft(%20r_t%20+%20%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%5E2%20%5Cright%5D%20%5C%5C%0A&amp;=%20%5Cmathbb%7BE%7D_%5Cmu%20%5Cleft%5B%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%5Cgamma%5Et%202%20%5Cleft(%20r_t%20+%20%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Cleft(%20r_t%20+%20%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%20%5Cright%5D%20%5Cnewline%0A&amp;=%20%5Cmathbb%7BE%7D_%5Cmu%20%5Cleft%5B%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%5Cgamma%5Et%202%20%5Cdelta%20%20%5Cleft(%20%5Cgamma%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%20%5Cright%5D%20%5C%5C%0A%5Cend%7Balign*%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20=%20r_t%20+%20%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)"> is the TD error.</p>
<p>this is the projected gradient of the projected Bellman error.</p>
<p>which would give us the following update rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5Cdelta%20%5Cleft(%20%5Cgamma%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%0A"></p>
<p>rather than the update rule we have in the algorithm:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5Cdelta%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bq%7D(S_%7Bt+1%7D,A_%7Bt+1%7D,%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(S_t,A_t,%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bq%7D(S_t,A_t,%5Cmathbf%7Bw%7D)%0A"></p>
<ol type="1">
<li>Can we do better than this?</li>
</ol>
<p>Hint: using a projected Bellman error, we may get a better algorithm than SARSA with function approximation.</p>
<ol type="1">
<li>Do we have any convergence guarantees for SARSA with function approximation?</li>
</ol>
<p>In the text book <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement sec. 10.1]</span> the authors state that for a constant policy <sup>1</sup>, this method converges in the same way that TD(0) does, with the same kind of error bound. Then they go on to say that for a non-constant policy, the convergence is a matter of ongoing research.</p>
<p>For the tabular setting, <span class="citation" data-cites="singh2000convergence">[@singh2000convergence]</span> the authors show that the algorithm has asymptotic converges to the optimal policy provided that the policies from the policy improvement operator is “greedy in the limit with infinite exploration”</p>
<p>For the function approximation setting, Empirical results show that the linear SARSA can chatter, i.e.&nbsp;the weight vector does not go to infinity (i.e., it does not diverge) but oscillates in a bounded region.</p>
<ol type="1">
<li>Since it is semi-gradient, we are not using the true value of the next state in the update rule.</li>
<li>Are we biased?</li>
<li>Does this algorithm have lower variance than the TD(0) algorithm?</li>
<li>When we choose the next action, are we on policy or off policy?</li>
<li>In the non terminal rule we use two samples, but they are highly correlated. Why is this a problem</li>
<li>How can we reduce this correlation?</li>
<li>What is the policy improvement operator?</li>
<li>What is the Lipschitz constant of this operator and why don’t we see this in the SARSA algorithm?</li>
</ol>
</div>
</div>
</section>
<section id="sec-l1g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g1">The update for Episodic SARSA with function approximation</h2>
<p>So far we have only been using function approximation to parametrize state value function,</p>
<p><span id="eq-parametize-state-value-function"><img src="https://latex.codecogs.com/png.latex?%0AV_%5Cpi(s)%20%E2%89%88%20%5Chat%7Bv%7D(s,w)%20%5Cdoteq%20%5Cmathbf%7Bw%7D%5ET%20%5Ccdot%20%5Cmathbf%7Bx%7D(S)%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<p>For SARSA we need a parametrized approximation <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D"> for the the action value function <img src="https://latex.codecogs.com/png.latex?q_pi">,</p>
<p><span id="eq-parametrized-action-value-function"><img src="https://latex.codecogs.com/png.latex?%0Aq_%5Cpi(s,a)%20%E2%89%88%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cdoteq%20%5Cmathbf%7Bw%7D%5ET%20%5Ccdot%20%5Cmathbf%7Bx%7D(s,a)%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>
</section>
<section id="episodic-sarsa-in-mountain-car-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="episodic-sarsa-in-mountain-car-video">Episodic SARSA in Mountain Car (Video)</h2>

<div class="no-row-height column-margin column-container"><div id="fig-mountain-car-environment" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-environment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/mountain-car-environment.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-environment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: The mountain car environment
</figcaption>
</figure>
</div></div></section>
<section id="feature-choices-in-episodic-sarsa-with-function-approximation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="feature-choices-in-episodic-sarsa-with-function-approximation">Feature Choices in Episodic SARSA with Function Approximation</h2>

<div class="no-row-height column-margin column-container"><div id="fig-mountain-car-feature-representations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-feature-representations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/mountain-car-feature-representations.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-feature-representations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: The feature representations for the mountain car problem
</figcaption>
</figure>
</div></div><p>What features do we use for the mountain car problem?</p>
<p>for the state:</p>
<ul>
<li>position</li>
<li>velocity</li>
</ul>
<p>for the action:</p>
<ul>
<li>accelerate left</li>
<li>accelerate right</li>
<li>do nothing</li>
</ul>
</section>
<section id="sec-l1g3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g3">Visualizing Value Function and Learning Curves</h2>

<div class="no-row-height column-margin column-container"><div id="fig-mountain-car-learned-values" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-learned-values-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/mountain-car-learned-values.png" class="img-fluid figure-img" data-group="slides">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-learned-values-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: The value function for the mountain car problem
</figcaption>
</figure>
</div><div id="fig-mountain-car-learned-values-trajectory" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-learned-values-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/mountain-car-learned-values-trajectory.png" class="img-fluid figure-img" data-group="slides">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-learned-values-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: The trajectory through the state space for the mountain car problem
</figcaption>
</figure>
</div><div id="fig-mountain-car-learning-curve" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-learning-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/mountain-car-learning-curve.png" class="img-fluid figure-img" data-group="slides">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-learning-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: The learning curve for the mountain car problem
</figcaption>
</figure>
</div></div>

<p>The first two figures show the learned value function for the mountain car problem. The first figure shows the value function for each state. The second shows a possible trajectory through the state space.</p>
<p>Then we look at the learning curve for the mountain car problem. This shows how the value function improves over time as the agent learns the optimal policy. We see the familiar exponential decay in the learning curves.</p>
<p>It worth noting that this is a very simple environment and that many more sophisticated deep learning techniques don’t do a very good job on this problem.</p>
</section>
<section id="expected-sarsa-with-function-approximation-video" class="level2">
<h2 class="anchored" data-anchor-id="expected-sarsa-with-function-approximation-video">Expected SARSA with Function Approximation (Video)</h2>
<p>Now we extend SARSA with under function approximation into Expected SARSA .</p>
<p>First, recall the update rule for Tabular SARSA:</p>
<p><span id="eq-tabular-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,A_t)%20%5Cleftarrow%20Q(S_t,A_t)%20+%20%5Calpha%20(R_%7Bt+1%7D%20+%20%5Cgamma%20Q(%20%5Ctextcolor%7Bred%7D%7B%20S_%7Bt+1%7D,%20A_%7Bt+1%7D%20%7D)%20-%20Q(S_t,A_t))%20%5Cqquad%0A%5Ctag%7B3%7D"></span></p>
<p>SARSA’s update target includes the action value for the next state in action.</p>
<p>Next, recall how Tabular Expected SARSA uses the expectation over its target policy instead.</p>
<p><span id="eq-tabular-expected-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,A_t)%20%5Cleftarrow%20Q(S_t,A_t)%20+%20%5Calpha%20(R_%7Bt+1%7D%20+%20%5Cgamma%20%5Ctextcolor%7Bred%7D%7B%20%5Csum_%7Ba'%7D%20%5Cpi%20(a'%20%5Cmid%20S_%7Bt+1%7D)%20Q(S_%7Bt+1%7D,a')%7D%20-%20Q(S_t,A_t))%20%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<p>We can compute the same expectation using function approximation.</p>
<p>First, recall the update for SARSA with function approximation. It looks similar to the tabular setting except the action value estimates are parameterized by the weight factor, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D">. i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?q_%5Cpi%20(s,%20a)%20%5Capprox%20%5Chat%7Bq%7D%20(s,%20a,%20%5Cmathbf%7Bw%7D)"></p>
<p><span id="eq-semi-gradient-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bq%7D(%5Ctextcolor%7Bred%7D%20%7B%20S_%7Bt+1%7D%20%7D%20,%20%5Ctextcolor%7Bred%7D%20%7B%20A_%7Bt+1%7D%7D,%20%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(S_t,%20A_t,%20%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bq%7D%20(S_t,%20A_t,%20%5Cmathbf%7Bw%7D)%20%5Cqquad%0A%5Ctag%7B5%7D"></span></p>
<p>Expected Sarsa with function approximation follows a similar structure.</p>
<p><span id="eq-semi-gradient-expected-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%20%20%5Ctextcolor%7Bred%7D%7B%20%5Csum_%7Ba'%7D%20%5Cpi(%20a'%20%5Cmid%20S_%7Bt+1%7D)%20%5Chat%7Bq%7D%20(S_%7Bt+1%7D,%20a'%20,%20%5Cmathbf%7Bw%7D%20)%7D%20-%20%5Chat%7Bq%7D(S_t%20,%20A_t%20,%20%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bq%7D(S_t%20,%20A_t%20,%20%5Cmathbf%7Bw%7D)%20%5Cqquad%0A%5Ctag%7B6%7D"></span></p>
</section>
<section id="sec-l1g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g4">How this extends to Q-learning easily, since it is a subset of Expected SARSA</h2>
<p>Finally, for Q-learning with function approximation as Q-learning is a special case of expected SARSA in which we take the greedy. So next we replace the expectation over the target policy by argmax action to derive the Q-learning update rule.</p>
<p>The Q-learning update rule is:</p>
<p><span id="eq-semi-gradient-Q-learning-update"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%20%5Ctextcolor%7Bred%7D%7B%20%5Cmax_%7Ba'%7D%20%5Chat%7Bq%7D(S_%7Bt+1%7D,a',%5Cmathbf%7Bw%7D)%7D%20%E2%88%92%5Chat%7Bq%7D(S_t,A_t,%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bq%7D(S_t,A_t,%5Cmathbf%7Bw%7D)%20%5Cqquad%0A%5Ctag%7B7%7D"></span></p>
</section>
</section>
<section id="lesson-2-exploration-under-function-approximation" class="level1">
<h1>Lesson 2: Exploration under Function Approximation</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Understanding</em> optimistically initializing your value function as a form of exploration #</label></li>
</ul>
</div>
</div>
<section id="exploration-under-function-approximation-video" class="level2">
<h2 class="anchored" data-anchor-id="exploration-under-function-approximation-video">Exploration under Function Approximation (Video)</h2>
</section>
<section id="sec-l2g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g1">Optimistic Initialization as a Form of Exploration</h2>
<p>In the tabular setting, we saw that we could use optimistic initialization as a form of early exploration. The way this works is that we initialize the value function to be very high, this way the agent will be encouraged to try to exploit one of the states. It will discover that the value of the state is not as high as it thought and will try to exploit another and so on until it have visited all the states and learned more realistic values for them. Over time the effect of the initial values will diminish. This however assumes two things:</p>
<ol type="1">
<li>the number of states is finite</li>
<li>the values are independent of each other</li>
</ol>
<p>In the function approximation setting, we can try to do the same thing. This time we will want to initialize the weights so as to make the value function high. We face a number of issues in this setting:</p>
<ol type="1">
<li>the values are not independent of each other so each weight may loose its optimistic value long before the agent has explored many of the states within the features neighborhood in the state space.</li>
<li>Unlike values we can’t be certain that high weights will lead to high values. While this may work for linear function approximation, for a non-linear function approximation using Neural Networks with tanh activation functions, positive weights may lead to negative values.</li>
</ol>
<p>epsilon-greedy exploration is easy to implement even with non-linear function approximation. However it is not as effective in the function approximation setting. Because it relies on randomness to explore states near those followed by the current policy. This is not as systematic as the exploration due to optimistic initialization in the tabular setting.</p>
<p>Improving exploration with function approximation is an open research question</p>
<p>In this course we will stick to epsilon-greedy exploration.</p>
<p>Q. Why is epsilon-greedy exploration ineffective in the function approximation setting?</p>
<ol type="1">
<li>like in the bandit setting, the agent keeps exploring even after it has found the optimal policy.</li>
<li>like in environments with a changing maze multiple epsilon-greedy exploration steps may be required to explore states required by the optimal policy that are not near the current policy. The chance for such an exploration is <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)%5Cepsilon%5En"> where <img src="https://latex.codecogs.com/png.latex?mu(s)"> is importance of the nearest state and n is the number of steps required to reach the target state. This can be vanishingly small for large state spaces. Which means it can take too long to find the optimal policy using epsilon-greedy exploration. We need to think of better ways to organize exploration in the function approximation setting.</li>
<li>Prioritizing using count based on a coarse coding may be more effective. Even better if we track the uncertainty in the value function for each if these features. This is a form of intrinsic motivation.</li>
</ol>
<p>However the last video by Satinder Singh discusses using intrinsic motivation to improve exploration in reinforcement learning systems. And in it he shows a different paradigm of exploration. Rather than getting agents to explore systematic one wants to explore in a way that is interesting to the agent.</p>
</section>
</section>
<section id="lesson-3-average-reward" class="level1 page-columns page-full">
<h1>Lesson 3: Average Reward</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Describe</em> the average reward setting #</label></li>
<li><label><input type="checkbox" checked=""><em>Explain</em> when average reward optimal policies are different from discounted solutions #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how differential value functions are different from discounted value functions #</label></li>
</ul>
</div>
</div>
<section id="average-reward-a-new-way-of-formulating-control-problems-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="average-reward-a-new-way-of-formulating-control-problems-video">Average Reward: A New Way of Formulating Control Problems (Video)</h2>
<p>You probably never thought about it, since discounting is familiar like a geometric series, but it can really skew the value function.</p>

<div class="no-row-height column-margin column-container"><div id="fig-near-sighted-mdp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-near-sighted-mdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/near-sighted-mdp.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>The near sighted MDP</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-near-sighted-mdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: The near sighted MDP. The lower discounting causes the agent to prefer the smaller immediate reward over the larger delayed reward.
</figcaption>
</figure>
</div></div><p>In most states, there’s only one action, so there are no decisions to be made. There’s only one state were a decision can be made. In this state, the agent can decide which ring to traverse.</p>
<p>This means there are two deterministic policies, traversing the left ring or traversing the right ring. The reward is zero everywhere except for in one transition in each ring. In the left ring, the reward is +1 immediately after state S. In the right ring, the reward is +2 immediately before state S. Intuitively, you would pick the right action because you know you will get +2 reward. But what would the value function tell us to do?</p>
<p><strong>If we use discounting, what are the values of state S under these two different policies?</strong></p>
<p>The policy that chooses the left action has a value of <img src="https://latex.codecogs.com/png.latex?v_l(S)%20=%20%5Cfrac%7B1%7D%7B1-%5Cgamma%5E5%7D">. How do we figure this out? If you write out the infinite discounted return, you will see this is a fairly straightforward geometric series with a closed form solution.</p>
<p>The policy that chooses the right action has a value of <img src="https://latex.codecogs.com/png.latex?v_r(S)%20=%20%5Cfrac%7B2%20%5Cgamma%5E4%7D%7B1-%5Cgamma%5E5%7D">. Let’s think of the value of state S under these two part policies for particular values of <img src="https://latex.codecogs.com/png.latex?%5Cgamma">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cgamma=%200.5%20%5Cimplies%20V_L%20%5Capprox%201%20%5Cqquad%20V_R%20%5Capprox%200.1"></p>
<p>This means the policy that takes the left action is preferable under this more myopic discount.</p>
<p>Let’s try</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cgamma=%200.9%20%5Cimplies%20V_L%20%5Capprox%202.4%20%5Cqquad%20V_R%20%5Capprox%203.2"></p>
<p>So now we prefer the other policy.</p>
<p>In fact, we can figure out the minimum value of <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> so that the agent prefers the policy that goes right. <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> needs to be at least 0.841. So the problem here is that the discount magnitude depends on the problem.</p>
<p>For this example, 0.85 is sufficiently large. But if the rings had 100 states each, this discount factor would need to be over 0.99.</p>
<p>In general, the only way to ensure that the agents actions maximize reward over time is to keep increasing the discount factor towards 1.</p>
<p>Depending on the problem, we might need <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> to be quite large. And we can’t set it to 1 in a continuing setting as the return might be infinite.</p>
<p>Now, what’s wrong with having larger <img src="https://latex.codecogs.com/png.latex?%5Cgamma">? Larger values of <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> can also result in larger and more variables sums, which might be difficult to learn. So is there an alternative?</p>
</section>
<section id="sec-l3g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g1">The Average Reward Setting</h2>
<p><span id="eq-average-reward"><img src="https://latex.codecogs.com/png.latex?%0Ar(%5Cpi)%20%5Cdoteq%20%5Clim%20_%7Bh%20%5Cto%20%5Cinfty%7D%20%5Cfrac%7B1%7D%7Bh%7D%20%5Csum_%7Bt=1%7D%5E%7Bh%7D%20%5Cmathbb%7BE%7D%5BR_t%20S_0,A_%7B0:t%E2%88%921%7D%20%5Csim%20%5Cpi%5D%20%20%5Cqquad%0A%5Ctag%7B8%7D"></span></p>
<p>Let’s discuss a new objective called the average reward. Imagine the agent has interacted with the world for H steps. This is the reward it has received on average across those H steps. In other words, it’s rate of reward. If the agents goal is to maximize this average reward, then it cares equally about nearby and distant rewards. We denote the average reward of a policy with <img src="https://latex.codecogs.com/png.latex?R_%5Cpi">.</p>
<p><span id="eq-average-reward-visitation"><img src="https://latex.codecogs.com/png.latex?%0Ar(%5Cpi)%20=%20%5Csum_%7Bs%7D%20%5Cmu_%5Cpi%20(s)%20%5Csum_%7Ba%7D%20%5Cpi(a%20%5Cmid%20s)%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)%20r%0A%5Ctag%7B9%7D"></span></p>
<p>More generally, we can write the average reward using the state visitation, <img src="https://latex.codecogs.com/png.latex?%5Cmu">. This inner term is the expected reward in a state under policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">. The outer sum takes the expectation over how frequently the policy is in that state. Together, we get the expected reward across states. In other words, the average reward for a policy.</p>
<p>In the nearsighted example, the two deterministic possible policies visit either the left loop or the right loop indefinitely. In both cases, the five states in each loop are visited equally many times. In the left loop, the immediate expected reward is +0 for all states except one, which gets +1. This results in an average reward of 1 every 5 steps or 0.2.</p>
<p><img src="https://latex.codecogs.com/png.latex?r(%5Cpi_L)=1/5=0.2%20%5Cqquad%20r(%5Cpi_R)=2/5=0.4"></p>
<p>Most states in the right loop also have +0 <img src="https://latex.codecogs.com/png.latex?%5Cmu"> to expected reward. But this time, the last state gets +2. This gives an average reward of 2 every 5 steps or 0.4.</p>

<div class="no-row-height column-margin column-container"><div id="fig-near-sighted-mdp-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-near-sighted-mdp-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/near-sighted-mdp-2.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Near sighted MDP return for Average rewards</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-near-sighted-mdp-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Return for the nearsighted MDP for average rewards. The average reward for the left policy is 0.4 and for the right policy is 1.4.
</figcaption>
</figure>
</div><div id="fig-near-sighted-mdp-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-near-sighted-mdp-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/near-sighted-mdp-3.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Near sighted MDP return for Average rewards</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-near-sighted-mdp-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Return for the nearsighted MDP for average rewards. The average reward for the left policy is <img src="https://latex.codecogs.com/png.latex?G_t=-1.8"> and for the right policy is <img src="https://latex.codecogs.com/png.latex?G_t=0.8">.
</figcaption>
</figure>
</div></div>
<p>We can see the average reward puts preference on the policy that receives more reward in total without having to consider larger and larger discounts.</p>
<p>The average reward definition is intuitive for saying if one policy is better than another, but <strong>how can we decide which actions from a state are better?</strong></p>
<p>What we need are action values for this new setting. The first step is to figure out what the return is. In the average reward setting, returns are defined in terms of differences between rewards and the average reward <img src="https://latex.codecogs.com/png.latex?R_%5Cpi">. This is called <strong>the differential return</strong>.</p>
<p><span id="eq-differential-return"><img src="https://latex.codecogs.com/png.latex?%0AG_t%20=%20R_%7Bt+1%7D%20%E2%88%92r_%5Cpi%20+%20R_%7Bt+2%7D%20%E2%88%92r_%5Cpi%20+%20R_%7Bt+2%7D%20%E2%88%92r_%5Cpi%20%5Cldots%20%5Cqquad%0A%5Ctag%7B10%7D"></span></p>
<p>Let’s look at what the <strong>differential returns</strong> are in our nearsighted MDP.</p>
<p>The differential return represents how much more reward the agent will receive from the current state in action compared to the average reward of the policy. Let’s look at the differential return starting in state <img src="https://latex.codecogs.com/png.latex?s">, first choosing action L and then following <img src="https://latex.codecogs.com/png.latex?%5Cpi"> L afterwards.</p>
<p>The average reward for this policy is 0.2.</p>
<p>The differential return is the sum of rewards into the future with the average reward subtracted from each one. This sum starts in state S with the action L. We can compute it by summing to some finite horizon H. Then taking the limit as H goes to infinity. We are simplifying things slightly with this limit notation. While notation provider works in many cases, we need to use a different technique when the environment is periodic. In this case, we compute the return using a more general limit called the <a href="https://en.wikipedia.org/wiki/Ces%C3%A0ro_summation">Cesàro sum</a>, but this technical detail is not critical. The main point here is the intuition. We find that the differential return is 0.4. Now, let’s look at the other action. This time, we can break the differential return into two parts. First the sum for a single trajectory through the right loop. We can write the sum explicitly and it’s equal to 1. Then the sum corresponding to taking the left action indefinitely. This sum is the same as the differential return we just computed, 0.4. Adding the two parts together, we find that the differential return is 1.4.</p>
<p>We can write the average reward using the state visitation, <img src="https://latex.codecogs.com/png.latex?%5Cpi">. This inner term is the expected reward in a state under policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">. The outer sum takes the expectation over how frequently the policy is in that state. Together, we get the expected reward across states. In other words, the average reward for a policy</p>
<p>In the average reward setting, returns are defined in terms of differences between rewards and the average reward <img src="https://latex.codecogs.com/png.latex?r_%5Cpi">, which is called the differential return. The differential return represents how much more reward the agent will receive from the current state in action compared to the average reward of the policy.</p>
<p>The differential return represents how much better it is to take an action in a state then on average under a certain policy. The differential return can only be used to compare actions if the same policy is followed on subsequent time steps. To compare policies, their average reward should be used instead</p>
<p>This quantity captures how much more reward the agent will get by starting in a particular state than it would get on average over all states if it followed a fixed policy.</p>
<p><span id="eq-differential-value-function"><img src="https://latex.codecogs.com/png.latex?%0Aq_%5Cpi(s,a)%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BG_t%20%5Cmid%20S_t%20=%20s,A_t%20=%20a%5D%20%5Cqquad%0A%5Ctag%7B11%7D"></span></p>
<p>Like in the discounted setting, differential value functions can be written as Bellman equations. They only differ in that they subtract r() from the immediate reward and there is no discounting. <span id="eq-differential-bellman"><img src="https://latex.codecogs.com/png.latex?%0Aq_%5Cpi(s,a)%20=%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)(r%20%E2%88%92r(%5Cpi))%20+%20%5Csum_%7Ba'%7D%20%5Cpi(a'%20%5Cmid%20s')q_%5Cpi(s',a')%20%5Cqquad%0A%5Ctag%7B12%7D"></span></p>
<p>Many algorithms from the discounted settings can be rewritten to apply to the average reward case.</p>
</section>
<section id="sec-l3g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g2">When Average Reward Optimal Policies are Different from Discounted Solutions</h2>
</section>
<section id="sec-l3g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g3">Differential Value Functions v.s. Discounted Value Functions</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Episodic Semi-gradient SARSA
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-differential-sg-sarsa" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-pseudocode-number="3" data-line-number="true" data-line-number-punc=":" data-no-end="false" data-indent-size="1.2em" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{Differential Semi-gradient SARSA for estimating ($\hat{q} \approx q_*$)}\begin{algorithmic} \State $\textbf{Input:}$ \State $\qquad \text{a differentiable action-value fn parameterization } \hat{q}: \mathcal{S} \times A \times \mathbb{R}^d \to \mathbb{R}$ \State $\textbf{Algorithm parameters:}$ \State $\qquad \alpha, \beta &gt; 0$ \State $\textbf{Initialize:}$ \State $\qquad$ value function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily, (e.g., $\mathbf{w} = 0$) \State $\bar{R} source \in \mathbb{R}$ avg reward estimate \State Initialize $S$ and action $A$ \For {each step} \State Take action A, observe R, S' \State Choose A' as a function of $\hat{q}(S',\cdot,\mathbf{w})$ (e.g., $\epsilon$-greedy) \State $\delta \leftarrow R - \bar{R} + \hat{q}(S', A', \mathbf{w}) - \hat{q}(S, A, \mathbf{w}) $ \State $ \bar{R}\leftarrow \bar{R} + \beta \delta$ \State $\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta \nabla \hat{q}(S, A, \mathbf{w})$ \State $S \leftarrow S'$; $A \leftarrow A'$ \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
</section>
<section id="satinder-singh-on-intrinsic-rewards-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="satinder-singh-on-intrinsic-rewards-video">Satinder Singh on Intrinsic Rewards (Video)</h2>

<div class="no-row-height column-margin column-container"><div id="fig-subtasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/L-9Pbhpk7pQ" title="From Reinforcement Learning to Artificial Intelligence ?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: This is a high level talk by Satinder Singh on AI and RL
</figcaption>
</figure>
</div><div id="fig-subtasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/jn1NE8uIxgw" title="Steps Towards Continual Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: This talk titled ‘Steps Towards Continual Learning’ by Satinder Singh on Reinforcement Learning at DLSS &amp; RLSS 2017 - Montreal
</figcaption>
</figure>
</div><div id="fig-subtasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Z7JhZx3urEY" title="Discovery in Reinforcement Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Talk titled ‘Discovery in Reinforcement Learning’ at Beijing Academy of Artificial Intelligence by Satinder Singh on Two Pieces of Research on Exploration in Reinforcement Learning.
</figcaption>
</figure>
</div></div>

<p>Satinder Singh is a professor at the University of Michigan. He is a leading researcher in reinforcement learning and has made significant contributions to the field. In this video, he discusses intrinsic rewards and how they can be used to improve learning in reinforcement learning systems. It’s worth noting that he is one of the researchers who has worked on options with Doina Precup.</p>
<p>Now Satinder Singh is a good speaker and he has lots of interesting research results to share. Unfortunately, this video is not his finest hour. I would definitely recommend watching some of his other talks linked above.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discussion prompt
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>What are the issues with extending some of the exploration methods we learned about bandits and Dyna to the full RL problem? How can we do visitation counts or UCB with function approximation?</p>
</blockquote>
<blockquote class="blockquote">
<p>A control agent with function approximation has to explore to find the best policy, learn a good state representation, and try to get a lot of reward, all at the same time. How might an agent balance these potentially conflicting goals?</p>
</blockquote>
</div>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>e.g.&nbsp;epsilon greedy policy without decay↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c3-w3.html</guid>
  <pubDate>Tue, 02 Apr 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Constructing Features for Prediction</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c3-w2.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.4-9.5.0, pp. 204-210]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.7, pp. 223-228]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
</ul>
</div>
</div>
</div>
<p>We discussed methods for representing large, an possibly continuous state spaces. Ways to construct features. A representation is an agent’s internal encoding of the state, the agent constructs features to summarize the current input. Whenever we are talking about features and representation learning, we are in the land of function approximation.</p>
</section>
<section id="lesson-1-feature-construction-for-linear-methods" class="level1 page-columns page-full">
<h1>Lesson 1: Feature Construction for Linear Methods</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Define</em> the difference between <strong>coarse coding</strong> and tabular representations #</label></li>
<li><label><input type="checkbox" checked=""><em>Explain</em> the trade-off when designing representations between discrimination and generalization #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how different coarse coding schemes affect the functions that can be represented #</label></li>
<li><label><input type="checkbox" checked=""><em>Explain</em> how tile coding is a (computationally?) convenient case of coarse coding #</label></li>
<li><label><input type="checkbox" checked=""><em>Describe</em> how designing the tilings affects the resultant representation #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> that tile coding is a computationally efficient implementation of coarse coding #</label></li>
</ul>
</div>
</div>
<section id="coarse-coding-video" class="level2">
<h2 class="anchored" data-anchor-id="coarse-coding-video">Coarse Coding (Video)</h2>
<p>In this video, Adam White introduces the concept of <strong>coarse coding</strong>, covering the first learning objective of this lesson.</p>
<p>Coarse coding are a way to represent states in a more general way than tabular representations. This allows for generalization across states. The trade-off is that the representation is less discriminative.</p>
</section>
<section id="sec-l1g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g1">The difference between <strong>coarse coding</strong> and tabular representations</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-coding_states.png" class="img-fluid figure-img"></p>
<figcaption>approximation</figcaption>
</figure>
</div></div><p>Recall that linear function approximation are paramertized by a weight vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> and a feature vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D(s)">.</p>
<p>As we saw in the previous unit tabular representations associates one feature per state, this is called a one-hot encoding of the state space.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-tabular_coding.png" class="img-fluid figure-img"></p>
<figcaption>one hot coding</figcaption>
</figure>
</div></div><p>We associate one hot encoding with an indicator function <img src="https://latex.codecogs.com/png.latex?%5Cdelta_%7Bij%7D(s)">. This is a very discriminative representation but it does generalize.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-state-aggregation.png" class="img-fluid figure-img"></p>
<figcaption>state aggregation</figcaption>
</figure>
</div></div><p>We also discussed using <strong>state aggregation</strong> for the 1000 state random walk example. In state aggregation we break the continuous state space into discrete regions and associate a feature with each region. This is a more general representation than tabular representations but less discriminative.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-coarse-coding.png" class="img-fluid figure-img"></p>
<figcaption>coarse coding</figcaption>
</figure>
</div></div><p><strong>Coarse coding</strong> uses multiple overlapping shapes to represent states. This is a more General representation than state aggregation but less discriminative. Features are the circles they are in. If the circles overlap, we can have items that are in multiple circles. I.e. they are characterized by multiple features. In the example shown there can be from one to three active features.</p>
<p>So the difference is that tabular representations are one hot encodings while coarse coding uses membership in multiple overlapping shapes to represent states.</p>
<p>How does coarse coding relates to state aggregation?</p>
<p>Coarse coding is also a generalization of state aggregation. In state aggregation we break the state space into discrete regions and associate a feature with each region. But we don’t let these regions overlap. In coarse coding we allow the regions to overlap which can give greater generalization as regions can share features.</p>
<p>In this video the term Reception Field is used to describe the region of the state space that a feature is associated with. This is an idea that comes from CNNs.</p>
</section>
<section id="generalization-properties-of-coarse-coding-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="generalization-properties-of-coarse-coding-video">Generalization Properties of Coarse Coding (Video)</h2>
<p>In this video Martha White discusses the generalization properties of coarse coding.</p>
<p>She looks at using small overlapping 1-d intervals to represent a 1-d function.</p>
<p>We see that changing shape size and number of effects the generalization properties of the representation.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/rl-scale-generalization.png" class="img-fluid figure-img"></p>
<figcaption>scale</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/rl-shape-generalization.png" class="img-fluid figure-img"></p>
<figcaption>shape</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/rl-shape-discrimination.png" class="img-fluid figure-img"></p>
<figcaption>discrimination</figcaption>
</figure>
</div></div>

<p>Next we looked at using short interval vs longer intervals to approximate a 1-d function. We see that the longer intervals give a smoother approximation.</p>
</section>
<section id="sec-l1g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g2">The trade-off between discrimination and generalization</h2>
</section>
<section id="tile-coding-video" class="level2">
<h2 class="anchored" data-anchor-id="tile-coding-video">Tile Coding (Video)</h2>
<p>In this video, Martha White introduces the concept of <strong>tile coding</strong>. This is simply a implementation of coarse coding using multiple overlapping grids.</p>
</section>
<section id="sec-l1g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g4">Explain how tile coding is a (computationally?) convenient case of coarse coding</h2>
<p>Tile coding is a computationally efficient implementation of coarse coding. We use multiple overlapping tilings to represent states. Each tiling is a grid of tiles. Each tile is a feature.</p>
<p>If we use one tiling we get state aggregation. If we use multiple tilings we get coarse coding. One tiling means we don’t discriminate between states that are in the same tile. Multiple tilings means we can discriminate between states that are in the same tile in one tiling but not in another.</p>
</section>
<section id="sec-l1g5" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g5">Describe how designing the tilings affects the resultant representation</h2>
<p>The textbook goes into some more details about how we can generalize using tile coding - using regular tilings generates in a diagonal pattern. Using random tilings generates more spherical regions.</p>
<p>However we also saw that the number size and shape of the tiles affects the generalization properties of the representation. And that increasing the overlap between the tiles an increase the discrimination properties of the representation.</p>
</section>
<section id="sec-l1g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g6"><em>Understand</em> that tile coding is a computationally efficient implementation of coarse coding</h2>
<p>Tile coding is a computationally efficient implementation of coarse coding. Since grids are uniform it is easy to compute which cells a state is in. A second reason is that end up with a sparse representations thus the dot product is just the sum of the weights of the active features for each state.</p>
<p>One caveat is that in high dimensional spaces we end up an exponential number of features. This is called the curse of dimensionality.</p>
</section>
<section id="using-tile-coding-in-td-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="using-tile-coding-in-td-video">Using Tile Coding in TD (Video)</h2>
<p>In this video, Adam White shows how to use tile coding in TD learning. He goes back to the 1000 state random walk example and shows how to use tile coding to approximate the value function. We end up needing six tiles.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-tile-coding-performance.png" class="img-fluid figure-img"></p>
<figcaption>tile coding v.s. state aggregation</figcaption>
</figure>
</div></div></section>
<section id="feature-construction-for-linear-methods" class="level2">
<h2 class="anchored" data-anchor-id="feature-construction-for-linear-methods">Feature Construction for Linear Methods</h2>
<p>In the textbook we see two forms of features for linear methods that are not covered in the videos.</p>
<p>The first are polynomials. We might use polynomials features for the state to represent the state space. This seems to be a good for problems where RL is dealing to a greater extent with interpolation or regression.</p>
<p>The following is given as an example of a polynomial feature representation of the state space. It took a bit of time to understand what was going on here.</p>
<p>They explain about the different combination of two features <img src="https://latex.codecogs.com/png.latex?s_1"> and <img src="https://latex.codecogs.com/png.latex?s_2"> doesn’t cover some edge cases but using four <img src="https://latex.codecogs.com/png.latex?(1,s_1,s_2,s_1s_2)"> covers all the possible combinations of the two features. We might also want to include higher powers of the atoms and that is what the polynomial representation is doing.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_i(s)%20=%20%5Cprod_%7Bj=1%7D%5Ek%20s_j%5E%7Bc_%7Bij%7D%7D%0A"></p>
<p>It important to point out that we are not using the polynomials as a function approximation basis function. What we are talking about is a formulation of multinomial from a set of fixed numbers <img src="https://latex.codecogs.com/png.latex?s_1%20%5Clsots%20s_k"> I.e. we are talking about all the possible products product from powers of these atoms.</p>
<p>The second are Fourier bases.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_i(s)%20=%20%5Ccos%5Cleft(%5Cfrac%7B2%5Cpi%20s%5ET%20a_i%7D%7Bb%7D%5Cright)%0A"></p>
<p>The book mentions that the Fourier basis is particularly useful for periodic functions.</p>
<p>There are many other orthogonal bases used as functnio expansions that could be used, as features for linear function approximation.</p>
<ul>
<li>Walsh functions and Haar wavelets have discrete support and are used in signal processing.</li>
<li>Legendre polynomials are used in physics.</li>
<li>Chebyshev polynomials are used in numerical analysis.</li>
</ul>
</section>
<section id="other-forms-of-coarse-coding" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="other-forms-of-coarse-coding">Other Forms of Coarse Coding</h2>
<p>In the textbook we see that there are other forms of coarse coding.</p>
<p>For example in section 9.5.5 we see using radial basis functions.</p>
<dl>
<dt>An RBF</dt>
<dd>
is a real-valued function whose value depends only on the distance between the input and a fixed point (called the center). &nbsp;
</dd>
</dl>
<p>Visualizing - Imagine a hill or bump centered at a specific point. The height of the hill at any other point depends solely on its distance from the center. The hill gets flatter as you move away from the cente</p>
<div class="page-columns page-full"><p> <img src="https://latex.codecogs.com/png.latex?%0Ax_i(s)%20=%20%5Cexp%5Cleft(-%5Cfrac%7B%5C%7Cs-c_i%5C%7C%5E2%7D%7B2%5Csigma_i%5E2%7D%5Cright)%0A"></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/posts/rl-radial-basis-functions.png" class="img-fluid" alt="one dimensional radial basis functions"></div></div>
<ul>
<li>Where</li>
<li><img src="https://latex.codecogs.com/png.latex?c_i"> is the center of the radial basis function and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csigma_i"> is the width.</li>
</ul>
<p>This is a form of coarse coding where the features are the distance from a set of centers. This is a more general representation than tile coding but less discriminative. The advantage of RBFs over tiles is that they are approximate functions that vary smoothly and are differentiable. However it appears there is both a computational cost and no real advantage in having continuous/differential features according to the book.</p>
<div id="cell-fig-radial-basis-functions" class="cell page-columns page-full" data-fig.height="3" data-fig.width="3" data-execution_count="1">

<div class="no-row-height column-margin column-container"><div class="cell-output cell-output-display">
<div id="fig-radial-basis-functions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-radial-basis-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/c3-w2_files/figure-html/fig-radial-basis-functions-output-1.png" width="662" height="470" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-radial-basis-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: One-dimensional radial basis functions with centers at -2, 0, and 2.
</figcaption>
</figure>
</div>
</div></div></div>
<p>I find this a bit disappointing as it seems like a nice intermediate step between linear function approximation with its convergence guarantees and neural networks which have no such guarantees.</p>
</section>
</section>
<section id="lesson-2-neural-networks" class="level1 page-columns page-full">
<h1>Lesson 2: Neural Networks</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Define</em> a neural network #</label></li>
<li><label><input type="checkbox" checked=""><em>Define</em> activation functions #</label></li>
<li><label><input type="checkbox" checked=""><em>Define</em> a feed-forward architecture #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how neural networks are doing feature construction #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how neural networks are a non-linear function of state #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how deep networks are a composition of layers #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> the tradeoff between learning capacity and challenges presented by deeper networks #</label></li>
</ul>
</div>
</div>
<section id="what-is-a-neural-network-video" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-neural-network-video">What is a Neural Network? (Video)</h2>
<p>In this video, Martha White introduces the concept of a neural network. We look at a simple one layer feed forward neural network. Where the <img src="https://latex.codecogs.com/png.latex?output=f(sW)"> is a non-linear function of the input.</p>
</section>
<section id="sec-l2g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g1">Define a neural network</h2>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/posts/img/rl-feedforward-nn.png" class="img-fluid"></div></div>
<p>A Neural network consists of a network of nodes which process and pass on information.</p>
<ul>
<li>The circles are the noes</li>
<li>The lines are the connections</li>
<li>The nodes are organized in layers</li>
</ul>
<p>Data starts at the input layer. It is passed through the connections to the hidden layer. The hidden layer is preforms some computation on the data and passes it to the output layer. This process repeats until the last layer produces the output of the network.</p>
</section>
<section id="deep-neural-networks-video" class="level2">
<h2 class="anchored" data-anchor-id="deep-neural-networks-video">Deep Neural Networks (Video)</h2>
<p>In this video, Martha White introduces the concept of neural networks with multiple hidden layers and activation functions.</p>
</section>
<section id="neural-networks-mechanics" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks-mechanics">Neural Networks Mechanics</h2>
<p>A node in the network is a function</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aoutput%20=%20f%5B(w_1%20%5Ctimes%20input_1)%20+%20(w_2%20%5Ctimes%20input_2)%20+%20%5Cldots%20+%20(w_n%20%5Ctimes%20input_n)%20+%20b%5D%0A"></p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?w_i"> are the weights,</li>
<li><img src="https://latex.codecogs.com/png.latex?input_i"> are the inputs, and</li>
<li><img src="https://latex.codecogs.com/png.latex?b"> is the bias.</li>
<li><img src="https://latex.codecogs.com/png.latex?f"> is the activation function.</li>
</ul></li>
</ul>
<p>The sum of the product of the weights and inputs is a linear operation. The activation function <img src="https://latex.codecogs.com/png.latex?f"> is where a non-linearity is introduced into the network.</p>
</section>
<section id="sec-l2g2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g2">Define activation functions</h2>
<p>Activation functions are non-linear functions that are applied to the output of a node. They introduce non-linearity into the network.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-activation-functions-tanh.png" class="img-fluid figure-img"></p>
<figcaption>tanh activation</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-activation-functions-relu.png" class="img-fluid figure-img"></p>
<figcaption>rectified linear activation function</figcaption>
</figure>
</div></div>
<p>Martha White also mentions threshold activation functions. However these are not used in practice as they are not differentiable. There is some work since this course came out on compressing neural networks to use threshold activation functions which are easy to compute on a CPU as matrix multiplication becomes a series of comparisons. However these are trained with a differentiable approximation of the threshold function and then quantized to the threshold function.</p>
</section>
<section id="the-neural-network-implementation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-neural-network-implementation">The Neural Network Implementation</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-nn-implementation.png" class="img-fluid figure-img"></p>
<figcaption>Neural Network Implementation</figcaption>
</figure>
</div></div><p>A neural network is a parameterized function that is a composition of linear and non-linear functions. It is a function of the state. The linear functions are the weights and the non-linear functions are the activation functions. The weights are learned from data.</p>
</section>
<section id="sec-l2g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g3">Define a feed-forward architecture</h2>
<p>A feed forward architecture is a neural network where the connections between nodes do not form a cycle. The data flows from the input layer to the output layer.</p>
<p>An example of a non-feed forward architecture is a recurrent neural network where the connections between nodes form cycles.</p>
</section>
<section id="non-linear-approximation-with-neural-networks-video" class="level2">
<h2 class="anchored" data-anchor-id="non-linear-approximation-with-neural-networks-video">Non-linear Approximation with Neural Networks (video)</h2>
</section>
<section id="sec-l2g4" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g4">How Neural Networks are doing feature construction</h2>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/rl-activation-1.png" class="img-fluid figure-img"></p>
<figcaption>neural feature 1</figcaption>
</figure>
</div>
<p>darker means greater activation for the feature</p>
</div><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/rl-activation-2.png" class="img-fluid figure-img"></p>
<figcaption>neural feature 2</figcaption>
</figure>
</div>
<p>the one generalize differently</p>
</div></div>
<p>We construct a non-linear function of the state using a neural network.</p>
<p>recall A node takes the form</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aoutput%20=%20f%5B(w_1%20%5Ctimes%20input_1)%20+%20%5Cldots%20+%20(w_n%20%5Ctimes%20input_n)%20+%20b%5D%0A"></p>
<p>We call this output of the node a feature! We can see that these features are a non-linear function of the inputs. We repeat this process until we evaluate all the nodes of the final layer. And the output of this final layer is called the representation.</p>
<p>Note: This is not very different from tile coding where we pass input to a tile coder and get back a new representation of the state.</p>
<p>In both cases we are constructing a non-linear mapping of the input of the features. And we take a nonlinear function of the representation to form the output - a nonlinear approximation of the state.</p>
<p>Recall that in tile coding we had to set some hyper-parameters: size shape of tiles + number of tiling. These are fixed before training. In a neural network we also have hyperparameters for the size of the layers, the number of layers, the activation functions. These too are fixed before training.</p>
<p>The difference is that Neural networks have weights that get updated during training. But tile coding does not change during training.</p>
</section>
<section id="sec-l2g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g5">How neural networks are a non-linear function of state</h2>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/rl-activation-3.png" class="img-fluid figure-img"></p>
<figcaption>neural feature 3</figcaption>
</figure>
</div>
<p>there are no hard boundaries</p>
</div><div class="">
<p><img src="https://orenbochman.github.io/posts/rl-activation-4.png" class="img-fluid" alt="neural feature 4"> this shows how it generelises</p>
</div></div>
<p>Neural networks are non linear functions of the because of the non-linear nature of the activation functions. These are applied recursively as we move to the final layer.</p>
</section>
<section id="deep-neural-networks-video-1" class="level2">
<h2 class="anchored" data-anchor-id="deep-neural-networks-video-1">Deep Neural Networks (Video)</h2>
</section>
<section id="sec-l2g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g6">How deep networks are a composition of layers</h2>
<p>Neural networks are modular. We can add or remove layers. Each layer is a function of the previous layer. The output of the previous layer is the input to the next layer.</p>
<p>Depth allows composition of features. Each layer can learn a different representation of the input. The final layer can learn a representation of the input that is a composition of the representations learned by the previous layers</p>
<p>We can design the network to remove undesirable features. For example we can design a network with a bottleneck that has less features than the input. This forces the network to learn a compressed representation of the input.</p>
</section>
<section id="sec-l2g7" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g7">The tradeoff between learning capacity and challenges presented by deeper networks</h2>
<p>Depth can increase the learning capacity of the network by allowing the network to learn complex compositions and abstractions. However, deeper networks are harder to train.</p>
</section>
</section>
<section id="lesson-3-training-neural-networks" class="level1 page-columns page-full">
<h1>Lesson 3: Training Neural Networks</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Compute</em> the gradient for a single hidden layer neural network #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how to compute the gradient for arbitrarily deep networks #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> the importance of initialization for neural networks #</label></li>
<li><label><input type="checkbox" checked=""><em>Describe</em> strategies for initializing neural networks #</label></li>
<li><label><input type="checkbox" checked=""><em>Describe</em> optimization techniques for training neural networks #</label></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discussion prompt
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>What properties of the representation are important for our online setting? This contrasts the offline, batch setting.</p>
</blockquote>
</div>
</div>
<section id="gradient-descent-for-training-neural-networks-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="gradient-descent-for-training-neural-networks-video">Gradient Descent for Training Neural Networks (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://orenbochman.github.io/posts/nn-notation.png" class="img-fluid"></p>
</div></div><p>If we use the square error loss then</p>
<p><span id="eq-loss"><img src="https://latex.codecogs.com/png.latex?%0AL(%5Chat%20y_k,y_k)%20=%20(%5Chat%20y_k-y_k)%5E2%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AA%20=%20A%20%E2%88%92%CE%B1%CE%B4%5EAs%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB%20=%20B%20%E2%88%92%CE%B1%CE%B4%5EBx%0A"></p>
<p>Let’s start at the output of the network and work backwards. Recall: <img src="https://latex.codecogs.com/png.latex?%0Ax%20=%20f_A(sA)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7By%7D%20=%20f_B(xB)%0A"></p>
<p>We start by taking the partial derivative of the loss function with respect to the first set of weights B.</p>
<p>We use the chain rule given the derivative of L with respect to <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D%20%5Ctimes%20%5Cfrac%7B%E2%88%82%5Chat%7By%7D%7D%7B%E2%88%82B%7D">. The next step is again to use the chain rule for this derivative.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82%5Chat%7By%7D_k%7D%20%5Cfrac%7B%E2%88%82%5Chat%7By%7D_k%7D%7B%E2%88%82B_%7Bjk%7D%7D%0A"></p>
<p>let’s introduce a new variable, θ where θ is the output of the hidden layer times the last set of weights.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%CE%B8%20%5Cdot%20=%20xB%0A"></p>
<p>Thus</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%20y%20%5Cdot%20=%20f_B(%CE%B8)%0A"></p>
<p>Rewriting we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82%5Chat%7By%7D_k%7D%20%5Cfrac%7B%E2%88%82f_B(%5Ctheta_k)%7D%7B%E2%88%82%5Ctheta%7D%20%5Cfrac%7B%E2%88%82%5Ctheta_k%7D%7B%E2%88%82B_%7Bjk%7D%7D%0A"></p>
<p>and since</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82%5Ctheta_k%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20x_j%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82%5Chat%7By%7D_k%7D%20%5Cfrac%7B%E2%88%82f_B(%5Ctheta_k)%7D%7B%E2%88%82%5Ctheta%7D%20x_j%0A"></p>
<p>now that we calculated the gradient for the last layer we can move to the previous layer.</p>
<p>we use</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi%20%5Cdot%20=%20%20sA%0A"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax%20%5Cdot%20=%20f_A(%5CPsi)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82A_%7Bij%7D%7D%20&amp;=%20%5Cdelta_k%5EB%20%5Cfrac%20%7B%E2%88%82%5Ctheta_k%7D%7B%E2%88%82A_%7Bij%7D%7D%20%5Cnewline%0A&amp;%20=%20%5Cdelta_k%5EB%20B_%7Bjk%7D%20%5Cfrac%20%7B%E2%88%82x_j%7D%7B%E2%88%82A_%7Bij%7D%7D%20%5Cnewline%0A%20%20&amp;%20=%20%5Cdelta_k%5EB%20B_%7Bjk%7D%20%5Cfrac%20%7B%E2%88%82f_A(%5CPsi_j)%7D%7B%E2%88%82%5CPsi_j%7D%20%5Cfrac%20%7B%E2%88%82%5CPsi_j%7D%7B%E2%88%82A_%7Bij%7D%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>since</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%20%7B%E2%88%82%5CPsi_j%7D%7B%E2%88%82A_%7Bij%7D%7D%20=%20s_%7Bij%7D%0A"></p>
<p>we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82A_%7Bij%7D%7D%20=%20%5Cdelta_k%5EB%20B_%7Bjk%7D%20%5Cfrac%20%7B%E2%88%82f_A(%5CPsi_j)%7D%7B%E2%88%82%5CPsi_j%7D%20s_%7Bij%7D%0A"></p>
<p>We can clean up this derivative by again, defining a term <img src="https://latex.codecogs.com/png.latex?%CE%B4_A">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%CE%B4%5EA_j%20=%20(B_%7Bjk%7D%CE%B4%5EB_k%20)%20%5Cfrac%7B%E2%88%82f_A(%CF%88_j)%7D%7B%E2%88%82%CF%88_j%7D%0A"></p>
<p>The final result will be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82A_%7Bij%7D%7D=%20%CE%B4%5EA_j%20s_i%0A"></p>
<p>Obtaining as a final result for both gradients the next expressions</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20%CE%B4%5EB_k%20x_j%0A"></p>
</section>
<section id="sec-l3g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g1">Computing the Gradient for a Single Hidden Layer Neural Network</h2>
<p>Let’s summerize the results:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20%CE%B4%5EB_k%20x_j%20%5Cqquad%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82A_%7Bij%7D%7D=%20%CE%B4%5EA_j%20s_i%0A"></p>
<p>where:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%CE%B4%5EB_k%20=%20%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82%5Chat%7By%7D_k%7D%20%5Cfrac%7B%E2%88%82f_B(%5Ctheta_k)%7D%7B%E2%88%82%5Ctheta%7D%20%5Cqquad%0A%CE%B4%5EA_j%20=%20(B_%7Bjk%7D%CE%B4%5EB_k%20)%20%5Cfrac%7B%E2%88%82f_A(%CF%88_j)%7D%7B%E2%88%82%CF%88_j%7D%0A"></p>
</section>
<section id="sec-l3g2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g2">Computing the Gradient for Arbitrarily Deep Networks</h2>

<div class="no-row-height column-margin column-container"><div id="fig-gradient-descent-pseudo-code" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-descent-pseudo-code-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/nn-backpop.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-descent-pseudo-code-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Gradient Descent Pseudo-code
</figcaption>
</figure>
</div><div id="fig-gradient-descent-RELU" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-descent-RELU-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/nn-backpop-relu.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-descent-RELU-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Gradient Descent Pseudo-code for RELU
</figcaption>
</figure>
</div></div>
<p>Now that we have estimated the gradient for a hidden layer neural network. We can use it to learn to optimize the weights of the network by updating the weights to minimize the error in the loss function in the direction of the negative gradient.</p>
<p>The pseudocode in the figure outlines how to implementing the backprop algorithm with Stochastic gradient descent.</p>
<p>For each data point s, y in our dataset, we first get our prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> from the network. This is the forward pass. Then we can estimate the loss using the actual value <img src="https://latex.codecogs.com/png.latex?y"></p>
<p>Next we compute the gradients starting from the output. We first compute <img src="https://latex.codecogs.com/png.latex?%CE%B4%5EB"> and the gradient for <img src="https://latex.codecogs.com/png.latex?B">, then we use this gradient to update the parameters <img src="https://latex.codecogs.com/png.latex?B">, with the step size <img src="https://latex.codecogs.com/png.latex?%CE%B1_B"> for the last layer.</p>
<p>Next, we update the parameters <img src="https://latex.codecogs.com/png.latex?A">. We compute <img src="https://latex.codecogs.com/png.latex?%CE%B4%5EA"> which reuses <img src="https://latex.codecogs.com/png.latex?%CE%B4%5EB">.</p>
<p>Notice, that by computing the gradients of the end of the network first, we avoid recomputing the same terms for A, that were already computed for <img src="https://latex.codecogs.com/png.latex?%CE%B4B">. We then compute the gradient for A and update A with this gradient using step size <img src="https://latex.codecogs.com/png.latex?%CE%B1_A">.</p>
<hr>
<p>Next we look at how we adapt the pseudocode to work with the ReLU activation on the hidden layer and a linear unit for the output.</p>
<p>First, we compute the error for the output layer, then we compute the derivative of the ReLU units with respect to <img src="https://latex.codecogs.com/png.latex?%5CPsi">, and finally, we use the aerial signal from the output layer along with you to compute the air signal for the hidden layer, the rest remains the same</p>
</section>
<section id="optimization-strategies-for-nns-video" class="level2">
<h2 class="anchored" data-anchor-id="optimization-strategies-for-nns-video">Optimization Strategies for NNs (Video)</h2>
</section>
<section id="sec-l3g3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g3">The Importance of Initialization for Neural Networks</h2>
<p>One simple yet effective initialization strategy for the weights, is to randomly sample the initial weights from a normal distribution with small variance Fig. 42. This way, each neuron has a different output from other neurons within its layer. This provides a more diverse set of potential features.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-feedforward-nn.png" class="img-fluid figure-img"></p>
<figcaption>Weights initialization</figcaption>
</figure>
</div></div><p>By keeping the variants small, we ensure that the output of each neuron is within the same range as its neighbors. One downside to this strategy is that, as we add more inputs to a neuron, the variance of the output grows.</p>
</section>
<section id="sec-l3g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g4">Strategies for Initializing Neural Networks</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0AW_%7Binit%7D%20~%20N(0,1)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AW_%7Binit%7D%20~%20%5Cfrac%7BN(0,1)%7D%7B%5Csqrt%7Bn_%7Bin%7D%7D%7D%0A"></p>
</section>
<section id="sec-l3g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g5">Optimization Techniques for Training Neural Networks</h2>
<ul>
<li>momentum update AKA heavy ball method <img src="https://latex.codecogs.com/png.latex?%0AW_%7Bt+1%7D%20%E2%86%90%20W_t%20%E2%88%92%CE%B1%E2%88%87_wL(W_t)%20+%20%CE%BBM_t%0A"></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AM_%7Bt+1%7D%20=%20%CE%BBM_t%20%E2%88%92%CE%B1%E2%88%87_wL%0A"></p>
<p>vector step size adaptation</p>
<ul>
<li>separate step size for each weight</li>
</ul>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/posts/nn-vector-step-size.png" class="img-fluid"></div></div>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c3-w2.html</guid>
  <pubDate>Mon, 01 Apr 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Constructing Features for Prediction</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c3-w2.1.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.4-9.5.0, pp. 204-210]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.7, pp. 223-228]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
</ul>
</div>
</div>
</div>
<p>This is not a video lecture or notes for a learning goal. This is however my attempts to cover some material from the readings from chapter 9 of <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement]</span> mentioned above.</p>
<p>I’ve added this material about a year after completing the specialization as I have been taking a course on deep reinforcement learning. I had felt that the material in this course had been both challenging on occasion and rather basic on others.</p>
<p>I don’t think I would have would have gone ahead and created this unit without looking at the material by Shangtong Zhang at https://github.com/ShangtongZhang/reinforcement-learning-an-introduction In which he reproduced some of the figures from the book. However this is my own implementation and though similar to some of the course material - I follow the Coursera Honor code and this is not likely to be much help to anyone working on the programming assignments which is a different environment, uses RL-glue and an implementation of tile coding.</p>
<p>What I felt was that I was not happy about the basics in this chapter. This includes the parameterization of the value function, the convergence results regarding linear function approximation. The ideas about why TD is a semi-gradient method etc.</p>
<p>I am also having many idea about both creating algorithms for creating features for RL environments. As I get familiar with gridwolds, atari games, sokoban etc I had many good ideas for making progress in both these environments and for improving algorithms for more general cases.</p>
<p>For example it appears that in many papers it turns out the agents are not learning very basic abilities. My DQN agent for space invaders was very poor at shooting bullets. I had an few ideas that should make a big difference. Like adding features for bullets, the invaders and so on. This are kind of challenging to implement in the current gymnasium environments. However I soon had a much more interesting idea that seems to be good for many of the atari environments and quite possibly even more broadly to most cnn based agents.</p>
<ul>
<li>In brief this would combine
<ul>
<li>a multiframe YOLO,</li>
<li>a generelised value function to replace YOLO’s supervision</li>
<li>a ‘robust’ causal attention mechanism to decide which pixels are more or less important
<ul>
<li>dropping them would not impact performance. e.g.&nbsp;bullets</li>
<li>which affect survival e.g.&nbsp;certain bullets</li>
<li>for scoring e.g.&nbsp;mother ship</li>
<li>which ones we can influence e.g.&nbsp;standing under an invader gets it to shoot at us.</li>
</ul></li>
</ul></li>
</ul>
<p>Note this is not the causal attention mechanism from NLP where one censored the future inputs but rather a mechanism that decides which pixels represent features that are potentialy the cause of the future states.</p>
<p>Clearly this Algorithm and its parts need to be worked out in a very simple environment. The YOLO part is all about modeling features using bounding boxes via a single pass. The GVFs are to replace yolo supervision loss with a RL compatible loss and the causal attention to reduce the overfitting and speed up learning.</p>
<p>I decided that converting some of these simple environments to gymnasium environments would be a good way to kick start some of these ideas, more so as reviewing papers and talks by Adam and Martha White shows that most experiments in RL environments turn out to be too complicated and access to simple environments turns out to be the way to get the experiments started.</p>
<p>In this chapter we use a simple environment called the 1000 state Random walk. I implemented this independently in Python.</p>
<p>We also learned the MC prediction algorithm and the TD(0) algorithms for function approximation.</p>
<p>We will use these algorithms to learn the value function of the Random Walk 1000 environment. We will also use tile coding and coarse coding to create features for the Random Walk 1000 environment.</p>
</section>
<section id="feature-construction-linear-function-approximation" class="level1">
<h1>Feature Construction &amp; linear Function approximation</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this module we consider environments that allow to consider simple state aggregationI’d like to cover the following topics:</p>
<ol type="1">
<li><label><input type="checkbox" checked="">create gymnasium compatible environment for the 1000 step Random Walk environment.</label>
<ul>
<li>This will allow to use it with many RL agents built using many libraries.</li>
<li><label><input type="checkbox" checked="">I’d also want to extend this environment to include a neighborhood size parameter - this will set the bound how far a left or right step will move us.</label></li>
<li><label><input type="checkbox" checked="">A dimension parameter to control dimensions of the state space. This can allow us to consider state aggregation in one two and three dimensions.</label></li>
</ul></li>
<li><label><input type="checkbox" checked="">Plot the trajectory of the random walk.</label></li>
<li><label><input type="checkbox">Implement the function approximation environment for gymnasium.</label>
<ul>
<li><label><input type="checkbox">extend to looking at interpolation and regression problems in the context of RL. (How - using a different dataset loaded by pandas?) Time series, Tabular data, Clustering, Classification, Regression, Pricing with elasticity. Multi-dimensional data.</label></li>
<li>This environment can be a basis for looking at how Temporal Abstraction aggregation play with function approximation in a highly simplified form.</li>
<li>This is a fundamental issue that <a href="https://www.youtube.com/watch?v=GntIVgNKkCI">Doina Precup</a> has raised in her talks as an ongoing area of research.</li>
<li>So such an environment might be useful in testing how different approaches can handle these issues in an environment that is very close to supervised learning.</li>
</ul></li>
<li>implement Gradient MC agent</li>
<li>implement Semi Gradient TD(0) agent</li>
<li>use these agents with and without state aggregation to learn the value function of the Random Walk 1000 environment.</li>
<li>implement coarse coding via tile coding to create features for the Random Walk 1000 environment.</li>
<li>implement use of <strong>polynomial features</strong> to create features for the Random Walk 1000 environment.</li>
<li>implement use of <strong>radial basis functions</strong> to create features for the Random Walk 1000 environment.</li>
<li>implement use of <strong>Fourier basis functions</strong> to create features for the Random Walk 1000 environment.</li>
</ol>
</div>
</div>
<section id="the-1000-step-random-walk-environment" class="level2">
<h2 class="anchored" data-anchor-id="the-1000-step-random-walk-environment">The 1000 Step Random Walk Environment</h2>
<p>In this lesson we implement the 1000 Random Walk example as an environment. This is good to demonstrate how to construct features for linear methods. We will use tile coding and coarse coding to create features for the Random Walk 1000 environment.</p>
<div id="6655d173" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gymnasium <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> gym</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> gymnasium <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> spaces</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> RandomWalk1000(gym.Env):</span>
<span id="cb1-6">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, neighborhood_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb1-7">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb1-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_states <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> num_states</span>
<span id="cb1-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> neighborhood_size</span>
<span id="cb1-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.observation_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> spaces.Discrete(num_states <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># add two states 0 and num_states + 1 as terminal states</span></span>
<span id="cb1-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.action_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> spaces.Discrete(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 0 for left, 1 for right</span></span>
<span id="cb1-12">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># start in the middle</span></span>
<span id="cb1-13">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.np_random, seed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gym.utils.seeding.np_random(seed)</span>
<span id="cb1-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.trajectory <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>]</span>
<span id="cb1-15"></span>
<span id="cb1-16">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> reset(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, options<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb1-17">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().reset(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seed)</span>
<span id="cb1-18">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span></span>
<span id="cb1-19">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.trajectory <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>]</span>
<span id="cb1-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state, {}</span>
<span id="cb1-21"></span>
<span id="cb1-22">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, action):</span>
<span id="cb1-23"></span>
<span id="cb1-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> action <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># move left</span></span>
<span id="cb1-25">             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># left neighbours</span></span>
<span id="cb1-26">            left_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size)</span>
<span id="cb1-27">            left_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state</span>
<span id="cb1-28">            num_left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> left_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> left_start</span>
<span id="cb1-29"></span>
<span id="cb1-30">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> left_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb1-31">                prob_terminate_left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> num_left) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size</span>
<span id="cb1-32">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-33">                prob_terminate_left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-34">            </span>
<span id="cb1-35">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.np_random.random() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> prob_terminate_left:</span>
<span id="cb1-36">               </span>
<span id="cb1-37">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, {} <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># terminate left</span></span>
<span id="cb1-38"></span>
<span id="cb1-39">            next_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.np_random.integers(low<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>left_start, high<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>left_end)</span>
<span id="cb1-40"></span>
<span id="cb1-41"></span>
<span id="cb1-42">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> action <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># move right</span></span>
<span id="cb1-43">             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># right neighbours</span></span>
<span id="cb1-44">            right_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-45">            right_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_states <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-46">            num_right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> right_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> right_start</span>
<span id="cb1-47">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> right_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_states <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb1-48">                 prob_terminate_right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> num_right) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size</span>
<span id="cb1-49">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-50">                prob_terminate_right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-51">            </span>
<span id="cb1-52">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.np_random.random() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> prob_terminate_right:</span>
<span id="cb1-53"></span>
<span id="cb1-54">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_states <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, {} <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># terminate right</span></span>
<span id="cb1-55"></span>
<span id="cb1-56">            next_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.np_random.integers(low<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>right_start, high<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>right_end)</span>
<span id="cb1-57">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-58">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">ValueError</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Invalid action"</span>)</span>
<span id="cb1-59"></span>
<span id="cb1-60">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> next_state</span>
<span id="cb1-61"></span>
<span id="cb1-62">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.trajectory.append(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state)</span>
<span id="cb1-63">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, {} <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># not terminated or truncated</span></span>
<span id="cb1-64"></span>
<span id="cb1-65"></span>
<span id="cb1-66"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-67"></span>
<span id="cb1-68"></span>
<span id="cb1-69"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_trajectory(trajectory, num_states):</span>
<span id="cb1-70">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Plots the trajectory of the random walk."""</span></span>
<span id="cb1-71">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(trajectory))</span>
<span id="cb1-72">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(trajectory)</span>
<span id="cb1-73">    </span>
<span id="cb1-74">    plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb1-75">    plt.plot(x, y, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-'</span>, markersize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb1-76">    plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Time Step'</span>)</span>
<span id="cb1-77">    plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'State'</span>)</span>
<span id="cb1-78">    plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Random Walk Trajectory'</span>)</span>
<span id="cb1-79">    plt.yticks(np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>))</span>
<span id="cb1-80">    plt.grid(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'y'</span>)</span>
<span id="cb1-81"></span>
<span id="cb1-82">    plt.tight_layout()</span>
<span id="cb1-83">    plt.show()</span></code></pre></div>
</details>
</div>
<div id="578e007e" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#import gymnasium as gym</span></span>
<span id="cb2-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#from random_walk_gym import RandomWalk1000</span></span>
<span id="cb2-3"></span>
<span id="cb2-4">env <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomWalk1000()</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Reset the env</span></span>
<span id="cb2-7">obs, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.reset()</span>
<span id="cb2-8">terminated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb2-9"></span>
<span id="cb2-10"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> terminated:</span>
<span id="cb2-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># For this environment, an action is not needed.</span></span>
<span id="cb2-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Here we pass in a dummy value</span></span>
<span id="cb2-13">    obs, reward, terminated, truncated, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.step(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb2-14">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"State: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, Reward: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>reward<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, Terminated: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>terminated<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-15"></span>
<span id="cb2-16">env.close()</span>
<span id="cb2-17"></span>
<span id="cb2-18">plot_trajectory(env.trajectory, num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>env.num_states)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>State: 420, Reward: 0, Terminated: False
State: 410, Reward: 0, Terminated: False
State: 363, Reward: 0, Terminated: False
State: 321, Reward: 0, Terminated: False
State: 235, Reward: 0, Terminated: False
State: 139, Reward: 0, Terminated: False
State: 128, Reward: 0, Terminated: False
State: 83, Reward: 0, Terminated: False
State: 44, Reward: 0, Terminated: False
State: 1, Reward: -1, Terminated: True</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/c3-w2.1_files/figure-html/cell-3-output-2.png" width="1142" height="374" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="d3f83acb" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">env <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomWalk1000(num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, neighborhood_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb4-2">obs, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.reset()</span>
<span id="cb4-3">terminated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb4-4">truncated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb4-5">trajectory <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb4-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> terminated <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> truncated:</span>
<span id="cb4-7">    action <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.action_space.sample()  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Replace with your policy</span></span>
<span id="cb4-8">    obs, reward, terminated, truncated, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.step(action)</span>
<span id="cb4-9">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>obs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>action<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>reward<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>terminated<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb4-10"></span>
<span id="cb4-11">plot_trajectory(env.trajectory, num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>env.num_states)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>obs=582, action=1, reward=0, terminated=False
obs=681, action=1, reward=0, terminated=False
obs=670, action=0, reward=0, terminated=False
obs=637, action=0, reward=0, terminated=False
obs=676, action=1, reward=0, terminated=False
obs=646, action=0, reward=0, terminated=False
obs=560, action=0, reward=0, terminated=False
obs=582, action=1, reward=0, terminated=False
obs=632, action=1, reward=0, terminated=False
obs=540, action=0, reward=0, terminated=False
obs=517, action=0, reward=0, terminated=False
obs=559, action=1, reward=0, terminated=False
obs=646, action=1, reward=0, terminated=False
obs=701, action=1, reward=0, terminated=False
obs=713, action=1, reward=0, terminated=False
obs=644, action=0, reward=0, terminated=False
obs=707, action=1, reward=0, terminated=False
obs=636, action=0, reward=0, terminated=False
obs=696, action=1, reward=0, terminated=False
obs=708, action=1, reward=0, terminated=False
obs=611, action=0, reward=0, terminated=False
obs=691, action=1, reward=0, terminated=False
obs=762, action=1, reward=0, terminated=False
obs=704, action=0, reward=0, terminated=False
obs=706, action=1, reward=0, terminated=False
obs=691, action=0, reward=0, terminated=False
obs=757, action=1, reward=0, terminated=False
obs=693, action=0, reward=0, terminated=False
obs=786, action=1, reward=0, terminated=False
obs=861, action=1, reward=0, terminated=False
obs=874, action=1, reward=0, terminated=False
obs=787, action=0, reward=0, terminated=False
obs=800, action=1, reward=0, terminated=False
obs=707, action=0, reward=0, terminated=False
obs=784, action=1, reward=0, terminated=False
obs=692, action=0, reward=0, terminated=False
obs=706, action=1, reward=0, terminated=False
obs=756, action=1, reward=0, terminated=False
obs=768, action=1, reward=0, terminated=False
obs=699, action=0, reward=0, terminated=False
obs=789, action=1, reward=0, terminated=False
obs=804, action=1, reward=0, terminated=False
obs=855, action=1, reward=0, terminated=False
obs=771, action=0, reward=0, terminated=False
obs=747, action=0, reward=0, terminated=False
obs=792, action=1, reward=0, terminated=False
obs=710, action=0, reward=0, terminated=False
obs=711, action=1, reward=0, terminated=False
obs=742, action=1, reward=0, terminated=False
obs=796, action=1, reward=0, terminated=False
obs=812, action=1, reward=0, terminated=False
obs=770, action=0, reward=0, terminated=False
obs=847, action=1, reward=0, terminated=False
obs=841, action=0, reward=0, terminated=False
obs=780, action=0, reward=0, terminated=False
obs=786, action=1, reward=0, terminated=False
obs=836, action=1, reward=0, terminated=False
obs=858, action=1, reward=0, terminated=False
obs=843, action=0, reward=0, terminated=False
obs=877, action=1, reward=0, terminated=False
obs=836, action=0, reward=0, terminated=False
obs=767, action=0, reward=0, terminated=False
obs=701, action=0, reward=0, terminated=False
obs=736, action=1, reward=0, terminated=False
obs=683, action=0, reward=0, terminated=False
obs=688, action=1, reward=0, terminated=False
obs=787, action=1, reward=0, terminated=False
obs=858, action=1, reward=0, terminated=False
obs=807, action=0, reward=0, terminated=False
obs=896, action=1, reward=0, terminated=False
obs=982, action=1, reward=0, terminated=False
obs=1001, action=1, reward=1, terminated=True</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/c3-w2.1_files/figure-html/cell-4-output-2.png" width="1142" height="374" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Lets simulate the random walk till success and plot its trajectory.</p>
<div id="16befd85" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">env <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomWalk1000(num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, neighborhood_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb6-2">obs, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.reset()</span>
<span id="cb6-3">terminated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb6-4">truncated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb6-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> terminated <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> truncated:</span>
<span id="cb6-6">    action <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.action_space.sample()  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Replace with your policy</span></span>
<span id="cb6-7">    obs, reward, terminated, truncated, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.step(action)</span>
<span id="cb6-8"></span>
<span id="cb6-9"></span>
<span id="cb6-10">plot_trajectory(env.trajectory, num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>env.num_states)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/c3-w2.1_files/figure-html/cell-5-output-1.png" width="1142" height="374" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="a-short-digression-on-einstein-tiling-for-rl" class="level2">
<h2 class="anchored" data-anchor-id="a-short-digression-on-einstein-tiling-for-rl">A short digression on Einstein Tiling for RL</h2>
<p>One (awful) Idea I keep returning to is to use Einstein Tiling for RL. I mentino that Einstein in this context is not the physicist but rather a pun on the word ‘Einstein’ which means ‘one stone’ in German.</p>
<p>Let’s quickly review why it is a bad idea, and then why it is also a fascinating idea.</p>
<ol type="1">
<li><p>Unlike A square tiling this is an aperiodic tiling so we need to generate it efficiently. Depending on the space it will may take some time to generate the tiling. We need to store the tiling in memory. For a square tiling we can generate the tiling in a few lines of code. We can access the tiling or tile using a simple formula.</p></li>
<li><p>We need a quick way to find which tile a point is in. This is not to hard for one tile. But as the number of tiles increases this becomes more difficult. It is trivial for a square tiling where again we have a formula to efficiently determine the tile a point belongs to.</p></li>
</ol>
<p>Some reasons why it is a fascinating idea.</p>
<ol type="1">
<li>We only need one tiling. If we have one we can map the first tile ton any other location it is in the same orientation and we will get a new tiling! This is due to the aperiodic nature of the tiling.</li>
<li>The hat tile is constructed by glueing together eight smaller kite tiles that are sixth of a hexagon. We can easily use larger kites that so we can use two such grids as coarse and coarser tilings.</li>
<li>Different can be similar locally but will tend to diverge. This suggest that we will get a good generalization.</li>
<li>There may be variant einsteins that are easier to generate and use</li>
<li>In https://www.ams.org/journals/proc/1995-123-11/S0002-9939-1995-1277129-X/ the authors show that for d&gt;=3 aperiodic tilings can naturally avoid more symmetries than just translations. I.e. we can have a periodic tilings in higher dimensions.</li>
</ol>
<p>I may be wrong but It may be possible to generate the tiling using a simple formula. Ok so far tiling generation is insanely complicated. Though this is not a judgment on the complexity of the tiling but rather the complexity of the code and mathematics to generate the tiling.</p>
<p>The hat tile allows one to create many different tilings of the state space in two dimensions. Each tiling is going to have a different set of features.</p>
<p>As the hat tile is constructed by glueing together eight smaller tiles. Tilings are created in a hierarchical manner. This suggests that we can will get fine and course feature in this process and that we can just keep going to increase discrimination.</p>
<p>Some issues - it is possible to get two tilings that are the same but for a ‘conway worm’ this is a curve in the tiling that is different. The problem here is that the features will be the same for every where except the worm. Not good for generalization.</p>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c3-w2.1.html</guid>
  <pubDate>Mon, 01 Apr 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>On-Policy Prediction with Approximation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c3-w1.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level1 page-columns page-full">
<h1>Introduction</h1>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<ul>
<li>We now start the third course in the reinforcement learning specialization.</li>
<li>In terms of the <strong>?@fig-rl-chart</strong> we are on the left branch of the tree.</li>
<li>This course is about prediction and control with function approximation.</li>
<li>The main difference in this course is that will start consider continuous state spaces and action spaces which cannot be represented as tables.
<ul>
<li>However many of the methods we will develop will be useful in handling large scale tabular problems as well.</li>
<li>We will use methods from supervised learning but only online methods that can handle non-stationary data.</li>
<li>The main differences are the use of weights to parameterize the value functions.</li>
<li>The use of function approximation to estimate value functions and policies in reinforcement learning.
<ul>
<li>Weights lead to using a loss function to estimate the value function.</li>
<li>Minimizing the continuous loss function leads to Gradient descent and</li>
<li>Using sampling leads to Stochastic gradient descent.</li>
</ul></li>
<li>The idea of learning weights rather than values is a key idea in this course.</li>
<li>The tradeoff between discrimination and generalization is also a key idea in this course.</li>
</ul></li>
<li>We will learn how to use function approximation to estimate value functions and policies in reinforcement learning.</li>
<li>We will also learn how to use function approximation to solve large-scale reinforcement learning problems.</li>
<li>We will see some simple linear function approximation methods and later</li>
<li>We will see modern nonlinear approximation methods using deep neural networks.</li>
</ul>
<p>I did not find the derivation of the SGD alg particularly enlightening and I have seen it several times. However the online setting is the best motivation for the use of SGD and makes perfect sense in the context of reinforcement learning. Minibatches are then a natural extension of this idea.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.1-9.4, pp. 194-209]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
</ul>
</div>
</div>
</div>
</section>
<section id="lesson-1-estimating-value-functions-as-supervised-learning" class="level1 page-columns page-full">
<h1>Lesson 1: Estimating Value Functions as Supervised Learning</h1>
<p>In this lesson we will cover some important notation that will remain with us till the end of the course. Mathematically most of it is trivial, but it is important to understand the notation - otherwise the rest of the course will be hard to follow.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><p><label><input type="checkbox" checked=""><em>Understand</em> how we can use <strong>parameterized functions</strong> to approximate value functions #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Explain</em> the meaning of <strong>linear value function approximation</strong> #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Recognize</em> that the tabular case is a special case of linear value function approximation #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Understand</em> that there are many ways to parameterize an approximate value function #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Understand</em> what is meant by <strong>generalization</strong> and <strong>discrimination</strong> #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Understand</em> how generalization can be beneficial #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Explain</em> why we want both generalization and discrimination from our function approximation #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Understand</em> how value estimation can be framed as a <strong>supervised learning</strong> problem #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Recognize</em> not all function approximation methods are well suited for reinforcement learning #</label></p></li>
</ul>
</div>
</div>
<section id="moving-to-parameterized-policies-video" class="level2">
<h2 class="anchored" data-anchor-id="moving-to-parameterized-policies-video">Moving to Parameterized Policies (video)</h2>
<p>This video covers the first four learning objectives.</p>
<p>This video covers parameterized policies and how they can be used to approximate value functions. The idea is that using a table has some limitations/ The first is that the tables can be very large. For continuous states they can become infinite.</p>
<p>The second is that the tables can be very sparse and in a table we don’t generalize between states.</p>
<p>We see that we don’t really want functions that directly approximate the value function. We want functions that have some structure that we can learn.</p>
<p>This is called a parameterized function. The ideas is to use a weighted sum of the features of the state. This allows us to learn the weights and use them to approximate the value function. This is called linear function approximation. The way to get around this which is two fold. We first represent the salient properties of a states into features.</p>
<p>Then we use weights to combine these features to approximate the value function. This is called linear function approximation.</p>
<p>More generally we can use non linear parameterized functions to approximate value functions.</p>
<p>Adam shows that is the features are not picked wisely we may not be able to discriminate between states - out function for one state will be the same as for another dissimilar state. Learning about one will make us forget what we learned about the other. This is called bias. On the other hand if we have too many features we may not be able to generalize between states. This is called variance. The goal is to balance between bias and variance.</p>
</section>
<section id="sec-l1g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g1">Understanding parameterized functions</h2>
<ul>
<li>In the previous courses we represented value functions as tables or arrays:
<ul>
<li>For <img src="https://latex.codecogs.com/png.latex?V(s)"> we had an array of size <img src="https://latex.codecogs.com/png.latex?%7CS%7C">,</li>
<li>For <img src="https://latex.codecogs.com/png.latex?Q(s,a)"> we had an array of size <img src="https://latex.codecogs.com/png.latex?%7CS%7C%20%5Ctimes%20%7CA%7C">. This becomes impractical as <img src="https://latex.codecogs.com/png.latex?%7CS%7C%20%5Crightarrow%20%5Cinfty">. We can use <strong>parameterized functions</strong> to approximate value functions. This is called <strong>function approximation</strong>.</li>
</ul></li>
<li><strong>Linear value function approximation</strong> is a simple and popular method.
<ul>
<li>We represent the value function as a linear combination of features:</li>
</ul></li>
</ul>
<p><span id="eq-fn-approx"><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(s,%20%5Cmathbb%7Bw%7D)%20%5Capprox%20v_%5Cpi(s)%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D()"> is the approximate value function</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> is a weight vector</li>
</ul></li>
<li>for example:</li>
</ul>
</section>
<section id="sec-l1g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g2">Linear value function approximation</h2>
<ul>
<li>We can write the approximate value function as a linear combination of features:</li>
</ul>
<p><span id="eq-lin-fn-approx"><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(s,%20%5Cmathbb%7Bw%7D)%20%5Cdot%20=%20w_1%20X%20+%20w_2%20+%20Y%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are features of the state <img src="https://latex.codecogs.com/png.latex?s"></li>
<li><img src="https://latex.codecogs.com/png.latex?w_1"> and <img src="https://latex.codecogs.com/png.latex?w_2"> are the weights of the features</li>
</ul></li>
<li>now learning becomes finding better weights that parameterize the value function.</li>
</ul>
<p>finding the weights that minimize the error between the approximate value function and the true value function:</p>
</section>
<section id="sec-l1g3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g3">Tabular case is a special case of linear value function approximation</h2>
<div class="page-columns page-full"><p> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Chat%7Bv%7D(s,%20%5Cmathbb%7Bw%7D)%20&amp;%20%5Cdot%20=%20%5Csum%20w_i%20x_i(s)%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%3C%5Cmathbf%7Bw%7D,%20%5Cmathbf%7Bx%7D(s)%3E%20%5Cqquad%0A%5Cend%7Balign*%7D%0A"></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/posts/img/c3-w2-parametrized_functions.png" id="fig-rl-linear-approximation" class="img-fluid" alt="Linear value function generalize the tabular case"></div></div>
<ul>
<li>here:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> is a weight vector</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D(s)"> is a feature vector that is 1 in the <img src="https://latex.codecogs.com/png.latex?i">-th position and 0 elsewhere.</li>
</ul></li>
<li>linear value function approximation is a generalization of the tabular case.</li>
<li>limitations of linear value function approximation:
<ul>
<li>the choice of features limits the expressiveness of the value function.</li>
<li>it can only represent linear relationships between the features and the value function.</li>
<li>it can only represent a limited number of features.</li>
</ul></li>
<li>so how are tabular functions a special case of linear value function approximation?
<ul>
<li>we can see from the figure that all we need is use one hot encoding for the features. Then the weighted vector will be the same as the value function in the table.</li>
</ul></li>
</ul>

<div class="no-row-height column-margin column-container"><div id="fig-rl-failure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-failure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/rl-linear-fn-fail.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-failure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Linear value function approximation failure
</figcaption>
</figure>
</div></div></section>
<section id="sec-l1g4" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g4">There are many ways to parameterize an approximate value function</h2>

<div class="no-row-height column-margin column-container"><div id="fig-neural-networks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neural-networks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/rl-non-linear-fn-approximation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neural-networks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: neural networks are non-linear fn approximators
</figcaption>
</figure>
</div></div><ul>
<li>We can use different types of functions to approximate the value function:
<ul>
<li>one hot encoding</li>
<li>linear functions</li>
<li>tile coding</li>
<li>neural networks</li>
</ul></li>
</ul>
</section>
<section id="generalization-and-discrimination-video" class="level2">
<h2 class="anchored" data-anchor-id="generalization-and-discrimination-video">Generalization and Discrimination (video)</h2>
<p>In this video Martha covers the next three learning objectives. The video is about:</p>
<ul>
<li>Generalization - using knowledge (V,Q,Pi) about similar states.</li>
<li>Discrimination - being able to distinguish between different states.</li>
</ul>
</section>
<section id="sec-l1g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g5">Understanding generalization and discrimination</h2>

<div class="no-row-height column-margin column-container"><div id="fig-generalization-discrimination-chart" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-generalization-discrimination-chart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/rl-generalization-discrimination-matrix.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generalization-discrimination-chart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: generalization and discrimination
</figcaption>
</figure>
</div></div><ul>
<li>Generalization:
<ul>
<li>the ability to estimate the value of states that were not seen during training.</li>
<li>in the case of policy evaluation, generalization is the ability of updates of value functions in one state to affect the value of other states.</li>
<li>in the tabular case, generalization is not possible because we only update the value of the state we are in.</li>
<li>in the case of function approximation, we can think of generalization as corresponding to an embedding of the state space into a lower-dimensional space.</li>
</ul></li>
<li>Discrimination: the ability to distinguish between different states.</li>
</ul>
</section>
<section id="sec-l1g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g6">How generalization can be beneficial</h2>
<ul>
<li>Generalization can be beneficial because:
<ul>
<li>It allows us to estimate the value of states that are similar to states seen during training.</li>
<li>This includes states that were not seen during training.</li>
<li>It allows us to estimate the value of states that are far from states seen during training. (So long as they are similar in terms of the features we are using to approximate the value function)</li>
</ul></li>
</ul>
</section>
<section id="sec-l1g7" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g7">Why we want both generalization and discrimination from our function approximation</h2>
<ul>
<li>We want both generalization and discrimination from our function approximation because:
<ul>
<li>generalization allows us to estimate the value of states that were not seen during training.</li>
<li>discrimination allows us to distinguish between different states.</li>
<li>generalization allows us to estimate the value of states that are similar to states seen during training.</li>
<li>discrimination allows us to estimate the value of states that are far from states seen during training.</li>
</ul></li>
</ul>
<p>We hear a lot about function approximation and gradient methods having a bias or high variance. I tracked this from wikipedia and statistical learning. While it makes sense for a Bayesian regression I’m not sure that it is quite correct for RL. Unfortunately I don’t have a better explanation, though reviewing <a href="https://www.youtube.com/watch?v=y3oqOjHilio">this policy gradient lecture might be helpful</a></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bias-variance tradeoff
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>an important result called the <a href="https://en.wikipedia.org/wiki/Bias-variance_tradeoff"><strong>bias-variance tradeoff</strong></a>:
<ul>
<li>Bias is the error introduced by approximating a real-world problem, which may be extremely complicated, by a much simpler model. This means that since we cannot discriminate between different states that share weights for the same feature vector we have errors we characterize as bias.</li>
<li>High bias corresponds to underfitting in our model.</li>
<li>Variance is the opposite issue arising from having more features than we need to discriminate between states. This means that updating certain weights will affect only some of these related states and not others. This type of error is called variance and is also undesirable.</li>
<li>High variance corresponds to overfitting in our model which can be due to our model fitting the noise in the data rather than the underlying signal.</li>
<li>In general for a model there is some optimal point where the bias and variance are balanced. Going forward from that point we observe a trade off between bias and variance so we need to choose one or the other.</li>
<li>This choice is usually governed by business realities and the nature of the data or the problem we are trying to solve.</li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="framing-value-estimation-as-supervised-learning-video" class="level2">
<h2 class="anchored" data-anchor-id="framing-value-estimation-as-supervised-learning-video">Framing Value Estimation as Supervised Learning (video)</h2>
<p>In this video we cover the next two learning objectives. Martha has a good background in supervised learning and she explains how many parts of RL can be framed as supervised learning problems.</p>
<p>Things we like to learn in RL in this course:</p>
<ul>
<li>Value fn approximation (V)</li>
<li>Action fn values (Q)</li>
<li>Policies (Pi)</li>
</ul>
<p>In reality we may want to learn other things as well which can be framed as supervised learning problems:</p>
<ul>
<li>State representations - i.e.&nbsp;better features (CNNs, RNNs, etc)</li>
<li>Models of Dynamics i.e.&nbsp;Transition probabilities (P)</li>
<li>Reward precesses (R) What is a good reward function? How do we learn it? This is an inverse reinforcement learning problem. It is ill posed because there are many reward functions that can explain the data. We need to find the simplest one that explains the data. This is intertwined with learning internal motivations and goals. c.f. <a href="https://www.youtube.com/watch?v=jn1NE8uIxgw">Satinder Singh</a>’s work on intrinsic motivation.</li>
<li>Generalized Value functions (Gvfs) c.f. <a href="https://www.youtube.com/watch?v=GmGL9cVfJG4&amp;t=1889s">Martha and Adam White’s work on GVFs</a>.</li>
<li>Options (Spatial or Temporal Aggregations of actions) c.f. Doina Precup’s work on options and semi-markov decision processes.</li>
</ul>
<p>Even more things that we might want to learn in RL that might be framed as supervised learning problems:</p>
<ul>
<li>Approximate/Compressed policies for sub goals AKA heuristics
<ul>
<li>Fully Pooled policies (all states are the same) - Think random uniform policy</li>
<li>Partial pooled policies, (some states are the same)</li>
<li>Unpooled policies (all states are different - tabular setting)</li>
<li>Priors for Policies</li>
<li>Hierarchies of options - Think Nethack</li>
</ul></li>
<li>Beliefs about policies</li>
<li>Beliefs about other agents (theory of mind)</li>
<li>Beliefs about the environment.</li>
<li>Causal models of the environment
<ul>
<li>what can we influence and what can’t we influence.</li>
</ul></li>
<li>Coordination and communication with other agents c.f. work by <a href="https://www.jakobfoerster.com/">Jakob Foerster</a>, <a href="https://natashajaques.ai/">Natasha Jacques</a>, and <a href="">Marco Baroni</a> on emergent communication.
<ul>
<li>What part of communication is cheap talk</li>
<li>What part of communication is credible</li>
</ul></li>
</ul>
</section>
<section id="sec-l1g8" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g8">How value estimation can be framed as a supervised learning problem</h2>
<ul>
<li><p>The problem of policy evaluation in reinforcement learning can be framed as supervised learning problem</p>
<ul>
<li>In the case of Monte Carlo methods,
<ul>
<li>the inputs are the states and</li>
<li>the outputs are the returns <img src="https://latex.codecogs.com/png.latex?G">.</li>
</ul></li>
<li>In the case of TD methods,
<ul>
<li>the inputs are the states and</li>
<li>the outputs are the one step bootstrapped returns. <img src="https://latex.codecogs.com/png.latex?U_t%20%5Cdot=R_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)"></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="sec-l1g9" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g9">Not all function approximation methods are well suited for reinforcement learning</h2>
<blockquote class="blockquote">
<p>In principle, any function approximation technique from supervised learning can be applied to the <strong>policy evaluation task</strong>. However, not all are equally well-suited. – Martha White</p>
</blockquote>
<ul>
<li>in RL the agent interacts with the environment and generates data, which corresponds to the online setting in supervised learning.</li>
<li>When we want to use supervised learning we need to choose a method that is well suited for the online setting which can handle
<ul>
<li>non-stationary data.</li>
<li>non-stationary and correlated data (which is the case in RL).</li>
</ul></li>
</ul>
<p>In fact much of the learning in RL is about learning such correlations and quickly adapting to non-stationary in the environment.</p>
<p>In TD learning the target depends on <img src="https://latex.codecogs.com/png.latex?w"> but in supervised learning the target is fixed and given.</p>
</section>
</section>
<section id="lesson-2-the-objective-for-on-policy-prediction" class="level1 page-columns page-full">
<h1>Lesson 2: The Objective for On-policy Prediction</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Understand</em> the <strong>mean-squared value error objective</strong> for policy evaluation #</label></li>
<li><label><input type="checkbox" checked=""><em>Explain</em> the role of the <strong>State distribution</strong> in the objective #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> the idea behind <strong>Gradient descent</strong> and <strong>Stochastic gradient descent</strong> #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> that gradient descent converges to stationary points #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how to use <strong>Gradient descent</strong> and <strong>Stochastic gradient descent</strong> to minimize the value error #</label></li>
<li><label><input type="checkbox" checked=""><em>Outline</em> the <strong>Gradient Monte Carlo</strong> algorithm for value estimation #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how <strong>state aggregation</strong> can be used to approximate the value function #</label></li>
<li><label><input type="checkbox" checked=""><em>Apply</em> <strong>Gradient Monte-Carlo</strong> with state aggregation #</label></li>
</ul>
</div>
</div>
<section id="the-value-error-objective-video" class="level2">
<h2 class="anchored" data-anchor-id="the-value-error-objective-video">The Value Error Objective (Video)</h2>
<p>In this video Adam White covers the first two learning objectives of the unit.<br>
The main subject about using the mean squared error as a loss for the approximate value function.</p>
<p>we get a sequence of <img src="https://latex.codecogs.com/png.latex?(S_1,v_%7B%5Cpi%7D(S_1)%20),(S_2,v_%7B%5Cpi%7D(S_2)%20),(S_3,v_%7B%5Cpi%7D(S_3)%20),%20%5Cldots"> and we want to approximate the value function . We can track how well we are approximating <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D(s)"> by using <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D(s,%5Cmathbb%7Bw%7D)">. The difference can be positive or negative so if we average it the sum will tend to cancel out. If we square the error we get a positive number we have a much better estimate of the error. And if normalize it by taking the mean we can get use it to compare runs of different lengths. This is called the mean squared error.</p>
<p>It turns out that this is not enough for RL and we need to take a weighted average using the state distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)">. This is because we care more about some states than others. The state distribution is the long run probability of visiting the state <img src="https://latex.codecogs.com/png.latex?s"> under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">. This weighted average is called <em>the mean squared <strong>value</strong> error</em>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Your Objective is My Loss
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In the optimization literature the <strong>loss</strong> function is called the <strong>objective</strong> function. This is because we are trying to optimize the weights of the function to minimize the loss. So we will often hear the term objective function or just objective. Life is simpler if we recall that this is just a loss function for a supervised learning problem.</p>
<p>A related point is that if we want to optimize our approximate value we can swap the with a different loss function or with a different approximation function and the outcome should remain the same, at least under certain conditions. This is how we can switch from the mean squared value error objective to the Monte Carlo objective and then to the TD learning objective.</p>
</div>
</div>
</div>
</section>
<section id="sec-l2g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g1">Understanding the mean-squared value error objective for policy evaluation</h2>

<div class="no-row-height column-margin column-container"><div id="fig-mse" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/rl-mean-squared-value-error.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: mean-squared value error objective
</figcaption>
</figure>
</div></div><ul>
<li>An idealized Scenario:
<ul>
<li>input: <img src="https://latex.codecogs.com/png.latex?%5C%7B(S_1,%20v_%5Cpi(S_1)),%20(S_2,%20v_%5Cpi(S_2)),%20%5Cldots,%20(S_n,%20v_%5Cpi(S_n))%5C%7D"></li>
<li>output: <img src="https://latex.codecogs.com/png.latex?%5Chat%20v(s,w)%20%5Capprox%20v_%5Cpi(s)"></li>
<li>however in reality we may get some some error in the approximation.
<ul>
<li>this could be due to our choice of the approximation.</li>
<li>but initially we just don’t have good weights - to fit the data.</li>
</ul></li>
<li>What we need is a way to measure the error in the approximation.</li>
<li>Also we may care more about some states than others and we can encode this using the state distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)">.</li>
</ul></li>
<li>The mean-squared value error objective for policy evaluation is to minimize the mean-squared error between the true value function and the approximate value function:</li>
</ul>
<p><span id="eq-msve"><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7BVE%7D%20=%20%5Csum_%7Bs%5Cin%20S%7D%5Cmu(s)%5Bv_%5Cpi(S)%20-%20%5Chat%7Bv%7D(S,%20%5Cmathbf%7Bw%7D)%5D%5E2%0A%5Ctag%7B3%7D"></span></p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverline%7BVE%7D"> is the mean-squared value error</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmu(s)"> is the state distribution</li>
<li><img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> is the true value of state <img src="https://latex.codecogs.com/png.latex?s"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)"> is the approximate value of state <img src="https://latex.codecogs.com/png.latex?s"> with weights <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"></li>
</ul></li>
<li>the goal is to find the weights that minimize the mean-squared value error.</li>
</ul>
</section>
<section id="sec-l2g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g2">Explaining the role of the state distribution in the objective</h2>
<ul>
<li>The state distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)"> is the long run probability of visiting the state <img src="https://latex.codecogs.com/png.latex?s"> under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</li>
<li>This makes more sense if our markov chain is ergodic - i.e.&nbsp;we can reach any state from any other state by following some transition trajectory.</li>
<li>The state distribution is important because it determines how much we care about the error in each state.</li>
<li>The state distribution is usually unknown, and hard to estimate as it has complex dependencies on the policy and the environment.</li>
<li>We will later see a result that shows how we can avoid the need to know the state distribution.</li>
<li>In the diagram we see that the state distribution is a probability distribution over the states of the MDP and that there is little probability mass of visiting states at the edges of the state space.</li>
<li>The mean square error has less impact in these low probability states.</li>
</ul>
</section>
<section id="sec-l2g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g3">The idea behind gradient descent and stochastic gradient descent</h2>
<ul>
<li>Gradient descent is an optimization algorithm that uses the gradient to find a local minimum of a function.</li>
<li>The gradients points in the direction of the steepest ascent of the function and our objective is to minimize the mean squared error we move in the opposite direction.</li>
<li>Hence the name gradient descent.</li>
<li>The gradient of the mean-squared value error with respect to the weights <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> is given by:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20w%20%5Cdot%20=%20%5Cleft%20%5B%20%5Cbegin%7Bmatrix%7D%20w_1%20%5C%5C%20%5Cvdots%20%5C%5C%20w_d%20%20%5Cend%7Bmatrix%7D%20%5Cright%20%5D%20%5Cqquad%20%5Cnabla%20f%20=%20%5Cleft%20%5B%20%5Cbegin%7Bmatrix%7D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20w_1%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20w_d%7D%20%20%5Cend%7Bmatrix%7D%20%5Cright%20%5D%20%5Cqquad%0A"></p>
<ul>
<li>for a linear function:</li>
</ul>
<p><span id="eq-grad-lin-fn"><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20=%20%5Csum%20%5Cmathbf%7Bw%7D%5ET%20%5Cmathbf%7Bx%7D(s)%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%7D%7B%5Cpartial%20w_i%7D%20=%20%5Cmathbf%7Bx_i%7D(s)%20%5C%5C%0A%5Cnabla%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20=%20%5Cmathbf%7Bx%7D(s)%20%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<ul>
<li>we can write the update rule for the weights as:</li>
</ul>
<p><span id="eq-grad-descent"><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bt+1%7D%20%5Cdot=%20w_t%20-%20%5Calpha%20%5Cnabla%20J(%5Cmathbf%7Bw_t%7D)%20%5Cqquad%0A%5Ctag%7B5%7D"></span></p>
<ul>
<li>Stochastic gradient descent is a variant of gradient descent that uses a random sample of the data to estimate the gradient.</li>
<li>Stochastic gradient descent uses mini-batches of data to estimate the gradient, which makes it computationally efficient and reduces the variance of the gradient estimate.</li>
<li>In practice we will use variants like:
<ul>
<li>Adam - which adapts the learning rate based on the gradient.</li>
<li>RMSProp - which uses a moving average of the squared gradient.</li>
<li>Adagrad - which uses a different learning rate for each parameter.</li>
<li>SGD - which uses a fixed learning rate.</li>
</ul></li>
</ul>
</section>
<section id="sec-l2g7" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g7">Gradient descent converges to stationary points</h2>

<div class="no-row-height column-margin column-container"><div id="fig-gradient-descent" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/rl-sgd-convergence.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: gradient descent
</figcaption>
</figure>
</div></div><ul>
<li>Gradient descent converges to stationary points because the gradient of the mean-squared value error is zero at the minimum.</li>
<li>Gradient descent can get stuck in a local minima, so it is important to use a good initialization and learning rate.</li>
<li>Stochastic gradient descent can escape a local minima because it uses a random sample of the data to estimate the gradient.</li>
<li>In general the optimizer is not guaranteed to find the global minimum of the function - just a local minima</li>
</ul>
</section>
<section id="sec-l2g8" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g8">How to use Gradient descent and Stochastic gradient descent to minimize the value error</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla%20J(%5Cmathbf%7Bw%7D)%20&amp;%20=%20%5Cnabla%20%5Csum_%7Bs%5Cin%20S%7D%20%5Cmu(s)%5Bv_%5Cpi(s)%20-%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%5D%5E2%20%5Cqquad%20%20%5Cqquad%20%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20=%20%3C%5Cmathbf%7Bw%7D,%5Cmathbf%7Bx%7D(s)%3E%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%20%5Csum_%7Bs%5Cin%20S%7D%20%20%5Cmu(s)%20%5Cnabla%20%5Bv_%5Cpi(s)%20-%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%5D%5E2%20%5Cqquad%20%20%5Cqquad%20%5Cnabla%20%20%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20=%20%5Cmathbf%7Bx%7D(s)%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%20%5Csum_%7Bs%5Cin%20S%7D%20%5Cmu(s)%202%20%5Bv_%5Cpi(s)%20-%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%5D%5Cnabla%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%0A%5Cend%7Balign*%7D%20%5Cqquad%0A"></p>
<section id="stochastic-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>If we have a sample of states <img src="https://latex.codecogs.com/png.latex?s_1,%20s_2,%20%5Cldots,%20s_n"> observed by following <img src="https://latex.codecogs.com/png.latex?pi"><br>
we can write the update rule for a pair of weights as:</p>
<p><span id="eq-sgd"><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bt+1%7D%20%5Cdot=%20w_t%20+%20%5Calpha%20%5Bv_%5Cpi(S_1)%20-%20%5Chat%7Bv%7D(s_1,%20%5Cmathbf%7Bw_1%7D)%5D%5Cnabla%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20%5Cqquad%0A%5Ctag%7B6%7D"></span></p>
<p>This allows us to decrease the error in the value function by making updates for one state at a time and moving the weights in the direction of the negative gradient. By making this type of update we might increase the error occasionally but in the long run we will decrease the error.</p>
<blockquote class="blockquote">
<p>This updating approach is called stochastic gradient descent, because it only uses a stochastic estimate of the gradient. In fact, the expectation of each stochastic gradient equals the gradient of the objective. You can think of this stochastic gradient as a noisy approximation to the gradient that is much cheaper to compute, but can nonetheless make steady progress to a minimum – Martha White</p>
</blockquote>
<p>we have here one issue - we don’t know the true value of the policy <img src="https://latex.codecogs.com/png.latex?v_pi(s_1)">, how do we get around this?</p>
<p>one option is to replace the true value with an estimate, one option is to use the return from the state <img src="https://latex.codecogs.com/png.latex?s_1">.</p>
<p>recall that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Av_%5Cpi(s)%20=%20%5Cmathbb%7BE%7D%5BG_t%20%5Cmid%20S_t%20=%20s%5D%20%5Cqquad%0A"></p>
<p>so we can substitute the true value with the return from the state <img src="https://latex.codecogs.com/png.latex?s_1">.</p>
<p><span id="eq-gradient-mc-update"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Aw_%7Bt+1%7D%20&amp;%20%5Cdot=%20w_t%20+%20%5Calpha%20%5Bv_%5Cpi(S_1)%20-%20%5Chat%7Bv%7D(s_1,%20%5Cmathbf%7Bw_1%7D)%5D%5Cnabla%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20%5Cqquad%20%5C%5C%0A%20%20%20%20%20%20%20%20&amp;%20%5Cdot%20=%20w_t%20+%20%5Calpha%20%5BG_1%20-%20%5Chat%7Bv%7D(s_1,%20%5Cmathbf%7Bw_1%7D)%5D%5Cnabla%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20%5Cqquad%0A%5Cend%7Balign*%7D%0A%5Ctag%7B7%7D"></span></p>
</section>
</section>
<section id="sec-l2g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g4">The gradient Monte Carlo algorithm for value estimation</h2>
<p>We now have a way to update the weights of the value function using the gradient of the mean-squared value error. Which allows us to present the gradient Monte Carlo algorithm for value estimation.</p>
<div id="nte--gradient-mc" class="alg callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;1: MC prediction fist visit for estimating <img src="https://latex.codecogs.com/png.latex?V%20%5Capprox%20v_%5Cpi">
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-gradient-mc" class="pseudocode-container quarto-float" data-line-number="true" data-no-end="false" data-pseudocode-number="1" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-line-number-punc=":" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{GradientMC($\pi$)}\begin{algorithmic} \State Input \State $\qquad \pi$ to be evaluated \State $\qquad \text{a differentiable function } \hat{v}: \mathcal{S} \times \mathbb{R}^d \rightarrow \mathbb{R}$ \State Algorithm parameters: \State $\qquad \alpha \in (0, 1]$ step size \State Initialize: \State $\qquad \mathbf{w} \leftarrow x \in \mathbb{R^d} \text{ arbiterly}$ (e.g. w=0) \For {each episode:} \State Generate an episode by following $\pi: S_0, A_0, R_1, S_1, A_1, R_2,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0$ \For {each step of episode, $t = 0, 1, \ldots, T-1$:} \State $\mathbf{w} \leftarrow w_t + \alpha [G_t - \hat{v}(S_t , \mathbf{w_1})] \nabla \hat{v}(S_t , \mathbf{w})$ \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<ul>
<li>The Gradient Monte Carlo algorithm is a policy evaluation algorithm that uses stochastic gradient descent to minimize the mean-squared value error.</li>
</ul>
</section>
<section id="sec-l2g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g5">How state aggregation can be used to approximate the value function</h2>

<div class="no-row-height column-margin column-container"><div id="fig-gradient-mc-state-agg" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-mc-state-agg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/rl-gradient-mc-with-state-agg.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-mc-state-agg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: gradient mc with state aggregation
</figcaption>
</figure>
</div></div><ul>
<li>State aggregation</li>
<li>is a method for reducing the dimensionality of the state space by grouping similar states together.</li>
<li>can be used to approximate the value function by representing each group of states as a single state.</li>
<li>can be used to reduce the number of parameters in the value function and improve generalization.</li>
<li>the example used a 1000 state MDP with 10 groups of 100 states each.</li>
<li>left and right jump left 1-100 states and right 1-100 states.</li>
<li>if they pass the terminal state they get to the terminal state.</li>
<li>state aggregation is a way to reduce the number of parameters in the value function by grouping similar states together.</li>
<li>it is an example of linear function approximation.</li>
<li>there is one feature for each group of states.</li>
<li>the weights are updated using the gradient of the mean-squared value error.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw%20%5Cleftarrow%20w%20+%20%5Calpha%20%5BG_t%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%0A"></p>
<p>and the gradient of the approximate value function is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%20=%20%5Cmathbf%7Bx%7D(S_t)%0A"></p>
<p>which is either 1 or 0 depending on the group of states.</p>
</section>
<section id="sec-l2g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g6">Applying Gradient Monte-Carlo with state aggregation</h2>
<ul>
<li>Gradient Monte Carlo with state aggregation is a policy evaluation algorithm that uses state aggregation to approximate the value function.</li>
<li>The algorithm works as follows:</li>
</ul>
</section>
</section>
<section id="lesson-3-the-objective-for-td" class="level1 page-columns page-full">
<h1>Lesson 3: The Objective for TD</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Understand</em> the <strong>TD-update</strong> for function approximation #</label></li>
<li><label><input type="checkbox" checked=""><em>Highlight</em> the advantages of TD compared to Monte-Carlo #</label></li>
<li><label><input type="checkbox" checked=""><em>Outline</em> the <strong>Semi-gradient TD(0)</strong> algorithm for value estimation #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> that TD converges to a <strong>biased</strong> value estimate #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> that TD converges much <strong>faster</strong> than Gradient Monte Carlo #</label></li>
</ul>
</div>
</div>
<section id="sec-l3g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g1">The TD-update for function approximation</h2>
<ul>
<li>recall the Monte Carlo update rule:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw%20%5Cleftarrow%20w%20+%20%5Calpha%20%5BG_t%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%0A"></p>
<ul>
<li>we can use other targets for the update rule than the return <img src="https://latex.codecogs.com/png.latex?G_t">.</li>
<li>we can replace the return with any estimate of the value of the next state.</li>
<li>we can call this target <img src="https://latex.codecogs.com/png.latex?U_t"> and if it is unbiased it converge to a local minimum of the mean squared value error.</li>
<li>we can use the one step bootstrapped return:</li>
</ul>
<p><span id="eq-td-target"><img src="https://latex.codecogs.com/png.latex?%0AU_t%20%5Cdot=R_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)%0A%5Ctag%7B8%7D"></span></p>
<ul>
<li>but this is not unbiased because it essential approximating the expected return using the current value of the state.</li>
<li>there is no guarantee that the the update will converge to a local minimum of the mean squared value error.</li>
<li>however the update has many advantages over the Monte Carlo update:
<ul>
<li>it has lower variance because it uses a single sample.</li>
<li>it can update the value function after every step.</li>
<li>it can learn online.</li>
<li>it can learn from incomplete episodes.</li>
<li>it can learn from non-episodic tasks.</li>
</ul></li>
<li>the TD-update is nor a true gradient update because the target is not the true value of the state. we call it a semi-gradient update.
<ul>
<li>let’s estimate the gradient of the mean squared value error with respect to the weights <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D">:</li>
</ul></li>
</ul>
<p><span id="eq-td-grad"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla%20J(%5Cmathbf%7Bw%7D)%20&amp;%20=%20%5Cnabla%20%5Cfrac%7B1%7D%7B2%7D%5BU_t%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%5D%5E2%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20(U_t%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D))%20(%5Cnabla%20U_t%20-%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D))%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20%5Cne%20-%20(U_t%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D))%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%20%5Cquad%20%5Ctext%7Bunless%7D%20%5Cquad%20%5Cnabla%20%20U_t%20=%200%0A%5Cend%7Balign*%7D%0A%5Ctag%7B9%7D"></span></p>
<ul>
<li>but for TD we have:</li>
</ul>
<p><span id="eq-td-grad-ut"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla%20U_t%20=%20&amp;%20=%20%5Cnabla%20(R_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D))%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cgamma%20%5Cnabla%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20%5Cne%200%0A%5Cend%7Balign*%7D%0A%5Ctag%7B10%7D"></span></p>
<ul>
<li>So the TD-update isn’t a true gradient update. However TD often converge in many cases we care updates.</li>
</ul>
</section>
<section id="sec-l3g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g2">Advantages of TD compared to Monte-Carlo</h2>
<ul>
<li>Adam point out that in Gradient Monte Carlo we need to run the alg for a long time and decay the step size to get convergence. But that in practice we don’t decay the step size and we use a fixed step size.<sup>1</sup></li>
<li>TD has several advantages over Monte-Carlo:
<ul>
<li>TD can update the value function after every step, while Monte-Carlo can only update the value function after the episode is complete.</li>
<li>TD can learn online, while Monte-Carlo can only learn offline.</li>
<li>TD can learn from incomplete episodes, while Monte-Carlo requires complete episodes.</li>
<li>TD can learn from non-episodic tasks, while Monte-Carlo can only learn from episodic tasks.</li>
</ul></li>
</ul>
</section>
<section id="sec-l3g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g3">The Semi-gradient TD(0) algorithm for value estimation</h2>
<ul>
<li>The Semi-gradient TD(0) algorithm is a policy evaluation algorithm that uses the TD-update for function approximation.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Semi-gradient TD(0) algorithm for estimating <img src="https://latex.codecogs.com/png.latex?v_%5Cpi">
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-td-zero" class="pseudocode-container quarto-float" data-line-number="true" data-no-end="false" data-pseudocode-number="2" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-line-number-punc=":" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{Semi-gradient TD(0) for estimating $v_\pi$}\begin{algorithmic} \State Input: \State $\qquad \pi \leftarrow \text{policy to be evaluated}$ \State $\qquad \text{a differentiable function } \hat{v}: \mathcal{S} \times \mathbb{R}^d \rightarrow \mathbb{R}$ \State Algorithm parameters: \State $\qquad \alpha \in (0, 1]$ step size \State $\qquad \gamma \in [0, 1]$ discount factor \State Initialize: \State $\qquad value function weights w \leftarrow x \in \mathbb{R}^d \quad \forall s \in \mathcal{S}$ (e.g. w=0) \FORALL {episode $e$:} \State $Initialize S$ \FORALL {step $S \in e$:} \State $\text{Choose } A \sim \pi(\cdot \mid S)$ \State Take action $A$, observe $R, S'$ \State $w \leftarrow w + \alpha [R + \gamma \hat{v}(S', \mathbf{w}) - \hat{v}(S, \mathbf{w})] \nabla \hat{v}(S, \mathbf{w})$ \State $S \leftarrow S'$ \State until $S$ is terminal \ENDFOR \ENDFOR \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
</section>
<section id="sec-l3g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g4">TD converges to a biased value estimate</h2>
<ul>
<li>TD converges to a biased value estimate because it updates the value function using an estimate of the next state.</li>
<li>The bias of TD can be reduced by using a smaller step size or by using a more accurate estimate of the next state.</li>
</ul>
</section>
<section id="sec-l3g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g5">TD converges much faster than Gradient Monte Carlo</h2>

<div class="no-row-height column-margin column-container"><div id="fig-early-learning-mc-vs-td" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-early-learning-mc-vs-td-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/rl-early-learning-mc-vs-td.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-early-learning-mc-vs-td-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: early learning experiment
</figcaption>
</figure>
</div></div><ul>
<li>We run the same random walk experiment for the 1000 episodes 1000 step random walk and we see that TD has a worse fit than MC on most of the range.</li>
<li>We run a second experiments with only 30 episodes to see early learning performance and we see that TD has a better fit than MC on most of the range. In this case we used the best alpha for each method. MC needed a much smaller alpha to get a good fit.</li>
<li>TD converges much faster than Gradient Monte Carlo because it updates the value function after every step.</li>
<li>Gradient Monte Carlo can only update the value function after the episode is complete, which can be slow for long episodes.</li>
<li>TD can learn online, while Gradient Monte Carlo can only learn offline.</li>
<li>TD can learn from incomplete episodes, while Gradient Monte Carlo requires complete episodes.</li>
<li>TD can learn from non-episodic tasks, while Gradient Monte Carlo can only learn from episodic tasks.</li>
</ul>
</section>
<section id="sec-l3g6" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g6">Doina Precup’s talk on Building Knowledge for AI Agents with Reinforcement Learning</h2>

<div class="no-row-height column-margin column-container"><div id="fig-generelization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-generelization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/rl-dorina-precup.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generelization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Dorina Precup
</figcaption>
</figure>
</div><div id="fig-options" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-options-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/rl-dorina-options.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-options-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Options are a temporal generalization
</figcaption>
</figure>
</div></div>
<p>In this talk, <a href="https://rl.cs.mcgill.ca/people/doina-precup/">Dorina Precup</a> discusses the challenges of building knowledge for AI agents using reinforcement learning.</p>
<ul>
<li>Dorina Precup is a professor at McGill University and a research team lead at DeepMind.</li>
<li>She is an expert in reinforcement learning and machine learning.</li>
<li>Her interests are in the areas of abstractions.</li>
<li>When I think about generalization in RL I think about:
<ul>
<li>Learning a parameterized value function that can be used to estimate the value of any state.</li>
<li>Learning a parameterized policy that can be used to select actions in any state.</li>
<li>Being able to transfer this policy to a similar task</li>
<li>Being able to learn using less interaction with the environment and more from replaying past experiences.</li>
<li>Being able to learn from a small number of examples.</li>
</ul></li>
<li>Dorina talks about two other aspects of generalization:
<ul>
<li>Action duration are one time step in an MDP, yet in reality some actions like traveling from one city to another require sticking to the action over an extended period of time.</li>
</ul></li>
<li>This might be happen through planning but idealy, agents should be able to learn skills which are sequences of actions that are executed over an extended period of time.</li>
<li>This has been formalized in the literature as options.</li>
<li>She references two sources
<ul>
<li><span class="citation" data-cites="Sutton1999BetweenMA">[@Sutton1999BetweenMA]</span> a paper from 1999 on options in reinforcement learning.</li>
<li><span class="citation" data-cites="precup2000temporal">[@precup2000temporal]</span> her doctoral thesis from 2000 on temporal abstraction in reinforcement learning.</li>
</ul></li>
<li>Options consists of
<ul>
<li>an initiation set <img src="https://latex.codecogs.com/png.latex?%5Ciota_%5Comega(s)"> the precondition which is a probability of starting the option in state <img src="https://latex.codecogs.com/png.latex?s">.</li>
<li>a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Comega(a%5Cmid%20s)"> that is executed in the option</li>
<li>a termination condition <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%5Comega(s)">. the termination condition is a probability of terminating the option in state <img src="https://latex.codecogs.com/png.latex?s">.</li>
</ul></li>
<li>Options are “chunks of behavior” that can be executed over an extended period of time.</li>
<li>The model will need to learn options and work with them.</li>
<li>IT needs expected reward over the option.</li>
<li>A transition model over the option.</li>
<li>These models are predictive models about outcomes conditioned on the model being executed.</li>
<li>Adding options to the model weakens the MDP assumption, because the option duration is not fixed so state now have a longer dependence is a sequence of actions that are not Markovian <sup>2</sup>.</li>
<li>Precup’s point out that combining temporal and spatial abstraction is an ongoing research challenge.</li>
<li>She also points out that the model needs to learn the options and the value function at the same time.</li>
<li>According to her profile Precup has a number of students working on this problem. Some additional references are:
<ul>
<li><span class="citation" data-cites="Bacon2016TheOA">[@Bacon2016TheOA]</span> a paper from 2016 on option-critic architecture which extends actor-critic algorithms to work with options.</li>
</ul></li>
<li>Earlier work uses the term macro-actions to refer to options.
<ul>
<li><span class="citation" data-cites="bradtke1994reinforcement">[@bradtke1994reinforcement]</span> a paper from 1994 on reinforcement learning with hierarchies of machines.</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Options &amp; CI
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>This type of formulation seems very similar to that used by <a href="https://www.youtube.com/watch?v=JxzjdTc15A0">Judea Pearl</a> in his structureal graphical model of Causality. If we can express options as a graph of states we can use his algorithms to infer the best options to take in a given state.</li>
<li>options are like do operations (interventions)</li>
<li>choosing between options is like conterfactual reasoning.</li>
</ul>
</div>
</div>
</section>
</section>
<section id="lesson-4-linear-td" class="level1">
<h1>Lesson 4: Linear TD</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Derive</em> the TD-update with linear function approximation #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> that tabular TD(0) is a special case of linear semi-gradient TD(0) #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> why we care about linear TD as a special case #</label></li>
<li><label><input type="checkbox" checked=""><em>Highlight</em> the advantages of linear value function approximation over nonlinear #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> the <strong>fixed point</strong> of linear TD learning #</label></li>
<li><label><input type="checkbox" checked=""><em>Describe</em> a theoretical guarantee on the mean squared value error at the <strong>TD fixed point</strong> #</label></li>
</ul>
</div>
</div>
<section id="sec-l4g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g1">Deriving the TD-update with linear function approximation</h2>
<ul>
<li>Linear function is both:</li>
<li>simple enough to be understood, yet</li>
<li>powerful enough that with TD to be useful to create agents that are stornger than human Atari games.</li>
<li>The TD-update with linear function approximation is a way to update the weights of the value function using the TD-error.</li>
<li>The TD-update with linear function approximation works as follows:
<ul>
<li>Compute the TD-error <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> as the difference between the one-step bootstrapped return and the approximate value of the next state.</li>
<li>Update the weights <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> in the direction of the TD-error.</li>
</ul></li>
</ul>
<p>recall the TD-update rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdelta%20%5Cdot=%20R_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%20%5C%5C%0Aw%20%5Cleftarrow%20w%20+%20%5Calpha%20%5Cdelta_t%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%0A"> in the linear case we can write the value function as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%20%5Cdot%20=%20%5Csum%20%5Cmathbf%7Bw%7D%5ET%20%5Cmathbf%7Bx%7D(S_t)%20%5C%5C%0A%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%20=%20%5Cmathbf%7Bx%7D(S_t)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw%20%5Cleftarrow%20w%20+%20%5Calpha%20%5Cdelta_t%20%5Cmathbf%7Bx%7D(S_t)%0A"></p>
</section>
<section id="sec-l4g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g2">Tabular TD(0) is a special case of linear semi-gradient TD(0)</h2>
<ul>
<li>Tabular TD(0) is a special case of linear semi-gradient TD(0) where the features are one-hot encoded.</li>
<li>In the tabular case, the weights are the same as the value function in the table.</li>
<li>In the linear case, the weights are the parameters of the value function.</li>
<li>Tabular TD(0) can be seen as a special case of linear semi-gradient TD(0) where the features are one-hot encoded.</li>
</ul>
</section>
<section id="sec-l4g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g4">Advantages of linear value function approximation over nonlinear</h2>
<ul>
<li>Linear value function approximation has several advantages over nonlinear value function approximation:
<ul>
<li>Linear value function approximation is computationally efficient and easy to implement.</li>
<li>Linear value function approximation is easy to interpret and understand.</li>
<li>Linear value function approximation is less prone to overfitting than nonlinear value function approximation.</li>
<li>Linear value function approximation can be used to approximate any function, while nonlinear value function approximation is limited by the choice of features.</li>
</ul></li>
<li>If we have access to expert knowledge we can use it to define good features and use linear value function approximation to learn the value function quickly.</li>
<li>Most of the theory of function approximation in reinforcement learning is based on linear value function approximation.</li>
</ul>
</section>
<section id="sec-l4g5" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g5">The fixed point of linear TD learning</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bt+1%7D%20%5Cdot=%20w_t%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%0A"> recall that in the linear case we defined the approximate value function as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)%20%5Ccdot=%20%20%5Cmathbf%7Bw%7D%5ET%20%5Cmathbf%7Bx%7D(S_%7Bt+1%7D)%0A"> with :</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> the weights of the value function</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D(S_%7Bt+1%7D)"> the features of the next state</li>
</ul>
<p>using this definition we can write the update rule as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Aw_%7Bt+1%7D%20&amp;%20=%20w_t%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Cmathbf%7Bw%7D%5ET%20%5Cmathbf%7Bx%7D_%7Bt+1%7D%20-%20%5Cmathbf%7Bw%7D%5ET%20%5Cmathbf%7Bx%7D%5D%20%5Cmathbf%7Bx%7D_t%20%5Cnewline%0A%20%20%20%20%20%20%20%20&amp;=%20%20w_t%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20%5Cmathbf%7Bx%7D_t%20-%20%20%5Cmathbf%7Bx%7D_t(%20%5Cmathbf%7Bx%7D_t%20-%20%5Cgamma%20%5Cmathbf%7Bx_%7Bt+1%7D%7D)%5ET%20%5Cmathbf%7Bw_t%7D%5D%20%5Cnewline%0A%5Cend%7Balign*%7D%0A"></p>
<p>let us now consider what this update looks like in expectation:</p>
<p>we can think about it as an expected update plus a noise term but the noise term is dominated by the behaviour of the expected update.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5B%5CDelta%20w_%7Bt%7D%5D%20%20=%20%5Calpha(b-Aw_t)%0A"> where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?b%20=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20%5Cmathbf%7Bx%7D_t%5D"> - expectation over the features and the rewards</li>
<li><img src="https://latex.codecogs.com/png.latex?A%20=%20%5Cmathbb%7BE%7D%5B%5Cmathbf%7Bx%7D_t(%20%5Cmathbf%7Bx%7D_t%20-%20%5Cgamma%20%5Cmathbf%7Bx_%7Bt+1%7D%7D)%5ET%5D"> - an expectation over the rewards</li>
</ul>
<p>note: this is a linear system of equations that looks like a <strong>linear regression problem</strong>.</p>
<p>when the weights do not change we have a fixed point:</p>
<p><span id="eq-fixed-point"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BE%7D%5B%5CDelta%20w_%7BTD%7D%5D%20&amp;%20=%20%5Calpha(b-Aw_%7BTD%7D)%20=%200%20%5Cnewline%0A%5Cimplies%20&amp;%20w_%7BTD%7D%20=%20A%5E%7B-1%7Db%0A%5Cend%7Balign*%7D%0A%5Ctag%7B11%7D"></span> more generally <img src="https://latex.codecogs.com/png.latex?w_%7BTD%7D"> is the solution to this equation and we could show that it minimises</p>
<p><span id="eq-min-lin-td"><img src="https://latex.codecogs.com/png.latex?%0A(b-Aw)%5ET(b-Aw)%0A%5Ctag%7B12%7D"></span></p>
<p>this is related to Bellman equations via the projected Bellman error.</p>
</section>
<section id="sec-l4g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g6">Theoretical guarantee on the mean squared value error at the TD fixed point</h2>
<p><span id="eq-td-fixed-point-and-minimum-ve"><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7BVE%7D(w_%7BTD%7D)%20%5Cleq%20%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%20%5Cmin_%7Bw%7D%20%5Coverline%7BVE%7D(w)%0A%5Ctag%7B13%7D"></span></p>
<ul>
<li>if <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Capprox%201"> then the mean squared value error at the TD fixed point can be large</li>
<li>if <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Capprox%200"> then the mean squared value error at the TD fixed point can be small</li>
<li>if the features representation is good then the two will be equal regardless of <img src="https://latex.codecogs.com/png.latex?%5Cgamma">. since both will be almost zero.</li>
</ul>
</section>
<section id="sec-l4g7" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g7">Semi-gradient TD(0) algorithm</h2>
<p>In the assignment I implemented the Semi-gradient TD(0) algorithm for value estimation.</p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>why not?↩︎</p></li>
<li id="fn2"><p>the property that the future is independent of the past given the present↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c3-w1.html</guid>
  <pubDate>Sun, 31 Mar 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>On-Policy Prediction with Approximation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c3-w1.1.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<p>Some of the notes I made in this course became a bit too long. Rather than break the flow of the lesson I decided to move them to a separate file. This is one of those notes.</p>
<section id="a-few-thought-on-generalization-and-discrimination-in-rl" class="level1">
<h1>A few Thought on Generalization and discrimination in RL</h1>
<p>There are a couple of issues on generalization.</p>
<section id="how-are-generalization-in-ml-is-closely-related-to-transfer-learning-in-rl." class="level2">
<h2 class="anchored" data-anchor-id="how-are-generalization-in-ml-is-closely-related-to-transfer-learning-in-rl.">How are generalization in ML is closely related to transfer learning in RL.</h2>
<p>In ML we have a rather clear understanding of generalization. We have a training set and a test set. We train on the training set and then test on the test set. The goal is to do well on the test set. The test set is a sample from the same distribution as the training.</p>
<p>Geometrically, for classification we want to a decision boundary that separates the classes with the least error and with a fewest number of parameters. This is the essence of the bias-variance tradeoff.</p>
<p>In RL we tend to think of generalization as the ability of an agent to perform well on a task that is different from the one it was trained on. The algorithms can take decades of CPU compute to solve a simple video game. But change a few pixels in the game and the agent can’t play at all. This suggest these agents are severely overfitting.</p>
<p>Would we be able to learn much faster if we could avoid overfitting i.e.&nbsp;if we could generalize better?</p>
<p>One point worth considering here is that solving a general problem is harder than a specific one.</p>
<p>E.g. To solve a maze we need a policy matrix. To solve all mazes we need an algorithm.</p>
<p>So in one sense it is harder to generalize than to solve a specific problem. However it also should allow us to discard most of the irrelevant information that take up most of the model’s capacity and end up slowing down learning.</p>
<ul>
<li>Agents learn a policy that is only suitable to a specific task. The policy doesn’t generalize to even small changes in the task, e.g.&nbsp;moving the start and goal in the same maze tasks.</li>
<li>Learned representation for features are not abstract and thus can’t be mapped to a slightly different task (e.g.&nbsp;changing a few pixels in a game)</li>
<li>We definitely can’t map the representation to different tasks.</li>
<li>Ideally, we would like to deal with challenging problems by reusing knowledge from agents trained on other problems.</li>
<li>One direction called options lays in decomposing learning a policy for a goal into reframing it into learning sub-goals, strategies and tactics and basic moves.</li>
<li>Another direction I call heuristics concerns finding minimal policies that are just strong enough to get the agent to the goal a high percentage of the time.</li>
<li>Learning should be aggregational and compositional. However, these terms require reinterpretation for each problem and at many levels of abstraction.</li>
</ul>
</section>
<section id="human-like-to-use-heuristic-which-are-are" class="level2">
<h2 class="anchored" data-anchor-id="human-like-to-use-heuristic-which-are-are">Human like to use Heuristic, which are are:</h2>
<pre><code>- A minimal sub-optimal policy that is suffiecnt to get the agent to its goal with high probability.
- In an MDP with lots of sub-goals, we may have benefit in learning learning heuristic style policy for each sub-goal and then compose them into a policy for the goal. 
- Composing heuristics is vague so let try make it clear.
    - We want to follow the heuristic policy until we reach a sub-goal.
    - We then switch to the policy for the next sub-goal. 
    - If we have well established entry and exit points for each heuristic we can have two benefits one is generalization and the other is discrimination.
        - Generalization is due to using the same heuristic from different starting points.
        - Discrimination is due to having different heuristics for different sub-goals.
        - A third advantage is that the heuristic policy is for a smaller state space and can be learned faster.
        - Third advantage is may be that of mapping different sub-problem to the same heuristic may allow us to discard some of the features of the state space that are not required for the heuristic to work.
    - Thus composing heuristics in this case is just about switching between heuristics at the right time.
    - Another direction is to use the heuristics as a form of  priors for the policy we want to learn.  
    - Simple models are often a good fit for more problems than complex models.
    - If we are good at learning to decompose problems into simpler sub problems and then we might be able to leverage the power of heuristics.

-   Heuristics don't always work but overall they capture the essence of the solution to the problem.
-   Heuristics are usually more general than an optimal policy.
-   A heuristic might be a very good behavior policy for off policy learning the optimal policy.
-   I don't see RL algorithms for heuristics.</code></pre>
</section>
<section id="models-in-rl-try-to-approximate-mdp-dynamics-using-its-transition-and-rewards" class="level2">
<h2 class="anchored" data-anchor-id="models-in-rl-try-to-approximate-mdp-dynamics-using-its-transition-and-rewards">Models in RL try to approximate MDP dynamics using its transition and rewards</h2>
<pre><code>-   In ML we often use boosting and bagging to aggregate very simple models.
-   In RL we often replace the model by sampling from a replay buffer of the agent's past experiences. </code></pre>
</section>
<section id="the-problem-for-a-general-ai-is-very-much-the-problem-of-transfer-learning-in-rl." class="level2">
<h2 class="anchored" data-anchor-id="the-problem-for-a-general-ai-is-very-much-the-problem-of-transfer-learning-in-rl.">The problem for a general ai is very much the problem of transfer learning in RL.</h2>
<ul>
<li>agents learn a very specific policy for a very specific task - the learned representation cannot be mapped to other tasks or even other states in the same task.</li>
<li>if agents learning was decomposed into
<ul>
<li>learning very general policies that solved more abstract problems and then</li>
<li>learning a good composition of these policies to solve the specific problem.</li>
<li>only after getting to this point would the agent try to optimize the policy for the specific task.</li>
<li>e.g.&nbsp;chess
<ul>
<li>learn the basic moves and average value of pieces</li>
<li>learning tactics - short term goals</li>
<li>learning about end game
<ul>
<li>update the value of pieces based on the ending</li>
</ul></li>
<li>learning about strategy
<ul>
<li>positional play
<ul>
<li>learn about pawn formations and weak square
<ul>
<li>value of pawn formations</li>
<li>how they can be used with learned tactics.</li>
</ul></li>
<li>the center
<ul>
<li>add value to pieces based on their position on the board</li>
</ul></li>
<li>open files and diagonals</li>
</ul></li>
<li>long term plans
<ul>
<li>minority attack, king side attack, central breakthrough</li>
<li>creating a passed pawn</li>
<li>exchanging to win in the end game</li>
<li>sacrificing material to get a better position</li>
<li>attacking the king</li>
</ul></li>
<li>castling</li>
<li>piece development and the center</li>
<li>tempo</li>
</ul></li>
<li>localize value of pieces in different positions on the board using the learned tactics and strategy.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="bayesian-models-and-hierarchical-model-encode-knowledge-using-priors-which-can-pool-or-bias-beliefs-towards-a-certain-outcome." class="level2">
<h2 class="anchored" data-anchor-id="bayesian-models-and-hierarchical-model-encode-knowledge-using-priors-which-can-pool-or-bias-beliefs-towards-a-certain-outcome.">Bayesian models and hierarchical model encode knowledge using priors which can pool or bias beliefs towards a certain outcome.</h2>
<pre><code>-   learning in Bayesian models is about updating the initial beliefs based on incoming evidence.</code></pre>
</section>
<section id="ci-may-be-useful-here" class="level2">
<h2 class="anchored" data-anchor-id="ci-may-be-useful-here">CI may be useful here</h2>
<ul>
<li>Is in a big way about mapping knowledge into
<ul>
<li>Statistical joint probabilities,</li>
<li>Casual concepts that are not in the joint distributions like interventions and Contrafactuals, latent, missing, mediators, confounders, etc.</li>
<li>Hypothesizing a causal structural model, deriving a statistical model and Testing it against the data.</li>
<li>Interventions in the form of actions and options -</li>
</ul></li>
<li>Many key ideas in RL are counterfactual reasoning
<ul>
<li>Off-policy learning is about learning from data generated by a different policy.</li>
<li>Options are like do operations (interventions)</li>
<li>Choosing between actions and options is like contrafactual reasoning.</li>
</ul></li>
<li>Using and verifying CI models could be the way to unify the spatial and temporal abstraction in RL.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c3-w1.1.html</guid>
  <pubDate>Sun, 31 Mar 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Sample-based Learning Methods</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c2-w4.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="lesson-1-what-is-a-model" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-1-what-is-a-model">Lesson 1: What is a model?</h2>
<section id="lesson-learning-goals" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Describe what a model is and how they can be used #</label></li>
<li><label><input type="checkbox">Classify models as <strong>distribution models</strong> or <strong>sample models</strong> #</label></li>
<li><label><input type="checkbox">Identify when to use a distribution model or sample model #</label></li>
<li><label><input type="checkbox">Describe the advantages and disadvantages of sample models and distribution models #</label></li>
<li><label><input type="checkbox">Explain why sample models can be represented more compactly than distribution models #</label></li>
</ul>
</section>
<section id="sec-l1g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g1">What is a model and how can it be used?</h3>
<ul>
<li><p>A model is a simplified representation of the environment dynamics</p></li>
<li><p>Models can be used to simulate the environment</p></li>
<li><p>In this course a Model is a function that predicts the next state and reward given the current state and action</p></li>
<li><p>a transition model <img src="https://latex.codecogs.com/png.latex?s_%7Bt_+1%7D%20=%20f(s_t,%20a_t)"> predicts the next state given the current state and action</p></li>
<li><p>a reward model <img src="https://latex.codecogs.com/png.latex?r_%7Bt_+1%7D%20=%20f(s_t,%20a_t)"> predicts the reward given the current state and action</p></li>
</ul>
<p>three other model types are mentioned in the <a href="https://sites.google.com/view/mbrl-tutorial">ICML Tutorial on Model-Based Reinforcement Learning</a></p>
<ul>
<li><strong>Inverse models</strong> predict the action given the current state and next state <img src="https://latex.codecogs.com/png.latex?a_%7Bt+1%7D%20=%20f_s%5E%7B-1%7D(s_t,%20s_%7Bt+1%7D)"></li>
<li><strong>Distances models</strong> predict the distance between the current state and the goal state <img src="https://latex.codecogs.com/png.latex?d_%7Bij%7D%20=d(s,%20s')"></li>
<li><strong>Future return models</strong> predict the future return given the current state and action <img src="https://latex.codecogs.com/png.latex?G_t=Q(s_t,%20a_t)"> or <img src="https://latex.codecogs.com/png.latex?G_t=V(s_t)"></li>
</ul>
<p>Why do we want to use models?</p>
<ul>
<li>model allow us to simulate the environment without interacting with it.</li>
<li>this can increase sample efficiency - e.g.&nbsp;by replaying past experiences to propergate learning from goal to all predecessor states we have visited</li>
<li>this can reduce risks - e.g.&nbsp;by simulating dangerous situations instead of actually experiencing them.</li>
<li>this can reduce costs - e.g.&nbsp;by simulating costly actions in a simulated environment instead of paying the cost in the real environment.</li>
<li>this could be much faster than real-time interaction with the environment. Often in robotics simulation is orders of magnitude faster than real-time interaction.</li>
</ul>
</section>
<section id="sec-l1g2" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l1g2">Types of models</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-models.png" class="img-fluid figure-img"></p>
<figcaption>models</figcaption>
</figure>
</div></div><ul>
<li><strong>Distribution models</strong> predict the probability distribution of the next state and reward</li>
<li><strong>Sample models</strong> predict a single next state and reward</li>
</ul>
<p>Also there are:Environment simulator</p>
<ul>
<li>Chess Programs typicaly can simulate all possible movers and evaluate the board position (tacticaly and strategically). The difference between the current board position and the board position after a move is the reward.</li>
<li><span class="citation" data-cites="Silver2016MasteringTG">[@Silver2016MasteringTG]</span> mentions an <strong>Environment simulator</strong> for the game of go</li>
<li><span class="citation" data-cites="Agostinelli2019SolvingTR">[@Agostinelli2019SolvingTR]</span> used a simulator of the rubik’s cube to train a reinforcement learning agent to solve the cube.</li>
<li><span class="citation" data-cites="Bellemare2012TheAL">[@Bellemare2012TheAL]</span> used a simulator of the game of atari to train a reinforcement learning agent to play atari games.</li>
<li><span class="citation" data-cites="Todorov2012MuJoCoAP">[@Todorov2012MuJoCoAP]</span> used a simulator of the physics of the real world to train a reinforcement learning agent to control a robot.</li>
<li><span class="citation" data-cites="Shen2018MWalkLT">[@Shen2018MWalkLT]</span> used a simulator to train agents to navigate a graph using MCTS.</li>
<li><span class="citation" data-cites="Ellis2019WriteEA">[@Ellis2019WriteEA]</span> used a REPL environment to train a reinforcement learning agent to write code.</li>
</ul>
</section>
<section id="sec-l1g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g3">When to use a distribution model or sample model</h3>
<ul>
<li><strong>Distribution models</strong> are useful when we need to know the probability of different outcomes</li>
<li><strong>Sample models</strong> are useful when we need to simulate the environment</li>
</ul>
</section>
<section id="sec-l1g4" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g4">Advantages and disadvantages of sample models and distribution models</h3>
<ul>
<li><p><strong>Sample models</strong> can be represented more compactly than distribution models</p></li>
<li><p><strong>Distribution models</strong> can be more accurate than sample models</p></li>
<li><p>exact expectations can be computed from distribution models</p></li>
<li><p>assessing risks and uncertainties is easier with distribution models</p></li>
</ul>
</section>
<section id="sec-l1g5" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g5">Why sample models can be represented more compactly than distribution models</h3>
<ul>
<li><p><strong>Sample models</strong> can be represented more compactly than distribution models because they only need to store a single next state and reward</p></li>
<li><p><strong>Distribution models</strong> need to store the joint probability of each possible next state and reward pair</p></li>
<li><p><strong>Sample models</strong> can be more efficient when we only need to simulate the environment</p></li>
</ul>
</section>
</section>
<section id="lesson-2-planning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-2-planning">Lesson 2: Planning</h2>
<section id="lesson-learning-goals-1" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-1">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Explain how planning is used to improve policies #</label></li>
<li><label><input type="checkbox">Describe random-sample one-step <strong>tabular Q-planning</strong> #</label></li>
</ul>
</section>
<section id="sec-l2g1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l2g1">How planning is used to improve policies</h3>
<ul>
<li><strong>Planning</strong> is the process of using a model to improve a policy or value function</li>
<li><strong>Planning</strong> can be used to improve a policy or value function without interacting with the environment</li>
<li><strong>Planning</strong> can be used to improve a policy or value function more efficiently than direct RL updates</li>
</ul>
<p>Random-sample one-step <strong>tabular Q-planning</strong> {#sec-l2g2}</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-wk5-random-sample-one-step-tabular-Q-learning.png" class="img-fluid figure-img"></p>
<figcaption>Q-planning alg overview</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-wk5-Q-planning.png" class="img-fluid figure-img"></p>
<figcaption>random sample one step tabular Q-planning</figcaption>
</figure>
</div></div>
<!-- replace with latex version -->
<ul>
<li><strong>Tabular Q-planning</strong> is a planning algorithm that uses a sample model to improve a policy or value function</li>
<li><strong>Tabular Q-planning</strong> uses a sample model to simulate the environment</li>
<li><strong>Tabular Q-planning</strong> uses the simulated experience to improve a policy or value function</li>
</ul>
<p>advantages of planning</p>
<ul>
<li><strong>Planning</strong> can be more efficient than direct RL updates</li>
<li><strong>Planning</strong> can be used to improve a policy or value function without interacting with the environment</li>
<li><strong>Planning</strong> can be used to improve a policy or value function more efficiently than direct RL updates</li>
</ul>
</section>
</section>
<section id="lesson-3-dyna-as-a-formalism-for-planning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-3-dyna-as-a-formalism-for-planning">Lesson 3: Dyna as a formalism for planning</h2>
<section id="lesson-learning-goals-2" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-2">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Recognize that direct RL updates use experience from the environment to improve a policy or value function #</label></li>
<li><label><input type="checkbox">Recognize that planning updates use experience from a model to improve a policy or value function #</label></li>
<li><label><input type="checkbox">Describe how both direct RL and planning updates can be combined through the <strong>Dyna architecture</strong> #</label></li>
<li><label><input type="checkbox">Describe the <strong>Tabular Dyna-Q algorithm</strong> #</label></li>
<li><label><input type="checkbox">Identify the direct-RL and planning updates in <strong>Tabular Dyna-Q</strong> #</label></li>
<li><label><input type="checkbox">Identify the model learning and search control components of <strong>Tabular Dyna-Q</strong> #</label></li>
<li><label><input type="checkbox">Describe how learning from both direct and simulated experience impacts performance #</label></li>
<li><label><input type="checkbox">Describe how simulated experience can be useful when the model is accurate #</label></li>
</ul>
</section>
<section id="sec-l3g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g1">Direct RL updates use experience from the environment to improve a policy or value function</h3>
<ul>
<li><strong>Direct RL updates</strong> use experience from the environment to improve a policy or value function</li>
<li><strong>Direct RL updates</strong> can be used to improve a policy or value function by interacting with the environment</li>
</ul>
</section>
<section id="sec-l3g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g2">Planning updates use experience from a model to improve a policy or value function</h3>
<ul>
<li><strong>Planning updates</strong> use experience from a model to improve a policy or value function</li>
<li><strong>Planning updates</strong> can be used to improve a policy or value function without interacting with the environment</li>
</ul>
</section>
<section id="sec-l3g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g3">Both direct RL and planning updates can be combined through the <strong>Dyna architecture</strong></h3>
<ul>
<li><strong>Dyna architecture</strong> combines direct RL updates and planning updates to improve a policy or value function</li>
<li><strong>Dyna architecture</strong> uses a model to simulate the environment</li>
<li><strong>Dyna architecture</strong> uses the simulated experience to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l3g4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l3g4">The <strong>Tabular Dyna-Q algorithm</strong></h3>
<ul>
<li><strong>Tabular Dyna-Q</strong> is a planning algorithm that uses a sample model to improve a policy or value function</li>
<li><strong>Tabular Dyna-Q</strong> uses a sample model to simulate the environment</li>
<li><strong>Tabular Dyna-Q</strong> uses the simulated experience to improve a policy or value function</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-wk5-tabular-dyna-Q.png" class="img-fluid figure-img"></p>
<figcaption>The Tabular Dyna-Q algorithm</figcaption>
</figure>
</div></div><p>Exercise 8.2 Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiments?</p>
<p>Dyna-Q+ is like a generalized UCB while Dyna-Q+ is like a generalized epsilon greedy alg. Dyna-Q+ is doing more efficent exploration. It will revisits will be more spread out more over time but it scheme also tends to increases in non independent way - probabilities for unvisited regions keep growing so if it starts exploring it may like doing an extended sequence till it gets to a dead end.<br>
Dyna Q exploration is independent for each state,action combo so retrying sequences get asymptotically less likely with time.</p>
<p>Exercise 8.3 Careful inspection of Figure 8.5 reveals that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?</p>
<p>Dyna-Q+ is more efficient at exploring so it learned a better policy, but since the environment was static Dyna-Q got to catch up, but it never reached the same policy.</p>
<p>Exercise 8.5 How might the tabular Dyna-Q algorithm shown on page 164 be modified to handle stochastic environments? How might this modification perform poorly on changing environments such as considered in this section? How could the algorithm be modified to handle stochastic environments and changing environments?</p>
<p>to hadle a stochastic environment one would need to to model probabilities of stochastic dynamics. One way to do this is to use Bayesian updating with a dericlet prior and a multinomial posterior.<br>
This modification would likely fare much worse since learning low probability transitions would require many visits to discover.<br>
In the case of changing environment it would also take much longer for new state to be reflected in the model (if a state was visited 10 with just one transition and then the transition changed to another state then it would take many more than 10 vistis to quash the old probability and get the new one correct to 10%<br>
This means that we adding a forgetting rule might be better then the plain derichlet-multinomial model.<br>
To handle both stochastic and changing updates we may want to<br>
1. track the recency of the last visit and reward this option like in dyna-q plus.<br>
2. decay old probabilities - would require storing the time for each visit - i.e.&nbsp;path dependent model.<br>
3. A better idea is to use a hirachial model with parial pooling representing short term and long term transitions - this could fix the problem of decay by simply giving greater weight to the smaller more recent model.<br>
The short term would track the last k visits in each state and the long term all the visits. We could then do partial pooling between these two estimators with much greater emphasis on the recent one!<br>
<br>
</p>
</section>
<section id="sec-l3g5" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g5">Direct-RL and planning updates in <strong>Tabular Dyna-Q</strong></h3>
<ul>
<li><strong>Tabular Dyna-Q</strong> uses direct RL updates to improve a policy or value function</li>
<li><strong>Tabular Dyna-Q</strong> uses planning updates to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l3g6" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g6">Model learning and search control components of <strong>Tabular Dyna-Q</strong></h3>
<ul>
<li><strong>Tabular Dyna-Q</strong> uses a sample model to simulate the environment</li>
<li><strong>Tabular Dyna-Q</strong> uses the simulated experience to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l3g7" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g7">Learning from both direct and simulated experience impacts performance</h3>
<ul>
<li>Learning from both direct and simulated experience can improve performance</li>
<li>Learning from both direct and simulated experience can be more efficient than direct RL updates</li>
</ul>
</section>
<section id="sec-l3g8" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g8">Simulated experience can be useful when the model is accurate</h3>
<ul>
<li>Simulated experience can be useful when the model is accurate</li>
<li>Simulated experience can be used to improve a policy or value function without interacting with the environment</li>
</ul>
</section>
</section>
<section id="lesson-4-dealing-with-inaccurate-models" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-4-dealing-with-inaccurate-models">Lesson 4: Dealing with inaccurate models</h2>
<section id="lesson-learning-goals-3" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-3">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Identify ways in which models can be inaccurate #</label></li>
<li><label><input type="checkbox">Explain the effects of planning with an inaccurate model #</label></li>
<li><label><input type="checkbox">Describe how <strong>Dyna</strong> can plan successfully with a partially inaccurate model #</label></li>
<li><label><input type="checkbox">Explain how model inaccuracies produce another exploration-exploitation trade-off #</label></li>
<li><label><input type="checkbox">Describe how <strong>Dyna-Q+</strong> proposes a way to address this trade-off #</label></li>
</ul>
</section>
<section id="sec-l4g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g1">Ways in which models can be inaccurate</h3>
<ul>
<li>Models can be inaccurate for many reasons</li>
<li>because they have not sampled all actions in all states</li>
<li>because the environment is has changed since the model was learned</li>
<li>if the environment is stochastic</li>
</ul>
</section>
<section id="sec-l4g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g2">Effects of planning with an inaccurate model</h3>
<ul>
<li>Planning with an inaccurate model can cause the value function to become worse</li>
<li>Planning with an inaccurate model can lead to sub-optimal policies</li>
</ul>
</section>
<section id="sec-l4g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g3"><strong>Dyna</strong> can plan successfully with a partially inaccurate model</h3>
<ul>
<li><strong>Dyna</strong> can plan successfully with a partially inaccurate model</li>
<li><strong>Dyna</strong> can use direct RL updates to improve a policy or value function as well as the model</li>
<li><strong>Dyna</strong> can use planning updates to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l4g4" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g4">Model inaccuracies produce another exploration-exploitation trade-off</h3>
<ul>
<li><p>Model inaccuracies produce another exploration-exploitation trade-off</p></li>
<li><p>exploit an inaccurate model to improve the policy</p></li>
<li><p>revisit states/actions with low value to update the model</p></li>
<li><p>Can we use an inverse sort of planning to identify states for which the model is inaccurate?</p></li>
<li><p>Model inaccuracies can lead to suboptimal policies</p></li>
<li><p>Model inaccuracies can lead to poor performance</p></li>
</ul>
</section>
<section id="sec-l4g5" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l4g5"><strong>Dyna-Q+</strong> proposes a way to address this trade-off</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-wk5-bonus-rewards-for-exploration.png" class="img-fluid figure-img"></p>
<figcaption>Dyna-Q+ solution</figcaption>
</figure>
</div></div><ul>
<li><strong>Dyna-Q+</strong> proposes a way to address this trade-off</li>
<li><strong>Dyna-Q+</strong> uses a bonus reward to encourage exploration</li>
<li><strong>Dyna-Q+</strong> can improve performance when the model is inaccurate</li>
</ul>
</section>
<section id="drew-bagnell-on-self-driving-cars-robotics-and-model-based-reinforcement-learning" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="drew-bagnell-on-self-driving-cars-robotics-and-model-based-reinforcement-learning">Drew Bagnell on self-driving cars robotics and model-based reinforcement learning</h3>
<p>Drew Bagnell is a professor at Carnegie Mellon University and the CTO at Aurora innovation.</p>
<p>He has worked on self-driving cars and robotics. He has also worked on model-based reinforcement learning. He point out a dirty little secret that model-based reinforcement learning is a key technology for robotics.</p>
<p>He points out that the real world is expensive and dangerous. Using model based reinforcement learning can reduce the number of interactions with the real world and along learning about risky actions in the simulated world to improve performance in the real world. Also as we pointer out before this can usually be done much faster than real-time interaction with the environment.</p>
<p>Sample complexity: how many real-world samples are required to achieve high performance? It takes exponentially fewer interactions with a model than without. Not really sure what exponentially fewer means here - but it’s a lot fewer.</p>
<p>Quadratic value function approximation goes back to optimal control in the 1960s. It’s continuous in states and actions. This is a method that should be part of the next course but isn’t covered there either</p>
<p>For linear transition dynamics with quadratic costs/rewards, it’s exact. For local convex / concave points, it is a good approximation of the true action-value function.</p>
<p>Here is the math from his slide:</p>
<p>Quadratic value function approximation</p>
<p><span id="eq-quadratic-value-function-approximation"><img src="https://latex.codecogs.com/png.latex?%0AQ_t(x,a)%20=%20%5Cbegin%7Bbmatrix%7D%20%20%20%20%20x%20%20%20%5C%5C%20a%20%20%20%5C%5C%20%5Cend%7Bbmatrix%7D%5ET%0A%5Cbegin%7Bbmatrix%7D%20%20%20%20%20Q_%7Bxx%7D%20%20%20&amp;&amp;%20%20Q_%7Bxa%7D%20%20%20%5C%5C%20Q_%7Bxa%7D%20%20%20&amp;&amp;%20%20Q_%7Buu%7D%20%20%20%5C%5C%20%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%20x%20%20%20%5C%5C%20a%20%20%20%5C%5C%20%5Cend%7Bbmatrix%7D%5ET%20+%0A%5Cbegin%7Bbmatrix%7D%20%20%20%20q_x%20%20%20%5C%5C%20q_a%20%5C%5C%20%5Cend%7Bbmatrix%7D%5ET%0A%5Cbegin%7Bbmatrix%7D%20%20%20%20%20x%20%20%20%5C%5C%20a%20%20%20%5C%5C%20%5Cend%7Bbmatrix%7D%20+%20const%0A%5Ctag%7B1%7D"></span></p>
<p>The approximation allows for calculating the optimal action-value in closed form (finite number of standard operations) even with continuous actions.</p>
<p>Differential dynamic programming takes advantage of the technique above.<br>
<br>
So this seems complicated - because of matrix maths. But intuitively this is something we like to do in physics - add the term for the second derivative<br>
in the taylor series approximation of our function.</p>
<p>The 2nd paper is particularly clear and easy to work through for the approach just described.</p>
<div class="column-page">
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">timeline

    title Bandit Algorithms Timeline

    
        1952 : Thompson Sampling
        1955 : Upper Confidence Bound (UCB)
        1963 : Epsilon-Greedy
        2002 : Bayesian UCB
        2011 : Bayesian Bandits
        2012 : Contextual Bandits
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<div class="column-page">
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">%%{init: {'theme': 'base', 'themeVariables': { 'timeline': { 'nodeSpacing': 50, 'sectionSpacing': 100, 'verticalStartPosition': 50, 'verticalSectionStartPosition': 50 }}}}%%
timeline
    direction TD
    title Reinforcement Learning Algorithms Timeline


    
        1948 : Monte Carlo Methods
        1950 : Bellman Optimality Equations
        1957 : Dynamic Programming
        1959 : Temporal Difference Learning (TD)
        1960 : Policy Iteration
        1963 : Value Iteration
        1983 : Q-Learning
        1984 : Expected SARSA
        1990 : Dyna-Q : Dyna-Q+
        1992 : SARSA
        1994 : Monte Carlo with E-Soft
        1995 : Monte Carlo with Exploring Starts
             : Generalized Policy Iteration (GPI)
        1998 : Semi-Gradient TD
        2000 : Differential Semi-Gradient SARSA
        2001 : Gradient Monte Carlo (Gradient MC)
        2003 : Gaussian Actor-Critic
             : Softmax Actor-Critic
             : Deep Q-Network (DQN)


</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<p>Materials from <a href="https://sites.google.com/view/mbrl-tutorial">ICML Tutorial on Model-Based Reinforcement Learning</a>:</p>
<p>the page above contains the following materials as well as an extensive bibliography.</p>
<ul>
<li><a href="https://docs.google.com/presentation/d/1f-DIrIvh44-jmTIKdKcue0Hx2RqQSw52t4k8HEdn5-c/edit?usp=sharing">Slides</a></li>
<li><a href="https://slideslive.com/38930488/modelbased-methods-in-reinforcement-learning-part-1-introduction-learning-models">Part 1: Introduction and Learning Models</a></li>
<li><a href="https://slideslive.com/38930486/modelbased-methods-in-reinforcement-learning-part-2-modelbased-control">Part 2: Model-Based Control</a></li>
<li><a href="https://slideslive.com/38930487/modelbased-methods-in-reinforcement-learning-part-3-modelbased-control-in-the-loop">Part 3: Model-Based Control in the Loop</a></li>
<li><a href="https://slideslive.com/38930489/modelbased-methods-in-reinforcement-learning-part-4-beyond-vanilla-mbrl">Part 4: Beyond Vanilla MBRL</a></li>
</ul>
<p>From Bagnell’s talk:</p>
<ul>
<li><a href="https://macrl-book.github.io/">Modern Adaptive Control and Reinforcement Learning</a></li>
<li><a href="https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf">Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization</a></li>
<li><a href="https://katefvision.github.io/katefSlides/trajectoryoptimization_katef.pdf">Optimal Control, Trajectory Optimization, Learning Dynamics</a></li>
</ul>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Sample-based Learning Methods</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c2-w4.html</guid>
  <pubDate>Sun, 03 Mar 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Temporal Difference Learning Methods for Control</title>
  <link>https://orenbochman.github.io/posts/c2-w3.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="module-3-temporal-difference-learning-methods-for-control" class="level1 page-columns page-full">
<h1>Module 3: Temporal Difference Learning Methods for Control</h1>
<section id="lesson-1-td-for-control" class="level2">
<h2 class="anchored" data-anchor-id="lesson-1-td-for-control">Lesson 1: TD for Control</h2>
<section id="lesson-learning-goals" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox" checked="">Explain how generalized policy iteration can be used with TD to find improved policies #</label></li>
<li><label><input type="checkbox" checked="">Describe the Sarsa Control algorithm #</label></li>
<li><label><input type="checkbox" checked="">Understand how the Sarsa control algorithm operates in an example MDP #</label></li>
<li><label><input type="checkbox" checked="">Analyze the performance of a learning algorithm in an MDP #</label></li>
</ul>
</section>
</section>
<section id="sec-l1g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g1">Generalized Policy Iteration with TD</h2>
<p>we would like now to combine TD with a planning algorithm to use TD for control. We This will be a GPI algorithm.</p>
<section id="generalized-policy-iteration---recap" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="generalized-policy-iteration---recap">Generalized Policy Iteration - Recap</h3>
<p>lets recap the Generalized Policy Iteration (GPI) algorithm:</p>
<ul>
<li><strong>Policy Evaluation</strong>: Update the value function V to be closer to the true value function of the current policy</li>
<li><strong>Policy Improvement</strong>: Improve the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> based on the current value function V</li>
<li><strong>Generalized Policy Iteration</strong>: Repeated steps of policy evaluation and policy improvement</li>
<li>GPI does not require full evaluation of the value function, just an improvement can be used to update the policy.</li>
<li>policy iteration
<ul>
<li>run policy evaluation to convergence</li>
<li>greedifing the policy</li>
</ul></li>
<li>GPI MC
<ul>
<li>each episode:
<ul>
<li>policy evaluation (nota full evaluation)</li>
<li>improvement per episode</li>
</ul></li>
</ul></li>
<li>GPI TD
<ul>
<li>each step:
<ul>
<li>policy evaluation (for just one action)</li>
<li>improvement pi after the single time step.</li>
</ul></li>
</ul></li>
</ul>
</section>
<p>Recall how in the first course we saw DP methods for solving MDPs using the four part dynamic function and its variants. We used the Bellman equation to write down a system of linear equations for the value function and solve them exactly. We then used the value function to find the optimal policy. So in DP we don’t need to interact with the environment or to learn. We can compute the value function and the optimal policy exactly.</p>
<p>In these course we relaxed the assumption of knowing the transition dynamics or the expected returns. This creates a new challange of learning V or Q from experience.</p>
<p>In the first lesson we saw how MC methods can help us learn the value function but with the caveat that we need to wait until the end of the episode to update the value function.</p>
<p>However we have now seen how the TD(0) algorithm uses recursive nature of the Bellman equation for the value function to make approximate updates to the value function. This allows us to learn Values of states directly from experience.</p>
<p>Once we are able to approximate the value function, we can use it to create new generalized policy iteration algorithms. This part of the GPI remains the same, we still evaluate the policy and improve it. But now we can do this in an online fashion, updating the value function after each step.</p>
<p>In SARSA we are making updates to the policy after a single step - this may lead to much faster convergence to the optimal policy. It also allows us to improve our plans during an episode or in a continuing task.</p>
<p>The advantage of TD methods is that they can be used in continuing tasks, where the agent interacts with the environment indefinitely. This is because the value function is updated after each step, and the agent can continue to learn and improve its policy as it interacts with the environment. But this advantage is better understood by considering the episodic tasks, where the agent can learn during an episode that consequences of its actions are sub optimal. This allows TD(0) based GPI to make more frequent updates to the policy within one episode.</p>
</section>
<section id="sec-l1g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g2">Sarsa: On-policy TD Control</h2>
<p>Next we consider how we can derive and use a similar approximate updating of the action-value function to learn the action-value function directly from experience.</p>
<p>lets recap the Bellman equation for the action-value function:</p>
<p><span id="eq-belman-action-value"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Aq_%5Cpi(s,a)%20&amp;%20%5Cdot%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BG_t%20%7C%20S_t%20=%20s,%20A_t%20=%20a%5D%20%5Cqquad%20%5Cnewline%0A&amp;%20=%20%5Csum_%7Bs',%20r%7D%20p(s',%20r%20%7C%20s,%20a)%20%5Br%20+%20%5Cgamma%20%5Csum_%7Ba'%7D%5Cpi(a'%20%5Cmid%20s')%20q_%5Cpi(s',%20a')%5D%20%5Cqquad%0A%5Cend%7Baligned%7D%0A%5Ctag%7B1%7D"></span></p>
<p>We can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.</p>
<p><span id="eq-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,%20A_t)%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20Q(S_%7Bt+1%7D,%20A_%7Bt+1%7D)%20-%20Q(S_t,%20A_t)%5D%0A%5Ctag%7B2%7D"></span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SARSA
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-sarsa" class="pseudocode-container quarto-float" data-line-number="true" data-pseudocode-number="1" data-line-number-punc=":" data-indent-size="1.2em" data-no-end="false" data-comment-delimiter="#" data-caption-prefix="Algorithm">
<div class="pseudocode">
\begin{algorithm} \caption{SARSA($\alpha,\epsilon$)}\begin{algorithmic} \State Initialize: \State $\qquad Q(s,a) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S^+}, a\in\mathcal{A}(s)\ except\ for\ Q(terminal,\cdot)=0$ \For {each episode e:} \State Initialize $S$ \State Choose A from S using policy derived from Q (e.g., $\epsilon$-greedy) \For {each step of e} \State Take action A, observe R, S' \State Choose A' from S' using policy derived from Q (e.g., $\epsilon$-greedy) \State $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma Q(S',A') - Q(S, A)]$ \State $S \leftarrow S'$; $A \leftarrow A'$ \EndFor \State until $S$ is terminal \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<p>The SARSA algorithm is due to Rummery, Gavin Adrian, and Mahesan Niranjan. The name Sarsa is due to Rich Sutton and comes from the fact that the algorithm uses the tuple <img src="https://latex.codecogs.com/png.latex?(S_t,%20A_t,%20R_%7Bt+1%7D,%20S_%7Bt+1%7D,%20A_%7Bt+1%7D)"> to update the action-value function. <span class="citation" data-cites="Rummery1994OnlineQU">[@Rummery1994OnlineQU]</span></p>
<p>SARSA is a sample-based algorithm to solve the Bellman equation for action-values. - It picks an action based on the current policy and then - It policy evaluation by a TD updates of Q the action-value function based on the reward and the next action. - Then it does a policy improvement.</p>
</section>
<section id="sec-l1g3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g3">SARSA in an Example MDP</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-sarsa-windy-gridworld.png" class="img-fluid figure-img"></p>
<figcaption>windy gridworld</figcaption>
</figure>
</div></div><p>In this grid world isn’t a good fit for MC methods as most policies never terminate. This is because the agent is pushed up by the wind and has to learn to navigate to the goal. Anyhow if the episode never terminates MC wont be able to update the value function.</p>
<p>But Sarsa can learn the optimal policy in this environment by abandoning the epsilon greedy policy in mid episode and finding better policies. We can see that early episodes take longer to terminate after the e-greedy policy stops peaks.</p>
</section>
<section id="sec-l1g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g4">Performance of Learning Algorithms in an MDP</h2>
<p>On the right side of the figure we see the performance of the learning algorithms in the windy grid world. We see that in this chart the Sarsa algorithm learns the optimal policy at Around step 7000 where the gradient becomes constant.</p>
<p>Q. why is SARSA called an on-policy algorithm?</p>
<p>this is because it learns by sampling from the policy induced by Q while following the same policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
</section>
<section id="lesson-2-off-policy-td-control-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="lesson-2-off-policy-td-control-q-learning">Lesson 2: Off-policy TD Control: Q-learning</h2>
<section id="lesson-learning-goals-1" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-1">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox" checked="">Describe the Q-learning algorithm #</label></li>
<li><label><input type="checkbox" checked="">Explain the relationship between q-learning and the Bellman optimality equations. #</label></li>
<li><label><input type="checkbox">Apply q-learning to an MDP to find the optimal policy #</label></li>
<li><label><input type="checkbox">Understand how Q-learning performs in an example MDP #</label></li>
<li><label><input type="checkbox">Understand the differences between Q-learning and Sarsa #</label></li>
<li><label><input type="checkbox">Understand how Q-learning can be off-policy without using importance sampling #</label></li>
<li><label><input type="checkbox">Describe how the on-policy nature of SARSA and the off-policy nature of Q-learning affect their relative performance #</label></li>
</ul>
</section>
<p>lets recap the Bellman optimality equation for the action-value function:</p>
<p><span id="eq-belman-optimality-action-value"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Aq_%7B%5Cstar%7D(s,a)%20=%20%5Csum_%7Bs',%20r%7D%20p(s',%20r%20%7C%20s,%20a)%20%5Br%20+%20%5Cgamma%20%5Cmax_%7Ba'%7D%20q_%7B%5Cstar%7D(s',%20a')%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B3%7D"></span></p>
<p>The following is an update rule for Q-learning:</p>
<p>We can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.</p>
<p><span id="eq-q-learning-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,%20A_t)%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Cmax_a%20Q(S_%7Bt+1%7D,%20a')%20-%20Q(S_t,%20A_t)%5D%0A%5Ctag%7B4%7D"></span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q-learning (off-policy TD control)
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-q-learning" class="pseudocode-container quarto-float" data-line-number="true" data-pseudocode-number="2" data-line-number-punc=":" data-indent-size="1.2em" data-no-end="false" data-comment-delimiter="#" data-caption-prefix="Algorithm">
<div class="pseudocode">
\begin{algorithm} \caption{Q-learning Off-policy TD control}\begin{algorithmic} \State Initialize: \State $\qquad Q(s,a) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S^+}, a\in\mathcal{A}(s)\ except\ for\ Q(terminal,\cdot)=0$ \For {each episode e:} \State Initialize $S$ \For {each step of e} \State Choose A from $A_B(S)$ using any ergodic Behavioural policy $B$ - perhaps the $\epsilon$-greedy induced by Q. \State Take action A, observe R, S' \State $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \max_a Q(S',a) - Q(S, A)]$ \State $S \leftarrow S'$ \EndFor \State until $S$ is terminal \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
<p>Note: I made some cosmetic changes to the psuedo code in the book to resolve the confusion I had about nature the behavioral policy.</p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The behavioral policy in Q-learning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Q-learning has a subtle issue I found confusing at first.</p>
<p>Here I first state the issue:</p>
<p>What is the behavioral policy we follow in these three td-learning algorithms when we sample the next action to follow?</p>
<p>We are not writing here that Q function is <img src="https://latex.codecogs.com/png.latex?Q_%5Cpi"> but the value functions are by definitions expectations under some policy. In these algorithms we keep updating the Q function using TD(0) updates. If we update the Q function in a sense that the best action changes at a given step then the updated function now uses a new policy. (In the case of Sarsa we can actually get a worse policy after the update.) I figured this out very quickly.</p>
<p>A fully specified Q-functions isn’t just defined by following a policy. It also <strong>induces a policy</strong>. This in generaly is a stochastic policy. But if we take the greedy action with arbitrary tie breaks we get one or more deterministic policies. So it seems that off policy algorithms like Q-learning and Expected Sarsa are following a sequence of policies that are induced by the Q function that is being learned.</p>
<p>In general off policy learning may be using S,A,R sequences that have been sampled like we clearly did in MC. So the question which arises is can sample from any ergodic policy as our behavioral policy in these off-policy algorithms or are we supposed to learn from experience and sample using the policy induced by latest and greatest Q function that we are learning?</p>
<p><strong>Luckily Martha White is very clear about this</strong>:</p>
<ul>
<li>The target policy is the easy part - we are targeting <img src="https://latex.codecogs.com/png.latex?Q_%7B%5Cpi_%5Cstar%7D">.</li>
<li>The behavior policy is the policy can be any policy so long as it is ergodic.
<ul>
<li>Using an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy policy derived from Q is very logical choice but we could use any other policy.</li>
</ul></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Do these algorithms converge?
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Is Q-learning guaranteed to converge ?</li>
<li>The course glossed over this in lectures perhaps referencing the text book – I will have to go back and check this.</li>
<li>However as this is introductory CS and not Mathematics I will try to suspend my disbelief that the algs are guarenteed to converge and return to the point.</li>
</ul>
</div>
</div>
<ol type="1">
<li>It is an off-policy algorithm.</li>
</ol>
<ul>
<li>The <strong>target policy</strong> is the easy part - we are targeting <img src="https://latex.codecogs.com/png.latex?Q_%7B%5Cpi_%5Cstar%7D">.</li>
<li>The <strong>behavior policy</strong> is the policy that we are following but what is that ?
<ul>
<li>it is clearly not <img src="https://latex.codecogs.com/png.latex?Q_%7B%5Cpi_%5Cstar%7D"> as we don’t know it yet.</li>
<li>we initialized Q(s,a) arbitrarily - so we may have a uniform random policy.</li>
<li>bu we actual have any random policy.
<ul>
<li>any action that is a legit transition from the current state is a valid action.</li>
<li>so long as their probabilities add up to 1.</li>
</ul></li>
<li>later Martha keeps saying that we need the ergodicity of the MDP to ensure that out policy will visit all states and actions with non-zero probability.</li>
<li>this anyhow is one source of confusion.</li>
<li>however an epsilon greedy policy of the induced policy from Q seems like a very good choice. Can we do better ?</li>
<li>another point to consider here is that this is a value iteration algorithm.
<ul>
<li>what can we say about the inermediate Q functions that we are learning ?</li>
<li>are they even a valid action value function ?</li>
<li>is the policy they induce a coherent probability distribution over actions ?</li>
</ul></li>
</ul></li>
</ul>
<ol start="2" type="1">
<li>When we select the action A’ what policy are we using ?</li>
</ol>
<ul>
<li><p>we are clearly not using the policy that we are learning <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Cstar"> - we dont know it yet.</p></li>
<li><p>we are could use an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy policy by greedifying Q. this seems the most logical</p></li>
<li><p>but we could pretty much use any other policy.</p></li>
<li><p>this is because Q-learning is an off-policy algorithm.</p></li>
<li><p>the confusion arises because it is not clear what “any policy derived from Q” means in the algorithm.</p></li>
<li><p>q-learning</p>
<ul>
<li>is a <span class="marked">value iteration algorithm</span></li>
<li>uses <span class="marked">the Bellman optimality equation</span> to update the action-value function.</li>
<li>selects the action based on greedyfing the current q-values and then</li>
</ul></li>
<li><p>it policy evaluation by a TD updates of Q the action-value function based on the reward and the next action.</p></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
why can’t q-learning account for the consequences of exploration in its policy ?
</div>
</div>
<div class="callout-body-container callout-body">
<p>q-learning learns the optimal policy but follows a some other policy. Let suppose the optimal policy is deterministic. And let’s suppose that the behavior policy is epsilon greedy based on that.</p>
<p>The alg does not follow the optimal policy - it follows the behavior policy and this will perform much worse because of exploration.</p>
<p>If we need to account for the consequences of exploration in the policy we need to use a different algorithm!</p>
</div>
</div>
</section>
<section id="sec-l2g4" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g4">Q-learning in an Example MDP</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-sarsa-windy-gridworld.png" class="img-fluid figure-img"></p>
<figcaption>windy gridworld</figcaption>
</figure>
</div></div><p>In this grid world isn’t a good fit for MC methods as most policies never terminate.</p>
<p>Q-learning can learn the optimal policy in this environment by abandoning the epsilon greedy policy in mid episode and finding better policies.</p>
<p>We can see that early episodes take longer to terminate after the e-greedy policy stops peaks.</p>
<p>However Q-learning does not take seem to factor in the consequences of exploration in its policy.</p>
<p>This is because it is learning the optimal policy and not the policy that it follows.</p>
<p>Q-learning does not need Importance sampling to learn off-policy. This is because it is learning action values.</p>
</section>
<section id="sec-l2g5" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g5">Comparing Sarsa and Q-learning</h2>
<p>Q-learning is an off-policy algorithm:</p>
<ul>
<li>the target policy is the optimal policy since the update rule approximates the Bellman optimality equation.</li>
<li>the behavior policy is initially given updated at each step from the inital get updated a bit towards the optimal policy at each step.</li>
</ul>
<p>because it is learning <img src="https://latex.codecogs.com/png.latex?%5Cpi_*"> (the optimal policy) but it samples a different policy.</p>
<p>This is in contrast to Sarsa, which is an on-policy algorithm because it learns the policy that it follows.</p>
</section>
<section id="lesson-3-expected-sarsa" class="level2">
<h2 class="anchored" data-anchor-id="lesson-3-expected-sarsa">Lesson 3: Expected SARSA</h2>
<section id="lesson-learning-goals-2" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-2">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Describe the Expected SARSA algorithm #</label></li>
<li><label><input type="checkbox">Describe Expected SARSA’s behavior in an example MDP #</label></li>
<li><label><input type="checkbox">Understand how Expected SARSA compares to SARSA control #</label></li>
<li><label><input type="checkbox">Understand how Expected SARSA can do off-policy learning without using importance sampling #</label></li>
<li><label><input type="checkbox">Explain how Expected SARSA generalizes Q-learning #</label></li>
</ul>
</section>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AQ(S_t,%20A_t)%20&amp;%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Cmathbb%7BE%7D%5BQ(S_%7Bt+1%7D,%20A_%7Bt+1%7D)%5D%20-%20Q(S_t,%20A_t)%5D%20%5Cnewline%0A&amp;%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Csum_a%20%5Cpi(a%7CS_%7Bt+1%7D)%20%5Ccdot%20Q(S_%7Bt+1%7D,%20a)%20-%20Q(S_t,%20A_t)%5D%0A%5Cend%7Baligned%7D%0A"></p>
<section id="sec-l3g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g1">Understanding Expected Sarsa</h3>
<p>lets recap the Bellman equation for the action-value function:</p>
<p><span id="eq-belman-action-value"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Aq_%5Cpi(s,a)%20&amp;%20%5Cdot%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BG_t%20%7C%20S_t%20=%20s,%20A_t%20=%20a%5D%20%5Cqquad%20%5Cnewline%0A&amp;%20=%20%5Csum_%7Bs',%20r%7D%20p(s',%20r%20%7C%20s,%20a)%20%5Br%20+%20%5Cgamma%20%5Csum_%7Ba'%7D%5Cpi(a'%20%5Cmid%20s')%20q_%5Cpi(s',%20a')%5D%20%5Cqquad%0A%5Cend%7Baligned%7D%0A%5Ctag%7B5%7D"></span></p>
<p>We can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.</p>
<p>in the sarsa update rule:</p>
<p><span id="eq-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,%20A_t)%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20Q(S_%7Bt+1%7D,%20A_%7Bt+1%7D)%20-%20Q(S_t,%20A_t)%5D%0A%5Ctag%7B6%7D"></span></p>
<p>we knows the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> so we can make a better update by replacing the sampled next action with the expected value of the next action under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>This is the basis of Expected Sarsa.</p>
<p>which uses the update rule:</p>
<p><span id="eq-expected-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,%20A_t)%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Csum_a%20%5Cpi(a%7CS_%7Bt+1%7D)%20%5Ccdot%20Q(S_%7Bt+1%7D,%20a)%20-%20Q(S_t,%20A_t)%5D%0A%5Ctag%7B7%7D"></span></p>
<p>Otherwise the algorithm is the same as Sarsa.</p>
<ul>
<li>This target is more stable than the Sarsa target because it is less noisy.</li>
<li>This makes it converge faster than Sarsa.</li>
</ul>
<p>this has a has a down side - it has more computation than Sarsa due to avaraging over many actions for every step.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expected Sarsa
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-q-learning" class="pseudocode-container quarto-float" data-line-number="true" data-pseudocode-number="3" data-line-number-punc=":" data-indent-size="1.2em" data-no-end="false" data-comment-delimiter="#" data-caption-prefix="Algorithm">
<div class="pseudocode">
\begin{algorithm} \caption{expected SARSA}\begin{algorithmic} \State Initialize: \State $\qquad Q(s,a) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S^+}, a\in\mathcal{A}(s)\ except\ for\ Q(terminal,\cdot)=0$ \For {each episode e:} \State Initialize $S$ \State Choose A from S using policy derived from Q (e.g., $\epsilon$-greedy) \For {each step of e} \State Take action A, observe R, S' \State Choose A' from S' using policy derived from Q (e.g., $\epsilon$-greedy) \State $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \sum_a \pi(a|S') Q(S',A') - Q(S, A)]$ \State $S \leftarrow S'$; $A \leftarrow A'$ \State $S$ is terminal \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summary - Connecting the dots
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Expected Sarsa is a generalization of Q-learning and Sarsa.</li>
<li>There are many RL algorithms in our specialization flowchart.</li>
<li>We would like to find a few or even one algorithm that may be widely applicable to many different settings, carrying over the insights we learned from each new algorithm.</li>
<li>The first step in this direction was introducing the <img src="https://latex.codecogs.com/png.latex?epsilon"> parameter to the bandit algorithms, which allowed us to treat the exploration-exploitation trade-off. We have seen additional strategies for exploration but we have been using either an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy strategy or a epsilon-soft strategy in most algorithms.
<ul>
<li>We also got the powerful idea of using confidence intervals as tie breakers in the case of multiple actions with the same expected reward.</li>
</ul></li>
<li>Another step in this direction was to introduce discounting of rewards which let us parameter discounting with <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> and thus treat episodic and continuing tasks in the same way.</li>
<li>The MC algorithms showed that this is not enough to fully generalize to episodic and continuing tasks. However we got a powerful new ideas of inverse sampling and doubly robust estimators. For use with off-policy learning.</li>
<li>Next we introduced GPI in which we combined policy evaluation and policy improvement algorithms to iteratively approximate the optimal policy in a single algorithm.</li>
<li>Another lesson was to using the TD error to bootstrap the value function. This let us update value functions after each step, rather than waiting until the end of the episode, increasing the data efficiency of the algorithms.
<ul>
<li>We also saw that we can use this idea with action-value functions, which is more fine grained than the value function and can lead to more efficient learning.</li>
</ul></li>
<li>Next we saw that Expected Sarsa is one such algorithm that can be used in many different settings.
<ul>
<li>It can be used in episodic and continuing tasks,</li>
<li>It can be used for on-policy and off-policy learning.</li>
<li>It is a GPI algorithm that uses the TD error to update the action-value function. And it the <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy strategy implicit in its action-value function.</li>
</ul></li>
</ul>
</div>
</div>


</section>
</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Sample-based Learning Methods</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c2-w3.html</guid>
  <pubDate>Sat, 02 Mar 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Temporal Difference Learning Methods for Prediction</title>
  <link>https://orenbochman.github.io/posts/c2-w2.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reading
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=91">RL Book§5.0-5.5 (pp.91-104)</a></label></li>
</ul>
</div>
</div>
</div>
<section id="lesson-1-introduction-to-temporal-difference-learning" class="level2">
<h2 class="anchored" data-anchor-id="lesson-1-introduction-to-temporal-difference-learning">Lesson 1: Introduction to Temporal Difference Learning</h2>
<section id="lesson-learning-goals" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Define temporal-difference learning #</label></li>
<li><label><input type="checkbox">Define the temporal-difference error #</label></li>
<li><label><input type="checkbox">Understand the TD(0) algorithm #</label></li>
</ul>
</section>
<section id="temporal-difference-learning-definition" class="level3">
<h3 class="anchored" data-anchor-id="temporal-difference-learning-definition">Temporal Difference Learning definition</h3>
<p>Now we turn to a new class of methods called Temporal Difference (TD) learning. According to Bass, TD learning is one of the key innovations in RL. TD learning is a method that combines the sampling of Monte Carlo methods with the bootstrapping of Dynamic Programming methods. The term <strong>temporal</strong> in the name references learning from two subsequent time steps and the term <strong>difference</strong> refers to using the difference between the values of each state.</p>
<p>Let us now derive the TD update rule for the value function:</p>
<p>Recall the definition of the value function from the previous course, is the expected return when starting in a particular state and following a particular policy.<br>
We write this as:</p>
<p><span id="eq-value-function"><img src="https://latex.codecogs.com/png.latex?%0Av_%5Cpi(s_t)%20%5Cdot%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BG_t%20%7C%20S_t%20=%20s%5D%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<p>We can motivate td update rule by considering the DP and MC update rules.</p>
<p>In The MC update rule a sample update based on the return for the entire episode. Which means that we can only update our value function at the end of the episode.</p>
<p><span id="eq-mc-online-update"><img src="https://latex.codecogs.com/png.latex?%0AV(S_t)%20%5Cleftarrow%20V(S_t)%20+%20%5Calpha%20%5B%5Cunderbrace%7BG_t%7D_%7B%5Ctext%7BMC%20target%7D%7D%20-V(S_t)%5D%20%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>
<p>where:</p>
<p><img src="https://latex.codecogs.com/png.latex?G_t"> is the return at time step <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?S_t"> is the state at time step <img src="https://latex.codecogs.com/png.latex?t">.</p>
<p>This is the actual return at time t.</p>
<p>In the DP update rule, the target is a full backup based on the entire distribution of possible next states which we can solve for exactly since we have the full dynamics of the environment.</p>
<p><span id="eq-dp-update"><img src="https://latex.codecogs.com/png.latex?%0AV(S_t)%20%5Cleftarrow%20V(S_t)%20+%20%5Csum_a%5Cpi(a%7CS_t)%20(%5Br(s,a)%20+%20%5Cgamma%20p(s'%5Cmid%20s,a)%20V(s')%5D)%20%5Cqquad%0A%5Ctag%7B3%7D"></span></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D"> is the reward at time step <img src="https://latex.codecogs.com/png.latex?t+1"></li>
<li><img src="https://latex.codecogs.com/png.latex?V(S_%7Bt+1%7D)"> is the approximate value of the state at time step <img src="https://latex.codecogs.com/png.latex?t+1"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the discount factor</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha"> is the learning rate</li>
</ul>
<p>How can we make updates at each time step?</p>
<p><span id="eq-return-definition"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AG_t%20&amp;%20%5Cdot=%20R_%7Bt+1%7D%20+%20%5Cgamma%20R_%7Bt+2%7D%20+%20%5Cgamma%5E2%20R_%7Bt+3%7D%20+%20%5Cldots%20%5Cqquad%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20&amp;=%20R_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%20%5Cqquad%0A%5Cend%7Baligned%7D%0A%5Ctag%7B4%7D"></span></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Av_%5Cpi(s_t)%20%20&amp;%20=%20%5Cmathbb%7BE%7D_%5Cpi%20%5BG_t%20%5Cmid%20S_t%20=%20s%5D%20&amp;%20%5Ctext%7Bdefinition%7D%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cmathbb%7BE%7D_%5Cpi%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%20%5Cmid%20S_t%20=%20s%5D%20&amp;%20%5Ctext%7B(subst.%20Recursive%20return)%7D%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BR_%7Bt+1%7D%20+%20%5Cgamma%20v_%5Cpi%20(S_%7Bt+1%7D)%20%7CS_t=s%5D%20%20%20%20&amp;%20%5Ctext%7B(subst.%20value%20function)%7D%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BR_%7Bt+1%7D%7CS_t=s%5D%20%20+%20%5Cgamma%20%5Cmathbb%7BE%7D_%5Cpi%5Bv_%5Cpi%20(S_%7Bt+1%7D)%20%7CS_t=s%5D%20&amp;%20%5Ctext%7B(by%20linearity%20of%20Expectation)%7D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20&amp;%20=%20R_%7Bt+1%7D%20+%20%5Cgamma%20v_%5Cpi%20(S_%7Bt+1%7D)%20&amp;%20%5Ctext%7B(by%20Expectation%20of%20constant%20RV)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>In this formula we have replaced <img src="https://latex.codecogs.com/png.latex?G_t"> with <img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D%20+%20%5Cgamma%20V(S_%7Bt+1%7D)">. We now have a recursive formula for the value functions in terms of the next value function. The next value function is a stand in for the value of the Return <img src="https://latex.codecogs.com/png.latex?G_%7Bt+1%7D"> which we don’t know.</p>
<p>we can use this formula to make an online update to our value function using an MC estimate of the return, without saving the full list of rewards.</p>
<p>since <img src="https://latex.codecogs.com/png.latex?G_t">, the MC update target is the return for the entire episode, we can only update our value function at the end of the episode. We will soon look at TD learning, which allows us to make updates at each time step.</p>
<p>the motivation for TD learning is based on the MC update rule. The MC update rule is a sample update based on the return for the entire episode. This means that we can only update our value function at the end of the episode. We will soon look at TD learning, which allows us to make updates at each time step.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AV_%5Cpi(S_t)%20%5Cleftarrow%20V(S_t)%20+%20%5Calpha%20%5B%0A%5Cunderbrace%7BR_%7Bt+1%7D%20+%20%5Cgamma%20V(S_%7Bt+1%7D)%7D_%7B%5Ctext%7BTD%20target%7D%7D%20-%20V(S_t)%5D%0A"></p>
<p>We can use the following to make online TD updates like we were able to update our value function without saving the full list of rewards. here:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?V(S_t)"> is the value of the state at time <img src="https://latex.codecogs.com/png.latex?t"></li>
<li><img src="https://latex.codecogs.com/png.latex?V(S_%7Bt+1%7D)"> is the value of the state at time <img src="https://latex.codecogs.com/png.latex?t+1"></li>
<li><img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D"> is the reward at time <img src="https://latex.codecogs.com/png.latex?t+1"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha"> is a constant the learning rate</li>
<li>the target is an estimate of the return</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AG_t%20&amp;%20%5Cdot%20=%20R_%7Bt+1%7D%20+%20%5Cgamma%20R_%7Bt+2%7D%20+%20%5Cgamma%5E2%20R_%7Bt+3%7D%20+%20%5Cldots%20%5Cnewline%0A&amp;=%20R_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%20%5Cnewline%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>The MC target is an estimate of the expected value (average of sampled return) - The DP target is an estimate because the true value of the state is not known - The TD target is an estimate for both reasons: a sample of the expected value of the reward, and the current estimate of the value of the state rather than the true value.</li>
</ul>
<p>Like with MC, the target is a sample updates based on a single observed transition. This is in stark contrast to DP, where the target is a full backup based on the entire distribution of possible next states which we can solve for exactly since we have the full dynamics of the environment.</p>
</section>
<section id="temporal-difference-error" class="level3">
<h3 class="anchored" data-anchor-id="temporal-difference-error">Temporal Difference Error</h3>
<ul>
<li>The TD error is the difference between the estimated value of a state and the value of the state at the next time step.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdelta_t%20%5Cdot%20=%20R_%7Bt+1%7D%20+%20%5Cgamma%20V(S_%7Bt+1%7D)%20-%20V(S_t)%0A"> this is one of the most important equations in reinforcement learning - we will see it again and again.</p>
</section>
<section id="td0-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="td0-algorithm">TD(0) Algorithm</h3>
<ul>
<li>The TD(0) algorithm is a TD learning algorithm that uses a bootstrapping method to estimate the value of a state.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The TD(0) algorithm for estimating <img src="https://latex.codecogs.com/png.latex?v_%5Cpi">
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-td-zero" class="pseudocode-container quarto-float" data-line-number-punc=":" data-caption-prefix="Algorithm" data-indent-size="1.2em" data-no-end="false" data-line-number="true" data-pseudocode-number="1" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{TD(0) for estimating $v_\pi$}\begin{algorithmic} \State Input: \State $\qquad \pi \leftarrow \text{policy to be evaluated}$ \State Initialize: \State $\qquad V(s) \leftarrow x \in \mathbb{R} \quad \forall s \in \mathcal{S}$ \FORALL {episode $e$:} \State $S \leftarrow \text{initial state of episode}$ \State $\text{Choose } A \sim \pi(\cdot \mid S)$ \FORALL {step $S \in e$:} \State Take action $A$, observe $R, S'$ \State $V(S) \leftarrow V(S) + \alpha [R + \gamma V(S') - V(S)]$ \State $S \leftarrow S'$ \State $\text{Choose} A' \sim \pi(\cdot \mid S)$ \State $S$ is terminal \ENDFOR \ENDFOR \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
</section>
</section>
<section id="lesson-2-advantages-of-td" class="level2">
<h2 class="anchored" data-anchor-id="lesson-2-advantages-of-td">Lesson 2: Advantages of TD</h2>
<section id="lesson-learning-goals-1" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-1">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Understand the benefits of learning online with TD #</label></li>
<li><label><input type="checkbox">Identify key advantages of TD methods over Dynamic Programming and Monte Carlo methods #</label></li>
<li><label><input type="checkbox">Identify the empirical benefits of TD learning #</label></li>
</ul>
</section>


</section>

 ]]></description>
  <category>Coursera</category>
  <category>Sample-based Learning Methods</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c2-w2.html</guid>
  <pubDate>Fri, 01 Mar 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Monte-Carlo Methods for Prediction &amp; Control</title>
  <link>https://orenbochman.github.io/posts/c2-w1.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<div class="tldr callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR This lesson in a nutshell
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>In this module we will embrace the paradigm of “learning from experience”.</li>
<li>This is called Sample based Reinforcement Learning and it we will let us relax some strong of the requirements of dynamic programming, namely knowing the table of MDP dynamics.</li>
<li>We will first use efficient Monte-Carlo ⚅🃁 methods for 🔮 prediction problem of estimating <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(S)"> value functions and action–value functions <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(a)"> from sampled episodes.</li>
<li>We will revise our algorithm to better handle exploration using exploring starts and <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">–soft policies.</li>
<li>We will adapt GPI algorithms for use with Mote-Carlo to solve the 🎮 control problem of policy improvement.</li>
<li>With off policy learning learn a policy using samples from another policy, by corrected using importance sampling.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reading
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=91">RL Book§5.0-5.5 (pp.91-104)</a></label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definitions
</div>
</div>
<div class="callout-body-container callout-body">
<p>Here are most of the definitions we need for this module. Let us review them before we start.</p>
<div id="dfn-value">
<dl>
<dt>Value Function <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"></dt>
<dd>
<p>a state’s value is its expected return</p>
</dd>
<dd>
<p><img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)%20%5Cdoteq%20%5Cmathbb%7BE%7D%5BG_t%7CS_t=s%5D"></p>
</dd>
</dl>
</div>
<div id="dfn-action-value">
<dl>
<dt>Action Value Function</dt>
<dd>
<p>is the expected return for taking action <img src="https://latex.codecogs.com/png.latex?a"> in state <img src="https://latex.codecogs.com/png.latex?s"> if we follow policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"></p>
</dd>
<dd>
<p><img src="https://latex.codecogs.com/png.latex?q_%5Cpi(a)%20%5Cdoteq%20%5Cmathbb%7BE%7D%5BG_t%20%5Cvert%20A_t=a%5D%20%5Cspace%20%5Cforall%20a%20%5Cin%20%5C%7Ba_1%20...%20a_k%5C%7D"></p>
</dd>
</dl>
</div>
<div id="dfn-bootstrap">
<dl>
<dt>Bootstrapping</dt>
<dd>
<p>“learning by guessing from a guess” or more formally</p>
</dd>
<dd>
<p>the process of updating an estimate of the value or action-value function based on other estimated values. It involves using the current estimate of the value function to update and improve the estimate itself.</p>
</dd>
</dl>
</div>
<div id="dfn-control">
<dl>
<dt>Control</dt>
<dd>
<p>to approximate optimal policies using the DP approach of GPI</p>
</dd>
</dl>
</div>
<div id="dfn-epsilon-soft">
<dl>
<dt>ϵ-Soft Policy</dt>
<dd>
<p>A policy in which each possible action is assigned at least <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20/%20%7CA%7C"> probability.</p>
</dd>
</dl>
</div>
<div id="dfn-exploring-starts">
<dl>
<dt>Exploring Starts</dt>
<dd>
<p>Learning the value or action values of a policy by trying each action starting in each state at least once and then following the policy.</p>
</dd>
<dd>
<p>This can include taking actions that are not part of the policy.</p>
</dd>
</dl>
</div>
<div id="dfn-mc">
<dl>
<dt>Monte-Carlo Methods</dt>
<dd>
<p>Estimation methods which relies on repeated random sampling. Also see <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte-Carlo methods <i class="bi bi-wikipedia"></i></a></p>
</dd>
</dl>
</div>
<div id="dfn-on-policy-learning">
<dl>
<dt>On-policy learning</dt>
<dd>
<p>learning a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> by sampling from <img src="https://latex.codecogs.com/png.latex?%5Cpi"></p>
</dd>
</dl>
</div>
<div id="dfn-off-policy-learning">
<dl>
<dt>Off-policy learning</dt>
<dd>
<p>learning a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> by sampling from some other policy <img src="https://latex.codecogs.com/png.latex?%5Cpi'"></p>
</dd>
</dl>
</div>
<div id="dfn-prediction">
<dl>
<dt>Prediction</dt>
<dd>
<p>Estimating <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> is called policy evaluation in the DP literature.</p>
<p>We also refer to it as the Prediction problem <sup>1</sup></p>
</dd>
</dl>
</div>
<div id="dfn-return">
<dl>
<dt>Return (<img src="https://latex.codecogs.com/png.latex?G_t">)</dt>
<dd>
<p><img src="https://latex.codecogs.com/png.latex?G_0%20%5Cdoteq%20R_1+%20%5Cgamma%5E1%20R_2%20+%20%5Ccdots+%20%5Cgamma%5En%20R_n"></p>
<p>i.e.&nbsp;the discounted sum of future rewards</p>
</dd>
</dl>
</div>
<div id="dfn-tqabular">
<dl>
<dt>Tabular methods</dt>
<dd>
<p>RL methods for which the action-values can be represented by a table</p>
</dd>
</dl>
</div>
</div>
</div>
<ul>
<li>Sample based methods learning from experience, without having prior knowledge of the underlying MDP model.</li>
<li>We will cover tabular methods in which the action-values can be represented by a table.</li>
</ul>
<section id="lesson-1-introduction-to-monte-carlo-methods" class="level1 page-columns page-full">
<h1>Lesson 1: Introduction to Monte-Carlo Methods</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand how Monte-Carlo can be used to estimate <img src="https://latex.codecogs.com/png.latex?v(s)"> value functions from sampled interaction #</label></li>
<li><label><input type="checkbox" checked="">Identify problems that can be solved using Monte-Carlo methods #</label></li>
<li><label><input type="checkbox" checked="">Use Monte-Carlo prediction to estimate the value function for a given policy. #</label></li>
</ul>
</div>
</div>
<ul>
<li>After completing the introduction we all think that MDPs and DP are the best?</li>
<li>Alas, Martha burst this bubble, introducing some shortcomings of DP, namely they require us to know a model of the dynamics <img src="https://latex.codecogs.com/png.latex?p(s,a%7Cs',r)"> and rewards <img src="https://latex.codecogs.com/png.latex?r"> of the MDP to estimate <img src="https://latex.codecogs.com/png.latex?v(s)"> or <img src="https://latex.codecogs.com/png.latex?q(a)">.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-mc-methods.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>MC methods for Policy evaluation</figcaption>
</figure>
</div></div><p>let us now try to understand how Monte-Carlo can be used to estimate <img src="https://latex.codecogs.com/png.latex?v(s)"> value functions from sampled interaction.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-mc-12-dice.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>12 dice</figcaption>
</figure>
</div></div><div id="exm-dp-dice" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Rolling 12 Dice)</strong></span> &nbsp;</p>
<ul>
<li>Say our MDP requires rolling 12 dice.
<ul>
<li>this is probably intractable to estimate theoretically using DP.</li>
<li>this is likely to be error prone (particularly and constitutionally).</li>
<li>this will be easy to estimate using MC methods</li>
</ul></li>
</ul>
</div>
<ul>
<li>For most MDPs knowing the dynamics and rewards is an unreasonably strong requirement.</li>
<li>If we can treat this like a bandit problem we can try to use the long term averages rewards to estimate value of a state</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-mc-bandit.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>MC bandits</figcaption>
</figure>
</div></div><p>more formally we can use the MC value prediction algorithm.</p>
<hr>
<p>Next we present an algorithm for estimating the value function of a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> using MC methods.</p>
<div id="nte-mc-value-prediction-any-visit" class="alg callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;1: MC prediction any visit for estimating <img src="https://latex.codecogs.com/png.latex?V%20%5Capprox%20v_%5Cpi">
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-prediction-any-visit" class="pseudocode-container quarto-float" data-line-number-punc=":" data-pseudocode-number="1" data-caption-prefix="Algorithm" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number="true">
<div class="pseudocode">
\begin{algorithm} \caption{MCFirstVisitValuePrediction($\pi$)}\begin{algorithmic} \State Input \State $\qquad \pi$ to be evaluated \State Initialize: \State $\qquad V(s) \leftarrow x \in \mathbb{R}$, arbitrarily, $\forall s \in \mathcal{S}$ \State $\qquad Returns(s)$ an empty list, $\forall s \in \mathcal{S}$ \For {each episode:} \State Generate an episode by following $\pi: S_0, A_0, R_1, S_1, A_1, R_2,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0$ \For {each step of episode, $t = T-1, T-2,\ldots, 0$:} \State $G \leftarrow \gamma G + R_{t+1}$ \State Append $G$ to $Returns(S_t)$ \State $V(S_t) \leftarrow average(Returns(S_t))$ \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<div id="nte-mc-value-prediction-first-visit" class="alg callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;2: MC prediction fist visit for estimating <img src="https://latex.codecogs.com/png.latex?V%20%5Capprox%20v_%5Cpi">
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-prediction-first-visit" class="pseudocode-container quarto-float" data-line-number-punc=":" data-pseudocode-number="2" data-caption-prefix="Algorithm" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number="true">
<div class="pseudocode">
\begin{algorithm} \caption{MCAnyVisitValuePrediction($\pi$)}\begin{algorithmic} \State Input \State $\qquad \pi$ to be evaluated \State Initialize: \State $\qquad V(s) \in \mathbb{R}$, arbitrarily, $\forall s \in \mathcal{S}$ \State $\qquad Returns(s)$ an empty list, $\forall s \in \mathcal{S}$ \For {each episode:} \State Generate an episode by following $\pi: S_0, A_0, R_1, S_1, A_1, R_2,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0$ \For {each step of episode, $t = T-1, T-2,\ldots, 0$:} \State $G \leftarrow \gamma G + R_{t+1}$ \If{$S_t \not \in S_0, S_1,\ldots,S_{t-1}$} \State Append $G$ to $Returns(S_t)$ \State $V(S_t) \leftarrow average(Returns(S_t))$ \EndIf \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Any visit / First-visit
</div>
</div>
<div class="callout-body-container callout-body">
<p>The book uses presents a small variation called the <em>first visit MC method</em>, We considered the any-visit case. This estimates <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> using the average of the returns following an episode’s first visit to <img src="https://latex.codecogs.com/png.latex?s">, whereas this the every-visit MC algorithms averages the returns following all visits to <img src="https://latex.codecogs.com/png.latex?s"></p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuition for the Algorithm
</div>
</div>
<div class="callout-body-container callout-body">

<p>The main idea is to use the recursive nature of the returns which is embodied in the following formula:</p>
<p><span id="eq-incremental-update-rule"><img src="https://latex.codecogs.com/png.latex?%0ANewEstimate%20%5Cleftarrow%20OldEstimate%20+%20StepSize%5BTarget%20-%20OldEstimate%5D%20%5Cqquad%20%5Ctext%7B(incremental%20update%20rule)%7D%0A%5Ctag%7B1%7D"></span></p>
<p>The key to understanding this algorithm is represented in the following diagram. At the top is a backup diagram for an discounted episode.</p>
<p>Martha explain that the MC uses the recursive nature of the returns to efficiently compute the average returns for each state by starting at the end of the episode and working backwards.</p>
<p>We can see that the returns from a series of equations that can be solved by substitution. Each return is the current reward and the discounted previous return that has been computed.</p>
<p>Thus we can compute all the returns for a state in a single pass through the episode. by solving this series of the full “telescoping” equations.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center callout-margin-content">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-mc-calc.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Efficient returns calculations</figcaption>
</figure>
</div></div><p>this bring us to our second example:</p>
<div id="exm-black-jack-mdp" class="theorem example page-columns page-full">
<p><span class="theorem-title"><strong>Example 2 (Blackjack MDP)</strong></span> &nbsp;</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-bj-example.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Blackjack example</figcaption>
</figure>
</div></div><ul>
<li><strong>Undiscounted</strong> MDP where each game of blackjack corresponds to an episode with
<ul>
<li>Rewards:
<ul>
<li>r= -1 for a loss</li>
<li>r= 0 for a draw</li>
<li>r= 1 for a win</li>
</ul></li>
<li>Actions : <img src="https://latex.codecogs.com/png.latex?a%5Cin%20%5C%7B%5Ctext%7BHit%7D,%20%5Ctext%7BStick%7D%5C%7D"></li>
<li>States S:
<ul>
<li>player has a usable ace (Yes/No) <sup>2</sup></li>
<li>sum of cards (12-21)<sup>3</sup></li>
<li>The card the dealer’s card shows (Ace-10)</li>
</ul></li>
<li>Cards are dealt with replacement<sup>4</sup></li>
<li>Policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">:
<ul>
<li>if sum &lt; 20, stick</li>
<li>otherwise, hit</li>
</ul></li>
</ul></li>
</ul>
</div>
<p>In the programming assignment we will produce the following graphs</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-bj-outcomes.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Blackjack outcomes</figcaption>
</figure>
</div></div><ul>
<li>In real world settings we typical don’t know theoretical functions like values, action values or rewards. Out best option is to sample reality in trial and error experiment of testing different interventions.</li>
<li>However under certain conditions such samples may be enough to perform the prediction task learn a value function or the action value function .</li>
<li>We can these function to learn better policies from this experience.</li>
<li>A second scenario involves historical samples collected from past interactions. We can use probabilistic methods like MCMC to estimate <img src="https://latex.codecogs.com/png.latex?q(a)">.</li>
</ul>
<p>we can use the MC prediction alg to estimate the expected returns for a state given a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The key limitations of <em>MC value estimation algorithm</em> is its requirement for episodic tasks and for completing such an episode before it starts. In some games an episode can be very long.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="emoji" data-emoji="bulb">💡</span> Is this really so? <span class="emoji" data-emoji="thinking">🤔</span>
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>If we work in the Bayesian paradigm with some prior and use Bayesian updating.</li>
<li>At every step we should have well defined means.</li>
<li>So it seems one can perhaps do sample based on non-episodic tasks</li>
<li>One more idea is to treat n_steps as an episode.</li>
<li>Without episodic we most likely lose the efficient updating. <span class="emoji" data-emoji="thinking">🤔</span></li>
<li>Perhaps we can use the online update rule for the mean.</li>
</ul>
</div>
</div>
<ul>
<li><p><label><input type="checkbox">TODO - try to implement this as an algorithm.</label></p></li>
<li><p>To ensure well-defined average sample returns, we define Monte Carlo methods only on episodic tasks that all eventually terminate - only on termination are value estimates and policies updated.</p></li>
</ul>
<p>Implications of MC Learning</p>
<ul>
<li>We don’t need to keep a large mode of the environment.</li>
<li>We estimate the values of each state independently of other states</li>
<li>Computation for updating values or each state is independent of the size of the MDP<sup>5</sup></li>
</ul>
</section>
<section id="sec-mc-control" class="level1 page-columns page-full">
<h1>Lesson 2: Monte Carlo for Control</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox">Estimate action-value functions using Monte Carlo #</label></li>
<li><label><input type="checkbox">Understand the importance of maintaining exploration in Monte Carlo algorithms #</label></li>
<li><label><input type="checkbox" checked="">Understand how to use Monte Carlo methods to implement a GPI algorithm. #</label></li>
<li><label><input type="checkbox" checked="">Apply Monte Carlo with exploring starts to solve an MDP #</label></li>
</ul>
</div>
</div>
<section id="sec-l2g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g1">MC Action-Value Functions</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-mc-action-values.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>action values</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-mc-backoff.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>back off</figcaption>
</figure>
</div></div>
<p>This back off diagram indicates that the value of a state S depends on the values of its actions.</p>
<ul>
<li>Recall that control is simply improving a policy using our action values estimate.</li>
<li>Policy improvement is done by <strong>Greedyfying</strong> a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> at a state <img src="https://latex.codecogs.com/png.latex?s"> by selecting the action <img src="https://latex.codecogs.com/png.latex?a"> with the highest action value.</li>
<li>If we are missing some action values we can make the policy worse!</li>
<li>We need to ensure that our RL algorithm engages the different actions of a state. There are two strategies:
<ul>
<li>Exploring starts</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-Soft strategies</li>
</ul></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-exploring-starts.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>exploring starts</figcaption>
</figure>
</div></div><p>The following is the MC alg with exploring start for estimation.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-monte-carlo-GPI-01.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>exploring starts pseudocode</figcaption>
</figure>
</div></div><p>Let’s recap how GPI looks:</p>
<ul>
<li>Keeping <img src="https://latex.codecogs.com/png.latex?%5Cpi_0"> fixed we do evaluation of <img src="https://latex.codecogs.com/png.latex?q_%5Cpi"> using MC–ES</li>
<li>We improve <img src="https://latex.codecogs.com/png.latex?%5Cpi_0"> by picking the actions with the highest values</li>
<li>We stop when we don’t improve <img src="https://latex.codecogs.com/png.latex?%5Cpi"></li>
</ul>
<p>Here, in the evaluation step, we estimate the action-values using MC prediction, with exploration driven by exploring Starts or an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-soft policy</p>
</section>
</section>
<section id="lesson-3-exploration-methods-for-monte-carlo" class="level1 page-columns page-full">
<h1>Lesson 3: Exploration Methods for Monte Carlo</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand why Exploring Starts can be problematic in real problems #</label></li>
<li><label><input type="checkbox" checked="">Describe an alternative exploration method for Monte Carlo control #</label></li>
</ul>
</div>
</div>
<p>Next we look into using exploring starts to learn action values using MC sampling.</p>
<p>Recall that we like action values over state values since they allow us to find optimal policies by quickly picking the best action in a state.</p>
<p>Mr White explains that we can’t use a deterministic policy to learn action values since we need to explore multiple actions in a state to pick the best one. To do this with a deterministic policy we use exploring starts - this means we start each simulation with a random state and action then follow the policy. This should eventually allow us to learn the action values for all actions in all states.</p>
<p>So here is how can use exploring starts or some other exploration strategy to ensure that we can learn the action values for all actions in all states.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Monte Carlo Exploring Start
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-es-control" class="pseudocode-container quarto-float" data-line-number-punc=":" data-pseudocode-number="3" data-caption-prefix="Algorithm" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number="true">
<div class="pseudocode">
\begin{algorithm} \caption{MonteCarloExploringStartsGPI()}\begin{algorithmic} \State Initialize: \State $\qquad \pi(s) \in A(s) \quad \forall s\in \mathcal{S}$ \State $\qquad Q(s,a) \leftarrow x \in \mathbb{R} \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)$ \State $\qquad Returns(s,a) \leftarrow \text {an empty list} \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)$ \For {each episode:} \State Choose $S_0 \in \mathcal{S} and A_0 \in \mathcal{A}(S_0) \text{randomly :} p(s,a)&gt;0 \forall s,a$ \comment{$\textcolor{blue}{Exploring Starts}$} \State Generate an episode e from $S_0, A_0$ by following $\pi: S_0, A_0, R_1,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0$ \For {each step of $e, t \in T-1, T-2,..., 0:} \comment{$\textcolor{blue}{Backward\ pass}$} \State $G \leftarrow \gamma G + R_{t+1}$ \IF {$S_t , A_t \not \in S_0 , A_0 , S_1 , A_1 \ldots , S_{t-1} , A_{t-1}$} \State Append $G$ to $Returns(S_t,A_t)$ \State $Q(S_t,A_t) \leftarrow average(Returns(S_t,A_t))\quad$ \comment{$\textcolor{blue}{\text{MC action-value estimate}}$} \State $\pi(S_t) \leftarrow \arg\max_a Q(S_t,a)\quad$ \comment{$\textcolor{blue}{\text{greedy policy improvement}}$} \EndIf \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Connecting the dots: MC vs DP
</div>
</div>
<div class="callout-body-container callout-body">
<p>Adam White points out how this is so much simpler than the DP methods we learned earlier. We don’t need to solve a set of simultaneous equations. We can just use the MC method to estimate the action values and then use GPI to improve the policy. The only thing we need to do is to ensure that we explore all the actions in all the states.</p>
</div>
</div>
<section id="sec-l3g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g1">Challenges for exploring starts</h3>
<p>Exploring start can be problematic as we may not able to say try all actions on all states.</p>
<ul>
<li>there may be too many states actions to try</li>
<li>testing certain actions in certain states it could be unethical <sup>6</sup> or risky <sup>7</sup></li>
<li>it could cost too much - we need too many experiments.</li>
</ul>
<p>Note: The Blackjack MDP can be improved using Exploring Starts since each initial state can be sampled. Recall there were 200 states.</p>
</section>
<section id="sec-l3g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g2">ϵ-Soft Policies</h3>
<p>AN ALTERNATIVE approach to policy improvement is an generalization of both the <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">- greedy policy and the random uniform, which we first learned in the contexts of the multi-armed bandits problem in the fundamentals course.</p>
<dl>
<dt>ϵ-soft policy</dt>
<dd>
<p>An ϵ-soft policy is one for which in each state, all actions have a probability of at least <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cepsilon%7D%7B%7CA%7C%7D"></p>
</dd>
</dl>
<p>The advantages of using an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-soft policy are: - we get a never ending exploration</p>
<p>However this means we can never reach a deterministic optimal policy - but we can get to a stochastic policy where the best choice is <img src="https://latex.codecogs.com/png.latex?1-%5Cepsilon+%5Cfrac%7B%5Cepsilon%7D%7B%7CA%7C%7D"></p>
<p>We can get to a deterministic policy if we use a decaying <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy policy or if we greedify the policy breaking ties randomly.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MC control with ϵ-soft policies
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-ϵ-soft-control" class="pseudocode-container quarto-float" data-line-number-punc=":" data-pseudocode-number="4" data-caption-prefix="Algorithm" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number="true">
<div class="pseudocode">
\begin{algorithm} \caption{MonteCarloϵ-SoftControl()}\begin{algorithmic} \State Initialize \State $\qquad \epsilon \in (0,1)$ \Comment{ $\textcolor{blue}{\ algorithm\ parameter}$} \State $\qquad \pi \leftarrow \epsilon$-soft policy \Comment{ $\textcolor{blue}{\ Initialize: policy}$} \State $\qquad Q(s) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S}$ \State $\qquad Returns(s)$ an empty list, $\quad \forall s \in \mathcal{S}$ \For {each episode:} \State Generate an episode by following $\pi: S_0, A_0, R_1,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0$ \For {each step of episode, $t \in T-1, T-2,..., 0$:} \State $G \leftarrow \gamma G + R_{t+1}$ \State Append $G$ to $Returns(S_t,A_t)$ \State $Q(S_t,A_t) \leftarrow average(Returns(S_t,A_t))$ \State $A^* \leftarrow \arg\max_a Q(S_t,a) \qquad \textcolor{blue}{\text{(ties broken arbitrarily)}}$ \For {each $a \in \mathcal{A}$} \If{$a = A^*$} \State $\pi(a \mid S_t) \leftarrow 1 - \epsilon + \frac{\epsilon}{|A(S_t)|}$ \Else \State $\pi(a \mid S_t) \leftarrow \frac{\epsilon}{|A(S_T)|}$ \EndIf \EndFor \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<p>The Highlights indicate modification of the Exploring Starts alg</p>
<ol type="1">
<li>We can start with Uniform-random as its epsilon-soft.</li>
<li>Episode generation uses the current <img src="https://latex.codecogs.com/png.latex?%5Cpi"> (<img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-soft policy) <em>before</em> it is improved.</li>
<li>We drop the first-visit check - this is an every-visit MC algorithm.</li>
<li>The new policy generated in each iteration is <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy w.r.t. the current action-value estimate, which is improved prior.</li>
<li>The optimal <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-soft policy is an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-soft policy.</li>
</ol>
</section>
<section id="lesson-4-off-policy-learning-for-prediction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-4-off-policy-learning-for-prediction">Lesson 4: Off-policy learning for prediction</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox">Understand how off-policy learning can help deal with the exploration problem #</label></li>
<li><label><input type="checkbox">Produce examples of target policies and examples of behavior policies. #</label></li>
<li><label><input type="checkbox">Understand importance sampling #</label></li>
<li><label><input type="checkbox">Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution. #</label></li>
<li><label><input type="checkbox">Understand how to use importance sampling to correct returns #</label></li>
<li><label><input type="checkbox">Understand how to modify the Monte Carlo prediction algorithm for off-policy learning. #</label></li>
</ul>
</div>
</div>
<section id="sec-l4g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g1">Off-policy learning</h3>
<ul>
<li>Off-policy learning is a way to learn a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> using samples from another policy <img src="https://latex.codecogs.com/png.latex?%5Cpi'">.</li>
<li>This is useful when we have a policy that is easier to sample from than the policy we want to learn.</li>
<li>A key idea is to correct the returns using importance sampling.</li>
</ul>
<p>For example suppose we can use a rule based model to generate samples of agent state, action and rewards - but we don’t really have an MDP, value function or policy. We could start with a uniform random policy and then use the samples to learn a better policy. However this would require us to interact with the environment and our agents may not be able to do this. In the case of Sugarscape model the agents are not really making decisions, they are following rules.</p>
<p>If we wished to develop agent that learn using RL with different rules on or off and other settings and use those to learn a policy using many samples. One advantage of the Sugarscape model is that it is highly heterogeneous so we get a rich set of samples to work with. A second advantage is that the rule based model can be fast to sample from and we can generate many samples by running it using hyper-parameters optimized test-bed.</p>
<p>So if we have lots of samples we may not need to explore as much initially, but rather learn to exploit the samples we have. Once we learn a near optimal policy for the samples we can use our agent to explore new vistas in our environment.</p>
</section>
<section id="sec-l4g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g2">Target and behavior policies</h3>
<ul>
<li>The target policy is the policy we want to learn.</li>
<li>The behavior policy is the policy we sample from.</li>
</ul>
</section>
<section id="sec-l4g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g3">Importance sampling</h3>
<ul>
<li>Importance sampling is a technique to estimate the expected value of a target distribution using samples from a different distribution.</li>
<li>Why cant we just use the samples from the behavior policy to estimate the target policy?</li>
<li>The answer is that the samples from the behavior policy are biased towards the behavior policy.</li>
<li>In the target policy we may have states that are never visited by the behavior policy.</li>
<li>For example we might want to learn a policy that focuses on trade rather than combat or Vica-versa. This extreme idea of introducing/eliminating some action would significantly change behavioral trajectories. Sample based methods could be able to handle these changes - if we can restrict them to each subset of actions but clearly the expected return of states will be diverge in the long run.</li>
<li>So what we want is someway to correct the returns from the behavior policy to the target policy.</li>
<li>It is used to correct returns from the behavior policy to the target policy.</li>
</ul>
<p>The probability of a trajectory under <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is:</p>
<p><span id="eq-trajectory-probability"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20P(A_t,%20S_%7Bt+1%7D,%20&amp;%20A_%7Bt+1%7D,%20...%20,S_T%20%7C%20S_t,%20A_%7Bt:T-1%7D%20%5Csim%20%5Cpi)%20%5Cnewline%0A%20%20&amp;%20=%20%5Cpi(A_t%7CS_t)p(S_%7Bt+1%7D%7CS_t,%20A_t)%5Cpi(A_%7Bt+1%7D,%20S_%7Bt+1%7D)%20%5Ccdot%5Ccdot%5Ccdot%20p(S_T%7CS_%7BT-1%7D,%20A_%7BT-1%7D)%20%5Cnewline%0A%20%20&amp;%20=%20%5Cprod_%7Bk=t%7D%5E%7BT-1%7D%20%5Cpi(A_k%7CS_k)p(S_%7Bk+1%7D%7CS_k,%20A_k)%0A%5Cend%7Balign*%7D%0A%5Ctag%7B2%7D"></span></p>
</section>
<section id="sec-l4g4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l4g4">Importance sampling ratio</h3>
<p><strong>Definition:</strong> The importance sampling ratio (rho, <img src="https://latex.codecogs.com/png.latex?%5Crho">) is the relative probability of the trajectory under the target vs behavior policy:</p>
<p><span id="eq-importance-sampling"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Crho_%7Bt:T-1%7D%20&amp;%20%5Cdoteq%20%5Cfrac%7B%5Cprod_%7Bk=t%7D%5E%7BT-1%7D%20%5Cpi(A_k%20%5Cmid%20S_k)%20%5Ccancel%7B%20p(S_%7Bk+1%7D%20%5Cmid%20S_k,%20A_k)%7D%7D%7B%5Cprod_%7Bk=t%7D%5E%7BT-1%7D%20b(A_k%20%5Cmid%20S_k)%20%5Ccancel%7B%20p(S_%7Bk+1%7D%20%5Cmid%20S_k,%20A_k)%7D%20%7D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cprod_%7Bk=t%7D%5E%7BT-1%7D%20%5Cfrac%7B%5Cpi(A_k%20%5Cmid%20S_k)%7D%7Bb(A_k%20%5Cmid%20S_k)%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B3%7D"></span></p>
<p><span id="eq-value1"><img src="https://latex.codecogs.com/png.latex?%0Av_%5Cpi(s)%20=%20%5Cmathbb%7BE%7D_b%5B%5Crho_%7Bt:T-1%7D%20%5Ccdot%20G_t%20%5Cmid%20S_t%20=%20s%5D%20%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<p><span id="eq-weighted-importance-sampling"><img src="https://latex.codecogs.com/png.latex?%0AV(s)%20%5Cdoteq%20%5Cfrac%7B%5Cdisplaystyle%20%5Csum_%7Bt%5Cin%20%5Cmathscr%20T(s)%7D%5Crho_%7Bt:T(t)%20-%201%7D%20%5Ccdot%20G_t%7D%7B%7C%5Cmathscr%20T%20(s)%7C%7D%20%5Cqquad%0A%5Ctag%7B5%7D"></span></p>
<p><span id="eq-weighted-importance-sampling2"><img src="https://latex.codecogs.com/png.latex?%0AV(s)%20%5Cdoteq%20%5Cfrac%7B%5Cdisplaystyle%20%5Csum_%7Bt%5Cin%20%5Cmathscr%20T(s)%7D%20%5CBig(%5Crho_%7Bt:T(t)%20-%201%7D%20%5Ccdot%20G_t%5CBig)%7D%7B%5Cdisplaystyle%20%5Csum_%7Bt%5Cin%20%5Cmathscr%20T(s)%7D%5Crho_%7Bt:T(t)%20-%201%7D%7D%20%5Cqquad%0A%5Ctag%7B6%7D"></span></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/wk2-importance-sampling-example.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>importance sampling example</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/wk2-off-policy-trajectories.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>off policy trajectories</figcaption>
</figure>
</div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Off-policy every visit MC prediction
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-off-policy-prediction" class="pseudocode-container quarto-float" data-line-number-punc=":" data-pseudocode-number="5" data-caption-prefix="Algorithm" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number="true">
<div class="pseudocode">
\begin{algorithm} \caption{OffPolicyMonteCarloPrediction()}\begin{algorithmic} \State Input: \State $\qquad \pi \leftarrow \text{policy to be evaluated}$ \State Initialize: \State $\qquad V(s) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S}$ \State $\qquad Returns(s) \text{ an empty list,} \quad \forall s \in \mathcal{S}$ \For {each episode:} \State Generate an episode by following $\pi: S_0, A_0, R_1,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0, W \leftarrow 1$ \For {each step of episode, $t \in T-1, T-2,..., 0$:} \State $G \leftarrow \gamma WG + R_{t+1}$ \State Append $G$ to $Returns(S_t)$ \State $V(S_t) \leftarrow average(Returns(S_t))$ \State $W \leftarrow W \frac{\pi(A_t|S_t)}{b(A_t|S_t)}$ \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Off-policy every visit MC control
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-off-control-prediction" class="pseudocode-container quarto-float" data-line-number-punc=":" data-pseudocode-number="6" data-caption-prefix="Algorithm" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number="true">
<div class="pseudocode">
\begin{algorithm} \caption{OffPolicyMonteCarloPrediction()}\begin{algorithmic} \State Input: \State $\qquad \pi \leftarrow \text{policy to be evaluated}$ \State Initialize: \State $\qquad V(s) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S}$ \State $\qquad Returns(s) \text{ an empty list,} \quad \forall s \in \mathcal{S}$ \For {each episode:} \State Generate an episode by following $b: S_0, A_0, R_1,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0, W \leftarrow 1$ \For {each step of episode, $t = T-1, T-2,\ldots, 0$:} \State $G \leftarrow \gamma WG + R_{t+1}$ \State Append $G$ to $Returns(S_t)$ \State $V(S_t) \leftarrow average(Returns(S_t))$ \State $W \leftarrow W \frac{\pi(A_t \mid S_t)}{b(A_t \mid S_t)}$ \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
</section>
<section id="emma-brunskill-batch-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="emma-brunskill-batch-reinforcement-learning">Emma Brunskill: Batch Reinforcement Learning</h3>
<p>These guest talks have a dual purpose:</p>
<ol type="1">
<li>to let the speakers share their passion for the field and introduce us to their research. this can be a good start for reading more about our own interests or for looking how to solve real problems that we are facing.</li>
<li>to show us how the concepts we are learning are being used in the real world.</li>
</ol>
<ul>
<li><a href="https://cs.stanford.edu/people/ebrun/">Emma Brunskill</a> is a professor at Stanford University.</li>
<li>Burnskill motivated her approach with an edutainment app in which the goal is to maximize student engagement in game based on historical data.</li>
<li>In batch RL we have a fixed dataset of samples and we want to learn a policy from this data.</li>
<li>This is useful when we have a fixed dataset of samples and we want to learn a policy from this data.</li>
<li>The key idea is to use importance sampling to correct the returns from the behavior policy to the target policy. We learned that the challenge this poses is primarily due to the bias of the behavior policy.</li>
<li><span class="marked">Importance sampling provides us with an unbiased estimate of the value function yet can have high variance</span>. These may can be exponentially large in the number of steps. So these results in very poor estimates for the value function if there are many steps in the trajectory.</li>
<li>Brunskill suggest that <span class="marked">the real challenge posed by batch RL is a sparsity of trajectories with actions leading to optimal next states under the target policy</span> in the historical data.<sup>8</sup></li>
<li>One point we learned about this is that we should seek algorithms that are more data efficient. However</li>
<li>A send idea is to use parametric models which are biased by can learn the transition dynamics and the reward function more efficiently.</li>
<li>Brunskill points out that since we have few samples we may need a better approach to get robust estimates of the value function.</li>
<li>This approach which comes from statistic is called <a href="">doubly robust stimators</a> and has been used in bandits and RL</li>
<li>She presents a chart from a 2019 paper with a comparison of different methods for RL in the cart-pole environment.
<ul>
<li>Off policy policy gradient with state Distribution Correction - dominates the other methods. And has a significantly narrower confidence interval for the value, if I understand the figure correctly.</li>
</ul></li>
<li>She also presents results from many papers on Generalization Guarantees for RL, which show that we can learn a policy that is close to the optimal policy with a small number of samples from another policy. However I cannot make much sense of the result in the slide.</li>
<li>An example of this is the Sugarscape model where we have a fixed dataset of samples from the rule-based model.</li>
<li>More generally, we can use batch RL to learn from historical data how to make better decisions in the future.</li>
</ul>
<dl>
<dt>Counterfactual</dt>
<dd>
<p>You don’t know what your life would be like if you weren’t reading this right now.</p>
</dd>
</dl>
<ul>
<li>Causal reasoning based on counterfactuals is a key idea to tackling this problem.</li>
</ul>
<dl>
<dt>Counterfactual or Batch Reinforcement Learning</dt>
<dd>
<p>In batch RL we have a fixed dataset of samples and we want to learn a new policy from this data.</p>
</dd>
</dl>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Doubly Robust Estimators
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://en.wikipedia.org/wiki/Inverse_probability_weighting#Interpretation_and_%22double_robustness%22">Doubly robust estimators</a> is a technique from statistics that and causal inference that allows us to combine to do importance sampling and model based learning and a propensity score to estimate the value function. combine the best of both worlds - they are robust to errors in the model and the policy.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%20V_%7BDR%7D%20=%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20%5Cleft%5B%20%5Crho_i%20(R_i%20+%20%5Cgamma%20Q(s_%7Bi+1%7D,%20%5Cpi(s_%7Bi+1%7D)))%20-%20%5Crho_i%20%5Chat%20Q_%7B%5Cpi%7D(s_i,%20a_i)%20+%20%5Chat%20Q_%7B%5Cpi%7D(s_i,%20a_i)%20%5Cright%5D%0A"> where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Crho_i"> is the importance sampling ratio for the <img src="https://latex.codecogs.com/png.latex?i">-th sample</li>
<li><img src="https://latex.codecogs.com/png.latex?R_i"> is the reward - <img src="https://latex.codecogs.com/png.latex?Q(s_%7Bi+1%7D,%20%5Cpi(s_%7Bi+1%7D))"> is the value of the next state under the target policy</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%20Q_%7B%5Cpi%7D(s_i,%20a_i)"> is the model based Q-function estimate</li>
<li><img src="https://latex.codecogs.com/png.latex?Q(s_%7Bi+1%7D,%20%5Cpi(s_%7Bi+1%7D))"> is the value of the next state under the target policy</li>
</ul>
</div>
</div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/ADD4B0bOZi4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>and paper</p>
<ul>
<li><a href="https://arxiv.org/abs/2007.08202">Provably Good Batch Reinforcement Learning Without Great Exploration</a></li>
<li><a href="https://arxiv.org/abs/1604.00923">Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning</a></li>
</ul>


</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><strong>Prediction</strong> in the sense that we want to predict for <img src="https://latex.codecogs.com/png.latex?%5Cpi"> how well it will preforms i.e.&nbsp;its expected returns for a state↩︎</p></li>
<li id="fn2"><p>worth either 1 or 11↩︎</p></li>
<li id="fn3"><p>face card are worth 10↩︎</p></li>
<li id="fn4"><p>this is a big simplifying assumption↩︎</p></li>
<li id="fn5"><p>in DP we had to solve <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%20n"> - simultaneous equations↩︎</p></li>
<li id="fn6"><p>think of a medical trial↩︎</p></li>
<li id="fn7"><p>think of a self driving car↩︎</p></li>
<li id="fn8"><ul>
<li>Can we learn form one or two examples by sampling ?</li>
<li>what if the good actions are never sampled by our algorithm?</li>
</ul>
↩︎</li>
</ol>
</section></div> ]]></description>
  <category>Coursera</category>
  <category>Sample-based Learning Methods</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c2-w1.html</guid>
  <pubDate>Thu, 29 Feb 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Dynamic Programming</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c1-w4.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="week-4-dynamic-programming" class="level1">
<h1>Week 4: Dynamic Programming</h1>
<p>In this week, we learn how to compute value functions and optimal policies, assuming you have the MDP model. You will implement dynamic programming to compute value functions and optimal policies and understand the utility of dynamic programming.</p>
<p>The ‘programming’ in dynamic programming really means solving an optimization problem. We have learned about using the Bellman equations as update rules. Now we look at some basic applications of this idea to solve MDP.</p>
<p>The intuition is pretty simple we have two tasks - one is to decide how good a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is - think <mark>discounted summation of the rewards from the best actions over the <img src="https://latex.codecogs.com/png.latex?s_ta_tr_t"> tree</mark>. This policy evaluation step is named <strong>prediction</strong>, as we don’t really know what the actual rewards of stochastic actions will be, only their expectation. But what we really want is to find near optimal policy which is called ‘control’. We have a strong theoretical result on how to go about this by iteratively improving a policy by picking its the actions with highest value at each steps.</p>
<p>What is surprising at first is that even starting with a uniform random policy we don’t need to explore the tree too deeply in the prediction step to be able to pick better actions. Also we can see from the maze like grid world that we really need to update one or two states every iteration. Which suggest that there is great room for improvement with smarter algorithms.</p>
</section>
<section id="lesson-1-policy-evaluation-prediction" class="level1">
<h1>Lesson 1: Policy Evaluation (Prediction)</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Read
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=72">RL Book§4.1-5,6-7, pp73-88</a></label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand the distinction between <strong>policy evaluation</strong> and <strong>control</strong>. #</label></li>
<li><label><input type="checkbox" checked="">Explain the setting in which dynamic programming can be applied, as well as its limitations. #</label></li>
<li><label><input type="checkbox" checked="">Outline the <strong>iterative policy evaluation algorithm</strong> for estimating state values under a given policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">. #</label></li>
<li><label><input type="checkbox" checked="">Apply iterative policy evaluation to compute value functions. #</label></li>
</ul>
</div>
</div>
<section id="sec-policy-evaluation-control" class="level2">
<h2 class="anchored" data-anchor-id="sec-policy-evaluation-control">Policy Evaluation and Control</h2>
<p>The distinction between policy evaluation and control:</p>
<dl>
<dt>policy evaluation (prediction)</dt>
<dd>
is the task of evaluating the future, i.e.&nbsp;the value function given some specific policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.
</dd>
<dt>control</dt>
<dd>
is the task of finding the optimal policy, given some specific value function <img src="https://latex.codecogs.com/png.latex?v">.
</dd>
<dt>planning</dt>
<dd>
is the task of finding the optimal policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cstar%7D"> and value function <img src="https://latex.codecogs.com/png.latex?v">, given a model of the environment. this is typically done by dynamic programming methods.
</dd>
</dl>
<p>Typically we need to solve the prediction problem before we can solve the control problem. This is because we need to know the value of the states under the policy to be able to pick the best actions.</p>
</section>
<section id="sec-dynamic-programming" class="level2">
<h2 class="anchored" data-anchor-id="sec-dynamic-programming">Dynamic Programming</h2>
<ul>
<li>Dynamic programming is a method for solving complex problems by breaking them down into simpler sub-problems.</li>
<li>It is a general approach to solving problems that can be formulated as a sequence of decisions.</li>
<li>Dynamic programming can be applied to problems that have the following properties:
<ul>
<li>Optimal substructure: The optimal solution to a problem can be obtained by combining the optimal solutions to its sub-problems.</li>
<li>Overlapping sub-problems: The same sub-problems are solved multiple times.</li>
</ul></li>
</ul>
</section>
<section id="sec-iterative-policy-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="sec-iterative-policy-evaluation">Iterative Policy Evaluation Algorithm</h2>
<p>Continuing with our goal of finding the optimal policy, we now turn to the an algorithms that will allow us to predict the value all the state starting with even the most naive policy.</p>
<p>The iterative policy evaluation algorithm is a simple iterative algorithm that estimates the value function for a given policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>We start with no knowledge of the value function or the policy. We set all the values to zero and we may even assume all actions are equally likely and all states are equally good. This is the uniform random policy. Alternatively we can start with some other policy.</p>
<p>These two assumptions are implemented in the initialization step of the algorithm.</p>
<p>The crux of the algorithm is the update step which is based on the recursive bellman equation for the value function under a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Av_%7B%5Cpi%7D(s)%20=%20%5Csum_%7Bs',r%7D%20p(s',r%7Cs,a)%5Br%20+%20%5Cgamma%20V(s')%5D%20%5Csum_%7Ba%7D%20%5Cpi(a%7Cs)%0A"> I rearranged the terms to make it clear that we are iterating over the states we use this equation to update the value of each state using</p>
<ol type="1">
<li>the four part dynamics function <img src="https://latex.codecogs.com/png.latex?p(s',r%7Cs,a)"> to get the probability of receiving a reward <img src="https://latex.codecogs.com/png.latex?r"> at a successor state <img src="https://latex.codecogs.com/png.latex?s'"> given the current state <img src="https://latex.codecogs.com/png.latex?s"> and action <img src="https://latex.codecogs.com/png.latex?a">.</li>
<li>the value of the next state <img src="https://latex.codecogs.com/png.latex?V(s')">. which we initially assumed is 0 and may have already updated</li>
<li>the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%7Cs)"> which we use to weigh the previous term</li>
</ol>
<p>Al this will give us the expected value of the state under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>The final part of the algorithm is the stopping condition. We stop when the change in the value function is less than a small threshold <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<p>The algorithm is guaranteed to converge to the value function for the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>Here is the concise statement of the algorithm with just one array in pseudo code:</p>
<div id="alg-Iterative-Policy-Evaluation" class="pseudocode-container quarto-float" data-line-number="true" data-pseudocode-number="1" data-caption-prefix="Algorithm" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number-punc=":" data-no-end="false">
<div class="pseudocode">
\begin{algorithm} \caption{Iterative Policy Evaluation, for estimating $V \approx v_{\pi}$}\begin{algorithmic} \State Input: $\pi$, the policy to be evaluated, default to uniform random policy \State Algorithm parameter: a small threshold $\theta &gt; 0$ determining accuracy of estimation \State $V(s) \leftarrow \leftarrow \vec 0 \forall s \in S$ \REPEAT \STATE $\Delta \leftarrow 0$ \FORALL { $s\in S$} \STATE $v \leftarrow V(s)$ \STATE $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]\quad$ \comment{ Bellman equation} \STATE $\Delta \leftarrow \max(\Delta, |v - V(s)|)$ \ENDFOR \UNTIL{$\Delta &lt; \theta$} \State Output: $V \approx v_{\pi}$ \end{algorithmic} \end{algorithm}
</div>
</div>
<p>note: the algorithm makes a couple of assumptions that are omitted in the pseudo code.</p>
<ol type="1">
<li>that we have access to the dynamics function <img src="https://latex.codecogs.com/png.latex?p(s',r%7Cs,a)"></li>
<li>that we have access to the reward function <img src="https://latex.codecogs.com/png.latex?r(s,a,s')"></li>
</ol>
</section>
<section id="sec-applying-iterative-policy-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="sec-applying-iterative-policy-evaluation">Applying Iterative Policy Evaluation</h2>
<p>The iterative policy evaluation algorithm can be applied to compute the value function for a given policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
</section>
</section>
<section id="lesson-2-policy-iteration-control" class="level1 page-columns page-full">
<h1>Lesson 2: Policy Iteration (Control)</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand the <strong>policy improvement theorem</strong>. #</label></li>
<li><label><input type="checkbox" checked="">Use a value function for a policy to produce a better policy for a given MDP. #</label></li>
<li><label><input type="checkbox" checked="">Outline the <strong>policy iteration algorithm for finding the optimal policy</strong>. #</label></li>
<li><label><input type="checkbox" checked="">Understand <strong>the dance of policy and value</strong>. #</label></li>
<li><label><input type="checkbox" checked="">Apply policy iteration to compute <strong>optimal policies</strong> and optimal <strong>value functions</strong>. #</label></li>
</ul>
</div>
</div>
<section id="sec-l2g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l2g1">Policy Improvement Theorem</h3>
<p>The policy improvement theorem states that given a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and the value function <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D">, we can construct a new policy <img src="https://latex.codecogs.com/png.latex?%5Cpi'"> that is as good as or better than <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
</section>
<section id="sec-l2g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l2g2">Value Function for a Policy</h3>
<p>The value function for a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s"> and following policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> thereafter.</p>
<p>The value function for a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is denoted by <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D(s)">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Av_%7B%5Cpi%7D(s)%20=%20%5Cmathbb%7BE%7D%5BG_t%20%5Cvert%20S_t%20=%20s%5D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?G_t"> is the return at time <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?S_t"> is the state at time <img src="https://latex.codecogs.com/png.latex?t">.</p>
</section>
<section id="sec-l2g3" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l2g3">Policy Iteration Algorithm</h3>
<p>The policy iteration algorithm is a simple iterative algorithm that alternates between policy evaluation and policy improvement.</p>
<p>The algorithm starts with an initial policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and iteratively evaluates the policy to get the value function <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D"> and then improves the policy to get a new policy <img src="https://latex.codecogs.com/png.latex?%5Cpi'">.</p>
<p>The algorithm continues this process until the policy no longer changes, which indicates that the optimal policy has been found.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi_0%20%5Cxrightarrow%7B%5Ctext%7BEvaluation%7D%7D%20v_%7B%5Cpi_0%7D%20%5Cxrightarrow%7B%5Ctext%7BImprovement%7D%7D%20%5Cpi_1%20%5Cxrightarrow%7B%5Ctext%7BEvaluation%7D%7D%20v_%7B%5Cpi_1%7D%20%5Cxrightarrow%7B%5Ctext%7BImprovement%7D%7D%20%5Cldots%20%5Cpi_*%20%5Cxrightarrow%7B%5Ctext%7BEvaluation%7D%7D%20v_%7B%5Cpi_*%7D%0A"></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-iterative-pl-eval1.png" class="img-fluid figure-img"></p>
<figcaption>starting with the uniform random policy</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-iterative-pl-eval2.png" class="img-fluid figure-img"></p>
<figcaption>we iterate to an optimal policy</figcaption>
</figure>
</div></div>
<p>Suppose we have computed for a deterministic policy <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D">, the value function for a deterministic policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>Now when would it be better to prefer some action ? <img src="https://latex.codecogs.com/png.latex?a%20%E2%89%A0%20%5Cpi(s)?"> in some state s?</p>
<p>It is better to switch to action a for state s if and only if: <span id="eq-action-switching-criterion"><img src="https://latex.codecogs.com/png.latex?%0Aq_%7B%5Cpi%7D(s,a)%20%3E%20v_%7B%5Cpi%7D(s)%0A%5Ctag%7B1%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?q_%7B%5Cpi%7D(s,a)"> is the value of taking action a in state s and then following policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>And, we can compute <img src="https://latex.codecogs.com/png.latex?q_%CF%80%20(s,a)"> from <img src="https://latex.codecogs.com/png.latex?v_%CF%80"> by:</p>
<p><span id="eq-action-value-function"><img src="https://latex.codecogs.com/png.latex?%0Aq_%7B%5Cpi%7D(s,a)%20=%20%5Csum_%7Bs',r%7D%20p(s',r%7Cs,a)%5Br%20+%20%5Cgamma%20v_%7B%5Cpi%7D(s')%5D%0A%5Ctag%7B2%7D"></span></p>
<p>this is the the key step the policy improvement step of the policy iteration algorithm.</p>
<div id="alg-Policy-Iteration" class="pseudocode-container quarto-float" data-line-number="true" data-pseudocode-number="2" data-caption-prefix="Algorithm" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number-punc=":" data-no-end="false">
<div class="pseudocode">
\begin{algorithm} \caption{Policy Iteration, for estimating $\pi \approx \pi_{\star}$}\begin{algorithmic} \State Input: $\pi$, the policy to be evaluated \State Algorithm parameter: a small threshold $\theta &gt; 0$ determining accuracy of estimation \State Initialize $V(s) \in \mathbb{R}, \quad \pi(s) \in A(s)\ \forall s\in S,\quad V(terminal)= 0$ \State \State {Policy Evaluation} \REPEAT \STATE $\Delta \leftarrow 0$ \FORALL { $s\in S$} \STATE $v \leftarrow V(s)$ \STATE $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]$ \STATE $\Delta \leftarrow \max(\Delta, |v - V(s)|)$ \ENDFOR \UNTIL{$\Delta &lt; \theta$} \State \State {Policy Improvement} \ForAll { $s\in S$} \State old-action $\leftarrow \pi(s)$ \State $\pi(s) \leftarrow \arg\max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]\quad$ \comment{ greedyfication} \If {old-action $\neq \pi(s)$} \State policy-stable $\leftarrow$ false \EndIf \EndFor \If {policy-stable} \Return {$V \approx v_\star,\ \pi \approx \pi_\star$} \Else \State go to Policy Evaluation \EndIf \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
<section id="sec-l2g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g5">Value Iteration</h2>
<p>Value iteration is an important example of Generalized Policy Iteration. It is an iterative algorithm that computes the optimal value function and the optimal policy for a given MDP but it does not directly referrence a particular policy.</p>
<p>In value iteration, the algorithm starts with an initial estimate of the value function and iteratively runs a single step of greedy polict evaluation per step, using the greedy value to update the state-value function.</p>
<p>updates the value function until it converges to the optimal value function.</p>
<div id="alg-Value-Iteration" class="pseudocode-container quarto-float" data-line-number="true" data-pseudocode-number="3" data-caption-prefix="Algorithm" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number-punc=":" data-no-end="false">
<div class="pseudocode">
\begin{algorithm} \caption{Value Iteration, for estimating $\pi \approx \pi_{\star}$}\begin{algorithmic} \State Input: $\pi$, the policy to be evaluated \State Algorithm parameter: a small threshold $\theta &gt; 0$ determining accuracy of estimation \State Initialize $V(s) \leftarrow \vec{0} \forall s \in \mathbb{R}$ \REPEAT \STATE $\Delta \leftarrow 0$ \FORALL { $s\in S$} \STATE $v \leftarrow V(s)$ \STATE $V(s) \leftarrow max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]$ \STATE $\Delta \leftarrow \max(\Delta, |v - V(s)|)$ \ENDFOR \UNTIL{$\Delta &lt; \theta$} \State Output: $V \approx v_{\pi}$ such that \State $\pi(s) = \arg\max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]$ \end{algorithmic} \end{algorithm}
</div>
</div>
<section id="sec-l2g4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l2g4">The Dance of Policy and Value</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-dance.png" class="img-fluid figure-img"></p>
<figcaption>Dance of Policy and Value</figcaption>
</figure>
</div></div><p>The policy iteration algorithm is called the dance of policy and value because it alternates between policy evaluation and policy improvement. The policy evaluation step computes the value function for the current policy, and the policy improvement step constructs a new better greedyfied policy based on the value function.</p>
<p>This is also true for other generalized policy iteration algorithms, such as value iteration, which alternates between policy evaluation and policy.</p>
</section>
</section>
</section>
<section id="lesson-3-generalized-policy-iteration" class="level1">
<h1>Lesson 3: Generalized Policy Iteration</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand the framework of <strong>generalized policy iteration</strong>. #</label></li>
<li><label><input type="checkbox" checked="">Outline <strong>value iteration</strong>, an important example of generalized policy iteration. #</label></li>
<li><label><input type="checkbox" checked="">Understand the distinction between <strong>synchronous</strong> and <strong>asynchronous</strong> dynamic programming methods. #</label></li>
<li><label><input type="checkbox" checked="">Describe brute force search as an alternative method for searching for an optimal policy. #</label></li>
<li><label><input type="checkbox" checked="">Describe <strong>Monte Carlo</strong> as an alternative method for learning a value function. #</label></li>
<li><label><input type="checkbox" checked="">Understand the advantage of Dynamic programming and <strong>bootstrapping</strong> over these alternative strategies for finding the optimal policy. #</label></li>
</ul>
</div>
</div>
<section id="sec-l3g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g1">Generalized Policy Iteration</h3>
<p>Generalized policy iteration is a framework for solving reinforcement learning problems that combines policy evaluation and policy improvement in a single loop. The idea is to alternate between evaluating the policy and improving the policy until the policy converges to the optimal policy.</p>
</section>
<section id="sec-l3g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g2">Value Iteration</h3>
<p>Value iteration is an important example of generalized policy iteration. It is an iterative algorithm that computes the optimal value function and the optimal policy for a given MDP. The algorithm starts with an initial estimate of the value function and iteratively updates the value function until it converges to the optimal value function.</p>
</section>
<section id="sec-l3g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g3">Synchronous and Asynchronous Dynamic Programming</h3>
<p>Synchronous dynamic programming methods update all states in the MDP in each iteration, while asynchronous dynamic programming methods update only a subset of states in each iteration. Synchronous dynamic programming methods are typically slower than asynchronous dynamic programming methods, but they are guaranteed to converge to the optimal policy.</p>
</section>
<section id="sec-l3g4" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g4">Brute Force Search</h3>
<p>Brute force search is an alternative method for searching for an optimal policy. It involves exploring all possible policies and selecting the policy that maximizes the expected return. Brute force search is computationally expensive and is not practical for large MDPs.</p>
</section>
<section id="sec-l3g5" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g5">Monte Carlo</h3>
<p>Monte Carlo is an alternative method for learning a value function. It involves estimating the value function by sampling returns from the environment. Monte Carlo is computationally expensive and is not practical for large MDPs.</p>
</section>
<section id="sec-l3g6" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g6">Advantage of Dynamic Programming</h3>
<p>Dynamic programming and bootstrapping are more efficient than brute force search and Monte Carlo for finding the optimal policy. Dynamic programming and bootstrapping exploit the structure of the MDP to update the value function iteratively, while brute force search and Monte Carlo do not.</p>
</section>
<section id="warren-powell-approximate-dynamic-programming-for-fleet-management" class="level3">
<h3 class="anchored" data-anchor-id="warren-powell-approximate-dynamic-programming-for-fleet-management">Warren Powell: Approximate dynamic programming for fleet management</h3>
<p>In this lecture Warren Powell talks about the application of dynamic programming to fleet management.</p>
<ul>
<li><p>We want to calculate the marginal value of a single driver.</p></li>
<li><p>This is a linear programming problem, solvable by <a href="https://www.gurobi.com/">Gurobi</a> and <a href="https://en.wikipedia.org/wiki/CPLEX">cplux</a>.</p></li>
<li><p>For each driver, we drop them out of the system and calculate the system’s new value.</p></li>
<li><p>The difference in values between the original and driver dropped value is the value of the driver.</p></li>
</ul>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Fundamentals</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c1-w4.html</guid>
  <pubDate>Wed, 04 May 2022 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Value Functions &amp; Bellman Equations</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c1-w3.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<p><a href="https://en.wikipedia.org/wiki/Decision_theory">Decision theory</a> is the branch of Mathematics dealing with the analysis of decisions by a single agent. <a href="https://en.wikipedia.org/wiki/Game_theory">Game theory</a> is the branch of Mathematics dealing with the analysis of decisions by multiple agents. The introduction of a second agent makes the problem more complex and introduces the notion of strategic behavior. Decision theory is in many ways a simplification of game theory. In <span class="citation" data-cites="silver2015">[@silver2015]</span>, Dave Silver responded to a question that a simple way of viewing MARL is that each agents are an independent decision maker.</p>
<p>Once the problem is formulated as an MDP, finding the optimal policy is more efficient when using value functions.</p>
<p>This week, we learn the definition of <em>policies</em> and <em>value functions</em>, as well as <em>Bellman equations</em>, which are the key technology behind all the algorithms we will learn.</p>
<p>For someone with a background in game theory, the concept of a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is not new in game theory, we call this a strategy and it is a mapping from states to actions. i.e.&nbsp;an assignment of some action to each state representing the best action that an agent should take in that state.</p>
<p>A second familiar concept is the value function. In game theory, we call this the payoff for an action. The payoffs are typically assigned to the terminal states of the game and can be backpropagated to non-terminal states using the laws of probability. Here we are interested in the expected value of the rewards that an agent can expect to receive when following a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> from a given state <img src="https://latex.codecogs.com/png.latex?s">.</p>
<p>I found the Policy and values functions somewhat families due to some background in game theory and markov processes.</p>
<p>I found the Bellman equations more of a challenge. I think the main issue is the unfamiliarity with the notation which make the material look like gibberish. However, the more I made myself more familiar with the notation, I came to see that these equations express a rather simple idea.</p>
<p>We describe a MDP as a linear process in time. However, it is really a tree of possible actions. What the Bellman equations express is that if we want to estimate the value <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> of a state or more specifically the value of an action <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,a)"> what we do is consider the immediate rewards and then we have have a copy of pretty much the same tree. As we move forward in time we will end up making ever smaller (discounted) corrections to our best assessment.</p>
<section id="lesson-1-policies-and-value-functions" class="level1 page-columns page-full">
<h1>Lesson 1: Policies and Value Functions</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Read
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=58">@sutton2018reinforcement§3.5-7, pp.&nbsp;58-67</a></label></li>
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=68">@sutton2018reinforcement§3.8, pp.&nbsp;68-69</a></label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Recognize that a policy is a distribution over actions for each possible state #</label></li>
<li><label><input type="checkbox" checked="">Describe the similarities and differences between stochastic and deterministic policies #</label></li>
<li><label><input type="checkbox" checked="">Identify the characteristics of a well-defined policy #</label></li>
<li><label><input type="checkbox" checked="">Generate examples of valid policies for a given MDP #</label></li>
<li><label><input type="checkbox" checked="">Describe the roles of state-value and action-value functions in reinforcement learning #</label></li>
<li><label><input type="checkbox" checked="">Describe the relationship between value functions and policies #</label></li>
<li><label><input type="checkbox" checked="">Create examples of valid value functions for a given MDP #</label></li>
</ul>
</div>
</div>
<div class="cell">
<details class="code-fold hidden">
<summary>Code</summary>
<div class="sourceCode cell-code hidden" id="cb1" data-startfrom="66" data-source-offset="0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 65;"><span id="cb1-66">dotStyles <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ({ <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">defaults</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span></span>
<span id="cb1-67"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  graph [</span></span>
<span id="cb1-68"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    labelloc = "b",</span></span>
<span id="cb1-69"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontname="Times",</span></span>
<span id="cb1-70"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontsize=12,</span></span>
<span id="cb1-71"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    overlap=false ];</span></span>
<span id="cb1-72"></span>
<span id="cb1-73"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  node [</span></span>
<span id="cb1-74"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    style=filled,</span></span>
<span id="cb1-75"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    shape=circle,</span></span>
<span id="cb1-76"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    colorscheme=accent5,</span></span>
<span id="cb1-77"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    color=1,</span></span>
<span id="cb1-78"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontname="Times",</span></span>
<span id="cb1-79"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontsize=12];</span></span>
<span id="cb1-80"></span>
<span id="cb1-81"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  edge [</span></span>
<span id="cb1-82"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontname="Times",</span></span>
<span id="cb1-83"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontsize=12,</span></span>
<span id="cb1-84"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    margin=2,</span></span>
<span id="cb1-85"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    overlap=false ];</span></span>
<span id="cb1-86"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span> })</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="ojs-cell-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell">
<details class="code-fold hidden">
<summary>Code</summary>
<div class="sourceCode cell-code hidden" id="cb2" data-startfrom="93" data-source-offset="0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 92;"><span id="cb2-93">{ </span>
<span id="cb2-94">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">flatten</span>(ll) {</span>
<span id="cb2-95">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ll<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">reduce</span>((acc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> l) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">...</span>acc]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">concat</span>(l)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> [])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-96">  }</span>
<span id="cb2-97">  </span>
<span id="cb2-98">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> numActions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-99">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> numNextStates <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-100">  </span>
<span id="cb2-101">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> actions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> numActions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-102">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> nextStates <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">flatten</span>(actions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> numNextStates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`s</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}${</span>j<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span>)))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-103">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> nextStateTransitions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">flatten</span>(actions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> numNextStates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`a</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;"> -&gt; s</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}${</span>j<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  [label="p, r"]`</span>)))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-104">    </span>
<span id="cb2-105">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dot</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span></span>
<span id="cb2-106"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">digraph G {</span></span>
<span id="cb2-107"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>dotStyles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">defaults</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb2-108"></span>
<span id="cb2-109"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph root {</span></span>
<span id="cb2-110"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb2-111"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=1 ];</span></span>
<span id="cb2-112"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s ;</span></span>
<span id="cb2-113"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb2-114"></span>
<span id="cb2-115"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph env1 {</span></span>
<span id="cb2-116"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb2-117"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=2 ];</span></span>
<span id="cb2-118"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>actions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`a</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;"> [label="s, a</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">"]`</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">join</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">';</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-119"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb2-120"></span>
<span id="cb2-121"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph sp {</span></span>
<span id="cb2-122"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb2-123"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=1, label="s'" ];</span></span>
<span id="cb2-124"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>nextStates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">join</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'; '</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-125"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb2-126"></span>
<span id="cb2-127"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>actions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(a <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`s -&gt; a</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>a<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;"> [label="&amp;pi;"];`</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">join</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb2-128"></span>
<span id="cb2-129"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>nextStateTransitions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">join</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb2-130"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">}`</span>     </span>
<span id="cb2-131">}</span></code></pre></div>
</details>
<div id="fig-ojs-cell-2" class="cell-output cell-output-display quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ojs-cell-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="ojs-cell-2" data-nodetype="expression">

</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ojs-cell-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Backup diagram for the V(s) - state value function
</figcaption>
</figure>
</div>
</div>
<div class="cell">
<details class="code-fold hidden">
<summary>Code</summary>
<div class="sourceCode cell-code hidden" id="cb3" data-startfrom="139" data-source-offset="0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 138;"><span id="cb3-139"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dot</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span></span>
<span id="cb3-140"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">digraph G {</span></span>
<span id="cb3-141"></span>
<span id="cb3-142"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>dotStyles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">defaults</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb3-143"></span>
<span id="cb3-144"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph root {</span></span>
<span id="cb3-145"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb3-146"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=2,label="s, a" ];</span></span>
<span id="cb3-147"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa ;</span></span>
<span id="cb3-148"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  } </span></span>
<span id="cb3-149"></span>
<span id="cb3-150"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph s1 {</span></span>
<span id="cb3-151"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb3-152"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=1, label="s'"];</span></span>
<span id="cb3-153"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s1;</span></span>
<span id="cb3-154"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s2;</span></span>
<span id="cb3-155"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s3;</span></span>
<span id="cb3-156"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb3-157"></span>
<span id="cb3-158"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph sp {</span></span>
<span id="cb3-159"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb3-160"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=2, label="s', a'" ];</span></span>
<span id="cb3-161"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa11; </span></span>
<span id="cb3-162"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa12; </span></span>
<span id="cb3-163"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa21; </span></span>
<span id="cb3-164"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa22; </span></span>
<span id="cb3-165"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa31; </span></span>
<span id="cb3-166"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa32;</span></span>
<span id="cb3-167"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  } </span></span>
<span id="cb3-168"></span>
<span id="cb3-169"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa -&gt; s1, s2, s3 [label="p, r"];</span></span>
<span id="cb3-170"></span>
<span id="cb3-171"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s1 -&gt; sa11, sa12 [label="&amp;pi;"];</span></span>
<span id="cb3-172"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s2 -&gt; sa21, sa22 [label="&amp;pi;"];</span></span>
<span id="cb3-173"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s3 -&gt; sa31, sa32 [label="&amp;pi;"];</span></span>
<span id="cb3-174"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">}`</span></span></code></pre></div>
</details>
<div id="fig-ojs-cell-3" class="cell-output cell-output-display quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ojs-cell-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="ojs-cell-3" data-nodetype="expression">

</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ojs-cell-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Backup diagram for the Q(a,s) action value function
</figcaption>
</figure>
</div>
</div>
<section id="sec-policy-definition" class="level2">
<h2 class="anchored" data-anchor-id="sec-policy-definition">Policy Definition</h2>
<ul>
<li>A policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a distribution over actions for each possible state.</li>
<li>It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%7Cs)">, which is the probability of taking action <img src="https://latex.codecogs.com/png.latex?a"> in state <img src="https://latex.codecogs.com/png.latex?s"> under policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</li>
</ul>
</section>
<section id="sec-stochastic-vs-deterministic" class="level2">
<h2 class="anchored" data-anchor-id="sec-stochastic-vs-deterministic">Stochastic vs Deterministic Policies</h2>
<ul>
<li>A policy can be deterministic or stochastic.</li>
<li>A deterministic policy is a policy that selects a single action in each state.
<ul>
<li>For example, the greedy policy selects the action with the highest value</li>
</ul></li>
<li>A stochastic policy is a policy that selects actions with some probability that can be conditioned on the state.
<ul>
<li>For example the uniform policy selects each action with equal probability.</li>
</ul></li>
</ul>
</section>
<section id="sec-value-functions" class="level2">
<h2 class="anchored" data-anchor-id="sec-value-functions">Value Functions</h2>
<ul>
<li>We generally want to evaluate the value of each state or better yet the value of each action in each state before we create the policy. To do this we define two types of value functions:</li>
</ul>
<section id="sec-state-value-functions" class="level3">
<h3 class="anchored" data-anchor-id="sec-state-value-functions">State-value functions <img src="https://latex.codecogs.com/png.latex?V_%5Cpi"></h3>
<p>The state-value function <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D(s)"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s"> and following policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> thereafter.</p>
<p><span id="eq-state-value-function"><img src="https://latex.codecogs.com/png.latex?%0Av_%5Cpi(s)%20%5Cdot%20=%20%5Cmathbb%7BE_%5Cpi%7D%5BG_t%7CS_t%20=%20s%5D%20%5Cquad%20%5Ctext%7Bfor%20policy%7D%20%5Cquad%20%5Cpi%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
</section>
<section id="sec-action-value-functions" class="level3">
<h3 class="anchored" data-anchor-id="sec-action-value-functions">Action-value functions <img src="https://latex.codecogs.com/png.latex?Q_%5Cpi"></h3>
<p>The action-value function <img src="https://latex.codecogs.com/png.latex?q_%7B%5Cpi%7D(s,a)"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s">, taking action <img src="https://latex.codecogs.com/png.latex?a">, and following policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> thereafter.</p>
<p><span id="eq-action-value-function"><img src="https://latex.codecogs.com/png.latex?%0Aq_%5Cpi(s,a)%20%5Cdot%20=%20%5Cmathbb%7BE_%5Cpi%7D%5BG_t%7CS_t%20=%20s,%20A_t%20=%20a%5D%20%5Cquad%20%5Ctext%7Bfor%20policy%7D%20%5Cquad%20%5Cpi%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>
</section>
<section id="relationship-between-value-functions-and-policies" class="level3">
<h3 class="anchored" data-anchor-id="relationship-between-value-functions-and-policies">Relationship between Value Functions and Policies</h3>
<p>In the short term, the value functions are more useful than the return G</p>
<ul>
<li>The return G is not immediately available</li>
<li>The return G can be non-deterministic.</li>
</ul>
<p>The value functions are deterministic and can be computed from the MDP.</p>
</section>
</section>
<section id="lesson-2-bellman-equations" class="level2">
<h2 class="anchored" data-anchor-id="lesson-2-bellman-equations">Lesson 2: Bellman Equations</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Derive the Bellman equation for state-value functions #</label></li>
<li><label><input type="checkbox" checked="">Derive the Bellman equation for action-value functions #</label></li>
<li><label><input type="checkbox" checked="">Understand how Bellman equations relate current and future values #</label></li>
<li><label><input type="checkbox" checked="">Use the Bellman equations to compute value functions the state value function is <img src="https://latex.codecogs.com/png.latex?v(s)"> #</label></li>
</ul>
</div>
</div>
</section>
<section id="sec-bellman-equation-state-value-functions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-bellman-equation-state-value-functions">Bellman Equation for State-Value Functions</h2>

<div class="no-row-height column-margin column-container"><div id="fig-backup-v" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-backup-v-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/posts/img/rl-backup-v.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-backup-v-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: backup diagram for <img src="https://latex.codecogs.com/png.latex?v_%5Cpi">
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bellman Equation intuition
</div>
</div>
<div class="callout-body-container callout-body">
<p>Richard Bellman was a uniquely gifted mathematician who worked on dynamic programming. The Bellman equations is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state. These equations form the basis of Dynamic Programming which is used in disparate problems including</p>
<ul>
<li>Schedule optimization</li>
<li>String algorithms (e.g.&nbsp;sequence alignment)</li>
<li>Graph algorithms (e.g.&nbsp;the shortest path problem)</li>
<li>Graphical algorithms (e.g.&nbsp;the Vitrebi algorithm)</li>
<li>Bioinformatics (e.g.&nbsp;lattice models)</li>
</ul>
<p>Although Bellman was one of the greatest problem solvers of the 20th century, he was not a great communicator. He was known for his terse and cryptic writing. The Bellman equations are a case in point. They are simple to understand once you get the hang of them but they are not easy to read for the first time. However the key to understanding the Bellman equations is to understand that they are a recursive equation based on some physical process</p>
<blockquote class="blockquote">
<p>The trick that one learns over time, a basic part of mathematical methodology, is to sidestep the equation and focus instead on the structure of the underlying physical process – Richard Bellman</p>
</blockquote>
<p>in the case of RL the recursive physical process is: <span id="eq-bellman-equation-intuition"><img src="https://latex.codecogs.com/png.latex?%0A%20%20S%20%5Crightarrow%20A%20%5Crightarrow%20R.%0A%5Ctag%7B3%7D"></span></p>
<p>and we can diagram it using a backup diagram as shown in the Figure&nbsp;3 above.</p>
<p>The name backup diagram comes from the idea that we are backing up the value of the state <img src="https://latex.codecogs.com/png.latex?v(s)"> from the successor state <img src="https://latex.codecogs.com/png.latex?v(s')">. I.e. we are going back up the tree of possible effects of some action <img src="https://latex.codecogs.com/png.latex?a"> starting from the state <img src="https://latex.codecogs.com/png.latex?s">.</p>
<p>While the Bellman equation are difficult to read, remember and to derive, the backup diagram are very easy to sketch even if you don’t remember the equations. Once you have sketch the backup diagram, you should be able to easily derive the Bellman equations.</p>
<p>This same intuition can be used for working through all the above dynamic programming algorithms!</p>
</div>
</div>
<p>The Bellman equation for state-value functions is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state.</p>
<p><span id="eq-bellman-state-value-function"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20v_%5Cpi(s)%20&amp;=%20%5Cmathbb%7BE_%5Cpi%7D%5BG_t%7CS_t=s%5D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Cmathbb%7BE_%5Cpi%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%7CS_t=s%5D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Cmathbb%7BE_%5Cpi%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20v_%5Cpi(S_%7Bt+1%7D)%7CS_t=s%5D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csum_a%20%5Cpi(a%7Cs)%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20%5Cmathbb%7BE_%5Cpi%7D%5BG_%7Bt+1%7D%7CS_%7Bt+1%7D=s'%5D)%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csum_a%20%5Cpi(a%7Cs)%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20v_%5Cpi(s'))%0A%5Cend%7Balign%7D%0A%5Ctag%7B4%7D"></span></p>
</section>
<section id="sec-bellman-equation-action-value-functions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-bellman-equation-action-value-functions">Bellman Equation for Action-Value Functions</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-backup-q.png" class="img-fluid figure-img"></p>
<figcaption>backup diagram for q(s,a) function</figcaption>
</figure>
</div></div><p>The Bellman equation for action-value functions is a recursive equation that decomposes the value of a state-action pair into the immediate reward and the discounted value of the successor state-action pair.</p>
<p><span id="eq-bellman-action-value-function"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20q_%5Cpi(s,a)%20&amp;%20%5Cdot%20=%20%5Cmathbb%7BE_%5Cpi%7D%5BG_t%7CS_t=s,%20A_t=a%5D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20%5Cmathbb%7BE_%5Cpi%7D%5BG_%7Bt+1%7D%7CS_%7Bt+1%7D=s'%5D)%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20%5Csum_%7Ba'%7D%20%5Cpi(a'%7Cs')%20%5Cmathbb%7BE_%5Cpi%7D%5BG_%7Bt+1%7D%7CS_%7Bt+1%7D=s',%20A_%7Bt+1%7D=a'%5D)%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20%5Csum_%7Ba'%7D%20%5Cpi(a'%7Cs')%20q_%5Cpi(s',a'))%0A%5Cend%7Balign%7D%0A%5Ctag%7B5%7D"></span></p>
</section>
<section id="bellman-equations" class="level2">
<h2 class="anchored" data-anchor-id="bellman-equations">Bellman Equations</h2>
<p>the bellman equations capture the relationship between the current value and the future value. The Bellman equations are a set of equations that express the relationship between the value of a state and the value of its successor states. The Bellman equations are used to compute the value functions of a Markov Decision Process (MDP).</p>
</section>
<section id="example-gridworld" class="level2">
<h2 class="anchored" data-anchor-id="example-gridworld">Example: Gridworld</h2>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td>A</td>
<td>B</td>
</tr>
<tr class="even">
<td>C</td>
<td>D</td>
</tr>
</tbody>
</table>
<p>In the 2x2 gridworld example, the agent can move up, down, left, or right. The agent receives a reward of 0 for each step taken unless it gets to location B for which it gets +5. The agent receives will return to the current cell if it bumping into the wall.</p>
<p>We will use the uniform random policy where the agent selects each action with equal probability.</p>
<p>gamma = 0.7</p>
<p>lets calculate the value of each state using the Bellman equation.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Av_%5Cpi(A)%20&amp;=%20%5Csum_a%20%5Cpi(a%7CA)%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7CA,a)%20(r%20+%20%5Cgamma%20v(s'))%20%5Cnewline%0A%20%20%20%20%20&amp;=%20%5Csum_a%20%5Cpi(a%7CA)%20(r%20+%200.7%20v_%5Cpi(s'))%20%5Cnewline%0A%20%20%20%20%20&amp;=%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(A)%20+%200.25%20%5Ctimes%20(5%20+%200.7%20%5Ctimes%20v(B))%20+%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(C)%20+%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(A)%20%5Cnewline%0Av_%5Cpi(B)%20&amp;=%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(A)%20+%200.5%20%5Ctimes%20(5%20+%200.7%20%5Ctimes%20v(B))%20+%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(D)%20%5Cnewline%0Av_%5Cpi(C)%20&amp;=%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(A)%20+%200.5%20%5Ctimes%20(0.7%20%5Ctimes%20v(B))%20+%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(C)%20%5Cnewline%0Av_%5Cpi(D)%20&amp;=%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(B)%20+%200.5%20%5Ctimes%200.7%20%5Ctimes%20v(C)%20+%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(D)%0A%5Cend%7Balign%7D%0A"> we can solve these equations to get the value of each state.</p>
<p>theses are</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Av_%5Cpi(A)%20&amp;=%204.2%20%5Cnewline%0Av_%5Cpi(B)%20&amp;=%206.1%20%5Cnewline%0Av_%5Cpi(C)%20&amp;=%202.2%20%5Cnewline%0Av_%5Cpi(D)%20&amp;=%204.2%20%5Cnewline%0A%5Cend%7Balign*%7D%0A"></p>
<p>We can use the Bellman equation to calculate the value of each state in the Gridworld. The value of each state is the expected return when starting in that state and following the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> thereafter. The value of each state is calculated by summing the immediate reward and the discounted value of the successor states.</p>
<p>For larger MDP the Bellman equations are not practical method to calculate the value of each state. Instead, we will use algorithms based on the Bellman equations to estimate the value of each state.</p>
<section id="lesson-3-optimality-optimal-policies-value-functions" class="level3">
<h3 class="anchored" data-anchor-id="lesson-3-optimality-optimal-policies-value-functions">Lesson 3: Optimality (Optimal Policies &amp; Value Functions)</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Define an optimal policy #</label></li>
<li><label><input type="checkbox" checked="">Understand how a policy can be at least as good as every other policy in every state. #</label></li>
<li><label><input type="checkbox" checked="">Identify an optimal policy for given MDPs.</label></li>
<li><label><input type="checkbox" checked="">Derive the Bellman optimality equation for state-value functions</label></li>
<li><label><input type="checkbox" checked="">Derive the Bellman optimality equation for action-value functions</label></li>
<li><label><input type="checkbox" checked="">Understand how the Bellman optimality equations relate to the previously introduced Bellman equations</label></li>
<li><label><input type="checkbox" checked="">Understand the connection between the optimal value function and optimal policies</label></li>
<li><label><input type="checkbox" checked="">Verify the optimal value function for given MDPs</label></li>
</ul>
</div>
</div>
</section>
</section>
<section id="sec-optimal-policy" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-optimal-policy">Optimal Policy</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-optimal-policy.png" class="img-fluid figure-img"></p>
<figcaption>Bellman Optimality Equation</figcaption>
</figure>
</div></div><ul>
<li>A policy <img src="https://latex.codecogs.com/png.latex?pi_1"> is better than a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_2"> if <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi_1%7D(s)%20%5Cgeq%20v_%7B%5Cpi_2%7D(s)"> for all states <img src="https://latex.codecogs.com/png.latex?s">.</li>
<li>Given any two policies <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi'">, if we pick the action that maximizes the value function, from either at every state, we will get a new policy that is at least as good as both <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi'">.</li>
<li>There is always at least one deterministic optimal policy for any MDP.</li>
<li>An optimal policy <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E*"> is a policy that is at least as good as every other policy in every state.</li>
<li>The optimal policy is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E*"> and is defined as:</li>
</ul>
<p><span id="eq-optimal-policy"><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%5E*%20%5Cdot%20=%20%5Carg%20%5Cmax_%7B%5Cpi%7D%20v_%7B%5Cpi%7D(s)%20%5Cquad%20%5Cforall%20s%5Cin%20S%0A%5Ctag%7B6%7D"></span></p>
</section>
<section id="sec-bellman-optimality-state-value-function" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-bellman-optimality-state-value-function">Bellman Optimality Equation for State-Value Functions</h2>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/posts/img/rl-backup-v-star.png" class="img-fluid"></div></div>
<p>The Bellman optimality equation for state-value functions is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state under the optimal policy.</p>
<p><span id="eq-bellman-optimality-state-value-function"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Av_*(s)%20&amp;%20%5Cdot%20=%20%5Cmax_%7B%5Cpi%7D%20v_%7B%5Cpi%7D(s)%20%5Cquad%20%5Cforall%20s%20%5Cin%20S%5Cnewline%0A%20%20%20%20%20%20%20&amp;%20=%20%5Cmax_a%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20v_*(S_%7Bt+1%7D)%7CS_t=s,%20A_t=a%5D%20%5Cnewline%0A%20%20%20%20%20%20%20&amp;%20=%20%5Cmax_a%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20v_*(s'))%0A%5Cend%7Balign%7D%0A%5Ctag%7B7%7D"></span></p>
</section>
<section id="sec-bellman-optimality-action-value-function" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-bellman-optimality-action-value-function">Bellman Optimality Equation for Action-Value Functions</h2>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/posts/img/rl-backup-q-star.png" class="img-fluid"></div></div>
<p>The Bellman optimality equation for action-value functions is a recursive equation that decomposes the value of a state-action pair into the immediate reward and the discounted value of the successor state-action pair under the optimal policy.</p>
<p><span id="eq-bellman-optimality-action-value-function"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Aq_*(s,a)%20&amp;%20%5Cdot%20=%20%5Cmax_%7B%5Cpi%7D%20q_%7B%5Cpi%7D(s,a)%20%5Cquad%20%5Cforall%20s%20%5Cin%20S,%20%5Cforall%20a%20%5Cin%20A%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Cmax_%7Ba'%7D%20q_*(S_%7Bt+1%7D,%20a')%7CS_t=s,%20A_t=a%5D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20%5Cmax_%7Ba'%7D%20q_*(s',a'))%0A%5Cend%7Balign%7D%0A%5Ctag%7B8%7D"></span></p>
<p>Martha White asks the question: “How can <img src="https://latex.codecogs.com/png.latex?%5CPi_3"> have better strictly better values than both <img src="https://latex.codecogs.com/png.latex?%5CPi_1"> and <img src="https://latex.codecogs.com/png.latex?%5CPi_2"> in all states if all we did is take the best action in each state from either <img src="https://latex.codecogs.com/png.latex?%5CPi_1"> or <img src="https://latex.codecogs.com/png.latex?%5CPi_2">?”</p>
<p>This is because if for example we found a fast path through a bottleneck for any state that is before the bottleneck will have a higher value in the other policies which may have had longer paths through the bottleneck.</p>
</section>
<section id="sec-optimal-value-functions" class="level2">
<h2 class="anchored" data-anchor-id="sec-optimal-value-functions">Optimal Value Functions</h2>
<ul>
<li>The optimal value function <img src="https://latex.codecogs.com/png.latex?v_*(s)"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s"> and following the optimal policy thereafter.</li>
<li>The optimal action-value function <img src="https://latex.codecogs.com/png.latex?q_*(s,a)"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s">, taking action <img src="https://latex.codecogs.com/png.latex?a">, and following the optimal policy thereafter.</li>
<li>An optimal policy can be obtained from the optimal action-value function by selecting the action with the highest value.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Fundamentals</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c1-w3.html</guid>
  <pubDate>Tue, 03 May 2022 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Markov Decision Processes</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c1-w2.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="lesson-1-introduction-to-markov-decision-processes" class="level1 page-columns page-full">
<h1>Lesson 1: Introduction to Markov Decision Processes</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§3.3, pp. 47-56]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=47">book</a></label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand Markov Decision Processes (MDP). #</label></li>
<li><label><input type="checkbox" checked="">Describe how the dynamics of an MDP are defined. #</label></li>
<li><label><input type="checkbox" checked="">Understand the graphical representation of a Markov Decision Process. #</label></li>
<li><label><input type="checkbox" checked="">Explain how many diverse processes can be written in terms of the MDP framework. #</label></li>
</ul>
</div>
</div>
<p>Before I started this course I viewed <span class="citation" data-cites="silver2015">[@silver2015]</span> online course by David Silver who in many ways is the face of RL. He is also the lead of the AlphaGo project, a principal researcher at DeepMind and a lecturer at University College London. Silver is also featured in one of the Lectures in this specialization. In his course he develops the MDP constructively starting with simpler structures as is often done in mathematics. I find this is a good way to learn how to think about how we generalize and specialize from more abstract to more concrete structures.</p>
<p>Many students of probability theory will be familiar with Markov Chains. And Markov Decision Processes are a generalization of Markov Chains.</p>
<p>In <span class="citation" data-cites="silver2015">[@silver2015]</span> he begins with a <strong>Markov Process</strong>, with states and transitions probabilities, by adding <strong>rewards</strong> he constructs a <em>Markov reward process</em>. Then by adding <strong>actions</strong> he constructs a <em>Markov decision process</em>. He explains these and in the notes covers three additional extensions. In the the notes he also add the following MDP extensions:</p>
<ul>
<li>Infinite and continuous MDPs - the case of optimal control</li>
<li>Partially observable MDPs.</li>
<li>Undiscounted, average reward MDPs.</li>
<li>David Silver’s 2015 <a href="https://www.davidsilver.uk/teaching/">UCL Course</a> <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">Video</a> and Slides.</li>
</ul>
<section id="sec-Markov-Process" class="level2">
<h2 class="anchored" data-anchor-id="sec-Markov-Process">Markov Process</h2>
<p>Silver goes into some detail on what we mean by state in RL:</p>
<ul>
<li>In th abstract state can be any function of the history.</li>
<li>The state should summarize the information on the previous actions, and rewards.</li>
<li>He points out that the history in RL can be very long, for Atari games it can include actions plus all the pixels for every screen in many plays of the game. In contrast the state tries to capture the bare essentials of the history for decision making at each time step. For Atari games they used the last 4 screens as the state.</li>
<li>A second point is that there is always a state. The full history is also a state, but not a very useful one. The internal representation of the ram in the Atari game is also a state, much smaller but this is the representation used by the environment and contains more information than is available to the agent. Ideally the agent would want to model this state, but again it contains lots more information than is available would need to male a decision.</li>
<li>Another useful property of the state is that it should have the Markov Property for a state space which is when the future is independent of the past given the present.</li>
</ul>
<p>A state S<img src="https://latex.codecogs.com/png.latex?_t"> is Markov if and only if:</p>
<p><span id="eq-markov-property"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D%5BS_%7Bt+1%7D%20%20%5Cvert%20%20S_%7Bt%7D%5D%20=%20%20%5Cmathbb%7BP%7D%5BS_%7Bt+1%7D%20%20%5Cvert%20%20S_1,...,S_t%5D%20%5Cqquad%20%5Ctext%7B(Markov%20Property)%7D%0A%5Ctag%7B1%7D"></span></p>
<p><strong>The state captures all relevant information from the history</strong> Once the state is known, the history may be thrown away i.e.&nbsp;<mark>The state is a sufficient statistic of the future</mark></p>
<p>Recall:</p>
<blockquote class="blockquote">
<p>a statistic satisfies the criterion for sufficency when no other statistic that can be calculated from the same <a href="https://en.wikipedia.org/wiki/Sample_(statistics)" title="Sample (statistics)">sample</a> provides any additional information as to the value of the parameter”. — <span class="citation" data-cites="doi:10.1098/rsta.1922.0009">[@doi:10.1098/rsta.1922.0009]</span></p>
</blockquote>
<p>For a Markov state <img src="https://latex.codecogs.com/png.latex?s"> and successor state <img src="https://latex.codecogs.com/png.latex?s'">, the state transition probability is defined by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D_%7Bss'%7D%20=%20%5Cmathbb%7BP%7D%5BS_%7Bt+1%7D=s'%20%20%5Cvert%20%20S_t=s%5D%0A"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Markov Process or Chain Definion
</div>
</div>
<div class="callout-body-container callout-body">
<dl>
<dt>A Markov Process is</dt>
<dd>
<p>a tuple <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8S,P%E2%9F%A9"></p>
</dd>
</dl>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?S"> is a (finite) set of states</li>
<li><img src="https://latex.codecogs.com/png.latex?P"> is a state transition probability matrix, <img src="https://latex.codecogs.com/png.latex?P_%7Bss'%7D%20=%20P%5BS_%7Bt+1%7D%20=%20s'%20%20%5Cvert%20S_t=s%5D"> State transition matrix <img src="https://latex.codecogs.com/png.latex?P_%7Bss'%7D"> defines transition probabilities from all states <img src="https://latex.codecogs.com/png.latex?s"> to all successor states <img src="https://latex.codecogs.com/png.latex?s'">,</li>
</ul>
<p><span id="eq-transition-matrix"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20P=%5Cleft(%20%5Cbegin%7Barray%7D%7Bcc%7D%0A%20%20%20%20%20%20p_%7B11%7D%20&amp;%20%5Ccdots%20&amp;%20p_%7B1n%7D%20%5Cnewline%0A%20%20%20%20%20%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5Cnewline%0A%20%20%20%20%20%20p_%7Bn1%7D%20&amp;%20%5Ccdots%20&amp;%20p_%7Bnn%7D%20%5Cend%7Barray%7D%20%5Cright)%0A%5Cend%7Balign*%7D%0A%5Ctag%7B2%7D"></span></p>
</div>
</div>
</section>
<section id="sec-MRP" class="level2">
<h2 class="anchored" data-anchor-id="sec-MRP">Markov Reward Process</h2>
<p>A Markov Reward Process <strong>(MRP)</strong> is a Markov chain with values.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Markov Reward Process Definition
</div>
</div>
<div class="callout-body-container callout-body">
<dl>
<dt>A Markov Reward Process is</dt>
<dd>
<p>a tuple <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8S,%20P,%20R,%20%5Cgamma%E2%9F%A9"></p>
</dd>
</dl>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?S"> is a finite set of states</li>
<li><img src="https://latex.codecogs.com/png.latex?P"> is a state transition probability matrix, where <img src="https://latex.codecogs.com/png.latex?P_%7Bss'%7D%20=%20%20%5Cmathbb%7BP%7D%5BS_%7Bt+1%7D%20=%20s'%20%5Cvert%20%20S_t%20=%20s%5D"></li>
<li><img src="https://latex.codecogs.com/png.latex?R"> is a reward function, <img src="https://latex.codecogs.com/png.latex?R_s%20=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20%5Cvert%20S_t%20=%20s%5D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is a discount factor, <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Cin%20%5B0,%201%5D"></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
the return definition
</div>
</div>
<div class="callout-body-container callout-body">
<dl>
<dt>The return <img src="https://latex.codecogs.com/png.latex?G_t"></dt>
<dd>
<p>is the total discounted reward from time-step t.</p>
</dd>
</dl>
<p><span id="eq-return"><img src="https://latex.codecogs.com/png.latex?%0AG_t%20=R_%7Bt+1%7D+%5Cgamma%20R_%7Bt+2%7D+...=%5Csum_%7Bk=0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20R_%7Bt+k+1%7D%0A%5Ctag%7B3%7D"></span></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?R_t"> is the reward at time-step <img src="https://latex.codecogs.com/png.latex?t"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> the discount factor <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Cin%20%5B0,%201%5D"> is the present value of future rewards.</li>
<li>The value of receiving reward <img src="https://latex.codecogs.com/png.latex?R"> after <img src="https://latex.codecogs.com/png.latex?k+1"> time-steps is <img src="https://latex.codecogs.com/png.latex?%5Cgamma%5Ek%20R"></li>
<li>This values immediate reward above delayed reward.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma%20=%200"> makes the agent short-sighted.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma%20=%201"> makes the agent far-sighted.</li>
</ul></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The value function
</div>
</div>
<div class="callout-body-container callout-body">
<dl>
<dt>The value function:</dt>
<dd>
<p>The state value function <img src="https://latex.codecogs.com/png.latex?v(s)"> of an MRP is the expected return starting from state <img src="https://latex.codecogs.com/png.latex?s"></p>
</dd>
</dl>
<p><img src="https://latex.codecogs.com/png.latex?%0Av(s)%20=%5Cmathbb%7BE%7D%20%5BG_t%20%20%5Cvert%20%20S_t%20=%20s%5D%0A"></p>
</div>
</div>
<section id="sec-bellman-MRP" class="level3">
<h3 class="anchored" data-anchor-id="sec-bellman-MRP">Bellman equations for MRP</h3>
<p>The value function can be decomposed into two parts:</p>
<ul>
<li>an immediate reward <img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D"> and</li>
<li>a discounted value of successor state <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20v(S_%7Bt+1%7D)"></li>
</ul>
<p>The Bellman equation for MRPs expresses this relationship:</p>
<p><span id="eq-bellman-mrp"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Av(s)%20&amp;=%20%5Cmathbb%7BE%7D%5BG_t%20%20%5Cvert%20%20S_t=s%5D%20%5Cnewline%0A&amp;%20=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20R_%7Bt+2%7D+%5Cgamma%5E2%20R_%7Bt+3%7D%20+%20...%20%20%5Cvert%20S_t%20=%20s%5D%20%20%20%5Cnewline%0A&amp;%20=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma(%20R_%7Bt+2%7D+%5Cgamma%5E2%20R_%7Bt+3%7D%20+%20...%20)%20%20%5Cvert%20S_t%20=%20s%5D%20%20%20%5Cnewline%0A&amp;%20=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%20%20%5Cvert%20%20S_t%20=%20s%5D%20%20%20%5Cnewline%0A&amp;%20=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20v(S_%7Bt+1%7D)%20%20%5Cvert%20S_t%20=%20s%5D%0A%5Cend%7Balign*%7D%20%5Cqquad%20%5Ctext%7B(Bellman%20Equation)%7D%0A%5Ctag%7B4%7D"></span></p>
<p>The Bellman equation can also be expressed in terms of the dynamics matrix for state transitions:</p>
<p><span id="eq-bellman-mrp-with-dynamics-matrix"><img src="https://latex.codecogs.com/png.latex?%0Av(s)%20=%20R_s%20+%20%CE%B3%20%5Csum_%7Bs'%5Cin%20S%7D%20P_%7Bss'%7Dv(s)%20%5Cqquad%20%5Ctext%7Bvalue%20with%20dynamics%7D%0A%5Ctag%7B5%7D"></span></p>
<p>where we use the dynamics matrix <img src="https://latex.codecogs.com/png.latex?P"> to express the expected value of the successor state.</p>
</section>
</section>
<section id="sec-MDP" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-MDP">Markov Decision Processes</h2>
<ul>
<li>The k-Armed Bandit problem does not account for the fact that different situations call for different actions.</li>
<li>Because it the problem is limited to a single state agents can only make decisions based on immediate reward so they fail to consider the long-term impact of their decisions - this is an inability to make plan.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-agent-env.png" class="img-fluid figure-img"></p>
<figcaption>The agent–environment interaction in a Markov decision process.</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MDP definition
</div>
</div>
<div class="callout-body-container callout-body">
<dl>
<dt>A Markov Decision Process is a Markov Reward Process with decisions.</dt>
<dd>
<p>a tuple <img src="https://latex.codecogs.com/png.latex?%E2%9F%A8S,%20A,%20P,%20R,%20%5Cgamma%E2%9F%A9"></p>
</dd>
</dl>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?S"> is a finite set of states</li>
<li><img src="https://latex.codecogs.com/png.latex?A"> is a finite set of actions</li>
<li><img src="https://latex.codecogs.com/png.latex?P"> is a state transition probability matrix, <img src="https://latex.codecogs.com/png.latex?P_%7Bss'%7D%5Ea%20=%20%5Cmathbb%7BP%7D%5BS_%7Bt+1%7D%20=%20s'%20%5Cvert%20S_t%20=%20s,%20A_t%20=%20a%5D"></li>
<li><img src="https://latex.codecogs.com/png.latex?R"> is a reward function, <img src="https://latex.codecogs.com/png.latex?R_s%5Ea%20=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20%5Cvert%20S_t%20=%20s,%20A_t%20=%20a%5D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is a discount factor, <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Cin%20%5B0,%201%5D"></li>
</ul>
</div>
</div>
<section id="sec-MDP-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="sec-MDP-dynamics">The dynamics of an MDP</h3>
<p>In a finite MDP, the sets of states, actions, and rewards (S, A and R) all have a finite number of elements. In this case, the random variables <img src="https://latex.codecogs.com/png.latex?S_t"> and <img src="https://latex.codecogs.com/png.latex?R_t"> have well defined discrete probability distributions dependent only on the preceding state and action.</p>
<p>The dynamics of an MDP are defined by the four argument dynamics function:</p>
<p><span id="eq-four-part-dynamics"><img src="https://latex.codecogs.com/png.latex?%0Ap(s',r%20%5Cvert%20s,a)%5C%20%5Cdot%20=%5C%20Pr%5C%7BS_t%20=%20s',%20R_t%20=%20r%20%5Cvert%20S_%7Bt-1%7D%20=%20s,%20A_%7Bt-1%7D%20=%20a%5C%7D%5Cqquad%20%20%5Cforall%20s',s%20%5Cin%20S%5C%20%5Cforall%20r%5Cin%20R%5C%20%5Cforall%20a%5Cin%20A%0A%5Ctag%7B6%7D"></span></p>
<p>where the sum of the probabilities over fixed set of s,a is 1:</p>
<p><span id="eq-dynamics-function-sum"><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bs'%20%5Cin%20S%7D%20%5Csum_%7Br%20%5Cin%20R%7D%20p(s',r%20%5Cvert%20s,a)%20=%201%20%5Cqquad%20%5Cforall%20s%20%5Cin%20S,%20%5Cforall%20a%20%5Cin%20A%20%5Cqquad%20%5Ctext%7B(Dynamics%20function)%7D%0A%5Ctag%7B7%7D"></span></p>
<p>This is just a regular function that takes a state and action and returns a probability distribution over the next state and reward.</p>
<p>In a tabular setting, we can also express this function as a table. Here is my solution for ex 3.4, a table for the recycling robot</p>
<div id="tbl-dynamics-function" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dynamics-function-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Dynamics function for a recycling robot MDP
</figcaption>
<div aria-describedby="tbl-dynamics-function-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th><img src="https://latex.codecogs.com/png.latex?s"></th>
<th><img src="https://latex.codecogs.com/png.latex?a"></th>
<th><img src="https://latex.codecogs.com/png.latex?s'"></th>
<th><img src="https://latex.codecogs.com/png.latex?r"></th>
<th><img src="https://latex.codecogs.com/png.latex?p(s',r%20%5Cmid%20s,a)"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>high</td>
<td>search</td>
<td>high</td>
<td><img src="https://latex.codecogs.com/png.latex?r_%7Bsearch%7D"></td>
<td><img src="https://latex.codecogs.com/png.latex?%5Calpha"></td>
</tr>
<tr class="even">
<td>high</td>
<td>search</td>
<td>low</td>
<td><img src="https://latex.codecogs.com/png.latex?r_%7Bsearch%7D"></td>
<td><img src="https://latex.codecogs.com/png.latex?1-%5Calpha"></td>
</tr>
<tr class="odd">
<td>low</td>
<td>search</td>
<td>high</td>
<td>-3</td>
<td><img src="https://latex.codecogs.com/png.latex?1-%5Cbeta"></td>
</tr>
<tr class="even">
<td>low</td>
<td>search</td>
<td>low</td>
<td><img src="https://latex.codecogs.com/png.latex?r_%7Bsearch%7D"></td>
<td><img src="https://latex.codecogs.com/png.latex?%5Cbeta"></td>
</tr>
<tr class="odd">
<td>high</td>
<td>wait</td>
<td>high</td>
<td><img src="https://latex.codecogs.com/png.latex?r_%7Bwait%7D"></td>
<td>1</td>
</tr>
<tr class="even">
<td>low</td>
<td>wait</td>
<td>low</td>
<td><img src="https://latex.codecogs.com/png.latex?r_%7Bwait%7D"></td>
<td>1</td>
</tr>
<tr class="odd">
<td>low</td>
<td>recharge</td>
<td>high</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>a couple of takeaways from this exercise are:</p>
<ol type="1">
<li><p>rewards are uniquely assigned for action at s resulting in s’ so we don’t need to make r another factor (i.e.&nbsp;list all possible rewards for (s,a,s’) tuple.</p></li>
<li><p>there are (s,a,s’,r) tuples for which we don’t have a non-zero probabilities - since there are no transition possible.</p>
<p>e.g.&nbsp;the robot wont charge when high, so there isn’t a transition from that state, nor a reward, nor a probability.</p></li>
</ol>
</section>
<section id="sec-MDP-graphical" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-MDP-graphical">Graphical representation of an MDP</h3>
<p>The graphical representation of an MDP is a directed graph where the nodes represent states and the edges represent actions. The graph is labeled with the probabilities of transitioning from one state to another given an action.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-mdp-graph.png" class="img-fluid figure-img"></p>
<figcaption>graphical MDP for cleaning robot</figcaption>
</figure>
</div></div><p>In an MDP, the probabilities given by four part dynamics function completely characterize the environment’s dynamics.</p>
<p>That is, the probability of each possible value for <img src="https://latex.codecogs.com/png.latex?S_%7Bt+1%7D"> and <img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D"> depends only on the immediately preceding state <img src="https://latex.codecogs.com/png.latex?S_t"> and action <img src="https://latex.codecogs.com/png.latex?A_t">.</p>
<p>This is best viewed a restriction not on the decision process, but on the state.</p>
<p>The state must include information about all aspects of the past agent–environment interaction that make a difference for the future.</p>
<p>If it does, then the state is said to have the Markov property.</p>
<p>We can use the four-part dynamics function to compute the <strong>state transition probabilities</strong> for a given state and action:</p>
<p><span id="eq-transition-probabilities"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20p(s'%20%5Cvert%20s,a)%20&amp;=%20%5Cmathbb%7BP%7D%5BS_t%20=%20s'%5Cvert%20S_%7Bt-1%7D%20=%20s,%20A_%7Bt-1%7D%20=%20a%5D%20%5Cnewline%0A%20%20&amp;=%20%5Csum_%7Br%20%5Cin%20R%7D%20p(s',r%20%5Cvert%20s,a)%20%5Cqquad%20%5Ctext%7B(state%20transition%20p)%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B8%7D"></span></p>
<p>where we summed over all possible rewards to get the state transition probability.</p>
<p>We can use the four-part dynamics function to compute the <strong>expected rewards</strong> for a given state and action:</p>
<p><span id="eq-expected-reward-for-state-action"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20r(s,a)%20&amp;=%20%5Cmathbb%7BE%7D%5BR_t%20%5Cvert%20S_%7Bt-1%7D%20=%20s,%20A_%7Bt-1%7D%20=%20a%5D%20%5Cnewline%0A%20%20&amp;=%20%5Csum_%7Br%20%5Cin%20R%7D%20r%20%20%5Ctimes%20%5Csum_%7Bs'%20%5Cin%20S%7D%20%20p(s',%20r,%20%5Cvert%20s,%20a)%0A%5Cend%7Balign%7D%0A%5Ctag%7B9%7D"></span></p>
<p>where we summed over all possible rewards and all successor state to get the expected reward.</p>
<p>We can also get the expected reward for a given state, action, and successor state using the four-part dynamics function:</p>
<p><span id="eq-expected-reward-for-state-action-successor-state"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20r(s,a,s')%20&amp;=%20%5Cmathbb%7BE%7D%5BR_t%20%5Cvert%20S_%7Bt-1%7D%20=%20s,%20A_%7Bt-1%7D%20=%20a,%20S_t%20=%20s'%5D%20%5Cnewline%0A%20%20&amp;=%20%5Csum_%7Br%20%5Cin%20R%7D%20r%20%5Ctimes%0A%20%20%5Cfrac%20%7B%20%20p(s',%20r%20%5Cvert%20s,%20a)%20%7D%20%7B%20%20p(s'%20%5Cvert%20s,%20a)%20%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B10%7D"></span></p>
<p>where we summed over all possible rewards to get the expected reward.</p>
<p>Note: perhaps implementing these in python might be further clarify the math for a given state transition graph.</p>
</section>
</section>
</section>
<section id="lesson-2-goal-of-reinforcement-learning" class="level1">
<h1>Lesson 2: Goal of Reinforcement Learning</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Describe how rewards relate to the goal of an agent. #</label></li>
<li><label><input type="checkbox" checked="">Understand episodes and identify episodic tasks. #</label></li>
</ul>
</div>
</div>
<section id="l2g1" class="level2">
<h2 class="anchored" data-anchor-id="l2g1">Rewards and the Goal of an Agent</h2>
<p>The agent interacts with the environment by taking actions and receiving rewards.</p>
<p>In the bandit setting, it is enough to maximize immediate rewards. In the MDP setting, the agent must consider the long-term consequences of its actions.</p>
<p>return <img src="https://latex.codecogs.com/png.latex?G_t"> is the total future reward from time-step <img src="https://latex.codecogs.com/png.latex?t">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AG_t%20%5Cdot=%20R_%7Bt+1%7D%20+%20R_%7Bt+2%7D%20+%20%20R_%7Bt+3%7D%20+%20%5Cldots%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?G_t"> is a random variable because both transition and the rewards can be stochastic.</li>
</ul>
<p>The goal of an agent is to maximize the expected cumulative reward.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The reward hypothesis
</div>
</div>
<div class="callout-body-container callout-body">
<p>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p>
<p>see <span class="citation" data-cites="SILVER2021103535">[@SILVER2021103535]</span></p>
</div>
</div>
<p>some basic formulations of the goal of an agent:</p>
<ol type="1">
<li>Maze runner: -1 for each time step until the goal is reached, then 0.</li>
<li>Recycling robot: +1 per can, 0 otherwise.</li>
<li>Chess: 1 for a win, 0 for a draw, -1 for a loss.</li>
</ol>
<p>Note there is more material on this subject in the guest lecture by Michael Littman.</p>
</section>
<section id="l2g2" class="level2">
<h2 class="anchored" data-anchor-id="l2g2">Episodes and Episodic Tasks</h2>
<p>An episode is a sequence of states, actions, and rewards that ends in a terminal state. An episodic task is a task with a well-defined terminal state.</p>
<p>An example of an episodic task is a game of chess, where the game ends when one player wins or the game is a draw. The opposite setting of an episodic task is a continuing task, where the agent interacts with the environment indefinitely.</p>
</section>
<section id="guest-lecture-with-michael-littman-on-the-reward-hypothesis" class="level2">
<h2 class="anchored" data-anchor-id="guest-lecture-with-michael-littman-on-the-reward-hypothesis">Guest Lecture with Michael Littman on The Reward Hypothesis</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Michael Littman
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://www.littmania.com/media#h.p_pIcJ-rzB3Cp6">His website</a></li>
<li></li>
</ul>
<p>Littman is a professor at Brown University and a leading researcher in reinforcement learning. He is known for his work on the reward hypothesis and the exploration-exploitation trade-off. He motivates the reward hypothesis with a humorous take on the old adage:</p>
<ul>
<li>Give a man a fish and he’ll eat for a day - traditional programming</li>
<li>Teach a man to fish and he’ll eat for a lifetime - supervised learning</li>
<li>Give a man a need for fish and he’ll figure it out - reinforcement learning</li>
</ul>
<p>I felt that the guest lecture was a bit of a let down. I was expecting more from a leading researcher in the field. The reward hypothesis is a fundamental concept and the lecture seemed all over the place. It raised many questions but didn’t answer them.</p>
<p>If we accept the hypothesis, then there are two areas need to be addressed:</p>
<ol type="1">
<li>What rewards should agents optimize?</li>
<li>Designing algorithms to maximize them.</li>
</ol>
<p>Some rewards are easy to define, like winning a game, but others are more complex, like driving a car. His example was of air conditioning in a car, where the reward is not just the temperature but also the comfort of the passengers. Running the air conditioning has a cost in terms of fuel, but the comfort of the passengers is much harder to quantify, particularly since each passenger may have different preferences.</p>
<p>Next he covered the two main approaches to setting up rewards in RL:</p>
<ul>
<li>Rewards can be expressed as a final goal, or no goal yet:
<ul>
<li>The goal based representation: Goal achieved = +1, and everything else is 0. This has a downside of not signaling, to the agent, the urgency of getting to the goal.</li>
<li>The action-penalty representation: a -1 could be awarded every step that the goal is not yet achieved. This can cause problems if there is a small probability of getting stuck and never reaching the goal.</li>
<li>It seems that there are many ways to set up rewards. If we take a lesson from game theory, we can see that the value of rewards might be important or the relative value or order of rewards might be important. In rl values are often more important than the ‘order’ of rewards. However it might be interesting to consider if we can encode preferences into the rewards and if this formulation would still make sense in the context of the reward hypothesis and the bellman equations.</li>
</ul></li>
</ul>
<p>Littleman then asked “Where do rewards come from?” and answered that they can come from - Programming - Human feedback - Examples - Mimic the rewards a human would give - Inverse reinforcement learning - learn the reward function from examples - Optimization - Evolutionary optimization like population dynamics. - The reward is the objective function - The reward is the gradient of the objective function</p>
<p>Next he discusses some challenges to the reward hypothesis:</p>
<ul>
<li>Target is something other than cumulative reward:
<ul>
<li>cannot capture risk averse behavior</li>
<li>cannot capture diversity of behavior</li>
</ul></li>
<li>is it a good match for a high level human behavior?
<ul>
<li>single minded pursuit of a goal isn’t characteristic of good people.</li>
<li>The goals we “should” be pursuing may not be immediately evident to us - we might need time to understand making good decisions.</li>
</ul></li>
</ul>
<p>The big elephant in the room is that we can reject the reward hypothesis and have agents that peruse multiple goals. The main challenge is that it becomes harder to decide when there is a conflict between goals. However, the field of multi-objective optimization has been around for a long time and there are many ways to deal with this problem. Some are more similar to the reward hypothesis but others can lead to more complex behavior based on preferences and pareto optimality.</p>
<section id="lesson-3-continuing-tasks" class="level1">
<h1>Lesson 3: Continuing Tasks</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Formulate returns for continuing tasks using discounting. #</label></li>
<li><label><input type="checkbox" checked="">Describe how returns at successive time steps are related to each other. #</label></li>
<li><label><input type="checkbox" checked="">Understand when to formalize a task as episodic or continuing. #</label></li>
</ul>
</div>
</div>
<section id="l3g1" class="level2">
<h2 class="anchored" data-anchor-id="l3g1">Returns for Continuing Tasks</h2>
<ul>
<li>In continuing tasks, the agent interacts with the environment indefinitely.</li>
<li>The return at time <img src="https://latex.codecogs.com/png.latex?t"> is the sum of the rewards from time <img src="https://latex.codecogs.com/png.latex?t"> to the end of the episode.</li>
<li>The return can be formulated using discounting, where the rewards are discounted by a factor <img src="https://latex.codecogs.com/png.latex?%5Cgamma">.</li>
</ul>
<p><span id="eq-discounted-return-continuing"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AG_t&amp;=R_%7Bt+1%7D%20+%20%5Cgamma%20R_%7Bt+2%7D%20+%20%5Cgamma%5E2%20R_%7Bt+3%7D%20+%20%5Cldots%20%5Cnewline%0A&amp;=%20%5Csum_%7Bk=0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20R_%7Bt+k+1%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B11%7D"></span></p>
<ul>
<li>returns have a recursive structure, where the return at time <img src="https://latex.codecogs.com/png.latex?t"> is related to the return at time <img src="https://latex.codecogs.com/png.latex?t+1"> by the discount factor <img src="https://latex.codecogs.com/png.latex?%5Cgamma">.</li>
</ul>
<p><span id="eq-return-recursive"><img src="https://latex.codecogs.com/png.latex?%20%20%0A%5Cbegin%7Balign%7D%0AG_t%20&amp;=%20R_%7Bt+1%7D%20+%20%5Cgamma%20R_%7Bt+2%7D%20+%20%5Cgamma%5E2%20R_%7Bt+3%7D%20+%20%5Cldots%20%5Cnewline%0A&amp;=%20R_%7Bt+1%7D%20+%20%5Cgamma%20(%20R_%7Bt+2%7D%20+%20%5Cgamma%20R_%7Bt+3%7D%20+%20%5Cldots%20)%20%5Cnewline%0A&amp;=%20R_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B12%7D"></span></p>
<p>this form of the return is called the recursive form of the return and is usefull in developing algorithms for reinforcement learning.</p>
</section>
<section id="l3g2" class="level2">
<h2 class="anchored" data-anchor-id="l3g2">Returns at Successive Time Steps</h2>
<ul>
<li>The return at time <img src="https://latex.codecogs.com/png.latex?t"> is related to the return at time <img src="https://latex.codecogs.com/png.latex?t+1"> by the discount factor <img src="https://latex.codecogs.com/png.latex?%5Cgamma">.</li>
<li>The return at time <img src="https://latex.codecogs.com/png.latex?t"> is the sum of the reward at time <img src="https://latex.codecogs.com/png.latex?t"> and the discounted return at time $t+1.</li>
</ul>
</section>
<section id="l3g3" class="level2">
<h2 class="anchored" data-anchor-id="l3g3">Episodic vs.&nbsp;Continuing Tasks</h2>
<ul>
<li>An episodic task has a well-defined terminal state, and the episode ends when the terminal state is reached.</li>
<li>A continuing task does not have a terminal state, and the agent interacts with the environment indefinitely.</li>
<li>To avoid infinite returns in continuing tasks, we use discounting to ensure that the return is finite.</li>
<li>The discount factor <img src="https://latex.codecogs.com/png.latex?%5Cgamma%5Cin(0,1)"> is the present value of future rewards.</li>
</ul>
<p><span class="citation" data-cites="sutton2018reinforcement">@sutton2018reinforcement</span> emphasizes that we can use the discount factor of [0,1] to unify both episodic and continuing tasks Here = 0 corresponds to myopic view of optimizing immediate rewards like in the k-armed bandit problem. The discount factor = 1 corresponds to the long-term view of optimizing undiscounted expected cumulative reward. into a single framework. This is a powerful idea that allows us to use the same algorithms for both types of tasks.</p>
<p>I think it is a good place to consider a couple of ideas raised by Littman in the guest lecture:</p>
<p>The first is hyperbolic discounting and the second risk aversion.</p>
<p>Behavioral economics has considered a notion of hyperbolic discounting where the discount factor is not constant but changes over time. This is a more realistic model of human behavior but is harder to work with mathematically. This idea is not covered in the course perhaps because it is a departure from the more rational exponential discounting model which we use.</p>
<p>two forms of hyperbolic discounting are:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AG(D)%20=%20%5Cfrac%7B1%7D%7B1%20+%20%5Cgamma%20D%7D%0A"></p>
<p><span id="eq-hyperbolic-discounting"><img src="https://latex.codecogs.com/png.latex?%0A%CF%86h(%CF%84)%20=%20(1+%20%CE%B1%CF%84)%E2%88%92%CE%B3/%CE%B1%0A%5Ctag%7B13%7D"></span> where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%CF%86h(%CF%84)"> is the hyperbolic discount factor at time <img src="https://latex.codecogs.com/png.latex?%CF%84"></li>
<li><img src="https://latex.codecogs.com/png.latex?%CE%B1"> is the rate of discounting</li>
<li><img src="https://latex.codecogs.com/png.latex?%CE%B3"> is the delay parameter</li>
</ul>
<p>there is also quasi-hyperbolic discounting which is a combination of exponential and hyperbolic discounting.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AG(D)%20=%20%5Cbegin%7Bcases%7D%0A%20%201%20&amp;%20%5Ctext%7Bif%20%7D%20t%20=%200%20%20%5Cnewline%0A%20%20%5Cbeta%5Ek%20%5Cdelta%5E%7BD%7D%20&amp;%20%5Ctext%7Bif%20%7D%20t%20%3E%200%0A%20%20%5Cend%7Bcases%7D%0A"></p>
<p>The notation for both terminal and non-terminal states is <img src="https://latex.codecogs.com/png.latex?S%5E+"></p>
<p>Exercise 2.10 proof of Equation 3.10</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AG_t%20&amp;=%20%5Csum_%7Bk=0%7D%5E%5Cinfty%20%5Cgamma%5Ek%20=%20lim_%7Bn%20%5Crightarrow%20%5Cinfty%7D%20(1%20+%20%5Cgamma%20+%20%5Cgamma%5E2%20+%20...%20+%20%5Cgamma%5En)%20%5Cnewline%0A&amp;=%20lim_%7Bn%20%5Crightarrow%20%5Cinfty%7D%20%5Cfrac%7B(1%20+%20%5Cgamma%20+%20%5Cgamma%5E2%20+%20...%20+%20%5Cgamma%5En)%20(1%20-%20%5Cgamma)%7D%7B(1%20-%20%5Cgamma)%7D%20%5Cnewline%0A&amp;=%20lim_%7Bn%20%5Crightarrow%20%5Cinfty%7D%20%5Cfrac%7B1%20-%20%5Cgamma%5E%7Bn+1%7D%7D%7B1%20-%20%5Cgamma%7D%20%5Cnewline%0A&amp;=%20%5Cfrac%7B1%7D%7B1%20-%20%5Cgamma%7D%0A%5Cend%7Balign*%7D%0A"></p>


</section>
</section>
</div>
</div>
</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Fundamentals</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c1-w2.html</guid>
  <pubDate>Mon, 02 May 2022 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The K-Armed Bandit Problem</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c1-w1.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="sec-lesson-k-armed-bandit" class="level1 page-columns page-full">
<h1>Lesson 1: The K-Armed Bandit</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Read
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=47">@sutton2018reinforcement§2.1-7, pp.&nbsp;25-36</a></label></li>
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=47">@sutton2018reinforcement§2.8, pp.&nbsp;42-43</a></label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand the temporal nature of the bandit problem #</label></li>
<li><label><input type="checkbox" checked="">Define k-armed bandit problem #</label></li>
<li><label><input type="checkbox" checked="">Define action-values and the greedy action selection method #</label></li>
<li><label><input type="checkbox" checked="">Define reward, time steps, and value functions #</label></li>
</ul>
</div>
</div>
<blockquote class="blockquote">
<p>In reinforcement learning, the agent generates its own training data by interacting with the world. The agent must learn the consequences of his own actions through trial and error, rather than being told the correct action – <span class="citation" data-cites="white2020fundamental">[@white2020fundamental]</span></p>
</blockquote>
<section id="sec-k-armed-bandit" class="level2">
<h2 class="anchored" data-anchor-id="sec-k-armed-bandit">K-armed bandits 🐙</h2>
<p>In the <strong>k-armed bandit</strong> problem there is an <strong>agent</strong> who is assigned a <strong>state</strong> <img src="https://latex.codecogs.com/png.latex?s"> by the environment and must learn which action <img src="https://latex.codecogs.com/png.latex?a"> from the possible set of <strong>actions</strong> <img src="https://latex.codecogs.com/png.latex?A"> leads to the goal state through a signal based on the greatest <strong>expected reward</strong>.</p>
<p>One way this can be achieved is using a Bayesian updating scheme starting from a uniform prior.</p>
</section>
<section id="sec-l1g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g1">Temporal nature of the bandit problem</h2>
<p>The <strong>bandit problem</strong> cam be static problem with a fixed reward distribution. However, more generally it is a <strong>temporal</strong> problem when the rewards distribution changes over time and agent must learn to adapt to these changes.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Difference between bandits and RL
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the typical <strong>bandit setting</strong> there is only one state. So after we pull the arm nothing in the problem changes.</p>
<p>Bandits problems where agents can discriminate between states are called <em>contextual bandits.</em></p>
<p>However, bandits embody one of the main themes of RL - that of estimating an expected reward for different actions.</p>
<p>In the more general <strong>RL setting</strong> we will be interested in more general problems where actions will lead the agent to new states and the goal is some specific state we need to reach.</p>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="img/multi_armed_bandit.webm" class="img-fluid" controls=""><a href="img/multi_armed_bandit.webm">Video</a></video></p>
<figcaption>bandit</figcaption>
</figure>
</div></div><div id="exm-clinical-trials" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Using Multi-armed bandit to randomize a medical trial)</strong></span> &nbsp;</p>
<ul>
<li>agent is the doctor</li>
<li>actions {blue, yellow, red} treatment</li>
<li>k = 3</li>
<li>the rewards are the health of the patients’ blood pressure.</li>
<li>a random trial in which a doctor need to pick one of three treatments.</li>
<li>q(a) is the mean of the blood pressure for the patient.</li>
</ul>
</div>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-clinical-trial.png" class="img-fluid figure-img"></p>
<figcaption>clinical trial</figcaption>
</figure>
</div></div><section id="sec-l1g3" class="level4">
<h4 class="anchored" data-anchor-id="sec-l1g3">Action Values and Greedy Action Selection</h4>
<p>The <strong>value</strong> of an action is its <strong>expected reward</strong> which can be expressed mathematically as:</p>
<p><span id="eq-action-value"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Aq_%7B%5Cstar%7D(a)%20&amp;%20%5Cdoteq%20%5Cmathbb%7BE%7D%5BR_t%20%20%5Cvert%20%20A_t=a%5D%20%5Cspace%20%5Cforall%20a%20%5Cin%20%5C%7Ba_1%20...%20a_k%5C%7D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Csum_r%20p(r%7Ca)r%20%5Cqquad%20%5Ctext%7B(action%20value)%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B1%7D"></span></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cdoteq"> means definition</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5Br%20%5Cvert%20a%5D"> means expectation of a reward given some action a Since agents want to maximize rewards, recalling the definition of expectations we can write this as:</li>
</ul>
<p>The goal of the agent is to maximize the expected reward which we can express mathematically as:</p>
<p><span id="eq-greedification"><img src="https://latex.codecogs.com/png.latex?%0A%5Carg%5Cmax_a%20q(a)=%5Csum_r%20p(r%20%5Cvert%20a)%20%5Ctimes%20r%20%5Cqquad%20%5Ctext%7B(Greedification)%7D%0A%5Ctag%7B2%7D"></span></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Carg%20%5Cmax_a"> means the argument <img src="https://latex.codecogs.com/png.latex?a"> maximizes - so the agent is looking for the action that maximizes the expected reward and the outcome is an action.</li>
</ul>
</section>
<section id="l1g4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="l1g4">Reward, Return, and Value Functions</h4>
<p>The <strong>reward</strong> <img src="https://latex.codecogs.com/png.latex?r"> is the immediate feedback from the environment after the agent takes an action.</p>
<p>The <strong>return</strong> <img src="https://latex.codecogs.com/png.latex?G_t"> is the total discounted reward from time-step <img src="https://latex.codecogs.com/png.latex?t">.</p>
<p>The <strong>value function</strong> <img src="https://latex.codecogs.com/png.latex?v(s)"> of an MRP is the expected return starting from state <img src="https://latex.codecogs.com/png.latex?s">.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-descion-problems.png" class="img-fluid figure-img"></p>
<figcaption>decisions</figcaption>
</figure>
</div></div><p>example of decisions under uncertainty:</p>
<ul>
<li>movie recommendation.</li>
<li>clinical trials.</li>
<li>music recommendation.</li>
<li>food ordering at a restaurant.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-why-bandits.png" class="img-fluid figure-img"></p>
<figcaption>why discuss bandits</figcaption>
</figure>
</div></div><p>It best to consider issues and algorithms design choices in the simplest setting first. The bandit problem is the simplest setting for RL. More advanced algorithms will incorporate parts we use to solve this simple settings.</p>
<ul>
<li>maximizing rewards.</li>
<li>balancing exploration and exploitation.</li>
<li>estimating expected rewards for different actions.</li>
</ul>
<p>are all problems we will encounter in both the bandit and the more general RL setting.</p>
</section>
</section>
</section>
<section id="sec-lesson-action-values" class="level1 page-columns page-full">
<h1>Lesson 2: What to learn: understanding Action Values</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><label><input type="checkbox" checked="">Define action-value estimation methods. #</label></li>
<li><label><input type="checkbox" checked="">Define exploration and exploitation #</label></li>
<li><label><input type="checkbox" checked="">Select actions greedily using an action-value function #</label></li>
<li><label><input type="checkbox" checked="">Define online learning #</label></li>
<li><label><input type="checkbox" checked="">Understand a simple online sample-average action-value estimation method #</label></li>
<li><label><input type="checkbox" checked="">Define the general online update equation #</label></li>
</ol>
</div>
</div>
<section id="L2G1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="L2G1">What are action-value estimation methods?</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-clinical-trial-q(a).png" class="img-fluid figure-img"></p>
<figcaption>estimating action values</figcaption>
</figure>
</div></div><p>In Tabular RL settings The action value function <img src="https://latex.codecogs.com/png.latex?q"> is nothing more than a table with one {state, action} pair per row and its value. More generally, like when we will consider function approximation in course 3, it is a mapping from {state, action} pair to a expected reward.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>State s</th>
<th>Action a</th>
<th>Action value q(s,a)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>red treatment</td>
<td>0.25</td>
</tr>
<tr class="even">
<td>0</td>
<td>yellow treatment</td>
<td>0.75</td>
</tr>
<tr class="odd">
<td>0</td>
<td>blue treatment</td>
<td>0.5</td>
</tr>
</tbody>
</table>
<p>The higher the action value <img src="https://latex.codecogs.com/png.latex?q(a)"> of an action <strong>a</strong>, the more likely it is to lead us to a better state which is closer to the objective. We can choose for each state the best or one of the best choices giving us a <strong>plan</strong> for navigating the state space to the goal state.</p>
<p><span id="eq-sample-average"><img src="https://latex.codecogs.com/png.latex?%0AQ_t(a)%20%5Cdoteq%20%5Cfrac%7B%5Ctext%7Bsum%20of%20rewards%20for%20action%20a%20taken%20time%20%7D%20t%7D%7B%5Ctext%7Bnumber%20of%20times%20action%20a%20was%20taken%20prior%20to%20%7D%20t%7D%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bt-1%7D%20R_i%7D%7Bt-1%7D%20%5Cqquad%0A%5Ctag%7B3%7D"></span></p>
<p>The main idea of RL is that we can propagate values from an one adjacent state to another. We can start with the uniform stochastic policy and use it to estimate/learn the action values. Action values will decrease for actions leads to a dead end. And it will increase in the direction of the goal but only once the influence of the goal has propagated. A continuing theme in RL is trying to increase the efficiency for propagation of rewards across the action values.</p>
<p>Knowing the minimum number of action needed to reach a goal can be an approximate indicator of the action value.</p>
<p>A second idea is that once we have let the influence of dead end and the goals spread enough we may have enough information to improve the initial action value to a point where each action is the one of the best choices. <mark>We call picking the one of the best action greedy selection and it leads to a deterministic policy.</mark> This is the optimal policy, it might not be unique since some actions might be tied in terms of their rewards. However for all of these we cannot do any better.</p>
</section>
<section id="L2G2" class="level3">
<h3 class="anchored" data-anchor-id="L2G2">Exploration and Exploitation definition and dilemma</h3>
<p>In the bandit setting we can define:</p>
<dl>
<dt>Exploration</dt>
<dd>
<p>Testing any action that might be better than our best.</p>
</dd>
<dt>Exploitation</dt>
<dd>
<p>Using the best action.</p>
</dd>
</dl>
<p>Should the doctor explore new treatments that might harm his patients or exploit the current treatment. In real life bacteria gain immunity to antibiotics so there is merit to exploring new treatments. However, a new treatment can be harmful to some patients. Ideally we want to enjoy the benefits of the best treatment but to be open to new and better alternatives but we can only do one at a time.</p>
<p><span class="marked">Since exploitation is by definition mutually exclusive with exploration we must choose one and give up the benefits of the other. This is the <strong>dilemma of Exploration and Exploitation</strong>.</span> How an agent resolves this dilemma in practice depends on the agent’s preferences and the type of state space it inhabits, if it has just started or encounters a <strong>changing landscape,</strong> it should make an effort to explore, on the other hand if it has explored enough to be certain of a global maximum it would prefer to exploit.</p>
</section>
<section id="L2G4" class="level3">
<h3 class="anchored" data-anchor-id="L2G4">Defining Online learning ?</h3>
<dl>
<dt>Online learning</dt>
<dd>
<p>learning by updating the agent’s value function or the action value function step by step as an agent transverses the states seeking the goal. Online learning is important to handle MDP which can change.</p>
</dd>
</dl>
<p>One simple way an agent can use online learning is to try actions by random and keep track of the subsequent states. Eventually we should reach the goal state. If we repeat this many times we can estimate the expected rewards for each action.</p>
</section>
<section id="L2G5" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="L2G5">Sample Average Method for estimating Action Values Incrementally</h3>
<p>Action values help us make decision. Let’s try and make estimate action values more formal using the following method:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aq_t(a)=%5Cfrac%7B%5Ctext%7Bsum%20or%20rewards%20when%20a%20taken%20prior%20to%20t%7D%7D%7B%5Ctext%7Bnumber%20of%20times%20a%20taken%20prior%20to%20t%7D%7D%0A%20%20%20%20%20%20%20=%5Cfrac%7B%5Csum_%7Bt=1%7D%5E%7Bt-1%7D%20R_i%20%5Cmathbb%7BI%7D_%7BA_i=a%7D%7D%7B%5Csum_%7Bt=1%7D%5E%7Bt-1%7D%5Cmathbb%7BI%7D_%7BA_i=a%7D%20%7D%20%5Cqquad%0A"></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-sample-avarage-method.png" class="img-fluid figure-img"></p>
<figcaption>example</figcaption>
</figure>
</div></div><p><span id="eq-sample-average-incremental-update-rule"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AQ_%7Bn+1%7D%20&amp;=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20R_i%20%5Cnewline%0A%20%20&amp;%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5CBigg(R_n%20+%20%5Csum_i%5E%7Bn-1%7D%20R_i%5CBigg)%20%5Cnewline%0A%20%20&amp;%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5CBigg(R_n%20+%20(n-1)%20%5Cfrac%7B1%7D%7B(n-1)%7D%5Csum_i%5E%7Bn-1%7D%20R_i%5CBigg)%20%5Cnewline%0A%20%20&amp;=%20%5Cfrac%7B1%7D%7Bn%7D%20%5CBig(R_n%20+%20(n-1)%20Q_%7Bn%7D%5CBig)%20%5Cnewline%0A%20%20&amp;=%20%5Cfrac%7B1%7D%7Bn%7D%20%5CBig(R_n%20+%20nQ_%7Bn%7D%20-Q_%7Bn%7D%20%5CBig)%20%5Cnewline%0A%20%20&amp;=%20Q_n%20+%20%5Cfrac%7B1%7D%7Bn%7D%20%5CBig%5BR_n%20-%20Q_%7Bn%7D%5CBig%5D%0A%5Cend%7Balign%7D%0A%5Ctag%7B4%7D"></span></p>
</section>
<section id="L2G6" class="level3">
<h3 class="anchored" data-anchor-id="L2G6">What are action-value estimation methods?</h3>
<p>We can now state this in English as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BNew%20Estimate%7D%20%5Cleftarrow%20%5Ctext%7BOld%20Estimate%7D%20+%20%5Ctext%7BStep%20Size%20%7D%20%5Ctimes%20%5B%5Ctext%7BTarget%7D%20-%20%5Ctext%7BOld%20Estimate%7D%5D%20%5Cqquad%0A"></p>
<p>here:</p>
<ul>
<li>step size can be adaptive - changing over time. but typically it is constant and in the range (0,1) to avoid divergence.</li>
<li>for the sample average method the step size is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7Bn%7D"> where n is the number of times the action has been taken.</li>
<li>(Target - OldEstimate) is called the <em>error</em>.</li>
</ul>
<p>More generally we will use the update rule as:</p>
<p><span id="eq-general-incremental-update-rule"><img src="https://latex.codecogs.com/png.latex?%0AQ_%7Bn+1%7D%20=%20Q_n%20+%20%5Calpha%20%5CBig%5BR_n%20-%20Q_%7Bn%7D%5CBig%5D%20%5Cqquad%20a%5Cin%20(0,1)%0A%5Ctag%7B5%7D"></span></p>
<div id="simple-epsilon-greedy-bandit-algorithm" class="pseudocode-container quarto-float" data-line-number-punc=":" data-line-number="true" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-pseudocode-number="1" data-no-end="false" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{Simple Bandit($\epsilon$)}\begin{algorithmic} \State $Q(a) \leftarrow 0\ \forall a\ $ \Comment{ $\textcolor{blue}{initialize\ action\ values}$} \State $N(a) \leftarrow 0\ \forall a\ $ \Comment{ $\textcolor{blue}{initialize\ counter\ for\ actions\ taken}$} \For{$t = 1, 2, \ldots \infty$} \State $A_t \leftarrow \begin{cases} \arg\max_a Q(a) &amp; \text{with probability } 1 - \epsilon \\ \text{a random action} &amp; \text{with probability } \epsilon \end{cases}$ \State $R_t \leftarrow \text{Bandit}(A_t)$ \State $N(A_t) \leftarrow N(A_t) + 1$ \State $Q(A_t) \leftarrow Q(A_t) + \frac{1}{N(A_t)}[R_t - Q(A_t)]$ \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
</section>
<section id="sec-lesson-exploration-exploitation" class="level1 page-columns page-full">
<h1>Lesson 3: Exploration vs Exploitation</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Define <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy #</li>
<li>Compare the short-term benefits of exploitation and the long-term benefits of exploration #</li>
<li>Understand optimistic initial values #</li>
<li>Describe the benefits of optimistic initial values for early exploration #</li>
<li>Explain the criticisms of optimistic initial values #</li>
<li>Describe the upper confidence bound action selection method #</li>
<li>Define optimism in the face of uncertainty #</li>
</ul>
</div>
</div>
<p>the following is a Bernoulli greedy algorithm</p>
<div id="alg-greedy-bandit" class="pseudocode-container quarto-float" data-line-number-punc=":" data-line-number="true" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-pseudocode-number="2" data-no-end="false" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{BernGreedy(K, α, β)}\begin{algorithmic} \For{$t = 1, 2, . . .$} \State \State \Comment{ estimate model} \For{$k = 1, . . . , K$} \State $\hat\theta_k \leftarrow a_k / (α_k + β_k)$ \EndFor \State \Comment{ select and apply action:} \State $x_t \leftarrow \arg\max_k \hat{\theta}_k$ \State Apply $x_t$ and observe $r_t$ \State \Comment{ update distribution:} \State $(α_{x_t}, β_{x_t}) \leftarrow (α_{x_t} + r_t, β_{x_t} + 1 − r_t)$ \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
<section id="sec-epsilon-greedy-policies" class="level2">
<h2 class="anchored" data-anchor-id="sec-epsilon-greedy-policies">Ɛ-Greedy Policies</h2>
<p>The Ɛ-greedy policy uses a simple heuristic to balance exploration with exploitation. The idea is to choose the best action with probability <img src="https://latex.codecogs.com/png.latex?1-%5Cepsilon"> and to choose a random action with probability <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">.</p>
<div id="alg-epsilon-greedy" class="pseudocode-container quarto-float" data-line-number-punc=":" data-line-number="true" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-pseudocode-number="3" data-no-end="false" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{EpsilonGreedy(K, α, β)}\begin{algorithmic} \For{$t = 1, 2, \ldots $} \State p = random() \If {$p &lt; \epsilon$} \State select radom action $x_t \qquad$ \Comment{explore} \Else \State select $x_t = \arg\max_k \hat{\theta}_k \qquad$ \Comment{exploit} \EndIf \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The problem with Ɛ-greedy policies
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>A problem with Ɛ-greedy is that it is not optimal in the long run.</li>
<li>Even after it has found the best course of action it will continue to explore with probability <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">.</li>
<li>This is because the policy is not adaptive.</li>
<li>One method is too reduce <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> over time. However unless there is a feedback from the environment this will likely stop exploring too soon or too late thus providing sub-optimal returns.</li>
</ul>
</div>
</div>
<p>The following is a simple implementation of the Ɛ-greedy algorithm in Python from <a href="https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/?ref=ml_lbp">geeksforgeeks.org</a></p>
<div id="22df15a2" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Import required libraries </span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span>
<span id="cb1-4">  </span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define Action class </span></span>
<span id="cb1-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Actions: </span>
<span id="cb1-7">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, m): </span>
<span id="cb1-8">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> m </span>
<span id="cb1-9">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-10">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-11">  </span>
<span id="cb1-12">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Choose a random action </span></span>
<span id="cb1-13">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> choose(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):  </span>
<span id="cb1-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.random.randn() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.m </span>
<span id="cb1-15">  </span>
<span id="cb1-16">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Update the action-value estimate </span></span>
<span id="cb1-17">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> update(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x): </span>
<span id="cb1-18">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-19">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.N)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x </span>
<span id="cb1-20">  </span>
<span id="cb1-21">  </span>
<span id="cb1-22"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> run_experiment(m1, m2, m3, eps, N): </span>
<span id="cb1-23">      </span>
<span id="cb1-24">  actions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [Actions(m1), Actions(m2), Actions(m3)] </span>
<span id="cb1-25">  </span>
<span id="cb1-26">  data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.empty(N) </span>
<span id="cb1-27">    </span>
<span id="cb1-28">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(N): </span>
<span id="cb1-29">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># epsilon greedy </span></span>
<span id="cb1-30">    p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.random() </span>
<span id="cb1-31">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> eps: </span>
<span id="cb1-32">      j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.choice(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) </span>
<span id="cb1-33">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>: </span>
<span id="cb1-34">      j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.argmax([a.mean <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> a <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> actions]) </span>
<span id="cb1-35">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> actions[j].choose() </span>
<span id="cb1-36">    actions[j].update(x) </span>
<span id="cb1-37">  </span>
<span id="cb1-38">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># for the plot </span></span>
<span id="cb1-39">    data[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x </span>
<span id="cb1-40">  cumulative_average <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.cumsum(data) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (np.arange(N) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) </span>
<span id="cb1-41">  </span>
<span id="cb1-42">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plot moving average ctr </span></span>
<span id="cb1-43">  plt.plot(cumulative_average) </span>
<span id="cb1-44">  plt.plot(np.ones(N)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>m1) </span>
<span id="cb1-45">  plt.plot(np.ones(N)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>m2) </span>
<span id="cb1-46">  plt.plot(np.ones(N)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>m3) </span>
<span id="cb1-47">  plt.xscale(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log'</span>) </span>
<span id="cb1-48">  plt.show() </span>
<span id="cb1-49">  </span>
<span id="cb1-50">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> a <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> actions: </span>
<span id="cb1-51">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(a.mean) </span>
<span id="cb1-52">  </span>
<span id="cb1-53">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> cumulative_average </span></code></pre></div>
</details>
</div>
<div id="1a5ed31a" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">c_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> run_experiment(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>) </span>
<span id="cb2-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print(c_1)</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/c1-w1_files/figure-html/cell-3-output-1.png" width="571" height="412" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.9986145710058402
1.9658660479954855
3.0003123552514848</code></pre>
</div>
</div>
<div id="a79852b6" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">c_05 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> run_experiment(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>) </span>
<span id="cb4-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print(c_05)</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/c1-w1_files/figure-html/cell-4-output-1.png" width="569" height="412" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.9953708104175287
2.0145256629031327
3.001903056693574</code></pre>
</div>
</div>
<div id="3c6823a4" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">c_01 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> run_experiment(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>) </span>
<span id="cb6-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print(c_01)</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/c1-w1_files/figure-html/cell-5-output-1.png" width="579" height="412" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1.036959235212011
1.9936684441473502
2.996446749853475</code></pre>
</div>
</div>
<div id="70983abc" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># log scale plot </span></span>
<span id="cb8-2">plt.plot(c_1, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'eps = 0.1'</span>) </span>
<span id="cb8-3">plt.plot(c_05, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'eps = 0.05'</span>) </span>
<span id="cb8-4">plt.plot(c_01, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'eps = 0.01'</span>) </span>
<span id="cb8-5">plt.legend() </span>
<span id="cb8-6">plt.xscale(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log'</span>) </span>
<span id="cb8-7">plt.show() </span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/c1-w1_files/figure-html/cell-6-output-1.png" width="569" height="412" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-benefits-of-exploitation-and-exploration" class="level2">
<h2 class="anchored" data-anchor-id="sec-benefits-of-exploitation-and-exploration">Benefits of Exploitation &amp; Exploration</h2>
<ul>
<li>In the short term we may maximize rewards following the best known course of action. However this may represent a local maximum.</li>
<li>In the long term agents that explore different options and keep uncovering better options until they find the best course of action corresponding to the global maximum.</li>
</ul>
<p>To get the best of both worlds we need to balance exploration and exploitation ideally using a policy that uses feedback to adapt to its environment.</p>
</section>
<section id="sec-optimistic-initial-values" class="level2">
<h2 class="anchored" data-anchor-id="sec-optimistic-initial-values">Optimistic initial values</h2>
<dl>
<dt>Optimistic initial values</dt>
<dd>
<p>Setting all initially action values greater than the algorithmically available values in [0,1]</p>
</dd>
</dl>
<p>The methods we have discussed are dependent on the initial action-value estimates, <img src="https://latex.codecogs.com/png.latex?Q_1(a)">. In the language of statistics, we call these methods biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once. For methods with constant <img src="https://latex.codecogs.com/png.latex?%5Calpha">, the bias is permanent, though decreasing over time.</p>
<div id="alg-optimitc-greedy-bandit" class="pseudocode-container quarto-float" data-line-number-punc=":" data-line-number="true" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-pseudocode-number="4" data-no-end="false" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{OptimisticBernGreedy(K, α, β)}\begin{algorithmic} \For{$t = 1, 2, . . .$} \State \State \Comment{ estimate model} \For{$k = 1, . . . , K$} \State $\hat\theta_k \leftarrow 1 \qquad$ \Comment{optimistic initial value} \EndFor \State \Comment{ select and apply action:} \State $x_t \leftarrow \arg\max_k \hat{\theta}_k$ \State Apply $x_t$ and observe $r_t$ \State \Comment{ update distribution:} \State $(α_{x_t}, β_{x_t}) \leftarrow (α_{x_t} + r_t, β_{x_t} + 1 − r_t)$ \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
<section id="sec-benefits-of-optimistic-initial-values-for-early-exploration" class="level2">
<h2 class="anchored" data-anchor-id="sec-benefits-of-optimistic-initial-values-for-early-exploration">Benefits of optimistic initial values for early exploration</h2>
<p>Setting the initial action values to be higher than the true values has the effect of causing various bandit algorithm to try to exploit them - only to find out that most values are not as rewarding as it was led to expect.</p>
<p>What happens is that the algorithm will initially explore more than it would have otherwise. Possibly even trying all the actions at least once.</p>
<p>In the short-term it will perform worse than Ɛ- greedy which tend to exploit. But as more of the state space is explored at least once the algorithm will beat an Ɛ-greedy policy which can take far longer to explore the space and find the optimal options.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl-optimistic-initial-conditions.png" class="img-fluid figure-img"></p>
<figcaption>The effect of optimistic initial action-value estimates</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Criticisms of optimistic initial values
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Optimistic initial values only drive early exploration. The agent will stop exploring once this is done.</li>
<li>For a non-stationary problems - this is inadequate.</li>
<li>In a real world problems the maximum reward is an unknown quantity.</li>
</ul>
</div>
</div>
</section>
<section id="sec-the-ucb-action-selection-method" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-the-ucb-action-selection-method">The UCB action selection method</h2>
<p>UCB is an acronym for Upper Confidence Bound. The idea behind it is to select the action that has the highest upper confidence bound. This has the advantage over epsilon greedy that it will explore more in the beginning and then exploit more as the algorithm progresses.</p>
<p>the upper confidence bound is defined as:</p>
<p><span id="eq-ucb"><img src="https://latex.codecogs.com/png.latex?%0AA_t%20=%20%5Carg%5Cmax%5C_a%20%5CBigg%5B%0A%20%20%5Cunderbrace%7BQ_t(a)%7D_%7Bexploitation%7D%20+%0A%20%20%5Cunderbrace%7Bc%20%5Csqrt%7B%5Cfrac%7B%5Cln%20t%7D%7BN_t(a)%7D%20%7D%7D_%7Bexploration%7D%0A%5CBigg%5D%20%5Cqquad%0A%5Ctag%7B6%7D"></span></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Q_t(a)"> is the action value</li>
<li><img src="https://latex.codecogs.com/png.latex?c"> is a constant that determines the degree of exploration</li>
<li><img src="https://latex.codecogs.com/png.latex?N_t(a)"> is the number of times action <img src="https://latex.codecogs.com/png.latex?a"> has been selected prior to time <img src="https://latex.codecogs.com/png.latex?t"></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/rl_wk1_ucb.png" class="img-fluid figure-img"></p>
<figcaption>UCB intuition</figcaption>
</figure>
</div></div><p>The idea is we the action for which the action value plus the highest possible uncertainty give the highest sum. We are being optimistic in assuming this choice will give the highest reward. In reality any value in the confidence interval could be the true value. Each time we select an action we reduce the uncertainty in the exploration term and we also temper our optimism of the upper confidence bound by the number of times we have selected the action. This means that we will prefer to visit the actions that have not been visited as often.</p>
<p>The main advantage of UCB is that it is more efficient than epsilon greedy in the long run. If we measure the cost of learning in terms of the regret - the difference between the expected reward of the optimal action and the expected reward of the action we choose. UCB has a lower regret than epsilon greedy. The downside is that it is more complex and requires more computation.</p>
<div id="alg-brn-UCB" class="pseudocode-container quarto-float" data-line-number-punc=":" data-line-number="true" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-pseudocode-number="5" data-no-end="false" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{UCB(K, α, β)}\begin{algorithmic} \For{$t = 1, 2, . . .$} \For { $k = 1, . . . , K$ } \State \Comment{ $\textcolor{blue}{compute\ UCBs}$} \State $U_k = \hat\theta_k + c \sqrt{\frac{\ln t}{N_k}}$ \EndFor \State \Comment{ $\textcolor{blue}{select\ and\ apply\ action}$} \State $x_t \leftarrow \arg\max_k h(x,U_x)$ \State Apply xt and observe $y_t$ and $r_t$ \State \Comment{ $\textcolor{blue}{estimate\ model}$} \For{$k = 1, . . . , K$} \State $\hat\theta_k \leftarrow a_k / (α_k + β_k)$ \EndFor \State \Comment{ select and apply action:} \State $x_t \leftarrow \arg\max_k \hat{\theta}_k$ \State Apply $x_t$ and observe $r_t$ \State \Comment{ update distribution:} \State $(α_{x_t}, β_{x_t}) \leftarrow (α_{x_t} + r_t, β_{x_t} + 1 − r_t)$ \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
<p>Note we can model UCB using an urn model.</p>
</section>
<section id="Sec-Thompson-Sampling" class="level2">
<h2 class="anchored" data-anchor-id="Sec-Thompson-Sampling">Thompson Sampling</h2>
<p>Thompson sampling is basically like UCB but taking the Bayesian approach to the bandit problem. We start with a prior distribution over the action values and then update this distribution as we take actions. The action we choose is then sampled from the posterior distribution. This has the advantage that it is more robust to non-stationary problems than UCB. The downside is that it is more computationally expensive.</p>
<section id="Sec-Thompson-Sampling-Algorithm" class="level3">
<h3 class="anchored" data-anchor-id="Sec-Thompson-Sampling-Algorithm">Thompson Sampling Algorithm</h3>
<p>The algorithm is as follows:</p>
<div id="alg-bernoulli-thompson-sampling" class="pseudocode-container quarto-float" data-line-number-punc=":" data-line-number="true" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-pseudocode-number="6" data-no-end="false" data-comment-delimiter="#">
<div class="pseudocode">
\begin{algorithm} \caption{BernTS(K, α, β)}\begin{algorithmic} \For{$t = 1, 2, . . .$} \State \State \Comment{ sample model} \For{$k = 1, . . . , K$} \State Sample $\hat\theta_k \sim beta(α_k, β_k)$ \EndFor \State \Comment{ select and apply action:} \State $x_t \leftarrow \arg\max_k \hat{\theta}_k$ \State Apply $x_t$ and observe $r_t$ \State \Comment{ update distribution:} \State $(α_{x_t}, β_{x_t}) \leftarrow (α_{x_t} + r_t, β_{x_t} + 1 − r_t)$ \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
<ul>
<li><a href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf">this is a tutorial on Thompson Sampling</a></li>
</ul>
</section>
</section>
<section id="L3G7" class="level2">
<h2 class="anchored" data-anchor-id="L3G7">Optimism in the face of uncertainty</h2>
<dl>
<dt>Optimism in the face of uncertainty</dt>
<dd>
<p>This is a heuristic to ensure initial exploration of all actions by assuming that untried actions have a high expected reward. We then try to exploit them but end up successively downgrading their expected reward when they do not match our initial optimistic assessment.</p>
</dd>
</dl>
<p>The downside to this approach is when the space of action is continuous so we can never get to the benefits of exploration.</p>
</section>
</section>
<section id="awesome-rl-resources" class="level1">
<h1>Awesome RL resources</h1>
<p>Let’s list some useful RL resources.</p>
<p><strong>Books</strong></p>
<ul>
<li>Richard S. Sutton &amp; Andrew G. Barto <a href="http://incompleteideas.net/book/RLbook2020.pdf">RL An Introduction</a></li>
<li><a href="https://tor-lattimore.com/">Tor Latimore’s</a> <a href="https://tor-lattimore.com/downloads/book/book.pdf">Book</a> and <a href="https://banditalgs.com/">Blog</a> on Bandit Algorithms.</li>
<li><a href="https://sites.ualberta.ca/~szepesva/">Csaba Szepesvari</a>’s <a href="https://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Book</a></li>
</ul>
<p><strong>Courses &amp; Tutorials</strong></p>
<ul>
<li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html">David Silver’s</a> 2015 <a href="https://www.davidsilver.uk/teaching/">UCL Course on RL</a> <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">Video</a> and Slides.</li>
<li><a href="https://faculty.cc.gatech.edu/~isbell/pubs/">Charles Isbell</a> and <a href="https://www.littmania.com/">Michael Littman</a> A free Udacity course on RL, with some emphasis on game theory proofs, and some novel algorithms like <a href="http://proceedings.mlr.press/v28/sodomka13.pdf">Coco-Q: Learning in Stochastic Games with Side Payments</a>.</li>
<li><strong>Contextual Bandits</strong> <a href="https://hunch.net/~rwil/">tutorial</a> <a href="https://vimeo.com/240429210">video</a> + papers from MS research videos on contextual bandit algorithms.</li>
<li>Interesting papers:
<ul>
<li>We discussed how Dynamic Programming can’t handle games like chess. Here are some RL methods that can.
<ul>
<li><a href="https://www.nature.com/articles/s41586-020-03051-4.epdf?sharing_token=kTk-xTZpQOF8Ym8nTQK6EdRgN0jAjWel9jnR3ZoTv0PMSWGj38iNIyNOw_ooNp2BvzZ4nIcedo7GEXD7UmLqb0M_V_fop31mMY9VBBLNmGbm0K9jETKkZnJ9SgJ8Rwhp3ySvLuTcUr888puIYbngQ0fiMf45ZGDAQ7fUI66-u7Y%3D">Muzero</a></li>
<li><a href="https://arxiv.org/abs/2202.06626">MuZero</a> and</li>
<li><a href="https://arxiv.org/abs/2111.00210">EfficentZero</a> <a href="https://github.com/YeWR/EfficientZero">code</a></li>
</ul></li>
</ul></li>
</ul>
<section id="coding-bandits-with-mesa" class="level2">
<h2 class="anchored" data-anchor-id="coding-bandits-with-mesa">Coding Bandits with MESA</h2>
<div id="6135b06a" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> tqdm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tqdm</span>
<span id="cb9-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mesa <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Model, Agent</span>
<span id="cb9-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mesa.time <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RandomActivation</span>
<span id="cb9-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb9-5"></span>
<span id="cb9-6"></span>
<span id="cb9-7"></span>
<span id="cb9-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> EpsilonGreedyAgent(Agent):</span>
<span id="cb9-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    This agent implements the epsilon-greedy </span></span>
<span id="cb9-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb9-12"></span>
<span id="cb9-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, unique_id, model, num_arms, epsilon<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>):</span>
<span id="cb9-14">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(unique_id,model)</span>
<span id="cb9-15">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_arms <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> num_arms</span>
<span id="cb9-16">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.epsilon <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> epsilon</span>
<span id="cb9-17">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(num_arms)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize Q-value estimates</span></span>
<span id="cb9-18">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.action_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(num_arms)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Track action counts</span></span>
<span id="cb9-19"></span>
<span id="cb9-20">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> choose_action(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb9-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> np.random.rand() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.epsilon:</span>
<span id="cb9-22">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Exploration: Choose random arm</span></span>
<span id="cb9-23">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.random.randint(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_arms)</span>
<span id="cb9-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb9-25">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Exploitation: Choose arm with highest Q-value</span></span>
<span id="cb9-26">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.argmax(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_values)</span>
<span id="cb9-27"></span>
<span id="cb9-28">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, model):</span>
<span id="cb9-29">        chosen_arm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.choose_action()</span>
<span id="cb9-30">        reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.get_reward(chosen_arm)</span>
<span id="cb9-31">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> reward <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Reward is not provided by the model"</span></span>
<span id="cb9-32">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.action_counts[chosen_arm] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb9-33">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_values[chosen_arm] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_values[chosen_arm] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.action_counts[chosen_arm] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> reward) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.action_counts[chosen_arm] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-34"></span>
<span id="cb9-35"></span>
<span id="cb9-36"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> TestbedModel(Model):</span>
<span id="cb9-37">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    This model represents the 10-armed bandit testbed environment.</span></span>
<span id="cb9-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb9-40"></span>
<span id="cb9-41">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, num_arms, mean_reward, std_dev,num_agents<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb9-42">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb9-43">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_agents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> num_agents</span>
<span id="cb9-44">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_arms <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> num_arms</span>
<span id="cb9-45">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean_reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_reward</span>
<span id="cb9-46">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.std_dev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> std_dev</span>
<span id="cb9-47">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.env_init()</span>
<span id="cb9-48">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#self.arms = [None] * num_arms  # List to store arm rewards</span></span>
<span id="cb9-49">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomActivation(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)</span>
<span id="cb9-50">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_agents):</span>
<span id="cb9-51">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.create_agent(EpsilonGreedyAgent, i, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>) </span>
<span id="cb9-52"></span>
<span id="cb9-53">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> env_init(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,env_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{}):</span>
<span id="cb9-54">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.arms <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_arms)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize arm rewards</span></span>
<span id="cb9-55"></span>
<span id="cb9-56">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> create_agent(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, agent_class, agent_id, epsilon):</span>
<span id="cb9-57">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb9-58"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Create an RL agent instance with the specified class and parameters.</span></span>
<span id="cb9-59"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb9-60">        agent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> agent_class(agent_id, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_arms, epsilon)</span>
<span id="cb9-61">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule.add(agent)</span>
<span id="cb9-62">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> agent</span>
<span id="cb9-63"></span>
<span id="cb9-64">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb9-65">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> agent <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule.agents:</span>
<span id="cb9-66">            chosen_arm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> agent.choose_action()</span>
<span id="cb9-67">            reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean_reward, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.std_dev)</span>
<span id="cb9-68">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.arms[chosen_arm] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reward  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Update arm reward in the model</span></span>
<span id="cb9-69">            agent.step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Pass the model instance to the agent for reward access</span></span>
<span id="cb9-70"></span>
<span id="cb9-71">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_reward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, arm_id):</span>
<span id="cb9-72">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Access reward from the stored list</span></span>
<span id="cb9-73">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.arms[arm_id]</span>
<span id="cb9-74"></span>
<span id="cb9-75"></span>
<span id="cb9-76"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example usage</span></span>
<span id="cb9-77">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TestbedModel(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create model with 10 arms</span></span>
<span id="cb9-78">num_runs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>                  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The number of times we run the experiment</span></span>
<span id="cb9-79">num_steps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The number of pulls of each arm the agent takes</span></span>
<span id="cb9-80"></span>
<span id="cb9-81"></span>
<span id="cb9-82"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Run simulation for multiple steps</span></span>
<span id="cb9-83"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tqdm(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_runs)):</span>
<span id="cb9-84">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_steps):</span>
<span id="cb9-85">        model.step()</span>
<span id="cb9-86">    model.step()</span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/home/oren/.local/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:

The AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.
We would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919

  0%|          | 0/200 [00:00&lt;?, ?it/s]  4%|▍         | 8/200 [00:00&lt;00:02, 78.08it/s]  8%|▊         | 16/200 [00:00&lt;00:02, 77.85it/s] 12%|█▏        | 24/200 [00:00&lt;00:02, 78.08it/s] 16%|█▌        | 32/200 [00:00&lt;00:02, 78.05it/s] 20%|██        | 40/200 [00:00&lt;00:02, 78.39it/s] 24%|██▍       | 48/200 [00:00&lt;00:01, 78.24it/s] 28%|██▊       | 56/200 [00:00&lt;00:01, 78.25it/s] 32%|███▏      | 64/200 [00:00&lt;00:01, 78.19it/s] 36%|███▌      | 72/200 [00:00&lt;00:01, 78.01it/s] 40%|████      | 80/200 [00:01&lt;00:01, 77.99it/s] 44%|████▍     | 88/200 [00:01&lt;00:01, 77.66it/s] 48%|████▊     | 96/200 [00:01&lt;00:01, 77.58it/s] 52%|█████▏    | 104/200 [00:01&lt;00:01, 77.93it/s] 56%|█████▌    | 112/200 [00:01&lt;00:01, 78.03it/s] 60%|██████    | 120/200 [00:01&lt;00:01, 78.24it/s] 64%|██████▍   | 128/200 [00:01&lt;00:00, 78.20it/s] 68%|██████▊   | 136/200 [00:01&lt;00:00, 77.77it/s] 72%|███████▏  | 144/200 [00:01&lt;00:00, 77.96it/s] 76%|███████▌  | 152/200 [00:01&lt;00:00, 77.79it/s] 80%|████████  | 160/200 [00:02&lt;00:00, 78.08it/s] 84%|████████▍ | 168/200 [00:02&lt;00:00, 78.29it/s] 88%|████████▊ | 176/200 [00:02&lt;00:00, 78.33it/s] 92%|█████████▏| 184/200 [00:02&lt;00:00, 78.30it/s] 96%|█████████▌| 192/200 [00:02&lt;00:00, 78.10it/s]100%|██████████| 200/200 [00:02&lt;00:00, 78.25it/s]100%|██████████| 200/200 [00:02&lt;00:00, 78.06it/s]</code></pre>
</div>
</div>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Fundamentals</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c1-w1.html</guid>
  <pubDate>Sun, 01 May 2022 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Course Introduction</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/c1-w0.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/posts/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<ul>
<li>The course instructors, Adam and Martha White, are world class researches researchers.</li>
<li>The University of Alberta is by far the strongest university for RL.</li>
<li>RL builds on the foundations of Markov Decision Processes, Dynamic Programming, and Monte Carlo methods as well as Machine Learning and Deep Neural Networks.
<ul>
<li>I found that learning these topics is better motivated and there fore easier to grasp when presented in the context of RL.</li>
</ul></li>
<li>I read reviews by a number of people have taken this specialization. A few have had some background in RL and breezed through the material. The rest suggested that the material can be challenging particularly the last two courses.</li>
<li>Some of the assignments appear in RL textbooks, but instead of a few lines of code for the main assignment, in the capstone we get to implement both Parts of the environment, the RL algorithm, the replay buffer, the neural net and the optimizer from scratch. This is a blessing in disguise as it really gives one more opportunity to get to grips with these complex subjects.</li>
<li>A big chunk of the programming assignments have to handle the environment rather than the RL algorithm - many students complain about this.
<ul>
<li>My insight has repeatedly been that in RL the algorithms are often very general and that the key to success for solving a particular problem with RL is all about thinking the environment through.</li>
<li>Many environments have different challenges and can reveal the weaknesses of many time tested algorithms.</li>
<li>Real world problems are much more challenging but the more environments you work with, the better you will be able to find good features that can make solving them with RL very different.</li>
<li>Another aspect of RL is called reward shaping which is defining a reward structure that can provide the algorithm with good signal so as to support better credit assignment to actions in the correct state. This is an integral part of environment design.</li>
<li>Finally I found that when you try to implement RL algorithms for real world problems you will want to modify the algorithms to take advantage of some structure in the environment.</li>
<li>Luckily there are today more environments available then I could list, it is easy to create new ones. The bottom line is that the challenges and algorithms of RL only make sense when considered in the context of environments.</li>
</ul></li>
<li>In my opinion many of the assignments are not very interesting - the coding problems often border on the trivial.</li>
<li>If you want to work with RL professionally you will need to do a lot more coding.
<ul>
<li>You should consider some of the extra coding assignments that ware included in the text book.</li>
<li>You should also consider implementing additional algorithms on the environments from the assignments.</li>
<li>There are tons of other more interesting environments to work with consider taking you implementation to the next level in these environments.
<ul>
<li>In reality even many of the environments we use in the assignments have surprising challenges.</li>
<li>The moon lander in the capstone for example tends to learn to hover for a long time to avoid crashing
<ul>
<li>The solution comes from a paper which suggests adding a time limit to the episode. However the assignment uses the time limit without mentioning this.</li>
<li>I thought that the time limit was there to speed up grading until I tried to dive deeper.</li>
</ul></li>
</ul></li>
</ul></li>
<li>We spend lots of time tinkering with RL-Glue a library that is no longer being maintained.</li>
<li>Although my background is in Mathematics I found the videos rather terse.
<ul>
<li>They often list goals at the start and at the end again, so all in all the instructional part can be very brief.</li>
<li>It would be easier to recap not just reference equations, update rules, and algorithms that have been covered a while back or in a previous course.</li>
<li>Many They frequently reference old material which you may not have <em>fully digested</em> yet.</li>
</ul></li>
<li>I discovered that I had worked though all the videos, quizzes and programming assignments scoring 100% but I hadn’t really digested the material.
<ul>
<li>These notes are my attempt to do so.</li>
<li>I try to make the material more accessible and to connect the dots.</li>
</ul></li>
<li>I don’t include the programming assignments in these notes.
<ul>
<li>I may add simple agents with from different environments and using libraries other than RL-Glue to avoid any issues raised by the Coursera honor code.</li>
</ul></li>
<li>I don’t include the quizzes either
<ul>
<li>I found that most can be aced by just reviewing the videos before taking them and then reviewing the answers.</li>
<li>I’d like to build a question/answer bank</li>
<li>But I think that coding is much better time investment.</li>
</ul></li>
<li>There are lots of good notes on the web on RL.
<ul>
<li>Summaries of the text book</li>
<li>Slides for the courses offered by Richard S. Sutton and Andrew G. Barto</li>
<li>Slides for the course offered by David Silver</li>
<li>Notes from more advanced courses</li>
<li>Books &amp; Notes from some of the Guest lectures in this course.</li>
</ul></li>
<li>So I was reluctant to add to the pile. I found out after completing the first two courses that I had not really digested the material.
<ul>
<li>I had to go back and review the material and then write these notes.</li>
<li>I found that the act of writing the notes helped me to understand some of the trickier bits from the book.</li>
<li>I also wanted to see how I could connect the dots between the different parts of the course.</li>
<li>I hope that you find these notes helpful.</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deceptively simple <span class="emoji" data-emoji="bulb">💡</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p><mark>This course is <strong>deceptively simple</strong></mark> - the chart in the margin provides a great summary of the material for the whole specialization. Only a handful of concepts are needed to master RL.</p>
<ul>
<li>This specialization is all about connecting the dots.</li>
<li>We revisit the same ideas over and over improving them in small but significant ways by relaxing the assumptions. e.g.&nbsp;from bandits with one state we move to MDP with many states and get the ability to formulate plans. From Dynamic programming with a fully specified model we move to model free settings where we might not be able to efficiently learn a model. From tabular methods where we treat each state as a separate entity we we move to function approximation and deep learning where we can generalize from one state to many others.</li>
<li>In this course and the more connections you make the better you will understand and remember material.</li>
<li>And the greater you facility to apply RL to new problems.</li>
</ul>
</div>
</div>
<p>The following are my tips for getting the most from this specialization</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Connecting The Dot to see the Forest For the Trees 🎄
</div>
</div>
<div class="callout-body-container callout-body">
<p>To connect the dots I <span class="emoji" data-emoji="heart">❤️</span> recommend:</p>
<ol type="1">
<li><strong>Annotate</strong> 🖊️ you e-copy of the book 📖</li>
<li><strong>Flash cards</strong> 🗂️ are your 🧑‍🤝‍🧑 friends.</li>
</ol>
<ul>
<li>We don’t need too many but they can help you keep the essentials (algorithms, definitions, some formulas, a few diagrams) fresh in your mind.</li>
<li>Keep reviewing these until you impress the folks at DeepMind in your interview. <span class="emoji" data-emoji="wink">😉</span></li>
</ul>
<ol start="3" type="1">
<li><strong>Review</strong> 👁️ the videos/quizzes until nothing seems surprising/confusing <sup>1</sup>.</li>
<li><strong>Review</strong> 👁️ your notes every time you complete a part of the specialization. Also a great idea if have an RL interview 💼</li>
<li><strong>Coding</strong>: If you have time do extra RL coding
<ol type="1">
<li>Start with developing more environments, simple and complex ⛩️</li>
<li>Implement more algorithms - from the course, the books, papers.⛩️</li>
<li>The notebooks also try to teach you experiments and analysis comparing algorithms performance. If you assimilate this part you are really going to shine. ⛩️</li>
</ol></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Mnemonics 😍
</div>
</div>
<div class="callout-body-container callout-body">
<p>As a Mathematics major I can attest that Mathematics becomes 10x easier so long as you can recall 🧠 the basic definitions and their notation.</p>
<p>I have extracted the essentials from the text book below. Best to memorize these or at least keep a copy handy and you are well on your way to grokking this course</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?G_t"> <strong>return</strong> at time t, for a <img src="https://latex.codecogs.com/png.latex?(s_t,%20a_t,%20r_t...)"> sequence discounted by <img src="https://latex.codecogs.com/png.latex?%5Cgamma%5Cin(0,1)">.</li>
<li><img src="https://latex.codecogs.com/png.latex?r(s,a)"> - <strong>expected immediate rewards</strong> for action <img src="https://latex.codecogs.com/png.latex?a"> in state <img src="https://latex.codecogs.com/png.latex?s"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpi"> <strong>policy</strong> - a decision making rule for every state.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpi_*"> <strong>optimal policy</strong> - which returns the maximum rewards.</li>
<li><img src="https://latex.codecogs.com/png.latex?p(s',r%20%5Cvert%20s,a)"> - <strong>transition probability</strong> to state <img src="https://latex.codecogs.com/png.latex?s'"> with reward <img src="https://latex.codecogs.com/png.latex?r"> from state <img src="https://latex.codecogs.com/png.latex?s"> via action <img src="https://latex.codecogs.com/png.latex?a"> AKA <strong>four valued dynamics</strong> function.</li>
<li><img src="https://latex.codecogs.com/png.latex?p(s'%20%5Cvert%20s,a)"> - <strong>transition probability</strong> to state <img src="https://latex.codecogs.com/png.latex?s'"> from state <img src="https://latex.codecogs.com/png.latex?s"> via action <img src="https://latex.codecogs.com/png.latex?a"> AKA <strong>Markov process transition matrix</strong></li>
<li><img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> - state’s <strong>value</strong> under policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> which is its expected return.</li>
<li><img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,a)"> - the <strong>action value</strong> in state <img src="https://latex.codecogs.com/png.latex?s"> under policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Guest Lectures
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are a number of guest lectures in this specialization - I think that about half are in the last course.</p>
<ul>
<li>These often provide a teaser into an area of research that extends the material in the course.</li>
<li>Many are boring but
<ul>
<li>almost all present papers and results that are worth some review.</li>
<li>a few present too many papers, which suggests a triage approach</li>
<li>sooner of later you might have a real problem that is similar to the one they are addressing.</li>
<li>so I suggest you try and spend some time</li>
</ul></li>
<li>RL was more or less invented 4 times in different fields - Control Theory, Operations Research, Machine Learning and Psychology.</li>
<li>The guest lectures often present the material from one of these fields.</li>
</ul>
</div>
</div>
<section id="a-long-term-plan-to-learning-deep-rl" class="level2">
<h2 class="anchored" data-anchor-id="a-long-term-plan-to-learning-deep-rl">A Long term plan to learning Deep RL</h2>
<ol type="1">
<li>do this specialization</li>
<li>read the rest of the book - there are a number of important subjects and algorithms that are not covered in the course.</li>
<li>solve more problems</li>
<li>read more papers</li>
</ol>
<ul>
<li>there are a zillion algorithms and a gazillion environments out there</li>
<li>reading a few papers will get you up to speed on the state of the art.</li>
</ul>
<ol start="5" type="1">
<li>implement more algorithms</li>
</ol>
<ul>
<li>this is what people are going to pay you for</li>
<li>start with gradient bandit algorithms</li>
<li>then Thompson Sampling using different distributions</li>
</ul>
<ol start="6" type="1">
<li>try and solve a real problem:</li>
</ol>
<ul>
<li>I created a custom RL algorithm to solve the Lewis signaling game quickly and efficient - this allows agents to learn to communicate.</li>
<li>I am now working on designing a multi-agent version of the game that should learn even faster.</li>
</ul>
<ol start="7" type="1">
<li>do the deep rl course from Hugging Face</li>
<li>find RL challenges on Kaggle</li>
</ol>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The annotated book and flashcards will help here. This material is really logical - if you are surprised/confused you never assimilated some part of the material. Once you do it should become almost intuitive to reason about from scratch.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Coursera</category>
  <category>Fundamentals</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/posts/c1-w0.html</guid>
  <pubDate>Sat, 30 Apr 2022 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/posts/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
