<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Notes on Reinfocement Learning</title>
<link>https://orenbochman.github.io/notes-rl/</link>
<atom:link href="https://orenbochman.github.io/notes-rl/index.xml" rel="self" type="application/rss+xml"/>
<description>Personal website, portfolio and blog</description>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Mon, 27 Jan 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>Training Classifiers with Natural Language Explanations</title>
  <link>https://orenbochman.github.io/notes-rl/posts/papers/BabbleLabble/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/BabbleLabble/cover.jpg" class="img-fluid figure-img"></p>
<figcaption>litrature review</figcaption>
</figure>
</div><div id="fig-vid01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vid01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/YBeAX-deMDg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vid01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Babble Labble: Learning From Natural Language Explanations (NIPS 2017 Demo)
</figcaption>
</figure>
</div><div id="fig-vid01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vid01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/4cgvIh9DUrg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vid01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Braden Hancock - Training Classifiers with Natural Language Explanations
</figcaption>
</figure>
</div></div>

<p>In <span class="citation" data-cites="hancock2018trainingclassifiersnaturallanguage">(Hancock et al. 2018)</span> the authors consider how to learn labeling functions from natural language explanations. Such explanations can come from a data labeler on Amazon mechnical Turk, a domain expert and perhaps from an LLM. Labeling function capture a heuristic for labeling data. The author ties it up with some work on <strong>data programming</strong> which lead to <a href="https://www.snorkel.org/">snorkel</a> another data labeling tool. This paper isn’t about RL at all. It is interesting for a number if reasons.</p>
<p>The hook for me was its approach to aggregation. Since Scott E. Page pointed out the challenges of aggregation in his book <a href="https://www.amazon.com/Difference-Diversity-Creates-Complexity-Scott/dp/0691138540">The Difference</a> I have been considering how it manifests in many forms - particularly in language emergence.</p>
<p>The chart I saw for the presentation was extremely similar to another chart I had seen in a paper on emergent languages. It looked like this paper was solving an aggregation problem I had been considering in emergent languages. It turned out to be a coincidence. These are different problems, and they aggregation is for different things. And yet there still seems to be an underlying similarity.</p>
<ol type="1">
<li>Another aspect of the paper is the approach to aggregating weak noisy classifier into a more powerful one. This aspect seems to also have merits for agents that are learning to discriminate together with learning a language. It is interesting that both Yoav Golderg and the Authors of this paper think that parsing accuracy is not that important since the parsers are good enough and their system is built to be robust to errors.</li>
<li>It uses <strong>semantic parsers</strong> to convert natural language explanations into labeling functions. Back in 2019 <a href="https://youtu.be/e12danHhlic?t=853">Yoav Goldberg</a> i suggested in that too often NLP devs use RegEx instead of a more robust approach based on syntax trees that can be constructed thanks to Spacy. I think semantic parsers are perhaps one step further where we take the parse tree and convert it to a program.</li>
<li>Seems similar to the approach of I saw in <a href="https://explosion.ai/blog/prodigy-annotation-tool-active-learning">Prodigy</a> by spacey core developer <a href="https://ines.io/">Ines Montani</a> and others.</li>
<li>It implements <a href="https://orenbochman.github.io/blog/notes/cognitivie-ai-cs7637/17-explanation-based-learning/17-explanation-based-learning.html">Explanation-Based Learning</a> from Cognitive AI (CS7637) course.</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - The paper
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>The paper in a nutshell</figcaption>
</figure>
</div>
<p>BabbleLabble is a framework for training classifiers in which annotators provide natural language explanations for labeling decisions. These explanations are parsed into labeling functions that generate noisy labels for unlabeled data. The framework consists of a semantic parser, filter bank, and label aggregator. Experiments on relation extraction tasks show that users can train classifiers 5-100× faster by providing explanations instead of just labels. The filter bank effectively removes incorrect labeling functions, and the label aggregator combines labels into probabilistic labels for training a discriminative model. The noisy labels provide weak supervision that promotes generalization.</p>
</div>
</div>

<div class="no-row-height column-margin column-container"><div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/BabbleLabble/fig-01.png" class="img-fluid figure-img"></p>
<figcaption>BabbleLabble UI</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: In BabbleLabble, the user provides a natural language explanation for each labeling decision. These explanations are parsed into labeling functions that convert unlabeled data into a large labeled dataset for training a classifier.
</figcaption>
</figure>
</div></div><section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose <em>BabbleLabble</em>, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic <em>labeling functions</em> that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100× faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices. — <span class="citation" data-cites="hancock2018trainingclassifiersnaturallanguage">(Hancock et al. 2018)</span></p>
</blockquote>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>

<div class="no-row-height column-margin column-container"><div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/BabbleLabble/fig-02.png" class="img-fluid figure-img"></p>
<figcaption>System Overview</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Natural language explanations are parsed into candidate labeling functions (LFs). Many incorrect LFs are filtered out automatically by the filter bank. The remaining functions provide heuristic labels over the unlabeled dataset, which are aggregated into one noisy label per example, yielding a large, noisily-labeled training set for a classifier.
</figcaption>
</figure>
</div><div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/BabbleLabble/fig-03.png" class="img-fluid figure-img"></p>
<figcaption>Seamntic Parse of An Explanation</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Valid parses are found by iterating over increasingly large sub-spans of the input looking for matches among the right hand sides of the rules in the grammar. Rules are either lexical (converting tokens into symbols), unary (converting one symbol into another symbol), or compositional (combining many symbols into a single higher-order symbol). A rule may optionally ignore unrecognized tokens in a span (denoted here with a dashed line).
</figcaption>
</figure>
</div></div>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li>Describes the standard protocol for collecting labeled data for training classifiers.</li>
<li>Highlights limitations of labeling: each label only provides a single bit of information.</li>
<li>Mentions previous works’ approaches to improve information gain from examples.</li>
<li>Presents <code>BabbleLabble</code>, a framework where annotators provide natural language explanations for each labeling decision.</li>
</ul>
</section>
<section id="the-babblelabble-framework" class="level3">
<h3 class="anchored" data-anchor-id="the-babblelabble-framework">The <code>BabbleLabble</code> Framework</h3>
<ul>
<li>Describes how <code>BabbleLabble</code> converts explanations and unlabeled data into a noisy training set.</li>
<li>Presents the three main components of <code>BabbleLabble</code>: semantic parser, filter bank, and label aggregator.</li>
<li>Mentions how explanations provide high-level information about patterns in the data.</li>
<li>Notes that the semantic parser converts explanations into a set of logical forms representing labeling functions.</li>
<li>Explains how the filter bank removes incorrect labeling functions based on semantic and pragmatic criteria.</li>
<li>Describes how the label aggregator combines labels from the labeling functions into probabilistic labels for each example.</li>
<li>Explains the benefits of training a discriminative model using the noisy labels instead of classifying directly with the label aggregator.</li>
</ul>
</section>
<section id="explanations" class="level3">
<h3 class="anchored" data-anchor-id="explanations">Explanations</h3>
<ul>
<li>Discusses the format and content of user-provided explanations.</li>
<li>Highlights that explanations should refer to specific aspects of the example.</li>
</ul>
</section>
<section id="semantic-parser" class="level3">
<h3 class="anchored" data-anchor-id="semantic-parser">Semantic Parser</h3>
<ul>
<li>Describes the goal of the semantic parser: generate a set of candidate labeling functions (LFs).</li>
<li>Presents the rule-based semantic parser used in <code>BabbleLabble</code> and its key features.</li>
<li>Discusses the parser’s grammar and the included predicates.</li>
<li>Notes that the parser is domain-independent, allowing transferability to new tasks.</li>
</ul>
</section>
<section id="filter-bank" class="level3">
<h3 class="anchored" data-anchor-id="filter-bank">Filter Bank</h3>
<ul>
<li>Explains the role of the filter bank: remove incorrect labeling functions without requiring ground truth labels.</li>
<li>Discusses the two types of filters: semantic and pragmatic.</li>
<li>Describes the purpose and operation of semantic and pragmatic filters.</li>
<li>Highlights the effectiveness of the filter bank in removing incorrect labeling functions.</li>
</ul>
</section>
<section id="label-aggregator" class="level3">
<h3 class="anchored" data-anchor-id="label-aggregator">Label Aggregator</h3>
<ul>
<li>Explains the function of the label aggregator: combine potentially conflicting labels from multiple labeling functions into a single probabilistic label.</li>
<li>Discusses the limitations of a simple majority vote approach.</li>
<li>Presents the data programming approach used in <code>BabbleLabble</code>, which models the relationship between true labels and labeling function outputs as a factor graph.</li>
</ul>
</section>
<section id="discriminative-model" class="level3">
<h3 class="anchored" data-anchor-id="discriminative-model">Discriminative Model</h3>
<ul>
<li>Discusses the advantages of using a discriminative model trained with noisy labels.</li>
<li>Explains how a discriminative model can leverage features not explicitly mentioned in explanations.</li>
<li>Notes that the noisy labels provide a form of weak supervision that promotes generalization.</li>
</ul>
</section>
<section id="experimental-setup" class="level3">
<h3 class="anchored" data-anchor-id="experimental-setup">Experimental Setup</h3>
<ul>
<li>Describes the three relation extraction tasks used for evaluation: Spouse, Disease, and Protein.</li>
<li>Presents details about each dataset: source, task description, and size.</li>
<li>Discusses the experimental settings: text preprocessing, semantic parser implementation, label aggregator, and discriminative model.</li>
<li>Mentions hyperparameter tuning and evaluation metrics.</li>
</ul>
</section>
<section id="experimental-results" class="level3">
<h3 class="anchored" data-anchor-id="experimental-results">Experimental Results</h3>
<ul>
<li>Presents the F1 scores achieved by <code>BabbleLabble</code> compared to traditional supervision.</li>
<li>Highlights the rate of improvement in F1 score with the number of user inputs.</li>
<li>Discusses the effectiveness of the filter bank in removing incorrect labeling functions.</li>
<li>Presents an analysis of the utility of incorrectly parsed labeling functions.</li>
<li>Compares using labeling functions as functions versus features.</li>
<li>Discusses the impact of unlabeled data on performance.</li>
</ul>
</section>
<section id="related-work-and-discussion" class="level3">
<h3 class="anchored" data-anchor-id="related-work-and-discussion">Related Work and Discussion</h3>
<ul>
<li>Discusses previous work on learning from natural language explanations.</li>
<li>Mentions research on learning from weak supervision, particularly in the context of relation extraction.</li>
<li>Highlights the potential of natural language as a high-bandwidth communication channel for machine learning.</li>
<li>Discusses future research directions, including applying the framework to other tasks and exploring more interactive settings.</li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My Thoughts
</div>
</div>
<div class="callout-body-container callout-body">
<section id="big-ideas" class="level3">
<h3 class="anchored" data-anchor-id="big-ideas">Big ideas</h3>
<ol type="1">
<li><mark>Always be on the lookout for tricks on how to convert<sup>1</sup> a supervised learning task into an unsupervised one</mark>. While this paper fails to do so, it does provide a step in the right direction and more so one that yields a 100x speed up in labeling tasks. In reality this claim is a marketing shtick for their paper, however the real point is that if you have a labeling function, its utility grows in proportion to the amount of unlabeled data you can bring to bear.</li>
<li>Branden Hancock keeps reiterating that <mark>the bulk of the time spent in Labeling is understanding the text/data not the annotation</mark>. The authors point that time to elicit explanation is only double the time of labeling. Highlighting evidence or even writing an explanation is thus a small burden in comparison to just annotating the most significant part of the text? Perhaps not yet if we include the benefits of the labeling functions that result in 6-10x more labels the math works out.</li>
<li>It is easy to miss the most significant aspect of the paper - <mark>the importance of the Filter Bank in creating value</mark>. However there is an extensive literature on PBE (Program By Example) that they do not seem to be aware of which can convert such examples into programs.</li>
<li>Another point made by Branden in support of Explanation is that <mark>you can’t highlight when the evidence is negative</mark> - i.e.&nbsp;when the classification is grounded in some context not being in the text.</li>
<li>A third reason to like explanations is that data-centric tasks are forever changing. There are new requirements (e.g.&nbsp;a new class) or drifts in the classifiers’s distributions. If you have collected labels you need to rethink the project but if you collected explanations it is easier to retain on more data then relabel. If there is a better semantic parser, you can swap it and re-train.</li>
</ol>
</section>
<section id="devils-and-details" class="level3">
<h3 class="anchored" data-anchor-id="devils-and-details">Devils and Details</h3>
<ol type="1">
<li>The most interesting aspects of the paper for RL are how an agent interacting with people can elicit explanation that can then be used to create labeling functions.</li>
<li>The idea of how the labeling function are aggregated is also instructive. That said, I can’t say that this is a big idea - it looks very much like ranking, weighting and even less powerful than TD-IDF. So what I mean is that when we learn a language probably want to learn features from functions not labels as labels do not generalize. If we also have a framing game that is driving language learning then we may have further uses for a sensible form of aggregation in our classifier. More abstractly, the RL agent needs to pick an action - that is usually like a classification of a state into an action space. For Life long learning we could</li>
<li>Q &amp; A are a good often considered in the Semantic Parsing literature.</li>
<li>LLM may well be useful for <mark>closing the weak supervision loop</mark>. I.e. one can use a general purpose query to elicit explanations from an LLM. With a range of prompts one should be able to get different explanations.</li>
</ol>
</section>
<section id="action-items" class="level3">
<h3 class="anchored" data-anchor-id="action-items">Action Items</h3>
<ol type="1">
<li>Try babble labble, snorkel, prodigy, and SippyCup.</li>
<li>Make a MVP of the filter bank.</li>
<li>make a cheat-sheet on how to convert a supervised learning tasks into an unsupervised ones. These are ideas that disrupted the field of ML.</li>
<li>close the loop on weak supervision with LLM for a RL agent</li>
<li>compare this approach to other PBE</li>
</ol>
</section>
</div>
</div>
<p>The people behind BabbleLabble are also behind Snorkel. Snorkel is a framework for building weakly supervised models. It is a more general framework that can be used for a wide range of tasks. BabbleLabble is a specific application of Snorkel to the task of training classifiers with natural language explanations. The key difference is that BabbleLabble focuses on the use of natural language explanations to generate labeling functions, while Snorkel is a more general framework for building weakly supervised models.</p>
<p>A point made on the Snorkel Site is that the people who worked on the tool moved on to building a platform. Is this just to monetize their work? My guess is not. In reality there are many other tools that do much the same thing. Labeling data is not very glamorous but building a tool is just a ui and a classifier. You can’t lock clients in and it is likely that each project required expensive customization. At some point it becomes simpler to do this in a form that is more general and can be used by future clients. This is harder in open source as you need consensus and a community. On the other hand in a platform each time you do a project you have added value to the platform.</p>
<p>From other talks by Snorkel people it seems that many companies invest massively in labeling datasets (i.e.&nbsp;teams with sizes of hundreds). Thus every small improvement in the process can have a big impact. And as a business model it makes a lot of sense to try to bring as many of those improvements in house.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Aggregation your’e No good for me
</div>
</div>
<div class="callout-body-container callout-body">
<p>Aggregation can take simple underlying components and combine them into arbitrarily complex structures. This is a powerful idea that is used in many areas of science and engineering. Here are a few examples:</p>
<ol type="1">
<li><p>The Statistics the Central limit theorem allow us to aggregate many random variables into a Normal distributed one under fairly broad conditions.</p>
<blockquote class="blockquote">
<p>“Aggregate statistics can sometimes mask important information”. – Ben Bernanke</p>
</blockquote></li>
<li><p>In Physics we can aggregate the behavior of many particles into that of an ensemble. We have a number of examples, the most famous being the ideal gas, statistical mechanics, the Ising model and the Potts model. Though the idea of aggregation is very much endemic to many areas of physics.</p>
<blockquote class="blockquote">
<p>“Imagine how difficult physics would be if electrons could think.” – Murray Gell-Mann</p>
</blockquote></li>
<li><p>In the case of a classifier we can aggregate many weak classifiers into a strong one. This is the idea behind <strong>boosting</strong> and <strong>bagging</strong>. This idea is behind well known algorithms like <strong>Random Forest</strong> and <strong>Gradient Boosting</strong> and even <strong>Stable Diffusion</strong></p></li>
<li><p>In the case of a language we can aggregate the meaning or semantics of many smaller units of meaning into larger ones. This is the idea behind <strong>semantics</strong>. This idea is behind well known algorithms like <strong>Word2Vec</strong> and <strong>BERT</strong> and even <strong>GPT-3</strong></p></li>
<li><p>Preferences can be aggregated into utility functions. This is the idea behind <strong>utility theory</strong>. This idea is behind demand theory.</p></li>
<li><p>Information aggregation is the notion that the wisdom of the crowd is better than the wisdom of the individual. <strong>The Wisdom of the Crowds</strong> points out that diverse opinions can play a big role here. There are other notions here like the <strong>Delphi Method</strong>.</p></li>
<li><p>Sensor Fusion via Kalman Filters, SLAM, and Particle Filters are ways to aggregate information from sensors. <strong>The Kalman Filter</strong> is a way to aggregate information from a sensor. <strong>SLAM</strong> is a way to aggregate information from a sensor. <strong>Particle Filters</strong> is a way to aggregate information from a sensor.</p></li>
<li><p>Social Networks have a number of ways to aggregate information. <strong>PageRank</strong> is a way to aggregate the importance of a node in a network. <strong>The Small World Phenomenon</strong> is a way to aggregate the number of hops between two nodes. <strong>The Strength of Weak Ties</strong> is a way to aggregate the number of connections between two nodes. <strong>The Friendship Paradox</strong> is a way to aggregate the number of friends a person has.</p></li>
<li><p>elections and voting have the notion of aggregation. <strong>Arrow’s Impossibility Theorem</strong> points out that there is no perfect way to aggregate preferences. <strong>The Condorcet Paradox</strong> points out that there is no perfect way to aggregate opinions. <strong>The Gibbard-Satterthwaite Theorem</strong> points out that there is no perfect way to aggregate votes.</p></li>
<li><p>In chaos theory many chaotic systems can become synchronized.</p></li>
</ol>
</div>
</div>
</section>
</section>
<section id="the-paper-annotated" class="level2">
<h2 class="anchored" data-anchor-id="the-paper-annotated">The Paper Annotated</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-hancock2018trainingclassifiersnaturallanguage" class="csl-entry">
Hancock, Braden, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher Ré. 2018. <span>“Training Classifiers with Natural Language Explanations.”</span> <a href="https://arxiv.org/abs/1805.03818">https://arxiv.org/abs/1805.03818</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>TODO: make a cheat-sheet on this topic!?↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>NLP</category>
  <category>Classification</category>
  <category>Named Entity Recognition</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/papers/BabbleLabble/</guid>
  <pubDate>Mon, 27 Jan 2025 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/papers/BabbleLabble/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Off-Policy Learning-to-Bid with AuctionGym</title>
  <link>https://orenbochman.github.io/notes-rl/posts/papers/Offpolicy-AuctionGym/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/Offpolicy-AuctionGym/cover.jpg" class="img-fluid figure-img"></p>
<figcaption>litrature review</figcaption>
</figure>
</div></div><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/YK0SELxAWeo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>So I don’t have much time for this today so here is a quick note on: <span class="citation" data-cites="jeunen2023offpolicy">(Jeunen, Murphy, and Allison 2023)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Learning to bid with AuctionGym
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>AuctionGym in a nutshell</figcaption>
</figure>
</div>
<ol type="1">
<li>The authors are using <strong>Thompson Sampling</strong>. This is a Bayesian method in RL.</li>
<li>Thier problem is an advert recommendation system. So they are integrating Thompson sampling into making recommendations.</li>
<li>They use a doubly robust estimation method. This is something I learned about from Emma Brunskill’s guest lecture in the Alberta Coursera course. And ever since I’ve been looking on how to do this in RL. Unforetunatly all I could find was work that used it in offline RL settings. So I was stoked to see it used as a central part of this paper. Using a doubly robust estimator is a sound technique for reducing variance without introducing a bias. And variance is the gratest impediment to learning quickly in RL. Also unlike some other ideas I’ve come accross it seems to align very well with Causal Inference.</li>
<li>The talk mentions a dataset the authors used for doing this work. Is this dataset available? I would like to try this out</li>
</ol>
</div>
</div>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Online advertising opportunities are sold through auctions, billions of times every day across the web. Advertisers who participate in those auctions need to decide on a bidding strategy: how much they are willing to bid for a given impression opportunity. Deciding on such a strategy is not a straightforward task, because of the interactive and reactive nature of the repeated auction mechanism. Indeed, an advertiser does not observe counterfactual outcomes of bid amounts that were not submitted, and successful advertisers will adapt their own strategies based on bids placed by competitors. These characteristics complicate effective learning and evaluation of bidding strategies based on logged data alone. The interactive and reactive nature of the bidding problem lends itself to a bandit or reinforcement learning formulation, where a bidding strategy can be optimised to maximise cumulative rewards. Several design choices then need to be made regarding parameterisation, model-based or model-free approaches, and the formulation of the objective function. This work provides a unified framework for such “learning to bid” methods, showing how many existing approaches fall under the value-based paradigm. We then introduce novel policy-based and doubly robust formulations of the bidding problem. To allow for reliable and reproducible offline validation of such methods without relying on sensitive proprietary data, we introduce AuctionGym: a simulation environment that enables the use of bandit learning for bidding strategies in online advertising auctions. We present results from a suite of experiments under varying environmental conditions, unveiling insights that can guide practitioners who need to decide on a model class. Empirical observations highlight the effectiveness of our newly proposed methods. AuctionGym is released under an open-source license, and we expect the research community to benefit from this tool.</p>
</blockquote>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My ideas
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Find what data set was used.</li>
<li>Is this dataset available?</li>
<li>Can we make a minimal version to quickly test this kind of agent?</li>
<li>Figure out a framework that extends tompson sampling to other RL problems.
<ul>
<li>need to add P(action|state) i.e.&nbsp;add conditioning of the bernulli on the state.</li>
<li>prehaps do simple counts of steps since starts or last reward.</li>
<li>prehaps using a succeror representation can help</li>
</ul></li>
<li>Marketing are the worst POMDPs. Testing real stuff is very hard so a good environment might help.</li>
<li>I want to make an petting zoo env to support single &amp; multiagent:
<ol type="1">
<li>auctions / non autions</li>
<li>advertising (rec sys) with costs</li>
<li>pricing with policies.</li>
</ol>
<ul>
<li>It should also allow incorperating real data from a dataset. Diretly or via sampling</li>
<li>It would be even neater to do this using a heirarchiacal model.</li>
<li>It would be even better if we can also incorportate the product, user hierecies.</li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="http://papers.adkdd.org/2022/papers/adkdd22-jeunen-learning.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="http://papers.adkdd.org/2022/paper-presentations/slides-adkdd22-jeunen-learning.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>slides</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/amzn/auction-gym.png" class="img-fluid figure-img"></p>
<figcaption>auctiongym code</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-jeunen2023offpolicy" class="csl-entry">
Jeunen, Olivier, Sean Murphy, and Ben Allison. 2023. <span>“Off-Policy Learning-to-Bid with AuctionGym.”</span> In <em>Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, 4219–28. KDD ’23. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3580305.3599877">https://doi.org/10.1145/3580305.3599877</a>.
</div>
</div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Advertising</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/papers/Offpolicy-AuctionGym/</guid>
  <pubDate>Fri, 17 Jan 2025 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/papers/Offpolicy-AuctionGym/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Learning to Bid with AuctionGym</title>
  <link>https://orenbochman.github.io/notes-rl/posts/papers/AuctionGym/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/AuctionGym/cover.jpg" class="img-fluid figure-img"></p>
<figcaption>litrature review</figcaption>
</figure>
</div></div><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/4wlOv9ThOuI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>So I don’t have much time for this today so here is a quick note on: <span class="citation" data-cites="jeunen2022learning">(Jeunen, Murphy, and Allison 2022)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Learning to bid with AuctionGym
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>AuctionGym in a nutshell</figcaption>
</figure>
</div>
<ol type="1">
<li>Advertising auctions are rarely incentive compatible.</li>
<li>Formulate the policy in terms of a utility AKA loss function.</li>
<li>They use a doubly robust estimation method. This is something I learned about from Emma Brunskill’s guest lecture in the Alberta Coursera course. And ever since I’ve been looking on how to do this in RL. Unforetunatly all I could find was work that used it in offline RL settings. So I was stoked to see it used as a central part of this paper. Using a doubly robust estimator is a sound technique for reducing variance without introducing a bias. And variance is the gratest impediment to learning quickly in RL. Also unlike some other ideas I’ve come accross it seems to align very well with Causal Inference.</li>
<li>The talk mentions a dataset the authors used for doing this work. Is this dataset available? I would like to try this out</li>
</ol>
</div>
</div>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">ABSTRACT</h2>
<blockquote class="blockquote">
<p>Online advertising opportunities are sold through auctions, billions of times every day across the web. Advertisers who participate in those auctions need to decide on a bidding strategy: how much they are willing to bid for a given impression opportunity. Deciding on such a strategy is not a straightforward task, because of the interactive and reactive nature of the repeated auction mechanism. Indeed, an advertiser does not observe counterfactual outcomes of bid amounts that were not submitted, and successful advertisers will adapt their own strategies based on bids placed by competitors. These characteristics complicate effective learning and evaluation of bidding strategies based on logged data alone.</p>
</blockquote>
<blockquote class="blockquote">
<p>The interactive and reactive nature of the bidding problem lends itself to a bandit or reinforcement learning formulation, where a bidding strategy can be optimised to maximise cumulative rewards. Several design choices then need to be made regarding parameterisation, model-based or model-free approaches, and the formulation of the objective function. This work provides a unified framework for such “learning to bid” methods, showing how many existing approaches fall under the value-based paradigm. We then introduce novel policy-based and doubly robust formulations of the bidding problem. To allow for reliable and reproducible offline validation of such methods without relying on sensitive proprietary data, we introduce AuctionGym: a simulation environment that enables the use of bandit learning for bidding strategies in online advertising auctions. We present results from a suite of experiments under varying environmental conditions, unveiling insights that can guide practitioners who need to decide on a model class. Empirical observations highlight the effectiveness of our newly proposed methods. AuctionGym is released under an open-source license, and we expect the research community to benefit from this tool.</p>
</blockquote>
</section>
<section id="the-bidding-objective" class="level2">
<h2 class="anchored" data-anchor-id="the-bidding-objective">The bidding Objective</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/AuctionGym/fig-01.png" class="img-fluid figure-img"></p>
<figcaption>High-level overview of a real-time-bidding flow in computational advertising</figcaption>
</figure>
</div>
<p>High-level overview of a real-time-bidding flow in computational advertising</p>
<p><span id="eq-utility"><img src="https://latex.codecogs.com/png.latex?%0AU%20=%20W(V%20%E2%88%92%20P)%0A%5Ctag%7B1%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?U"> is the utility, <img src="https://latex.codecogs.com/png.latex?W"> is the weight, <img src="https://latex.codecogs.com/png.latex?V"> is the value, and <img src="https://latex.codecogs.com/png.latex?P"> is the price. The value <img src="https://latex.codecogs.com/png.latex?V"> is the expected value of the impression, and the price <img src="https://latex.codecogs.com/png.latex?P"> is the bid amount.</p>
<p>The utility <img src="https://latex.codecogs.com/png.latex?U"> is the loss function. The goal is to maximize the utility <img src="https://latex.codecogs.com/png.latex?U"> according to some contextual policy <img src="https://latex.codecogs.com/png.latex?%5Cpi(B%5Cmid%20A;%20X)">.</p>
<p>Choosing a Counterfactual Estimator</p>
<ol type="1">
<li>Value-based Estimation (The “Direct Method”) High Bias model <img src="https://latex.codecogs.com/png.latex?P(win%7Cbid)"></li>
<li>Policy-based Estimation (IPS) High Variance</li>
<li>Doubly Robust Estimation Unbiased, lower variance</li>
</ol>
<p>How do you evaluate this?</p>
<ul>
<li><strong>Offline</strong>: use counterfactual estimators . . . &gt; “When a measure becomes a target, it ceases to be a good measure” (Goodhart’s Law)</li>
<li><strong>Online</strong>: A/B-tests span weeks, require production-level prototypes, …</li>
<li><strong>Simulate</strong></li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My ideas
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What do they mean that “auctions are not incentive compatible”?</li>
<li>Marketing are the worst POMDPs. Testing real stuff is very hard so a good environment might help.</li>
<li>Simulation is very powerfull as it allows to know the ground truth.</li>
<li>However its not easy to simulate the real world and any discrepency may lead to unrealistic results.</li>
</ul>
</div>
</div>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="http://papers.adkdd.org/2022/papers/adkdd22-jeunen-learning.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="http://papers.adkdd.org/2022/paper-presentations/slides-adkdd22-jeunen-learning.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>slides</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/amzn/auction-gym.png" class="img-fluid figure-img"></p>
<figcaption>auctiongym code</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-jeunen2022learning" class="csl-entry">
Jeunen, Olivier, Sean Murphy, and Ben Allison. 2022. <span>“Learning to Bid with AuctionGym.”</span>
</div>
</div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Advertising</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/papers/AuctionGym/</guid>
  <pubDate>Fri, 17 Jan 2025 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/papers/AuctionGym/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Dynamic collaborative filtering Thompson Sampling for cross-domain advertisements recommendation</title>
  <link>https://orenbochman.github.io/notes-rl/posts/papers/DCTS/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/DCTS/cover.jpg" class="img-fluid figure-img"></p>
<figcaption>litrature review</figcaption>
</figure>
</div></div><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/DAL7cpE3K7E" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>So I don’t have much time for this today so here is a quick note on: <span class="citation" data-cites="ishikawa2022dynamiccollaborativefilteringthompson">(Ishikawa, Chung, and Hirate 2022)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Dynamic collaborative filtering via Thompson Sampling
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>DCTS in a nutshell</figcaption>
</figure>
</div>
<ol type="1">
<li>The authors are using <strong>Thompson Sampling</strong>. This is a Bayesian method in RL.</li>
<li>Thier problem is an advert recommendation system. So they are integrating Thompson sampling into making recommendations.</li>
<li>The talk mentions a dataset the authors used for doing this work. Is this dataset available? I would like to try this out</li>
</ol>
</div>
</div>
<p>One line on Thompson sampling, one of the oldest technique in the RL playbook which uses the following rule: pick an action at random from the posterior distribution of the action values and then use the outcome to update the posterior distribution for the next step.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My ideas
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Find what data set was used.
<ul>
<li><a href="">Rakuten Ichiba data</a></li>
<li><a href="">Rakuten Travel dataset</a></li>
</ul></li>
<li>Is this dataset available?</li>
<li>Can we make a minimal version to quickly test this kind of agent?</li>
<li>Figure out a framework that extends tompson sampling to other RL problems.
<ul>
<li>need to add P(action|state) i.e.&nbsp;add conditioning of the bernulli on the state.</li>
<li>prehaps do simple counts of steps since starts or last reward.</li>
<li>prehaps using a succeror representation can help</li>
</ul></li>
<li>Marketing are the worst POMDPs. Testing real stuff is very hard so a good environment might help.</li>
<li>I want to make an petting zoo env to support single &amp; multiagent:
<ol type="1">
<li>auctions / non autions</li>
<li>advertising (rec sys) with costs</li>
<li>pricing with policies.</li>
</ol>
<ul>
<li>It should also allow incorperating real data from a dataset. Diretly or via sampling</li>
<li>It would be even neater to do this using a heirarchiacal model.</li>
<li>It would be even better if we can also incorportate the product, user hierecies.</li>
</ul></li>
</ul>
</div>
</div>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-ishikawa2022dynamiccollaborativefilteringthompson" class="csl-entry">
Ishikawa, Shion, Young-joo Chung, and Yu Hirate. 2022. <span>“Dynamic Collaborative Filtering Thompson Sampling for Cross-Domain Advertisements Recommendation.”</span> <a href="https://arxiv.org/abs/2208.11926">https://arxiv.org/abs/2208.11926</a>.
</div>
</div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Bandit</category>
  <category>Advertising</category>
  <category>Collaborative Filtering</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/papers/DCTS/</guid>
  <pubDate>Fri, 17 Jan 2025 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/papers/DCTS/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>On Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain</title>
  <link>https://orenbochman.github.io/notes-rl/posts/papers/Becoming-a-successful-loser/</link>
  <description><![CDATA[ 




<p>I tracked this paper due to it being highlighted in <span class="citation" data-cites="Skyrms2010signals">(Skyrms 2010)</span> as the source of a model that learns a signaling systems faster. I got me started with the loss domain. I was eventually able to find how to speed up learning by in the Lewis signaling game by considering the much more likely mistakes. Once I got on this algorithm I was thinking that using this idea for Bayesian updating of beliefs. This eventually led me to a second algorithm that was able to rapidly adjust a belief regarding the the state of the world in lewis signaling game with changing distributions.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>On Learning to become a successful loser in a nutshell</figcaption>
</figure>
</div>
<p>Besides having this amazing title this research paper compares five mathematical models that predict student behavior in repeated decision-making tasks involving gains and losses. <mark>The core issue is how to accurately represent the effect of losses on learning, as observed deviations from expected utility theory exist</mark> They conducted an experiment and find that learning in the loss domain can be faster than in the gain domain. <mark>The main results suggest that adding a constant to the payoff matrix can accelerate the learning process</mark>, supporting the adjustable reference point (ARP) abstraction of the effect of losses proposed by <span class="citation" data-cites="Roth1995Learning">(Roth and Erev 1995)</span>.</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<p>I’ts worth noteing that although this paper is about reinforment learning methods, the approach taken is more in line with how this is considered in the field of economics and pholosophy and less from machine learning or continous control. This is a good example of how the same problem can be approached from different fields. I would hazzard to say that the approches also align with the bandit settings.</p>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>One of the main difficulties in the development of descriptive models of learning in repeated choice tasks involves the abstraction of the effect of losses. The present paper explains this difficulty, summarizes its common solutions, and presents an experiment that was designed to compare the descriptive power of the specific quantifications of these solutions proposed in recent research. The experiment utilized a probability learning task. In each of the experiment’s 500 trials participants were asked to predict the appearance of one of two colors. The probabilities of appearance of the colors were different but fixed during the entire experiment. The experimental manipulation involved an addition of a constant to the payoffs. The results demonstrate that learning in the loss domain can be faster than learning in the gain domain; adding a constant to the payoff matrix can affect the learning process. These results are consistent with by <span class="citation" data-cites="Roth1995Learning">(Roth and Erev 1995)</span> adjustable reference point abstraction of the effect of losses, and violate all other models</p>
<p>— <span class="citation" data-cites="bereby1998learning">(Bereby-Meyer and Erev 1998)</span></p>
</blockquote>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline:</h2>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li>Highlights the difficulty in developing descriptive models of learning in repeated choice tasks that involve potential losses.</li>
<li>Presents the main goal of the paper: to compare the descriptive power of five distinct solutions to this difficulty and to identify a robust approximation of learning in simple decision tasks.</li>
</ul>
</section>
<section id="the-challenge-and-alternative-solutions" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-challenge-and-alternative-solutions">The Challenge and Alternative Solutions</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/Becoming-a-successful-loser/table-01.png" class="img-fluid figure-img"></p>
<figcaption>Models and Solutions</figcaption>
</figure>
</div></div><ul>
<li>Discusses the difficulty in abstracting the effect of losses given the approximately linear relationship between choice probabilities and the ratio of accumulated payoffs observed in the gain domain.</li>
<li>Introduces probability learning tasks used to derive and compare the predictions of different models.</li>
<li>Presents five alternative solutions to the problem:
<ul>
<li>Low Reference Point (LRP):
<ul>
<li>Transforms objective payoffs into non-negative rewards by subtracting the worst possible outcome</li>
</ul></li>
<li>Adjustable Reference Point and Truncation (ARP)
<ul>
<li>Uses an evolving reference point to distinguish gains and losses and truncates negative values to ensure positive propensities</li>
</ul></li>
<li>Exponential Response Rule (EDS, EFP, EWA)
<ul>
<li>Applies an exponential function to propensities, eliminating the need for handling negative values directly. Examples include Exponential Discounted Sum (EDS), Exponential Fictitious Play (EFP), and Experience Weighted Attractions (EWA) models</li>
</ul></li>
<li>Cumulative Normal Response Rule (CNFP)
<ul>
<li>Uses a cumulative normal distribution to model the relationship between payoffs and propensities- Employs the cumulative normal distribution function to map propensities (which can be negative) to choice probabilities (which are always between 0 and 1). The CNFP model exemplifies this.</li>
</ul></li>
<li>Relative Reinforcement solutions (CLO)
<ul>
<li>Uses outcome-specific parameters to determine the impact of different outcomes on choice probabilities. The Cardinal Linear Operator (CLO) model demonstrates this.</li>
</ul></li>
</ul></li>
<li>Describes the specific implementations of these solutions through different models, including their assumptions and parameterizations.</li>
</ul>
</section>
<section id="experiment" class="level3">
<h3 class="anchored" data-anchor-id="experiment">Experiment</h3>
<ul>
<li>Describes the experimental method, including the participants, the procedure, and the payoff structure of the probability learning task across three conditions.</li>
</ul>
</section>
<section id="results" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="results">Results</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/Becoming-a-successful-loser/table-01.png" class="img-fluid figure-img"></p>
<figcaption>Models and Solutions</figcaption>
</figure>
</div></div><ul>
<li>Presents the aggregated experimental results, showing a significant effect of the reward condition on the proportion of optimal choices.</li>
<li>Compares the quantitative predictive and descriptive power of the models using correlation and mean squared deviation (MSD) measures.</li>
<li>Discusses the between-subject variability observed in the data and the limitations of the models in capturing this variability.</li>
<li>Conducts a model-based analysis to evaluate the robustness of the condition effect.</li>
<li>Performs a sensitivity analysis to assess the robustness of the ARP model’s predictions to changes in parameter values.</li>
</ul>
</section>
<section id="discussion" class="level3">
<h3 class="anchored" data-anchor-id="discussion">Discussion</h3>
<ul>
<li>Discusses the main finding that the addition of constants to payoffs affects the speed of learning, highlighting the role of the distinction between gains and losses.</li>
<li>Notes the advantage of the ARP model in capturing the observed results and acknowledges the potential validity of other solutions under specific assumptions or parameterizations.</li>
<li>Addresses the generality of the findings by discussing:
<ul>
<li>Settings where the ARP model’s predictions are consistent with previous research (probability learning, signal detection).</li>
<li>Settings where the model might fail (learning among only positive outcomes, influence of other players’ payoffs).</li>
</ul></li>
</ul>
</section>
<section id="conclusions" class="level3">
<h3 class="anchored" data-anchor-id="conclusions">Conclusions</h3>
<ul>
<li>Concludes that human learning is affected by the distinction between gains and losses.</li>
<li>Emphasizes that modeling this distinction, particularly through the adjustable reference point approach, improves the descriptive power of adaptive learning models.</li>
<li>Acknowledges the need for further research to refine the quantification of the reference point for a more accurate and generalizable model.</li>
</ul>
</section>
</section>
<section id="key-takeaways" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ul>
<li>In most RL settings rewards are sparse. One way to speed up learning is to try and increase our reward signal.</li>
<li>This is the basis for seeling out to decompose the reward signal into an internal motivation for the agent and an external motivation for the problem designer.</li>
<li>Another approach though is to consider the loss domain. If we can get signals out of losses we can speed up learning and RL agents are slow learners - especially deep RL agents.</li>
<li>A third approach that I was able to make use of is to use both the loss domain and the gain domain to update beliefs about the possible states of the world. This was allowed me to speed up Bayesian learning algorithm for a coordination task.</li>
</ul>
<p>Besides this the paper has a lot of possible options for potential update rules to get this potential speed up.</p>
<ol type="1">
<li>How does adding a constant to all payoffs in a decision task affect learning, and which model best explains this effect?</li>
</ol>
<p>One of the results I learned is that adding a constant to the payoff matrix doesn’t change it. In fact linear transformations of the payoff matrix don’t change the outcomes. In policy gradient methods this we call this trick learning with baselines. What we see is that it doesn’t bias the estimator but can drastically reduce the variance of the estimator. And this variance is the noise that slows down learning by the agent. So adding a constant can surprisingly impact learning speed. The ARP model uniquely predicts this: subtracting a constant to introduce losses speeds learning compared to a purely gain-based scenario. This highlights the psychological impact of the gain-loss framing.</p>
<p>Another insight I had about this is while trying to abstract the RL algorithms. Was that under some conditions we can convert the reward function into a distance metric. Having a metric makes navigation states space much simpler. I really can’t think of a better feature.</p>
<ol start="2" type="1">
<li>What are the limitations of the ARP model?</li>
</ol>
<ul>
<li>The ARP model, with its current parameters, assumes an initial reference point of zero and a slow adjustment process. This might not hold when:
<ul>
<li>All options are positive: The model would predict slow learning even when clear differences exist.</li>
<li>Social comparison exists: People may adjust their reference point based on other players’ payoffs, a factor not currently incorporated in the model.</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li>How would you define the loss domain and the gain domain ?</li>
</ol>
<ul>
<li><p>The gain domain is when choice probabilities are approximately linearly related to the ratio of accumulated reinforcement.</p></li>
<li><p>The loss domain is when negative payoffs are possible.</p></li>
<li><p>In the gain domain, the probabilities of choosing an alternative match the ratio of accumulated reinforcement, meaning that individuals are more likely to choose options that have yielded higher rewards in the past.</p></li>
<li><p>Descriptive models have to assume that choice probabilities are determined by a function of the accumulated reinforcements, which must have strictly positive values. However, this presents a problem when losses are possible because negative payoffs can result in negative values for the function.</p></li>
</ul>
<ol start="4" type="1">
<li>In the paper the autors mention the value function from prospect theory c.f. <span class="citation" data-cites="kahneman1979econ">(Kahneman 1979)</span>. How does this relate to the ARP model?</li>
</ol>
<p>The authors state that models that use solutions other than the adjustable reference point can account for the results of the study under the assumption that the model’s parameters can be affected by the payoffs. One way to account for this is to use reinforcement functions with the characteristics of Prospect Theory’s value function. Prospect theory, developed by Kahneman and Tversky, suggests that individuals make decisions based on the potential value of losses and gains rather than the final outcome, and that losses have a greater impact on individuals than gains do. This relates to the ARP model because it also assumes that reinforcements are evaluated relative to a reference point, meaning outcomes above the reference point are perceived as gains (reinforcements) and outcomes below the reference point are perceived as losses (punishments).</p>
<ol start="5" type="1">
<li>Is there a formal definition of this prospect theoretic value function?</li>
</ol>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/Becoming-a-successful-loser/value-function.png" class="img-fluid figure-img"></p>
<figcaption>a value function</figcaption>
</figure>
</div></div><p>A key element of this theory is the value function, which exhibits these characteristics: - It’s defined on deviations from a reference point. - It’s generally concave for gains and convex for losses. - It’s steeper for losses than for gains, meaning an equivalent loss has a greater psychological impact than the corresponding gain.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bereby1998learning" class="csl-entry">
Bereby-Meyer, Yoella, and Ido Erev. 1998. <span>“On Learning to Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain.”</span> <em>Journal of Mathematical Psychology</em> 42 (2-3): 266–86.
</div>
<div id="ref-kahneman1979econ" class="csl-entry">
Kahneman, Daniel. 1979. <span>“Econ Ometrica i Ci.”</span> <em>Econometrica</em> 47 (2): 263–91.
</div>
<div id="ref-Roth1995Learning" class="csl-entry">
Roth, Alvin, and Ido Erev. 1995. <span>“Learning in Extensive-Form Games: Experimental Data and Simple Dynamic Models in the Intermediate Term.”</span> <em>Games and Economic Behavior</em> 8 (1): 164–212. <a href="https://EconPapers.repec.org/RePEc:eee:gamebe:v:8:y:1995:i:1:p:164-212">https://EconPapers.repec.org/RePEc:eee:gamebe:v:8:y:1995:i:1:p:164-212</a>.
</div>
<div id="ref-Skyrms2010signals" class="csl-entry">
Skyrms, Brian. 2010. <span>“<span class="nocase">Signals: Evolution, Learning, and Information</span>.”</span> In. Oxford University Press. <a href="https://doi.org/10.1093/acprof:oso/9780199580828.003.0013">https://doi.org/10.1093/acprof:oso/9780199580828.003.0013</a>.
</div>
</div></section></div> ]]></description>
  <category>Reinforcement Learning</category>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/papers/Becoming-a-successful-loser/</guid>
  <pubDate>Wed, 01 Jan 2025 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/papers/Becoming-a-successful-loser/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Temporal Abstraction in Reinforcement Learning with the Successor Representation</title>
  <link>https://orenbochman.github.io/notes-rl/posts/papers/Temp-Abstraction/</link>
  <description><![CDATA[ 




<p>This paper review is an extended introduction to temporal abstraction using options. It covers lots of advanced concepts in reinforcement learning that were introduced by Guest Speaker Doina Precup from Coursera Specialization on Reinforcement Learning by Martha White and Adam White. The paper is a deep dive into the topic of options and the successor representation. It is a long paper with lots of advanced concepts and algorithms. The paper is a great resource for anyone interested in reinforcement learning and temporal abstraction.</p>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Ever since I saw <span class="citation" data-cites="Martha2022SubTasks">(White 2022)</span> the video lecture on subtasks by Martha White about learning tasks in parallel. However the video does not address the elephant in the room - how to discover the options.</p>

<div class="no-row-height column-margin column-container"><div id="fig-subtasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/GmGL9cVfJG4" title="Martha White - Developing Reinforcement Learning Agents that Learn Many Subtasks?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.
</figcaption>
</figure>
</div><div id="fig-option-discovery" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-option-discovery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/m9gYmYEYuIs" title="Marlos C. Machado - Representation-driven Option Discovery in Reinforcement Learning?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-option-discovery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Talk at Cohere.AI by Marlos C. Machado on Representation-driven Option Discovery in Reinforcement Learning. He discusses the Representation-driven Option Discovery (ROD) cycle and how it can be used to discover options in reinforcement learning. The talk covers much of the material in the paper as well as some more recent follow up work.
</figcaption>
</figure>
</div></div>
<p>This is a hefty paper 70 pages with 8 algorithms many figures and citations from research spanning thirty years. It is filled to the brim with fascinating concepts that are developed by the authors but builds on lots of work by earlier researchers. It may seem to cover a niche topic but <span class="citation" data-cites="Machado2024Cohere">(c.f. Machado 2024, time 773)</span> makes an eloquent argument that this paper deals with a fundamental question of where options come and if we put aside the jargon for a second we are trying to capture a form of intelingence that includes elements of generalization, planning, problem solving, learning at a level much closer what we are familiar with. And these familiar forms of mental abstractions much harder to consider in the context of Supervised or Unsupervised learning which lack the ineraction with the environment that is the hallmark of reinforcement learning.</p>
<p>I came about this paper by accident. I a quick summary before I realized how long it was and I put out my first pass, and I hope to flesh it including perhaps a bit of code.</p>
<p>I’ve been developing my own ideas regarding the creation and aggregation of options in reinforcement learning. My thinking to date has been different. I am exploring a Bayesian based tasks. I’ve considered creating shared semantics via emergent symbolic semantics and looking at a number of composability mechanisms for state, language and of options including using hierarchial bayesian models. While working on coding environments for this subjects a search led to this amazing paper!</p>
<p>In <span class="citation" data-cites="Machado2024Cohere">(Machado 2024)</span> Marlos C. Machado, has given a talk that explains many of the complex ideas within this paper. This talk is available on YouTube.</p>
<p>Marlos C. Machado is a good speaker and going over that paper and the video certainly helps to understand the challenges of temporal abstractions as well as the solutions that the paper proposes.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Option Discovery with Successor Representations
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>Option Discovery in a nutshell</figcaption>
</figure>
</div>
<p>This paper posits that <strong>successor representations</strong>, which encode states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstraction like options if these are not known.</p>
<p>Options are a powerful form of temporal abstraction that allows agents to make predictions and to operate at different levels of abstraction within an environment in ways idiosyncratic of human approach to tackle many problems. One of the key questions has been how to discover good options. The paper presents a rather simple yet powerful answer to this.</p>
<p>This paper is a quite challangeing. You might listen to this lighthearted deep dive courtesy by notebooklm to ease you into the topics.</p>
<audio controls="1">
<source src="deepdive.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Abstract
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation, which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the successor representation can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these results as instantiations of a general framework for option discovery in which the agent’s representation is used to identify useful options, which are then used to further improve its representation. This results in a virtuous, never-ending, cycle in which both the representation and the options are constantly refined based on each other. Beyond option discovery itself, we also discuss how the successor representation allows us to augment a set of options into a combinatorially large counterpart without additional learning. This is achieved through the combination of previously learned options. Our empirical evaluation focuses on options discovered for temporally-extended exploration and on the use of the successor representation to combine them. Our results shed light on important design decisions involved in the definition of options and demonstrate the synergy of different methods based on the successor representation, such as eigenoptions and the option keyboard.</p>
<p>— <span class="citation" data-cites="machado2023temporal">(Machado et al. 2023)</span></p>
</blockquote>
</div>
</div>
</div>
</section>
<section id="the-review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-review">The Review</h2>
<section id="introduction-1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="introduction-1">Introduction</h3>

<div class="no-row-height column-margin column-container"><div id="fig-option-framework" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-option-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/GntIVgNKkCI" title="DeepHack.RL: Doina Precup - Temporal abstraction in reinforcement learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-option-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Doina Precup’s Talk at DeepHack.RL on Temporal abstraction in reinforcement learning covers both the intro and the background material on options.
</figcaption>
</figure>
</div></div><p>In this section, the authors introduce the reinforcement learning problem and the options framework. Next they discuss the benefits of using options and highlight the option discovery problem. Next they present the successor representation (SR) as a representation learning method that is conducive to option discovery, summarizing its use cases and connecting it to neuroscience They go on to describe the paper’s focus on temporally-extended exploration and the use of eigenoptions and covering options. The finnish the introduction by highlight the paper’s evaluation methodology and the use of toy domains and navigation tasks for clarity and intuition.</p>
<p>In <span class="citation" data-cites="Doina2017DeepHack">(Precup 2017)</span> Doina precup gives a talk on temporal abstraction in reinforcement learning. This talk covers both the introduction and the background material on options and is on YouTube.</p>
</section>
</section>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<ul>
<li><p>Defines the reinforcement learning problem, covering Markov Decision Processes, policies, value functions, and common algorithms such as Q-learning.</p></li>
<li><p>Introduces the options framework <span class="citation" data-cites="Sutton1999BetweenMA">(Richard S. Sutton, Precup, and Singh 1999)</span>, <span class="citation" data-cites="precup2000temporal">(Precup and Sutton 2000)</span>, defining its components (initiation set, policy, termination condition), execution models, and potential benefits.</p></li>
</ul>
<p>An option <img src="https://latex.codecogs.com/png.latex?%5Comega%20%5Cin%20%5COmega"> is a 3-tuple</p>
<p><span id="eq-6"><img src="https://latex.codecogs.com/png.latex?%0A%5Comega%20=%20%3CI_%5Comega%20,%20%5Cpi_%5Comega%20,%20%5Cbeta_%5Comega%20%3E%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<ul>
<li>where
<ul>
<li><img src="https://latex.codecogs.com/png.latex?I_%5Comega%20%E2%8A%86%20S"> the options’s initiation set,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Comega%20:%20%5Cmathcal%7BS%7D%20%5Ctimes%20%5Cmathcal%7BA%7D%20%5Crightarrow%20%5B0,%201%5D"> the option’s policy, such that <img src="https://latex.codecogs.com/png.latex?%5Csum_a%20%5Cpi_%5Comega%20(%C2%B7,%20a)%20=%201">, and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta_%5Comega%20:%20%5Cmathcal%7BS%7D%20%5Crightarrow%20%5B0,%201%5D"> the option’s termination condition <sup>1</sup></li>
</ul></li>
</ul>
</section>
<section id="a-framework-for-option-discovery-from-representation-learning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-framework-for-option-discovery-from-representation-learning">A Framework for Option Discovery from Representation Learning</h2>

<div class="no-row-height column-margin column-container"><div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" width="250px" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/papers/Temp-Abstraction/fig_1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Representation-driven Option Discovery (ROD) cycle <span class="citation" data-cites="Machado2019EfficientEI">(Machado 2019)</span>. The option discovery algorithms discussed in this paper can be seen as instantiating this cycle. The incoming arrow to Collect samples depicts the start of the process. The arrow from Define option to Option set highlights the output generated by the ROD cycle. Note that other generated artifacts can also be used by the agent outside the ROD cycle, such as the learned representation.
</figcaption>
</figure>
</div></div><ul>
<li><p>Introduces a general framework for option discovery driven by representation learning, named the <strong>Representation-driven Option Discovery</strong> (ROD) cycle.</p>
<ul>
<li>Collect samples</li>
<li>Learn a representation</li>
<li>Derive an intrinsic reward function from the representation</li>
<li>Learn to maximize intrinsic reward</li>
<li>Define option</li>
</ul></li>
<li><p>Presents a step-by-step explanation of the ROD cycle, outlining its iterative and constructivist nature, as depicted in the figure.</p></li>
</ul>
</section>
<section id="the-successor-representation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-successor-representation">The Successor Representation</h2>

<div class="no-row-height column-margin column-container"><div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" width="250px" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/papers/Temp-Abstraction/fig_3.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Example similar to Dayans 1993 of the SR, w.r.t. the uniform random policy, of state A (left). Consider a navigation task where the agent has access to its (x, y) coordinates. It is tempting to use some distance metric such as the Euclidean distance to define distance between states. However, if one considers the gray tiles to be walls, an agent in point A can reach point B much quicker than point C. The SR captures this distinction, ensuring that, in this representation, point A is closer to point B than it is to point C. The plots of the SR were generated using a discretization of the grid, where each tile is a state. Red represents larger values while blue represents smaller values (states that are temporally further away). Recall the SR of a state, in the tabular case, is an |S|-dimensional representation, thus allowing us to depict it as a heatmap over the state space.
</figcaption>
</figure>
</div></div><ul>
<li><p>Presents the successor representation (SR) as a method to extract representations from observations.</p></li>
<li><p>In <span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 4.1)</span> Defines the SR in the tabular setting, explaining its ability to capture environment dynamics by encoding expected future state visitation, as shown in Equation 7 and the figure.</p></li>
</ul>
<p><span id="eq-7"><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi_%7B%5Cpi%7D(s,s')%20=%20%5Cmathbb%7BE%7D_%7B%5Cpi,p%7D%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%20%CE%B3%5Et%20%5Cmathbb%7B1%7D_%7B%5C%7BS_t=s'%7D%5C%7D%20%7C%20%7BS_0%20=%20s%20%7D%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>

<div class="no-row-height column-margin column-container"><div id="fig-4" class="quarto-float quarto-figure quarto-figure-center anchored" width="250px" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/papers/Temp-Abstraction/fig_4.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: First three PVFs in the (a) four-room domain. Gray squares represent walls and white squares represent accessible states. Four actions are available: up, down, right, and left. The transitions are deterministic and the agent is not allowed to move into a wall. (b-d) These plots depict the first, second, and third eigenvectors associated with each state. The axes are rotated for clarity. The bottom left corner of the four-room domain is the state closer to the reade
</figcaption>
</figure>
</div></div><ul>
<li>Discusses the estimation of the SR with temporal-difference learning, its connection to general value functions <sup>2</sup>, and its relationship to the transition probability matrix Equation 9.</li>
</ul>
<p><span id="eq-8"><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi(S_t,j)%20%5Cleftarrow%20%5Chat%7B%5CPsi%7D(S_t,%20j)%20+%20%5Ceta%20%5B%5Cmathbb%7B1%7D_%7B%5C%7BS_t=j%5C%7D%7D%20+%20%5Cgamma%20%5Chat%7B%5CPsi%7D(S_%7Bt+1%7D,%20j)%20%E2%88%92%20%5Chat%7B%5CPsi%7D(S_t,%20j)%5D%0A%5Ctag%7B3%7D"></span></p>
<p><span id="eq-9"><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi_%5Cpi%20=%20%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20(%5Cgamma%20P_%5Cpi)%5Et%20=%20(I-%5Cgamma%20P_%5Cpi)%5E%7B-1%7D%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<ul>
<li><p>Introduces successor features (SFs) as a generalization of the SR to the function approximation setting, extending the definition of the SR to arbitrary features, as shown in Equation 11.</p></li>
<li><p>Highlights the relationship between the SR and PVFs.</p></li>
</ul>
</section>
<section id="temporally-extended-exploration" class="level2">
<h2 class="anchored" data-anchor-id="temporally-extended-exploration">Temporally-Extended Exploration</h2>
<ul>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 5)</span> discusses temporally-extended exploration with options and its potential to enhance exploration in RL.</p></li>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 5.1)</span> introduces eigenoptions, which are options defined by the eigenvectors of the SR. &gt; “Eigenoptions are options defined by the eigenvectors of the SR.2 Each eigenvector assigns an intrinsic reward to every state in the environment.”</p>
<ul>
<li><p>Explains the concept of eigenoptions using the four-room domain as an example (Figure 5).</p></li>
<li><p>Describes how to learn eigenoptions’ policies using an intrinsic reward function derived from the eigenvectors of the SR.</p></li>
<li><p>Defines the initiation set and termination condition of eigenoptions, as shown in Equation 16.</p></li>
<li><p>Presents Theorem 1, which guarantees the existence of at least one terminal state for each eigenoption.</p></li>
</ul></li>
<li><p>Introduces covering options, which are point options defined by the bottom eigenvector of the graph Laplacian and aim to minimize the environment’s cover time.</p>
<ul>
<li>Explains the concept of covering options using the four-room domain (Figure 7).</li>
<li>Describes how to learn covering options’ policies using a simplified intrinsic reward function.</li>
<li>Defines the initiation set and termination condition of covering options.</li>
<li>Highlights the iterative nature of covering option discovery, where options are added one by one at each iteration.</li>
</ul></li>
</ul>
</section>
<section id="evaluation-of-temporally-extended-exploration-with-options" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="evaluation-of-temporally-extended-exploration-with-options">Evaluation of Temporally-Extended Exploration with Options</h2>
<ul>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 6)</span>Evaluates eigenoptions and covering options in the context of temporally-extended exploration.</p></li>
<li><p>Uses the diffusion time, a task-agnostic metric, to quantify exploration effectiveness by measuring the expected number of decisions required to navigate between states.</p></li>
<li><p>Presents results comparing eigenoptions and covering options:</p>
<ul>
<li>Shows that both approaches can reduce diffusion time in the four-room domain when computed in closed form (Figure 8).</li>
<li>Discusses the impact of different initiation set sizes, highlighting the trade-off between avoiding sink states and ensuring option availability.</li>
</ul></li>
<li><p>Investigates the effectiveness of eigenoptions and covering options in an online setting:</p>
<ul>
<li>Demonstrates the robustness of eigenoptions to online SR estimation (Figure 11).</li>
<li>Reveals the challenges of using covering options online, particularly due to their restrictive initiation set and reliance on a single eigenvector (Figure 12).</li>
</ul></li>
<li><p>Explores the impact of using options on reward maximization in a fixed task:</p>
<ul>
<li>Shows that eigenoptions can accelerate reward accumulation when used for temporally-extended exploration in Q-learning (Figure 9).</li>
<li>Observes that covering options do not consistently improve reward maximization in this setting, likely due to their sparse initiation set.</li>
</ul></li>
</ul>

<div class="no-row-height column-margin column-container"><div id="fig-9" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/papers/Temp-Abstraction/fig_9.png" class="img-fluid figure-img" data-group="figures" width="250">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: figure 9
</figcaption>
</figure>
</div></div></section>
<section id="iterative-option-discovery-with-the-rod-cycle" class="level2">
<h2 class="anchored" data-anchor-id="iterative-option-discovery-with-the-rod-cycle">Iterative Option Discovery with the ROD Cycle</h2>
<ul>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 7)</span> introduces Covering Eigenoptions (CEO), a new algorithm that performs multiple iterations of the ROD cycle for option discovery.</p></li>
<li><p>Describes the steps of CEO, emphasizing its use of eigenoptions and online SR estimation, as outlined in Algorithm 2.</p></li>
<li><p>Demonstrates the benefits of multiple ROD cycle iterations with CEO, showing a significant reduction in the number of steps needed to visit all states in the four-room domain (Figure 14).</p></li>
<li><p>Illustrates the behavior of CEO over multiple iterations, highlighting its ability to progressively discover more complex options (Figure 14).</p></li>
<li><p>Combining Options with the Option Keyboard</p></li>
<li><p>Discusses the option keyboard as a way to combine existing options to create new options without additional learning, potentially expanding the agent’s behavioral repertoire.</p></li>
<li><p>Introduces Generalized Policy Evaluation (GPE) and Generalized Policy Improvement (GPI), generalizations of standard policy evaluation and improvement.</p></li>
<li><p>Explains how to use GPE and GPI to synthesize options from linear combinations of rewards induced by eigenvectors of the SR, as outlined in Algorithm 3.</p></li>
<li><p>Combining Eigenoptions with the Option Keyboard</p></li>
<li><p>Demonstrates the synergy of eigenoptions and the option keyboard.</p></li>
<li><p>Presents a qualitative analysis of options generated by combining eigenoptions with the option keyboard (Figures 16 and 17).</p></li>
<li><p>Shows that the option keyboard leads to a combinatorial explosion of new options, as evidenced by the number of unique options generated (Figure 18).</p></li>
<li><p>Demonstrates the diversity of options generated by the option keyboard through heatmaps showing the frequency of termination in different states (Figures 19 and 20).</p></li>
<li><p>Presents a quantitative analysis of the diffusion time induced by eigenoptions combined with the option keyboard, highlighting the improvement in exploration effectiveness (Figures 21 and 22).</p></li>
</ul>
</section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">Related Work</h2>
<ul>
<li>Discusses option discovery methods for planning and bottleneck options, including those based on spectral clustering and normalized cuts.</li>
<li>Mentions other option discovery methods for temporally-extended exploration, such as diffusion options.</li>
<li>Outlines extensions of the SR and option discovery methods to function approximation, including linear and non-linear function approximation techniques.</li>
<li>Discusses the connection of the SR to other reinforcement learning concepts, such as proto-value functions, slow-feature analysis, and dual representations.</li>
<li>Highlights the relationship of the SR to neuroscience, including its potential to model hippocampal place fields and grid cell activations.</li>
<li>Mentions the SR’s application to explaining human behavior and decision-making.</li>
</ul>
</section>
<section id="conclusion" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<ul>
<li>Highlights the potential of using the SR as the main substrate for temporal abstraction, pointing out promising directions for future work.</li>
<li>Emphasizes the importance of iterative option discovery and its role in building intelligent agents capable of continual learning and complex skill acquisition.</li>
</ul>
<p>Here are the successor representations algorithms from the paper:</p>

<div class="no-row-height column-margin column-container"><div id="fig-11" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/papers/Temp-Abstraction/alg_1.png" class="img-fluid figure-img" data-group="figures" width="250">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: successor representations algorithms
</figcaption>
</figure>
</div></div><p>Next is the covering eigenoptions algorithm:</p>

<div class="no-row-height column-margin column-container"><div id="fig-12" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/papers/Temp-Abstraction/alg_2.png" class="img-fluid figure-img" data-group="figures" width="250">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Covering Eigenoptions algorithm
</figcaption>
</figure>
</div></div></section>
<section id="study-guide-for-the-paper" class="level2">
<h2 class="anchored" data-anchor-id="study-guide-for-the-paper">Study guide for the paper</h2>
<ol type="1">
<li>What is an option in reinforcement learning?</li>
</ol>
<p>We actually took the definition from the paper. But here is another from the video. This is perhaps a more elegant definition. It comes from []</p>
<p>In reinforcement learning, an <strong>option</strong> is a temporally extended course of actions that allows an agent to operate at different levels of abstraction within an environment. Options are a form of temporal abstraction that enables agents to make predictions and execute actions over extended time horizons, providing a way to structure and organize the agent’s behavior. <img src="https://latex.codecogs.com/png.latex?%0Av_%7B%5Cpi,%5Cbeta%7D%5E%7Bc,z%7D(s)%20=%20%5Cmathbb%7BE%7D_%7B%5Cpi,%5Cbeta%7D%20%5Cleft%5B%20%5Csum_%7Bj=1%7D%5EK%20c(S_j)%20+%20%5Cgamma%5E%7BK-1%7D%20z(S_k)%20%7C%20S_0%20=%20s%20%5Cright%5D%20%5Cqquad%20%5Ctext%7Bfor%20all%20%7D%20s%20%5Cin%20S%0A"></p>
<ul>
<li>where
<ul>
<li><img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi,%5Cbeta%7D%5E%7Bc,z%7D(s)"> is the value function of the option,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpi"> is the policy,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is the termination condition,</li>
<li><img src="https://latex.codecogs.com/png.latex?c"> is the extrinsic reward function, <img src="https://latex.codecogs.com/png.latex?z"> is the intrinsic reward function,</li>
<li><img src="https://latex.codecogs.com/png.latex?S_j"> is the state at time <img src="https://latex.codecogs.com/png.latex?j">,</li>
<li><img src="https://latex.codecogs.com/png.latex?K"> is the duration of the option, and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the discount factor.</li>
</ul></li>
</ul>
<ol type="1">
<li>How can options be used ?</li>
</ol>
<ul>
<li>For planning: you can use eigenvectors of the SR to identify bottleneck, states that are difficult to reach under a random walk, and then use options to guide the agent to those states. c.f. <span class="citation" data-cites="Solway2014OptimalBH">(Solway et al. 2014)</span></li>
<li>For exploration: you can use eigenoptions to encourage exploration by driving the agent toward states that are difficult to reach under a random walk.</li>
</ul>
<ol type="1">
<li>Explain the successor representation</li>
</ol>
<p>The <strong>successor representation (SR)</strong> is a method in reinforcement learning that represents states based on their expected future visits under a given policy. It captures the environment’s dynamics by encoding how likely an agent is to visit each state in the future, starting from a particular state.</p>
<ul>
<li>The SR is denoted as <img src="https://latex.codecogs.com/png.latex?%CE%A8_%CF%80">, where <img src="https://latex.codecogs.com/png.latex?%CF%80"> represents the agent’s policy.</li>
<li>It can be estimated online using <strong>temporal difference learning</strong> and generalized to function approximation using <strong>successor features</strong>.</li>
</ul>
<p>The SR allows for <strong>Generalized Policy Evaluation (GPE)</strong>: once the SR is learned, an agent can immediately evaluate its performance under any reward function that can be expressed as a linear combination of the features used to define the SR.</p>
<p>The SR offers a powerful tool for discovering and using temporal abstractions in reinforcement learning, enabling the development of more intelligent and efficient agents. It is used in option discovery methods like eigenoptions and covering options, providing a natural framework for identifying and leveraging temporally extended courses of actions.</p>
<p>Here is a breakdown of the mathematical definition of the SR:</p>
<p><span id="eq-SR"><img src="https://latex.codecogs.com/png.latex?%0A%CE%A8_%5Cpi%20(s,%20s')%20=%20%20%5Cmathbb%7BE%7D_%7B%CF%80,p%7D%20%5B%5Csum%5E%5Cinfty_%7Bt=0%7D%20%CE%B3%5Et%5Cmathbb%7B1%7D_%7BS_t%20=%20s'%7D%20%7C%20S_0%20=%20s%20%5D%20%5Cqquad%0A%5Ctag%7B5%7D"></span></p>
<ul>
<li>Where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?s,%20s'"> are states in the environment.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the discount factor, determining the weight of future rewards.</li>
<li>The <strong>expectation (E)</strong> is taken over the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and the transition probability kernel <img src="https://latex.codecogs.com/png.latex?p">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7B1%7D_%7BS_t%20=%20s'%7D"> is an indicator function that equals 1 if the agent is in state s’ at time <img src="https://latex.codecogs.com/png.latex?t">, and 0 otherwise.</li>
</ul></li>
</ul>
<p>This equation calculates the expected discounted number of times the agent will visit state s’ in the future, given that it starts in state s and follows policy π. The SR matrix stores these expected visitations for all state pairs.</p>
<ol start="2" type="1">
<li>Explain what is an eigenoption a covering option and the difference</li>
</ol>
<p><strong>Eigenoptions</strong> and <strong>covering options</strong> are two methods for option discovery in RL that use the successor representation (SR). Options represent temporally extended courses of actions.</p>
<p><strong>Eigenoptions</strong> are options defined by the eigenvectors of the SR.</p>
<ul>
<li>Each eigenvector of the SR assigns an intrinsic reward to every state in the environment.</li>
<li>An eigenoption aims to reach the state with the highest (or lowest) value in the corresponding eigenvector.</li>
<li>They encourage exploration by driving the agent toward states that are difficult to reach under a random walk.</li>
<li>Eigenoptions have a broad initiation set, meaning they can be initiated from many states.</li>
<li>They terminate when the agent reaches a state with a (locally) maximum value in the eigenvector, meaning the agent can’t accumulate more positive intrinsic reward.</li>
<li>Eigenoptions tend to have different durations based on the eigenvalue they are derived from, allowing the agent to operate at different timescales.</li>
</ul>
<p><strong>Covering options</strong> are defined by the bottom eigenvector of the graph Laplacian, which is equivalent to the top eigenvector of the SR under certain conditions.</p>
<ul>
<li>They aim to minimize the environment’s expected cover time, which is the number of steps needed for a random walk to visit every state.</li>
<li>Each covering option connects two specific states: one with the lowest value and one with the highest value in the corresponding eigenvector.</li>
<li>They are discovered iteratively. After each option is discovered, the environment’s graph is updated, and the process repeats.</li>
<li>Covering options have a restrictive initiation set, containing only the single state with the lowest value in the eigenvector.</li>
<li>They terminate when they reach the state with the highest value in the eigenvector.</li>
</ul>
<p>Here’s a table summarizing the <strong>key differences</strong> between eigenoptions and covering options:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 37%">
<col style="width: 53%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Eigenoption</th>
<th style="text-align: left;">Covering Option</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Definition</strong></td>
<td style="text-align: left;">Based on any eigenvector of the SR</td>
<td style="text-align: left;">Based on the bottom eigenvector of the graph Laplacian (equivalent to the top eigenvector of the SR under certain conditions)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Goal</strong></td>
<td style="text-align: left;">Reach states with high/low values in the corresponding eigenvector</td>
<td style="text-align: left;">Minimize environment’s cover time</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Initiation Set</strong></td>
<td style="text-align: left;">Broad (many states)</td>
<td style="text-align: left;">Restrictive (single state)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Termination Condition</strong></td>
<td style="text-align: left;">Reaching a (local) maximum in the eigenvector</td>
<td style="text-align: left;">Reaching the state with the highest value in the eigenvector</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Discovery Process</strong></td>
<td style="text-align: left;">Can be discovered in parallel, in a single iteration</td>
<td style="text-align: left;">Discovered iteratively, one option at a time</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Timescale</strong></td>
<td style="text-align: left;">Different eigenoptions can have different durations</td>
<td style="text-align: left;">Generally have similar durations</td>
</tr>
</tbody>
</table>
<p>Both eigenoptions and covering options can be effective for exploration, but they have different strengths and weaknesses. Eigenoptions can learn more diverse behaviors and capture different timescales, while covering options may be simpler to implement and can guarantee improvement in the environment’s cover time.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>There are a few blog posts that dive deeper into some of the concepts in the paper.</p>
<ul>
<li><a href="https://medium.com/@marlos.cholodovskis/the-representation-driven-option-discovery-cycle-e3f5877696c2">The Representation-driven Option Discovery</a></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-Machado2019EfficientEI" class="csl-entry">
Machado, Marlos C. 2019. <span>“Efficient Exploration in Reinforcement Learning Through Time-Based Representations.”</span> <em>Revue De Geographie Alpine-Journal of Alpine Research</em>. PhD thesis.
</div>
<div id="ref-Machado2024Cohere" class="csl-entry">
———. 2024. <span>“Representation-Driven Option Discovery in Reinforcement Learning.”</span> <em>YouTube</em>. <a href="https://www.youtube.com/watch?v=m9gYmYEYuIs">https://www.youtube.com/watch?v=m9gYmYEYuIs</a>.
</div>
<div id="ref-machado2023temporal" class="csl-entry">
Machado, Marlos C., Andre Barreto, Doina Precup, and Michael Bowling. 2023. <span>“Temporal Abstraction in Reinforcement Learning with the Successor Representation.”</span> <em>Journal of Machine Learning Research</em> 24 (80): 1–69. <a href="http://jmlr.org/papers/v24/21-1213.html">http://jmlr.org/papers/v24/21-1213.html</a>.
</div>
<div id="ref-Doina2017DeepHack" class="csl-entry">
Precup, Doina. 2017. <span>“DeepHack.RL: Temporal Abstraction in Reinforcement Learning.”</span> <em>YouTube</em>. <a href="https://www.youtube.com/watch?v=GntIVgNKkCI">https://www.youtube.com/watch?v=GntIVgNKkCI</a>.
</div>
<div id="ref-precup2000temporal" class="csl-entry">
Precup, Doina, and Richard S. Sutton. 2000. <span>“Temporal Abstraction in Reinforcement Learning.”</span> PhD thesis, University of Massachusetts Amherst.
</div>
<div id="ref-Solway2014OptimalBH" class="csl-entry">
Solway, Alec, Carlos Diuk, N. Córdova, Debbie M. Yee, A. Barto, Y. Niv, and M. Botvinick. 2014. <span>“Optimal Behavioral Hierarchy.”</span> <em>PLoS Computational Biology</em> 10.
</div>
<div id="ref-sutton1988learning" class="csl-entry">
Sutton, Richard S. 1988. <span>“Learning to Predict by the Methods of Temporal Differences.”</span> <em>Machine Learning</em> 3: 9–44.
</div>
<div id="ref-Sutton1999BetweenMA" class="csl-entry">
Sutton, Richard S., Doina Precup, and Satinder Singh. 1999. <span>“Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning.”</span> <em>Artificial Intelligence</em> 112 (1): 181–211. https://doi.org/<a href="https://doi.org/10.1016/S0004-3702(99)00052-1">https://doi.org/10.1016/S0004-3702(99)00052-1</a>.
</div>
<div id="ref-Martha2022SubTasks" class="csl-entry">
White, Martha. 2022. <span>“Developing Reinforcement Learning Agents That Learn Many Subtasks.”</span> <em>YouTube</em>. <a href="https://www.youtube.com/watch?v=GmGL9cVfJG4&amp;t=6s">https://www.youtube.com/watch?v=GmGL9cVfJG4&amp;t=6s</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>the probability that option <img src="https://latex.codecogs.com/png.latex?%CF%89"> will terminate at a given state.↩︎</p></li>
<li id="fn2"><p>“the SR can be estimated from samples with temporal-difference learning methods <span class="citation" data-cites="sutton1988learning">(Richard S. Sutton 1988)</span>, where the reward function is replaced by the state occupancy”↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Reinforcement Learning</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Temporal Abstraction</category>
  <category>Options</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/papers/Temp-Abstraction/</guid>
  <pubDate>Sat, 09 Nov 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/papers/Temp-Abstraction/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Emergent Communication of Generalizations</title>
  <link>https://orenbochman.github.io/notes-rl/posts/papers/Emergent-communication/</link>
  <description><![CDATA[ 




<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>I think this is an amazing paper. I read it critically and made copious notes to see what I could learn from it. The paper point out some limitations of Lewis referential games^[In this game the sender sees images, it needs to classify them into some representation and sends a message. The reciever gets the same or similar images + distractors, it needs to run a classifier and needs to select the correct one. Learning an image classifier per agent is expensive and requires access to both an an image classification is likely shared. This however presents a problem…. It allows the agents? ] and suggest a couple of extentions that can over come these limitations. There is</p>
<p><span class="citation" data-cites="mu2022emergentcomms">(Mu and Goodman 2021)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-mu2022emergentcomms" class="csl-entry">
Mu, Jesse, and Noah D. Goodman. 2021. <span>“Emergent Communication of Generalizations.”</span> <em>CoRR</em> abs/2106.02668. <a href="https://arxiv.org/abs/2106.02668">https://arxiv.org/abs/2106.02668</a>.
</div></div></section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>To build agents that can collaborate effectively with others, recent research has trained artificial agents to communicate with each other in Lewis-style referential games. However, this often leads to successful but uninterpretable communication. We argue that this is due to the game objective: communicating about a single object in a shared visual context is prone to overfitting and does not encourage language useful beyond concrete reference. In contrast, human language conveys a rich variety of abstract ideas. To promote such skills, we propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. We find that these games greatly improve systematicity and interpretability of the learned languages, according to several metrics in the literature. Finally, we propose a method for identifying logical operations embedded in the emergent languages by learning an approximate compositional reconstruction of the language.</p>
</section>
<section id="the-video" class="level2">
<h2 class="anchored" data-anchor-id="the-video">The Video</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/LVW_t7p42X0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<p>Here is the paper with my annotation and highlights.</p>
</section>
<section id="annotations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="annotations">Annotations</h2>

<div class="no-row-height column-margin column-container"><div class="">
<blockquote class="blockquote">
<p>To promote such skills, propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. <sup>1</sup></p>
<div id="fn1"><p><sup>1</sup>&nbsp;Significant modification the game: tweak payoffs, assign categories to symbols and allow sending of categories.</p></div></blockquote>
<blockquote class="blockquote">
<p>We argue that the reference games typically used in these studies are ill-suited to drive linguistic <em>systematicity</em> for two reasons <sup>2</sup></p>
<div id="fn2"><p><sup>2</sup>&nbsp;The best the original Lewis signaling game can do is establish a one to one convention between a sender’s siganal of states and reciever action per states. This is just a coordination part of communication.</p></div></blockquote>
<blockquote class="blockquote">
<p>These tasks are more difficult <sup>3</sup></p>
<div id="fn3"><p><sup>3</sup>&nbsp;adding categories can result in a combinatorial increase the total messages. So that the agents need to coordinate on one of many more equilibria. Also you now want the agents to learn a much narrower subset of those possible equilibrium i.e.&nbsp;those that are are faithfull to certain structures in of the states. This is essentially a new problem which could be embodies as a second step after the Lewis game. There is no guarantee in general that such a structure exists. And as the authors suggest other structures are not considered</p></div></blockquote>
<blockquote class="blockquote">
<p>In the <strong>set reference</strong> (setref) game, a teacher must communicate to a student not just a single object, but rather a group of objects belonging to a concept <sup>4</sup></p>
<div id="fn4"><p><sup>4</sup>&nbsp;this is an interesting game - and also similar to <span class="cn">add reference</span> one modifies the game to refer to set of states by adding and, or and not operators giving an agent an basic reasoning ability. The new signaling systems allows specifying many more states. This can be useful in many applications. Of course learning to send additional operators becomes trivial conceptually if not in the practical sense</p></div></blockquote>
<blockquote class="blockquote">
<p>These tasks are more difficult than traditional reference games <sup>5</sup></p>
<div id="fn5"><p><sup>5</sup>&nbsp;a “concept” like a red triangle is a specific type of a set. so this should be a easier task than the set reference. The difficulty seems to be not in the language or the categories but in the added classification of varied visual representation of seagulls</p></div></blockquote>
</div></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf#page=1" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>
<hr>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf#page=2" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


 ]]></description>
  <category>signaling games</category>
  <category>stub</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/papers/Emergent-communication/</guid>
  <pubDate>Tue, 08 Oct 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/papers/Emergent-communication/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The Evolution of Language</title>
  <link>https://orenbochman.github.io/notes-rl/posts/papers/Evolution-of-language/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/Evolution-of-language/cover.jpg" class="img-fluid figure-img"></p>
<figcaption>litrature review</figcaption>
</figure>
</div></div><p>I came accross this paper after reading <span class="citation" data-cites="Skyrms2010signals">(Skyrms 2010)</span> and <span class="citation" data-cites="lewis1969convention">(Lewis 1969)</span> and I was looking at models that give some ideas on how languages might evolve but in particular how parctical constraints might shape the evolution of languages. This paper is one such stepping stone towards a more principled approach to engineering language for RL agents.</p>
<p>This is an early paper on emergence of languages. It quite a bit of a challenge to read. The authors are experts on evolutionary game theory and this is another aspect which is less familiar to me. However there is lots of clever thinking here and the more I ponder this paper the more I get to develop my own ideas and intuitions.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - The Evolution of Language
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/papers/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>Evolution of Language in a nutshell</figcaption>
</figure>
</div>
<p>This paper examines at emergence of languages evolution through the lens of <strong>resilience to errors</strong>. The game introduced by the authors is very similar to the Lewis signaling game, but on close inspection it may lead to much richer outcomes and if my intuition is correct, this formulation may have advantages for Deep learning and RL. While the results on the role of error is central to the paper, e.g.&nbsp;showing how limited sets of phonemes (30) are better then the full inventory (300+). My main take away is that we should add an error parameters to signaling games. They errors can lead to more than just robust equilibria, they seem to have self organizing effects. I also think that the learning of self organizing mappings is the key to development of more generalization and compositionality in signaling systems. So the big take aways are dont be afraid of errors in communication, they are a natural part of the process of language evolution. And that <span class="citation" data-cites="nowak1999evolution">(Nowak and Krakauer 1999)</span> has a formulation that is potentially more general then the Lewis signaling game and friendly to RL, Gradient Descent, Probabilistic methods as well as Information theory. Also at a glance the three matrix product in the formulas below looks like it can be used in a transformer network which can evaluate them in parallel with or without positional encodings.</p>
<p>Going over the paper kept generating many more insights then the material in the actual paper. I’ll try and keep my own ideas in callouts.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Some challenges in reading
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>The paper is not an easy read.</li>
<li>Many details are omitted, but later intimated in the endless captions of the figures. If you read it over a number of times it becomes possible to piece together more of the details. And some follow up papers also fill in additional details.</li>
<li>The mathematical approach seems cryptic. It is not aligned to RL or or Lewis Games in Game theory. It is based on papers and text on Evolutionary game theory which I am less familiar with.</li>
</ol>
</div>
</div>
<section id="abstract" class="level3">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<blockquote class="blockquote">
<p>The emergence of language was a defining moment in the evolution of modern humans. It was an innovation that changed radically the character of human society. Here, we provide an approach to language evolution based on evolutionary game theory. We explore the ways in which proto-languages can evolve in a nonlinguistic society and how specific signals can become associated with specific objects. <mark>We assume that early in the evolution of language,errors in signaling and perception would be common. We model the probability of misunderstanding a signal and show that this limits the number of objects that can be described bya protolanguage. This “error limit” is not overcome by employing more sounds but by combining a small set of more easily distinguishable sounds into words</mark>. The process of “word formation” enables a language to encode an essentially unlimited number of objects. Next, we analyze how words can be combined into sentences and specify the conditions for the evolution of very simple grammatical rules. We argue that grammar originated as a simplified rule system that evolved by natural selection to reduce mistakes in communication.Our theory provides a systematic approach for thinking about the origin and evolution of human language.</p>
<p>— <span class="citation" data-cites="nowak1999evolution">(Nowak and Krakauer 1999)</span></p>
</blockquote>
</section>
<section id="the-review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-review">The Review</h2>
<section id="the-evolution-of-signalobject-associations." class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-evolution-of-signalobject-associations.">The Evolution of Signal–Object Associations.</h3>
<p>This authors describes a setup that is very similar to Lewis signaling game. Each agent is both a sender and a receiver. They payoffs are also made symmetric as they get the averaged expected payoffs for each individual. I.e. each might have a different expected payoffs (one might be a poor sender but a good reciever) The other might be great at both. But since the payoffs are averaged, the game becomes symmetric and therefore cooperative!</p>
<ol type="1">
<li><p>Each agents <img src="https://latex.codecogs.com/png.latex?A_i"> is initially assigned a randomized language <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D_i=%3CP,Q%3E"> . There are two agents per interaction but many agents in the population. Payoffs are symmetric but with can communicate better get a higher payoffs. So this game also combines cooperation and competition.</p></li>
<li><p>I think that instead of working with binary permutations matrices the agents use probability matrices. These are continuous and may be easier to optimize using gradient descent.</p></li>
<li><p>For RL - we can have a richer reward signal. (Though we could also get it for the lewis signaling) It is the mean of each agent’s expected communication success with the other agents.</p></li>
<li><p>In terms of modeling it demonstrates that we can combine</p>
<ul>
<li>a cooperative communication paradigm with (symmetric payoff)</li>
<li>a competitive evolutionary paradigm (zero sum game)</li>
</ul></li>
<li><p>The formulation is probabilistic.</p>
<ul>
<li>The output matrix can be interpreted as a probabilistic mapping and analyzed by by expanding the terms as nested sums of the products of the input matrices.<br>
</li>
<li>The notion of using a phonemic uncertainty matrix is very interesting. It is a simple way to model the uncertainty in the mapping between signals and states. And is mapped back to the similarity of the phonemes.</li>
<li>It could be extended to add salience, signaling risk levels, if they are formulated in terms of probabilities. By multiplying additional matrices.</li>
<li>We could also use a diagonal block matrix to model normal subgroup to model subspace of the state space. Which would maker the</li>
<li>We might be able to add Welch-Hadamard blocks in the diagonal to create entangled error correcting subspace. This would allow us to model bound states in the state space. These are embedding subspace.</li>
<li>Finally I think that we might also use Self-Organizing maps to learn the subspace structure of the state space and use it to find a matching compositional communication structures.</li>
</ul></li>
</ol>
<p>What remains to be seen is if it leads to more desireable set of equilibria that are easier to learn, more robust to perturbation and other desiderata that enhance a signaling system into a language.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Some challenges in the approach
</div>
</div>
<div class="callout-body-container callout-body">
<p>The agents are not learning through communications but rather being selected in proportion to their fitness (Expected communication ability). Which means the agent with the best ability to communicate with most other agents will dominate the population, rather than the agent with the best signaling system.</p>
<p>If we dig a bit deeper. If there are two types of agents - SS_i agents which a maximally incompatible signaling system<sup>1</sup> - CP_i which uses a completely pooling system (one that ignores the input and randomizes the output)</p>
<p>then we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?0=F(SS_i,SS_j)%20%3C%20F(CP,SS_i)%20=%201/n%20%5Cqquad%20%5Cforall%20i">.</p>
<p>i.e.&nbsp;guessing is better then perfect miss-coordination.</p>
<p>Of course this kind of assignment is unlikely to come up in a random assignment. But it indicates that perfect signaling systems are less likely to evolve if they must compete with many more imperfect ones with mutual that can partially coordinate with more agents.</p>
<p>I.e. agents with poor multilingual capabilities will have a comparative advantage over agents with monolingual abilities if the monolingual ones are sufficiently sparse.</p>
<p>if we swap out the CP agent with a PP agent (partial pooling) the</p>
<p>I guess this issue could be mitigated by having very many more agents then equilibria. But as signaling systems are grow as <img src="https://latex.codecogs.com/png.latex?N!"> and they are the sparse in the possible equilibria, this is not feasible for large signaling systems.</p>
<p>To explore these notions we need an efficient learning algorithm that can learn the equilibria that are also robust to errors. My Bayesian Adaptive RL algorithm can handle errors, so it might be a good start. However I have yet to consider it complexity in the number of agents and signals.</p>
<p>A new algorithm in the works that is also aware of actions of normal subgroups to structure state into subspaces is likely to be more efficient to learn and to scale better.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
basic Evolutionary language game vs Lewis signaling game
</div>
</div>
<div class="callout-body-container callout-body">
<p>The paper makes no mention of the Lewis signaling game and only cites sources on evolutionary game theory, population dynamics and biological signaling. But the basic evolutionary language game is very similar to the Lewis signaling game. I have not analysed it but from the text in figure 1, I think it has the same types of equilibria.</p>
<p>I also simulated some algorithms in which agents had (P,Q) and (P’,Q’) belief or urn matracies when learning the Lewis signaling game. But my algorithms were RL and Bayesian RL.</p>
<p>What seems differrnt from the lewis is that the agents have different Languages L=(P,Q) and L’=(P’,Q’) and they evolve through population dynamics. But the agent’s intial linguistic endowment is not necessarily optimal. I.e. we are not told <img src="https://latex.codecogs.com/png.latex?P*Q%20=%201"> rather that <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20p_%7Bij%7D%20=1"> they have random values. In such a setup it unclear if evolution will lead to a separating equilibrium or just reinforce the initial bias.</p>
</div>
</div>
<p>In the basic <strong>evolutionary language game</strong> with</p>
<ul>
<li>m sounds (signals)</li>
<li>n objects</li>
<li>The <strong>active matrix</strong> P with entries <img src="https://latex.codecogs.com/png.latex?p_%7Bij%7D">, denoting the probability that for a speaker that an object <img src="https://latex.codecogs.com/png.latex?i"> is associated with sound <img src="https://latex.codecogs.com/png.latex?j">.</li>
<li>A <strong>passive matrix</strong> Q contains the entries <img src="https://latex.codecogs.com/png.latex?q_%7Bji%7D">, that denote the probability that for a listener that a sound <img src="https://latex.codecogs.com/png.latex?j"> is associated with object <img src="https://latex.codecogs.com/png.latex?i">.</li>
</ul>
<p>It is not clear how an agent uses the active matrix to produce a signal given a some object j.</p>
<ul>
<li>Do they pick a signal i with probability <img src="https://latex.codecogs.com/png.latex?p_%7Bij%7D"> - this is a Bayesian interpretation of the active matrix witch each agent having a subjective Language <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D">.</li>
<li>Do they use a MAP estimate, i.e.&nbsp;pick the most likely signal, - this approach is closest to the Lewis signaling game. This is a deterministic interpretation of the active matrix.</li>
<li>Do they they send all signals at once weighted by the <img src="https://latex.codecogs.com/png.latex?p_%7Bij%7D">?</li>
</ul>
<p>I prefer 1 as it is how I tend to simulate it.</p>
<blockquote class="blockquote">
<p>Suppose <em>A</em> sees object <em>i</em> and signals, then <em>B</em> will infer object <em>i</em> with probability <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bj=1%7D%5Em%20p_%7Bij%7D%20q_%7Bji%7D"></p>
</blockquote>
<blockquote class="blockquote">
<p>The overall payoff is symmetric for communication between A and B is taken as the average of A’s ability to convey information to B, and B’s ability to convey information to A.</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BE%7D%5B%5Ctext%7BPayoffs%7D%5Cmid%20L,L'%5D%20&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%20%5Coverbrace%7B%5Csum_%7Bi=1%7D%5En%20%5Csum_%7Bj=1%7D%5Em%20p_%7Bij%7D%20q'_%7Bij%7D%7D%5E%7B%5Ctext%7BA's%20ability%20to%20send%20interpretable%20messages%7D%7D%20%20+%20%5Cfrac%7B1%7D%7B2%7D%20%20%5Coverbrace%7B%5Csum_%7Bi=1%7D%5En%20%5Csum_%7Bj=1%7D%5Em%20p'_%7Bji%7D%20q_%7Bji%7D%7D%5E%7B%5Ctext%7BB's%20ability%20to%20interpret%20messages%7D%7D%20%5C%5C&amp;=%0A%5Cfrac%7B1%7D%7B2%7D%20%20%5Csum_%7Bi=1%7D%5En%20%5Csum_%7Bj=1%7D%5Em%20(%20p_%7Bij%7D%20q'_%7Bij%7D%20+p'_%7Bji%7D%20q_%7Bji%7D%20)%20=%0A%5Cend%7Balign*%7D%0A"></p>
<p>The original formula is rather cryptic. So this version breaks it into parts and annotates it.</p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?L%20=%20%3CP,Q%3E"> is the language of speaker and</li>
<li><img src="https://latex.codecogs.com/png.latex?L'=%20%3CP',Q'%3E"> is the language of the listener.</li>
</ul></li>
</ul>
<p>This seems like an expectation of the joint probability of the speaker and listener.</p>
<p>so far this seems to be very much aligned with the Lewis signaling game.</p>

<div class="no-row-height column-margin column-container"><div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/papers/Evolution-of-language/fig_1.png" class="img-fluid figure-img" data-group="figures" width="250">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: emergence of a language by population dynamics
</figcaption>
</figure>
</div></div><p>Figure 1 shows how a signaling system emerges through population dynamics.</p>
<p>Notes:</p>
<ol type="1">
<li>the agents don’t actually learn during the evolution, rather generate offsprings in proportion to their fitness. So with a bit of luck one agent will eventually dominate the population and all the other types will die out.</li>
<li>Agents with lower fitness are replaced by agents with higher fitness. This means that we are just reinforcing the initial bias towards the most central agent in the cluster as it will have the highest fitness.</li>
<li>Due to the layouts of the figures, I always think that the agents were on a grid talking with their neighbors, but I believe they are on a simplex and all talk to each other.</li>
</ol>
<p>The agents evolve a language that</p>
<p>The paper again lacks some important details. Are the agents the details</p>
</section>
<section id="word-formation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="word-formation">Word Formation</h3>

<div class="no-row-height column-margin column-container"><div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/papers/Evolution-of-language/fig_2.png" class="img-fluid figure-img" data-group="figures" width="250">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: figure 2
</figcaption>
</figure>
</div></div><p>If we increase our basic signals (think phonemes) we can handle more states. However phonemes exist in a restricted space and as more are added it becomes harder to distinguish between them. This is exacerbated by the fact that we add an explicit chance of communications error based on the phonemic similarity.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BE%7D%5B%5Ctext%7BPayoffs%7D%5Cmid%20L,L'%5D%20&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%20%5Csum_%7Bi=1%7D%5En%20%5Csum_%7Bj=1%7D%5Em%20%5Cleft%20%5B%20%20p_%7Bij%7D%20%5Cleft(%20%5Csum_%7Bj=1%7D%5Em%20u'_%7Bjk%7Dq'_%7Bki%7D%20%5Cright)%20+%20p'_%7Bij%7D%20%5Cleft(%20%5Csum_%7Bj=1%7D%5Em%20u_%7Bjk%7Dq_%7Bki%7D%20%5Cright)%20%5Cright%5D%20=%0A%5Cend%7Balign*%7D%0A"></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?U"> are matrices with <img src="https://latex.codecogs.com/png.latex?u_%7Bij%7D=s_%7Bij%7D%5Csum_%7Bk=1%7D%5Em%20s_%7Bik%7D"> and</li>
<li><img src="https://latex.codecogs.com/png.latex?s_ij"> is the similarity between sounds (signals) <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j">.</li>
</ul>
</section>
<section id="the-evolution-of-basic-grammatical-rules." class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-evolution-of-basic-grammatical-rules.">The Evolution of Basic Grammatical Rules.</h3>

<div class="no-row-height column-margin column-container"><div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/papers/Evolution-of-language/fig_3.png" class="img-fluid figure-img" data-group="figures" width="250">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: figure 3
</figcaption>
</figure>
</div><div id="fig-4" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/papers/Evolution-of-language/fig_4.png" class="img-fluid figure-img" data-group="figures" width="250">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: figure 4
</figcaption>
</figure>
</div></div>
</section>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<p><a href="https://www.pnas.org/doi/full/10.1073/pnas.96.14.8028">the source</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>embeded paper</figcaption>
</figure>
</div>
</section>
<section id="some-ideas" class="level2">
<h2 class="anchored" data-anchor-id="some-ideas">Some ideas</h2>
<p>I spent a large amount of time to see how the game is related to the Lewis signaling game.</p>
<p>What I slowly came to realize is that the formulation in this paper is potentially more general then the Lewis signaling game.</p>
<p>What I understood is that in the Lewis signaling agents are trying to learning invertible mapping from an to signal space.</p>
<p>In this as far as I can tell, the agents are ‘modeling’ such mappings using mixtures (of states or signals). If the Lewis signaling uses binary matrices to model connections between signals and states, this formulation is uses continuous random variables to model the connections. We can go to a binary matrix by using the MAP estimate of the mixture.</p>
<p>I found this formulation is rather annoying at first glance but I stated to see its potential in the second variant. In this extension agents also apply a mapping that corresponds to phonemic uncertainty. This can also be viewed as a noisy filter. Anyhow they end up with a game with more complex mappings in which similar signals are more likely to lead to the wrong state.</p>
<p>It is a very simple way to model the uncertainty in the mapping between signals and states. One of the advantages of this approach is that it can be expanded to add saliency, risk levels that can allow us to understand how such constraints shape natural language.</p>
<p>Based on the similarity of the phonemes, the agents can make errors in communication. This is a very interesting idea that I have not seen before. It is a very simple way to model the uncertainty in the mapping between signals and states.</p>
<p>In this the agents compose a mapping corresponding to phonemic uncertainty. Based on the similarity of the phonemes, the agents can make errors in communication. This is a very interesting idea that I have not seen before. It is a very simple way to model the uncertainty in the mapping between signals and states. . This is a very interesting idea that I have not seen before. It is a very simple way to model the uncertainty in the mapping between signals and states.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-lewis1969convention" class="csl-entry">
Lewis, David Kellogg. 1969. <em>Convention: A Philosophical Study</em>. Harvard University Press. <a href="https://books.google.co.il/books?id=_dLWAAAAMAAJ">https://books.google.co.il/books?id=_dLWAAAAMAAJ</a>.
</div>
<div id="ref-nowak1999evolution" class="csl-entry">
Nowak, Martin A, and David C Krakauer. 1999. <span>“The Evolution of Language.”</span> <em>Proceedings of the National Academy of Sciences</em> 96 (14): 8028–33.
</div>
<div id="ref-Skyrms2010signals" class="csl-entry">
Skyrms, Brian. 2010. <em><span class="nocase">Signals: Evolution, Learning, and Information </span></em>. Oxford University Press. <a href="https://doi.org/10.1093/acprof:oso/9780199580828.003.0013">https://doi.org/10.1093/acprof:oso/9780199580828.003.0013</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>this is a new idea. IF we look at the n! permutations matrices we will generally find a shared signal state mapping. But if we restrict to one permutation matrix say I and shit it down once we get no two mappings with a match. e.g.&nbsp;[1,2,3] and [2,3,1] and [3,2,1] are a maximally incompatible set for n = 3.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Signaling Games</category>
  <category>Emergent Languages</category>
  <category>Evolutionary game theory</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/papers/Evolution-of-language/</guid>
  <pubDate>Wed, 17 Apr 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/papers/Evolution-of-language/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Policy Gradient</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w4.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div id="fig-episodic-semi-gradient-sarsa" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-episodic-semi-gradient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithm decision tree</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-episodic-semi-gradient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The algorithms discussed in this lesson are all part of the policy gradient family. These allow us to consider both discrete and continuous actions in the the average rewards settings. We will consider Softmax Actor-Critic, Gaussian Actor-Critic, and the REINFORCE algorithm. The last is missing from the chart.
</figcaption>
</figure>
</div></div>
<p>Sometimes, the behavior codified in the policy is much simpler than the action value function. Thus, learning the policy directly can be more efficient. Learning policies is an end-to-end solution for solving many real-world RL problems. Coding such end-to-end solutions may be done under the umbrella of policy gradient methods. Once we cover the policy gradient theorem, we will see how we still need to use action value approximations to estimate the gradient of the average reward objective. A second way that we will use value function approximations is in the actor-critic algorithms. Here, the policy is called the actor, and the value function is called the critic. The critic evaluates the policy, and the actor is used to update the policy. The actor-critic algorithms are a hybrid of policy gradient and value function methods. They are more stable than policy gradient methods and can be used in more complex environments.</p>

<div class="no-row-height column-margin column-container"><div id="fig-policy-gradient-theorem-proof" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-theorem-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-13-pg-theorem.png" class="img-fluid figure-img"></p>
<figcaption>the policy gradient theorem</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-gradient-theorem-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: The policy gradient theorem &amp; proof from <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement pp.325]</span> In it <img src="https://latex.codecogs.com/png.latex?%5Cnabla"> are with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the parameter of the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> but is omitted for brevity.
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR This lesson in a nutshell
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>in a nutshell</figcaption>
</figure>
</div>
<p>I found policy gradient methods to be rather complicated at first blush. Eventually, as the finer points sunk in, I developed bits and pieces of intuition that made me feel more comfortable.</p>
<p><strong>Parametrized policies</strong> are much easier to conceptualize than <em>parametrized value functions</em>. One intuition is that the parameters are weights for the action, and the policy’s actions are drawn in proportion to these weights. The <strong>softmax policy</strong> does precisely this type of weighting. My Bayesian-trained intuition for these weights comes from the <em>categorical distribution</em> <sup>1</sup>. For this distribution, we can define <strong>success</strong> as the action that gets my agent closest to its goal. This intuition is just my first-order mental model; we will develop more sophisticated machinery as we go along.</p>
<p>The obvious question that will arise as soon as you deal with some environment is:</p>
<blockquote class="blockquote">
<p>“How can we get some <em>arbitrary</em> feature <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to participate in the parametrized policy?”</p>
</blockquote>
<p>The answer that comes to mind is <em>use it to build the weights</em>.</p>
<p>The usual suspect is a (Bayesian) linear regression that includes the feature .</p>
<blockquote class="blockquote">
<p>How is my feature <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> going to participate in the decisions made by <img src="https://latex.codecogs.com/png.latex?%5Cpi">?</p>
</blockquote>
<p>If we tweak this feature, how will the policy give us better returns? Since we want to maximize returns, we should adjust the weights in the direction that provides us with the best returns.</p>
<p>That is also the intuition for a parametrized policy’s <strong>update rule</strong>. The direction is just the gradient of the policy. The big hurdle lies in estimating this gradient.</p>
<p>The course material is very concise and laser-focused on very specific learning goals. <strong>The policy gradient theorem</strong> is the key result that allows us to estimate policy gradients. Unfortunately, the course instructors did not cover the proof, which is silently relegated to the readings.</p>
<p>This proof is not as simple as represented in its preamble in the book. I saw both longer and shorter versions of proofs and felt that there is a bit of hand waving in one of the steps<sup>2</sup>. Many people who are seriously interested in RL will be compelled to go through the proof in detail. Also, more experienced students can make greater leaps.</p>
<p>One of my goals is to make satisfactory proofs for episodic and continuing cases that I can walk though at ease.</p>
</div>
</div>
<p>Also, I was disappointed that this course does not cover more modern algorithms, such as <a href="https://arxiv.org/abs/1502.05477">TRPO</a>, <a href="https://arxiv.org/abs/1707.06347">PPO</a>, or other Deep learning algorithms. I cannot stress this point enough.</p>
<p>In the last video in the previous lecture’s notes by <a href="https://scholar.google.com/citations?user=8RgDBoEAAAAJ&amp;hl=en">Satinder Singh</a>, all of the research on using Meta gradients to learn intrinsic rewards is also built on top of policy gradient methods - where he and his students looked at propagating these gradients through multiple the planing algorithms and later through the learning algorithm to learn a reward function and tackle the issues of exploration.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Course Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§13 pp. 321-336]</span> Figure&nbsp;18</label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Extra Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Extra resources I found useful to review though not required, nor part of the course material</p>
<ul>
<li><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html">Intro to Policy Optimization @ Spinning Up</a></li>
<li><a href="https://johnwlambert.github.io/policy-gradients/">Understanding Policy Gradients</a></li>
<li><a href="https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146">Policy Gradient Explained</a></li>
<li><a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/">Going Deeper Into Reinforcement Learning: Fundamentals of Policy Gradients</a></li>
<li><a href="https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/">Notes on the Generalized Advantage Estimation Paper</a></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Videos
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">



<p>Here are three extra videos by experts in the field that delve deeper into this topic. Each of these instructors have published papers with some of the most groundbreaking algorithms in the field and have a lot of insights to share.</p>
<ol type="1">
<li><p>In this lecture <a href="http://joschu.net/">John Schulman</a> covers Deep Reinforcement Learning Policy Gradients and Q-Learning. John Schulman is a research scientist at OpenAI and has published many papers on RL and Robotics. John Schulman who developed PPO and TRPO and Chat-GPT</p></li>
<li><p>In this lecture <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> covers policy gradients and advantage estimation. Pieter Abbeel is a professor at UC Berkeley and has published many papers on RL and Robotics.</p></li>
<li><p>In this lesson from a Deep Mind Course <a href="https://hadovanhasselt.com/about/">Hado van Hasselt</a> covers some advantages as well as challenges of policy gradient methods.</p></li>
</ol>
</div>
</div>
</div><div class="no-row-height column-margin column-container"><div id="fig-deep-reinforcement-learning" class="quarto-float quarto-figure quarto-figure-center anchored callout-4-contents callout-collapse collapse callout-margin-content">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deep-reinforcement-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/PtAIh9KSnjo" title="Deep Reinforcement Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-deep-reinforcement-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Talk titled ‘Deep Reinforcement Learning Policy Gradients and Q-Learning’ by John Schulman on Reinforcement Learning at at the Deep Learning School on September 24/25, 2016
</figcaption>
</figure>
</div><div id="fig-policy-gradients-and-advantage-estimation" class="quarto-float quarto-figure quarto-figure-center anchored callout-4-contents callout-collapse collapse callout-margin-content">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradients-and-advantage-estimation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/AKbX1Zvo7r8" title="Policy Gradients and Advantage Estimation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-gradients-and-advantage-estimation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Talk titled ‘Policy Gradients and Advantage Estimation’ by Pieter Abbeel. in his ‘Foundations of Deep RL Series’
</figcaption>
</figure>
</div><div id="fig-policy-grad-limitations" class="quarto-float quarto-figure quarto-figure-center anchored callout-4-contents callout-collapse collapse callout-margin-content">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-grad-limitations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/y3oqOjHilio" title="Policy-Gradient and Actor-Critic methods" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-grad-limitations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Research Scientist Hado van Hasselt from Deep Mind covers policy algorithms that can learn policies directly and actor critic algorithms that combine value predictions for more efficient learning. From DeepMind x UCL | Deep Learning Lecture Series 2021
</figcaption>
</figure>
</div></div>
<section id="lesson-1-learning-parameterized-policies" class="level1 page-columns page-full">
<h1>Lesson 1: Learning Parameterized Policies</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand how to define policies as parameterized functions #</label></li>
<li><label><input type="checkbox" checked="">Define one class of parameterized policies based on the softmax function #</label></li>
<li><label><input type="checkbox" checked="">Understand the advantages of using parameterized policies over action-value based methods #</label></li>
</ul>
</div>
</div>
<section id="learning-policies-directly-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="learning-policies-directly-video">Learning policies directly (Video)</h2>

<div class="no-row-height column-margin column-container"><div id="fig-policy-gradient" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-01.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Energy pumping policy</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: In the mountain car environment, the parameterized value function is complex, but the parameterized policy is simple.
</figcaption>
</figure>
</div></div><p>In this lesson course instructor Adam White introduces the idea of learning policies directly. He contrasts this with learning value functions and explains why learning policies directly can be more flexible and powerful.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Rethinking policies
</div>
</div>
<div class="callout-body-container callout-body">
<p>Moving on we will need to think very clearly about policies.</p>
<p>To this end it is worth spending a minute to quickly recap the definition properties and notation of a policy from the previous lessons:</p>
<ol type="1">
<li><p><em>Intuitively</em> a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is just decision making rule.</p></li>
<li><p>A <em>deterministic policy</em> is just a function that maps a state to an action. <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%20:%20s%5Cin%20%5Cmathcal%7BS%7D%20%5Cto%20a%20%5Cin%20%5Cmathcal%7BA%7D%20%5Cqquad%20%5Ctext%7B(deterministic%20policy)%7D%0A"></p></li>
<li><p>A stochastic policy is a function that maps a state to a probability distribution over actions. Stochastic policies are more general and include deterministic policies as a special case. So while we may talk of deterministic policies, we will use the mathematical form of a stochastic policy.</p></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%20:%20s%5Cin%20%5Cmathcal%7BS%7D%20%5Cto%20%5Cmathbb%7BP%7D(%5Cmathcal%7BA%7D)%20%5Cqquad%20%5Ctext%7B(stochastic%20policy)%7D%0A"></p>
<ol type="1">
<li>Formally, the policy is defined probabilistically as follows:</li>
</ol>
<p><span id="eq-policy-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%5Cmid%20s)%20%5Cdoteq%20Pr(A_t%20=%20a%20%5Cmid%20S_t%20=%20s)%20%5Cqquad%20%5Ctext%7B(policy)%7D%0A%5Ctag%7B1%7D"></span></p>
<ol type="1">
<li>note that this is a shorthand for the following:</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%5Cmid%20s)%20=%20%5Cmathbb%7BE%7D%5BA_t%20%5Cmid%20S_t%20=%20s%5D%20%5Cqquad%20%5Ctext%7B(policy)%7D%0A"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a probability distribution over actions given a state.</p>
</div>
</div>
</section>
<section id="sec-l1g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g1">How to Parametrize a Policies?</h2>

<div class="no-row-height column-margin column-container"><div id="fig-policy-gradient-param" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-param-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-02.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>policy parametrization</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-gradient-param-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: When we parametrize a policy we will use the greek letter <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> to denote the parameters of the policy.
</figcaption>
</figure>
</div><div id="fig-policy-gradient-constraint" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-constraint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-03.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>policy parametrization constraints</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-gradient-constraint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Constraints on the policy parameters can be used to ensure that the policy is valid.
</figcaption>
</figure>
</div></div>
<p>So far we have been mostly looking at learning value functions. But when it comes to function approximation, it is often simpler to learn a policy directly.</p>
<p>In the mountain car environment we see the power pumping policy which accelerates the car in the direction it is moving. This is a near optimal policy for this environment. The policy is simple and can be learned directly and it makes no use of value functions. This may not always be the case.</p>
<p>A visual summary of the policy parametrization is shown in the figure. Recall that the policy is a function that takes in a state and outputs a probability distribution over actions. We will use the greek letter <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> to denote the parameters of the policy. This way we can reference the parameters of <img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D(s,a,w)"> the action value function are denoted by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D">.</p>
<p>The parametrized policy is defined as follows:</p>
<p><span id="eq-parametrized-policy-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%5Cdoteq%20Pr(A_t%20=%20a%20%5Cmid%20S_t%20=%20s,%20%5Ctheta)%20%5Cqquad%20%5Ctext%7B(parametrized%20policy)%7D%0A%5Ctag%7B2%7D"></span></p>
<p>is a probability distribution over actions given a state and the policy parameters.</p>
<p>Since we are dealing with probabilities, the policy parameters must satisfy certain constraints. For example, the probabilities must sum to one. This is shown in the figure. These policy parameters constraints will ensure that the policy is valid.</p>
<p>Policy Gradient use gradient ascent:</p>
<p><span id="eq-gradient-ascent"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7Bt+1%7D%20=%20%5Ctheta_t%20+%20%5Calpha%20%5Cnabla_%5Ctheta%20J(%5Ctheta)%20%5Cqquad%20%5Ctext%7B(gradient%20ascent)%7D%0A%5Ctag%7B3%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is the step size and <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta%20J(%5Ctheta)"> is the gradient of the objective function <img src="https://latex.codecogs.com/png.latex?J(%5Ctheta)"> with respect to the policy parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<ul>
<li>methods that follow this update rule are called <strong>policy gradient methods</strong>.</li>
<li>methods that learn both a value function and a policy are called <strong>actor-critic methods</strong>.</li>
</ul>
</section>
<section id="sec-l1g2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g2">Define one class of parameterized policies based on the softmax function</h2>

<div class="no-row-height column-margin column-container"><div id="fig-policy-softmax" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-softmax-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-04-softmax-policy.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>softmax properties</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-policy-softmax-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9
</figcaption>
</figure>
</div></div><p>The <strong>Softmax policy</strong> based on the Boltzmann distribution is a probability distribution over actions given a state. It is parameterized by a vector of action preferences <img src="https://latex.codecogs.com/png.latex?h(s,%20a,%20%5Ctheta)">.</p>
<p><span id="eq-softmax-policy"><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%5Cdoteq%20%5Cfrac%7Be%5E%7Bh(s,%20a,%20%5Ctheta)%7D%7D%7B%5Csum_%7Bb%5Cin%20%5Cmathcal%7BA%7D%7D%20e%5E%7Bh(s,%20b,%20%5Ctheta)%7D%7D%20%5Ctext%7B(softmax%20policy)%7D%20%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<ul>
<li>the numerator is the exponential of the action preference</li>
<li>the denominator is the sum of the exponentials of all action preferences</li>
</ul>
<p>Some properties of the softmax policy are that it can take in a vector of weights for different actions and output a probability distribution over actions. A second property is that the softmax policy generalizes the max function. A third property is that unlike the max function which is discontinuous the softmax policy is differentiable, making it amenable to gradient-based optimization.</p>
<ul>
<li>negative values of h lead to positive action probabilities.</li>
<li>equal values of h lead to equal action probabilities.</li>
<li>the softmax policy is a better option over than the <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy policy over the action-value based methods.</li>
</ul>
</section>
<section id="advantages-of-policy-parameterization-video" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-policy-parameterization-video">Advantages of Policy Parameterization (Video)</h2>
<p>In this video we consider the advantages of using parameterized policies over action-value based methods. We will see that parameterized policies are more flexible than action-value based methods and can start off stochastic and then become deterministic.</p>
</section>
<section id="sec-l1g3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g3">Advantages of using parameterized policies over action-value based methods</h2>

<div class="no-row-height column-margin column-container"><div id="fig-policy-action-preferences" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-action-preferences-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-05-action-preferences.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>softmax policy v.s. epsilon-greedy</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-action-preferences-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Softmax policy v.s. <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy
</figcaption>
</figure>
</div><div id="fig-short-corridor-env" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-short-corridor-env-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-06-stochastic-policy.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Short corridor with switched action</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-short-corridor-env-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: In the Short corridor with switched action environment a deterministic policy fails to reach the goal. The only optimal policy is stochastic.
</figcaption>
</figure>
</div></div>
<blockquote class="blockquote">
<p>One advantage of parameterizing policies according to the softmax in action preferences is <strong>that the approximate policy can approach a deterministic policy</strong>, whereas with <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy action selection over action values there is always an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> probability of selecting a random action.</p>
</blockquote>
<blockquote class="blockquote">
<p>A second advantage of parameterizing policies according to the softmax in action preferences is that <strong>it enables the selection of actions with arbitrary probabilities</strong>. In problems with significant function approximation, the best approximate policy may be stochastic.</p>
</blockquote>
<p>For example, in card games with imperfect information the optimal play is often a mixed strategy which means you should take two different actions each with a specific probability, such as when bluffing in Poker.</p>
<p>Action-value methods have no natural way of finding stochastic optimal policies, whereas policy approximating methods can, as shown in The short Corridor environment</p>
</section>
</section>
<section id="lesson-2-policy-gradient-for-continuing-tasks" class="level1 page-columns page-full">
<h1>Lesson 2: Policy Gradient for Continuing Tasks</h1>
<ul>
<li>parameterized policies are more flexible than action-value based methods</li>
<li>can start off stochastic and then become deterministic</li>
</ul>
<p>In function approximation, the optimal policy is not necessarily deterministic. Thus it is best to be able to learn stochastic policies.</p>
<p>Example where the optimal policy is stochastic:</p>
<ul>
<li>Sometimes it is just easier to learn a stochastic policy.</li>
<li>E.g. in mountain car, the parameterized value function is complex, but the parameterized policy is simple.</li>
</ul>

<div class="no-row-height column-margin column-container"><div id="fig-mountain-car-policy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-07-stochastic-simpler.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>mountain car environment values and policy</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: In mountain car, the parameterized value function is complex, but the parameterized policy is simple.
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Describe the objective for policy gradient algorithms #</label></li>
<li><label><input type="checkbox" checked="">Describe the results of the policy gradient theorem #</label></li>
<li><label><input type="checkbox" checked="">Understand the importance of the policy gradient theorem #</label></li>
</ul>
</div>
</div>
<section id="the-objective-for-learning-policies-video" class="level2">
<h2 class="anchored" data-anchor-id="the-objective-for-learning-policies-video">The Objective for Learning Policies (Video)</h2>
<p>In this video Martha White dive deep into the objective for policy gradient algorithms. She then contrasts this with the gradient of the value fn objectives and lists some challenges for estimating the gradient of the average reward objective. We will learn that we can use the policy gradient theorem and to overcome these challenges and derive a gradient with an neat update rule .</p>
</section>
<section id="sec-l2g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g1">The objective for policy gradient algorithms</h2>
<p>Formalizing the goal as an Objective</p>
<p><span id="eq-objectives"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AG_t%20&amp;=%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%20R_%7Bt%7D%20%20%5Cquad%20&amp;&amp;%20%5Ctext%7B(episodic)%7D%20%5Cnewline%0AG_t%20&amp;=%20%5Csum_%7Bt=0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Et%20R_%7Bt%7D%20%20%20%5Cquad%20&amp;&amp;%20%5Ctext%7B(continuing%20-%20discounted%20reward)%7D%20%5Cnewline%0AG_t%20&amp;=%20%5Csum_%7Bt=0%7D%5E%7B%5Cinfty%7D%20R_%7Bt%7D%20-%20r(%5Cpi)%20%20%5Cquad%20&amp;&amp;%20%5Ctext%7B(continuing%20-%20avg.%20reward)%7D%0A%5Cend%7Balign*%7D%0A%5Ctag%7B5%7D"></span></p>
<p>The average reward Objective for a policy is as follows: <span id="eq-average-reward-objective"><img src="https://latex.codecogs.com/png.latex?%0Ar(%5Cpi)%20=%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cmu(s)%20%5Csum_%7Ba%7D%20%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)%20r%20%5Cquad%20%5Ctext%7B(avg.%20reward%20objective)%7D%0A%5Ctag%7B6%7D"></span></p>
<p>What does this mean?</p>
<ul>
<li>the last sum is the expected reward for a state-action pair. <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BR_t%20%5Cmid%20S_t%20=%20s%20,%20A_t=a%5D"></li>
<li>the last two sums together are the expected reward for a state under weighted by the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">. <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%5Cpi%5BR_t%20%5Cmid%20S_t%20=%20s%5D"></li>
<li>full sum ads the time we spend in state <img src="https://latex.codecogs.com/png.latex?s"> under <img src="https://latex.codecogs.com/png.latex?%5Cpi"> therefore the expected reward for a state under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and the environment dynamics <img src="https://latex.codecogs.com/png.latex?p">. <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%5Cpi%5BR_t%5D"></li>
</ul>
<p>to optimize the average reward, we need to estimate the gradient of the avg. objective</p>
<p><span id="eq-average-reward-objective-gradient"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20r(%5Cpi)%20=%20%5Cnabla_%5Ctheta%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Ctextcolor%7Bred%7D%7B%5Cunderbrace%7B%5Cmu(s)%7D_%7B%5Ctext%7BDepends%20on%20%7D%5Ctheta%7D%7D%20%5Csum_%7Ba%7D%20%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)%20r%20%5Cqquad%0A%5Ctag%7B7%7D"></span></p>
<ol type="1">
<li>Methods based on this are called policy gradient methods.</li>
<li>We are trying to maximize the average reward.</li>
</ol>
<p>There are a few challenges with using the gradient in the above equation:</p>
<p>According to the lesson <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)"> depends on <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Martha White point out that this state importance though parameterized only by s actually depends on the the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> which will evolve during its training based on the values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Which means out notation here is a bit misleading. She then contrasts it with the value function gradient is being evaluated using a fixed policy.</p>
<p><span id="eq-value-function-gradient"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla_w%20%5Cbar%7BVE%7D%20&amp;=%20%5Cnabla_w%20%5Csum_%7Bs%7D%5Ctextcolor%7Bred%7D%7B%5Cunderbrace%7B%5Cmu(s)%7D_%7B%5Ctext%7BIndependent%20of%20%7D%5Cmathbf%7Bw%7D%7D%7D%20%20%5BV_%7B%5Cpi%7D(s)-%5Cbar%7Bv%7D(s,w)%5D%5E2%20%5Cnewline%0A&amp;=%5Csum_%7Bs%7D%20%5Ctextcolor%7Bred%7D%7B%5Cmu(s)%7D%20%5Cnabla_w%20%5BV_%7B%5Cpi%7D(s)-%5Cbar%7Bv%7D(s,w)%5D%5E2%0A%5Cend%7Balign*%7D%20%5Ctext%7B(value%20function%20gradient)%7D%20%5Cqquad%0A%5Ctag%7B8%7D"></span></p>
<p>We can avg reward as an objective for policy optimization and the its for the stochastic gradient ascent. Next we will consider how the policy gradient theorem can help us estimate the gradient of the average reward objective despite these setbacks.</p>
</section>
<section id="the-policy-gradient-theorem-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-policy-gradient-theorem-video">The Policy Gradient Theorem (Video)</h2>

<div class="no-row-height column-margin column-container"><div id="fig-policy-gradient-theorem" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-08-gridworld-policy.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Understanding the pg theorem up</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13
</figcaption>
</figure>
</div><div id="fig-policy-gradient-theorem" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-09-gridworld-policy.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Understanding the pg theorem left</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14
</figcaption>
</figure>
</div><div id="fig-policy-gradient-theorem" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-10-gridworld-policy.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Understanding the pg theorem all</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-policy-gradient-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15
</figcaption>
</figure>
</div></div>

<p>In this video course instructor Martha White explains the policy gradient theorem, a key result for optimizing policy in reinforcement learning. The goal is to maximize the average reward by adjusting policy parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> using gradient ascent. The challenge is estimating the gradient of the average reward, which initially involves a complex expression with the gradient of the stationary distribution over states (<img src="https://latex.codecogs.com/png.latex?%5Cmu(s)">).</p>
</section>
<section id="sec-l2g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g2">The results of the policy gradient theorem</h2>
<p>The policy gradient theorem simplifies this by providing a new expression for the gradient. This expression involves summing, over all states and actions, the product of the gradient of the policy and the action-value function (Q). The gradient of the policy indicates how to adjust parameters to increase the probability of an action, while the action-value function represents the expected return from taking that action.</p>
<p>The video illustrates this with a grid world example, showing how gradients for different actions point in different directions. By weighting these gradients with the corresponding action values, the theorem provides a direction to update the policy parameters that increases the probability of high-value actions and decreases the probability of low-value actions.</p>
<p>The product rule <span id="eq-product-rule"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla(f(x)g(x))%20=%20%5Cnabla%20f(x)g(x)%20+%20f(x)%5Cnabla%20g(x)%20%5Cqquad%20%5Ctext%7B(product%20rule)%7D%0A%5Ctag%7B9%7D"></span></p>
<p>therefore:</p>
<p><span id="eq-policy-gradient-naive"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla_%5Ctheta%20r(%5Cpi)%20&amp;=%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cnabla%20%5Cmu(s)%20%5Csum_%7Ba%7D%20%5Cpi(a%20%5Cmid%20s,%5Ctheta)%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)%20r%20%5Cnewline%20&amp;+%20%20%5Csum_%7Bt=0%7D%5E%7BT%7D%20%5Cmu(s)%20%5Cnabla%20%5Csum_%7Ba%7D%20%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)%20r%0A%5Cend%7Balign*%7D%0A%5Ctag%7B10%7D"></span></p>
<p>The first term is the gradient of the stationary distribution and the second term is the gradient of the policy. The policy gradient theorem simplifies this expression by eliminating the need to estimate the gradient of the stationary distribution.</p>
<p><span id="eq-policy-gradient-theorem"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla_%5Ctheta%20r(%5Cpi)%20&amp;=%20%5Csum_%7Bs%5Cin%20%5Cmathcal%7BS%7D%7D%20%5Cmu(s)%20%5Ctextcolor%7Bred%7D%7B%20%5Csum_%7Ba%5Cin%7B%5Cmathcal%7BA%7D%7D%7D%20%5Cnabla%20%5Cpi(a%20%5Cmid%20s,%5Ctheta)%20%20q_%5Cpi(s,a)%20%7D%0A%5Cend%7Balign*%7D%0A%5Ctag%7B11%7D"></span></p>
<p>The policy gradient theorem provides a new expression for the gradient of the average reward objective. This expression involves summing, over all states and actions, the product of the gradient of the policy and the action-value function (Q). The gradient of the policy indicates how to adjust parameters to increase the probability of an action, while the action-value function represents the expected return from taking that action.</p>
<p>Martha White points out that this expression is much easier to estimate.</p>
<p>Now let’s try to understand how we use the theorem to estimate the gradient.</p>
<p>What we will use it to approximate the gradient. Computing the sum over states is impractical.</p>
<p>What we will do do is take a stochastic samples. This involves updating the policy parameters based on the gradient observed at the current state.</p>
<p>To simplify the update rule, the concept of expectations is introduced. By re-expressing the gradient as an expectation under the <strong>stationary distribution of the policy</strong>, the update can be further simplified to involve only a single action sampled from the current policy.</p>
<p>The final update rule resembles other learning rules seen in the course, where the policy parameters are adjusted proportionally to a stochastic gradient of the objective. The magnitude of the step is controlled by a step-size parameter</p>
<p>The actual computation of the stochastic gradient requires two components: the gradient of the policy and an estimate of the action-value function</p>
</section>
<section id="the-policy-gradient-theorem" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-policy-gradient-theorem">The policy gradient theorem</h2>
<p>We need some preliminary results and definitions.</p>
<ol type="1">
<li>The four part dynamics function from the <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement pp. 48]</span> book:</li>
<li>Next we need the result from Exercise 3.18 in <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement pp. 62]</span></li>
<li>Next we need the result from Exercise 3.19 in <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement pp. 62]</span></li>
</ol>
<p><span id="eq-dynamics-function"><img src="https://latex.codecogs.com/png.latex?%0Ap(s',%20r%20%5Cmid%20s,%20a)%20%5Cdoteq%20Pr%5C%7BS_t=s',%20R_t=r%20%5Cmid%20S_%7Bt-1%7D%20=%20s%20,%20A_%7Bt-1%7D=%20a%5C%7D%20%5Cqquad%20%5Ctext%7B(S.B.%203.2)%7D%0A%5Ctag%7B12%7D"></span></p>
<div id="note-318" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 3.18
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>The value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:</p>
</blockquote>

<blockquote class="blockquote">
<p>Give the equation corresponding to this intuition and diagram for the value at the root node, <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)">, in terms of the value at the expected leaf node, <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,%20a)">, given <img src="https://latex.codecogs.com/png.latex?S_t%20=%20s">. This equation should include an expectation conditioned on following the policy, <img src="https://latex.codecogs.com/png.latex?%5Cpi">. Then give a second equation in which the expected value is written out explicitly in terms of <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%20%5Cmid%20s)"> such that no expected value notation appears in the equation.</p>
</blockquote>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fig-ex-3-18" class="quarto-float quarto-figure quarto-figure-center anchored callout-8-contents callout-collapse collapse show callout-margin-content">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ex-3-18-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/ex-3-18.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>backup diagram from v() to q()</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ex-3-18-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: backup diagram from <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> to <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,a)">
</figcaption>
</figure>
</div></div><section id="solution" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="solution">Solution</h3>
<p><span id="eq-3-18-solution"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AV_%5Cpi(s)%20&amp;=%20%5Cmathbb%7BE_%5Cpi%7D%5Bq(s_t,a)%20%20%5Cmid%20s_t%20=%20s,%20a_t=a%20%5D%20&amp;&amp;%20%5Ctext%7B(def.%20of%20Value)%7D%20%5Cnewline%0A&amp;=%20%5Csum_a%20Pr(a%20%5Cmid%20s)%20q_%5Cpi(s,a)%20&amp;&amp;%20%5Ctext%7B(def.%20of%20Expectation)%7D%20%5Cnewline%0A&amp;=%20%5Ctextcolor%7Bred%7D%7B%5Csum_a%20%5Cpi(a%20%5Cmid%20s)%7D%20%5Ctextcolor%7Bgreen%7D%7Bq_%5Cpi(s,a)%7D%20&amp;&amp;%20%5Ctext%20%7B(def.%20of%20policy)%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B13%7D"></span></p>
<div id="note-319" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 3.19
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>The value of an action, <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,%20a)">, depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state—action pair) and branching to the possible next states:</p>
</blockquote>

<blockquote class="blockquote">
<p>Give the equation corresponding to this intuition and diagram for the action value, <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,%20a)">, in terms of the expected next reward, <img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D">, and the expected next state value, <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(S_%7Bt+1%7D)">, given that <img src="https://latex.codecogs.com/png.latex?S_t%20=%20s"> and <img src="https://latex.codecogs.com/png.latex?A_t%20=%20a">. This equation should include an expectation but not one conditioned on following the policy. Then give a second equation, writing out the expected value explicitly in terms of <img src="https://latex.codecogs.com/png.latex?p(s_0,%20r%20%5Cmid%20s,%20a)"> defined by eq 3.2, such that no expected value notation appears in the equation.</p>
</blockquote>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fig-ex-3-19" class="quarto-float quarto-figure quarto-figure-center anchored callout-9-contents callout-collapse collapse show callout-margin-content">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ex-3-19-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/ex-3-19.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>backup diagram from q() to v()</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ex-3-19-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: backup diagram from <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,a)"> to <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s')">
</figcaption>
</figure>
</div></div></section>
<section id="solution-1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="solution-1">Solution</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Aq_%5Cpi(s,%20a)%20&amp;=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20v_%5Cpi%20(s_%7Bt+1%7D)%20%5Cmid%20s_t%20=%20s,%20a_t%20=%20a%5D%20%5Cnewline%0A&amp;=%20%5Ctextcolor%7Bblue%7D%7B%5Csum_%7Bs',%20r%7D%20p(s',%20r%20%5Cmid%20s,%20a)%7D%20%5Ctextcolor%7Bpink%7D%7B%5Br%20+%20v_%5Cpi(s')%5D%7D%0A%5Cend%7Balign*%7D%20%5Cqquad%0A"> {#ex-319-solution}</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Aq_%5Cpi(s,%20a)%20&amp;=%20%5Csum_%7Bs'%7D%20%5Cmathbb%7BE%7D_%5Cpi%5BG_%7Bt%7D%20%5Cmid%20S_%7Bt+1%7D=s'%5D%20Pr%5C%7BS_%7Bt+1%7D%20=%20s'%20%5Cmid%20S_t%20=%20s,%20A_t%20=%20a%5C%7D%0A%5Cnewline%20&amp;=%20%5Csum_%7Bs'%7D%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%20%5Cmid%20S_t%20=%20s,%20A_t%20=%20a,%20S_%7Bt+1%7D%20=%20s'%5D%20Pr%5C%7BS_%7Bt+1%7D%20=%20s'%20%5Cmid%20S_t%20=%20s,%20A_t%20=%20a%5C%7D%0A%5Cnewline%20&amp;=%20%5Csum_%7Bs',r%7D%20%5Cleft(%20r%20+%20%5Cgamma%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BG_%7Bt+1%7D%20%5Cmid%20S_%7Bt+1%7D%20=%20s'%5D%7D_%7Bv_%5Cpi%7C(s')%7D%20%5Cright)%20p(s',%20r%20%5Cmid%20s,%20a)%0A%5Cnewline%20&amp;=%20%5Csum_%7Bs',r%7D%20%5B%20r%20+%20%5Cgamma%20v_%5Cpi(s')%5D%20p(s',%20r%20%5Cmid%20s,%20a)%0A%5Cend%7Balign*%7D%0A"></p>
<p>Here is my version of the proof:</p>
<p><span id="eq-policy-gradient-theorem-proof"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Ctextcolor%7Bcyan%7D%7B%5Cnabla_%5Ctheta%20V_%5Cpi(s)%7D%20&amp;=%20%5Cnabla_%5Ctheta%20%5Csum_a%20%5Ctextcolor%7Bred%7D%7B%5Cpi(a%20%5Cmid%20s)%7D%20%5Ctextcolor%7Bgreen%7D%7B%20q_%5Cpi(s,a)%20%7D%20&amp;&amp;%20%5Ctext%7Bbackup%20%7D%20v_%5Cpi%20%5Cto%20q_%5Cpi%20%5Ctext%7B%20(Ex%203.18)%7D%0A%5Cnewline%20&amp;=%20%5Csum_a%20%5Cnabla_%5Ctheta%20%5Cpi(a%20%5Cmid%20s)%20q_%5Cpi(s,a)%20+%20%5Cpi(a%20%5Cmid%20s)%20%5Cnabla_%5Ctheta%20q_%5Cpi(s,a)%20&amp;&amp;%20%5Ctext%7Bproduct%20rule%7D%0A%5Cnewline%20&amp;=%20%5Csum_a%20%5Cnabla_%5Ctheta%20%5Cpi(a%20%5Cmid%20s)%20q_%5Cpi(s,a)%20+%20%5Cpi(a%20%5Cmid%20s)%20%5Cnabla_%5Ctheta%20%5Csum_%7Bs'%7D%20%5Ctextcolor%7Bblue%7D%7BP(s',r%20%5Cmid%20s,%20a)%7D%20%5Ctextcolor%7Bpink%7D%7B%5Br%20+%20V_%5Cpi(s')%5D%7D%20&amp;&amp;%20%5Ctext%7Bbackup%20%7D%20q_%5Cpi%20%5Cto%20v_%5Cpi%20%5Ctext%7B%20(Ex%203.19)%7D%0A%5Cnewline%20&amp;=%20%5Csum_a%20%5Cnabla_%5Ctheta%20%5Cpi(a%20%5Cmid%20s)%20q_%5Cpi(s,a)%20+%20%5Cpi(a%20%5Cmid%20s)%20%5Csum_%7Bs'%7D%20P(s',r%20%5Cmid%20s,%20a)%20%5Cnabla_%5Ctheta%20V_%5Cpi(s')%20&amp;&amp;%20P,%20r%20%5Ctext%7B%20are%20const%20w.r.t.%20%7D%20%5Ctheta%20%5Cnewline%0A=&amp;%20%5Csum_%7Ba%20%5Cin%20%5Cmathcal%7BA%7D%7D%20%5CBig(%20%5Cnabla_%5Ctheta%20%5Cpi(a%20%5Cmid%20s)Q_%5Cpi(s,%20a)%20+%20%5Cpi(a%20%5Cmid%20s)%20%5Csum_%7Bs'%7D%20P(s'%20%5Cmid%20s,a)%20%20%5Ctextcolor%7Bcyan%7D%7B%5Cnabla_%5Ctheta%20V_%5Cpi(s')%7D%20%5CBig)%20&amp;&amp;%20%5Ctext%7Btotal%20rule%20of%20probability%20on%20r%20for%20P%20%7D%0A%5Cnewline%20&amp;%20%5Cblacksquare%20&amp;&amp;%20%5Cqquad%0A%5Cend%7Balign*%7D%0A%5Ctag%7B14%7D"></span></p>
</section>
</section>
<section id="sec-l2g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g3">The importance of the policy gradient theorem</h2>
<p>Crucially, the policy gradient theorem eliminates the need to estimate the gradient of the stationary distribution (<img src="https://latex.codecogs.com/png.latex?%5Cmu">), making the gradient much easier to estimate from experience. This sets the stage for building incremental policy gradient algorithms, which will be discussed in the next lecture.</p>
</section>
</section>
<section id="lesson-3-actor-critic-for-continuing-tasks" class="level1">
<h1>Lesson 3: Actor-Critic for Continuing Tasks</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Derive a sample-based estimate for the gradient of the average reward objective #</label></li>
<li><label><input type="checkbox" checked="">Describe the actor-critic algorithm for control with function approximation, for continuing tasks #</label></li>
</ul>
</div>
</div>
<section id="sec-l3g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g1">Derive a sample-based estimate for the gradient of the average reward objective</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7Bt+1%7D%20%5Cdoteq%20%5Ctheta_t%20+%20%5Calpha%20%5Cfrac%7B%20%5Cnabla_%20%5Cpi%20(a_t%20%5Cmid%20s_t,%20%5Ctheta)%7D%7B%5Cpi%20(a_t%20%5Cmid%20s_t,%20%5Ctheta)%7D%20q_%5Cpi(s_t,%20a_t)%20%5Cqquad%20%5Ctext%7B()%7D%0A"></p>
<p><span id="eq-gradient-ascent"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7Bt+1%7D%20=%20%5Ctheta_t%20+%20%5Calpha%20%5Cnabla_%5Ctheta%20ln%20%5Cpi(a_t%20%5Cmid%20s_t,%20%5Ctheta)%20q_%5Cpi(s_t,%20a_t)%20%5Cqquad%20%5Ctext%7B()%7D%0A%5Ctag%7B15%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is the step size and <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta%20J(%5Ctheta)"> is the gradient of the objective function <img src="https://latex.codecogs.com/png.latex?J(%5Ctheta)"> with respect to the policy parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<p><span id="eq-log-derivative"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla%20%5Cln%20(f(x))%20=%20%5Cfrac%7B%5Cnabla%20f(x)%7D%7Bf(x)%7D%20%5Cqquad%20%5Ctext%7B(log%20derivative)%7D%0A%5Ctag%7B16%7D"></span></p>
</section>
<section id="reinforce-algorithm-extra" class="level2">
<h2 class="anchored" data-anchor-id="reinforce-algorithm-extra">Reinforce Algorithm (Extra)</h2>
<p>The reinforce algorithm isn’t covered in the course. However, it is in the readings. Also the reinforce algorithm is said to be the most direct implementation of the policy gradient theorem. Finaly the reinforce algorithm is used in one of my research projects and this seems to be a great opportunity to understand it better.</p>
<p>So without further ado, let’s dive into the reinforce algorithm.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-30-reinfoce.png" class="img-fluid figure-img"></p>
<figcaption>Reinforce Algorithm</figcaption>
</figure>
</div>
<p><strong>Reinforce</strong> reveals the main issues with the policy gradient theorem. While the policy gradient theorem provides an unbiased estimate of the gradient of the average reward objective, it is a high variance estimator. This means that the gradient is very noisy and can lead to slow learning.</p>
<p>One wat to reduce the variance of the policy gradient theorem is to use a baseline. A baseline is a function that is subtracted from the reward to reduce the variance of the policy gradient theorem. Subtracting the baseline does not change the expected value of the gradient<sup>3</sup>, but it can reduce the variance of the gradient estimate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-31-reinforce-with-baselines.png" class="img-fluid figure-img"></p>
<figcaption>Reinforce Algorithm</figcaption>
</figure>
</div>
<p>the change is in the last three lines. The baseline is subtracted from the return G and the gradient is scaled by the baseline.</p>
</section>
<section id="sec-l3g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g2">Describe the actor-critic algorithm for control with function approximation, for continuing tasks</h2>
</section>
</section>
<section id="lesson-4-policy-parameterizations" class="level1">
<h1>Lesson 4: Policy Parameterizations</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Derive the actor-critic update for a softmax policy with linear action preferences #</label></li>
<li><label><input type="checkbox" checked="">Implement this algorithm #</label></li>
<li><label><input type="checkbox" checked="">Design concrete function approximators for an average reward actor-critic algorithm #</label></li>
<li><label><input type="checkbox" checked="">Analyze the performance of an average reward agent #</label></li>
<li><label><input type="checkbox" checked="">Derive the actor-critic update for a gaussian policy #</label></li>
<li><label><input type="checkbox" checked="">Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions #</label></li>
</ul>
</div>
</div>
<section id="actor-critic-with-softmax-policies-video" class="level2">
<h2 class="anchored" data-anchor-id="actor-critic-with-softmax-policies-video">Actor-Critic with Softmax Policies (video)</h2>
<p>Adam White discusses one specific implementation of the actor-critic reinforcement learning algorithm using a linear function approximation of the action value with tile coding and a Softmax policy parameterization.</p>
<p>Actor-critic methods combine direct policy optimization (actor) with value estimation (critic) using temporal difference learning.</p>
<p>The critic evaluates the policy by updating state value estimates, while the actor updates policy parameters based on feedback from the critic. This implementation is designed for finite action sets and continuous states. It employs a Softmax policy that maps state-dependent action preferences to probabilities, ensuring these probabilities are positive and sum to one. Each state effectively has its own Softmax distribution, and actions are sampled proportionally to these probabilities.</p>
<p>Both the value function and action preferences are parameterized linearly. The critic uses a feature vector representing the current state to estimate the value function. For the actor, the action preferences depend on both state and action, necessitating a state-action feature vector. The parameterization requires duplicating state feature vectors for each action, resulting in a policy parameter vector (θ) larger than the critic’s weight vector (W).</p>
<p>The algorithm’s update equations include:</p>
<p>Critic Update: A straightforward semi-gradient TD update using the feature vector scaled by the temporal difference residual (TDR). Actor Update: A more complex gradient that involves two components: State-action features for the selected action. A sum over all actions of state-action features scaled by the policy probabilities.</p>
</section>
<section id="sec-l4g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g1">Derive the actor-critic update for a softmax policy with linear action preferences</h2>
<p>The critic’s update rule is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%CE%B1%5E%5Cmathbf%7Bw%7D%20%5Cdelta%20%5Cnabla%20%5Chat%7Bv%7D(S,w)%0A"></p>
<p>which uses semigradient TD(0) to update the value function.</p>
<p>The actor uses the tf-error from the critic to update the policy parameters: <img src="https://latex.codecogs.com/png.latex?%0A%CE%B8%20%5Cleftarrow%20%CE%B8%20+%20%CE%B1%5E%CE%B8%20%CE%B4%20%E2%88%87%20%5Cln%20%5Cpi%20(A%20%5Cmid%20S,%5Ctheta)%0A"></p>
<p>policy update with a softmax policy is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20%5Cdoteq%20%5Cfrac%7Be%5E%7Bh(s,%20a,%20%5Ctheta)%7D%7D%7B%5Csum_%7Bb%5Cin%20%5Cmathcal%7BA%7D%7D%20e%5E%7Bh(s,%20b,%20%5Ctheta)%7D%7D%0A"></p>
<p>this is like having a different softmax for each state</p>
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-21.png" class="img-fluid"> <img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-20.png" class="img-fluid"> <img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-22-features.png" class="img-fluid" alt="stacking"></p>
<p>Feature of the action preferences function</p>
<p>for the critic <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(s,w)%20%5Cdoteq%20w%5ET%20x(s)%0A"></p>
<p>for the actor</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah(s,a,%CE%B8)%20%5Cdoteq%20%CE%B8%5ET%20x_h(s,a)%0A"></p>
<p>we can do this by stacking</p>
<p>So with the softmax policy the critic’s update is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw%20%20%5Cleftarrow%20w%20+%20%CE%B1%5Ew%20%5Cdelta%20x(s)%0A"></p>
<p>and the actor’s update to the preferences looks as follows.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla%20%5Cln%20%5Cpi(a%20%5Cmid%20s,%20%5Ctheta)%20=%20x_h(s,a)%20-%20%5Csum_b%20%5Cpi(b%20%5Cmid%20s,%20%5Ctheta)%20x_h(s,b)%0A"></p>
<p>The gradient has two parts.</p>
<p>The first is the state action features for the selected action xh(s,a).</p>
<p>The second part is the state action features multiplied by the policy summed over all actions ∑ b π(b|s,θ)xh(s,b).</p>
</section>
<section id="sec-l4g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g2">Implement this algorithm</h2>
</section>
<section id="sec-l4g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g3">Design concrete function approximators for an average reward actor-critic algorithm</h2>
</section>
<section id="sec-l4g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g4">Analyze the performance of an average reward agent</h2>
</section>
<section id="sec-l4g5" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g5">Derive the actor-critic update for a gaussian policy</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-11-actor-critic.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Actor-Critic</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-12-actor-critic-alg.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Actor-Critic Continuing</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/pg-32-actor-critic.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Actor-Critic Episodic</figcaption>
</figure>
</div>
</section>
<section id="sec-l4g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g6">Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discussion prompt
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Are tasks really ever continuing? Everything eventually breaks or dies. It’s clear that individual people do not learn from death, but we don’t live forever. Why might the continuing problem formulation be a reasonable model for long-lived agents?</p>
</blockquote>
</div>
</div>
<div id="fig-chapter-13-policy-gradient" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chapter-13-policy-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="ch13-policy-gradient.pdf" class="col-page" width="700" height="1000"></p>
<figcaption>Chapter 13</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chapter-13-policy-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Chapter 13 of <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement]</span> covering policy gradient methods.
</figcaption>
</figure>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>which models a single roll of a die based on its historical performance. It generalizes the Bernoulli and a special case of the Multinomial for a single trial↩︎</p></li>
<li id="fn2"><p>A step that does not logically follow from the previous one↩︎</p></li>
<li id="fn3"><p>the bias↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w4.html</guid>
  <pubDate>Wed, 03 Apr 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Control with Approximation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w3.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div id="fig-episodic-semi-gradient-sarsa" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-episodic-semi-gradient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithm decision tree</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-episodic-semi-gradient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The algorithms we will be discussing in this lesson are function approximation versions of SARSA, Expected SARSA, and Q-learning. These are all sample-based algorithms that solve the Bellman equation for action-values. They differ in how they estimate the action-values and how they update them.
</figcaption>
</figure>
</div></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§10 pp. 243-246]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=243">book</a></label></li>
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§10.3 pp. 249-252]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=249">book</a></label></li>
</ul>
</div>
</div>
</div>
<section id="lesson-1-episodic-sarsa-with-function-approximation" class="level1 page-columns page-full">
<h1>Lesson 1: Episodic SARSA with Function Approximation</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Explain</em> the update for Episodic SARSA with function approximation #</label></li>
<li><label><input type="checkbox" checked=""><em>Introduce</em> the feature choices, including passing actions to features or stacking state features #</label></li>
<li><label><input type="checkbox" checked=""><em>Visualize</em> value function and learning curves #</label></li>
<li><label><input type="checkbox" checked=""><em>Discuss</em> how this extends to Q-learning easily, since it is a subset of Expected SARSA #</label></li>
</ul>
</div>
</div>
<section id="episodic-sarsa-with-function-approximation-video" class="level2">
<h2 class="anchored" data-anchor-id="episodic-sarsa-with-function-approximation-video">Episodic SARSA with function approximation (Video)</h2>
<p>In this video, Adam White, discusses the algorithm for “Episodic SARSA with function approximation”. He explains how it can be used to solve reinforcement learning problems with large or continuous state-spaces. He also delineates the importance of feature choices in this algorithm and how they can impact the performance of the system.</p>
</section>
<section id="sec-l1g2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g2">Two ways to construct action dependent features?</h2>

<div class="no-row-height column-margin column-container"><div id="fig-feature-stacking" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-feature-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/nn-feature-stacking.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Stacking</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-feature-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Stacking involves concatenating the state features with the action features
</figcaption>
</figure>
</div><div id="fig-feature-inputs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-feature-inputs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/nn-feature-inputs.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Passing</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-feature-inputs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Passing Actions to Features involves passing the state features through a neural network that also takes the action as input
</figcaption>
</figure>
</div></div>
<p>We see two techniques for constructing action dependent features. It is worthwhile noting that these two techniques are quite generally used in Machine Learning. For example Concatenating is used in CNN and with multi-head attention in Transformers.</p>
<ol type="1">
<li><strong>Stacking</strong> involves concatenating the state features with the action features. This is a simple and effective way to construct action dependent features.</li>
</ol>
<p>Stacking can used both for linear function approximation and for neural networks in much the same way. Stacking is very simple but it has a problem, it keeps the same state features used for different actions separate. This seems to be both an over-parameterization (i.e.&nbsp;overfitting) and a an impediment to learning a good representation of the state.</p>
<ol start="2" type="1">
<li><strong>Passing Actions to Features</strong> attempts to remedy this issue. In this technique, the state features are passed through a neural network that also takes the action as input. This allows the network to learn a better representation of the state that is dependent on both the state and the action. This technique is more complex but can lead to better performance.</li>
</ol>
</section>
<section id="how-to-use-sarsa-in-episodic-tasks-with-function-approximation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="how-to-use-sarsa-in-episodic-tasks-with-function-approximation">How to use SARSA in episodic tasks with function approximation</h2>
<p>Next we will see how to use SARSA in episodic tasks with function approximation. The main idea is to use a similar update rule as before, but with the action value function approximated by a function approximator, i.e.&nbsp;it will be parametrized by weights <strong>w</strong>. Also we will need to add a the gradient of the to the update rule. As we will want to move the weights in the direction of the gradient of the action value function.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SARSA
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-sarsa" class="pseudocode-container quarto-float" data-line-number-punc=":" data-pseudocode-number="1" data-no-end="false" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-comment-delimiter="#" data-line-number="true">
<div class="pseudocode">
\begin{algorithm} \caption{SARSA($\alpha,\epsilon$)}\begin{algorithmic} \State Initialize: \State $\qquad Q(s,a) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S^+}, a\in\mathcal{A}(s)\ except\ for\ Q(terminal,\cdot)=0$ \For {each episode e:} \State Initialize $S$ \State Choose A from S using policy derived from Q (e.g., $\epsilon$-greedy) \For {each step of e} \State Take action A, observe R, S' \State Choose A' from S' using policy derived from Q (e.g., $\epsilon$-greedy) \State $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma Q(S',A') - Q(S, A)]$ \State $S \leftarrow S'$; $A \leftarrow A'$ \EndFor \State until $S$ is terminal \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<p>SARSA is a sample-based algorithm to solve the Bellman equation for action-values.</p>
<ul>
<li>It picks an action based on the current policy and then</li>
<li>It policy evaluation by a TD updates of Q the action-value function based on the reward and the next action.</li>
<li>Then it does a policy improvement.</li>
</ul>

<div class="no-row-height column-margin column-container"><div id="fig-episodic-semi-grafient-sarsa" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-episodic-semi-grafient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/episodic-semi-grafient-sarsa.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Episodic SARSA with function approximation</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-episodic-semi-grafient-sarsa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: This figure shows the algorithm for Episodic semi-gradient SARSA which uses function approximation. The update rule is modified from tabular case, with an approximate action value function <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)"> as well as the inclusion of the gradient of the action value function <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Chat%7Bq%7D%20(S_t,%20A_t,%20%5Cmathbf%7Bw%7D)"> . These two modification allows us to update the weights of the function approximator The rest of the algorithm remains unchanged.
</figcaption>
</figure>
</div></div><p><span class="citation" data-cites="Rummery1994OnlineQU">[@Rummery1994OnlineQU]</span> introduced SARSA, but the name is due to Rich Sutton.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Episodic Semi-gradient SARSA
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-td-sarsa" class="pseudocode-container quarto-float" data-line-number-punc=":" data-pseudocode-number="2" data-no-end="false" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-comment-delimiter="#" data-line-number="true">
<div class="pseudocode">
\begin{algorithm} \caption{Episodic Semi-gradient SARSA for estimating ($\hat{q} \approx q_*$)}\begin{algorithmic} \State $\textbf{Input:}$ \State $\qquad \text{a differentiable action-value fn parameterization } \hat{q}: \mathcal{S} \times A \times \mathbb{R}^d \to \mathbb{R}$ \State $\textbf{Initialize:}$ \State $\qquad$ value function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily, (e.g., $\mathbf{w} = 0$) \For {each episode e:} \State Initialize $S$ \State Choose A from S using policy derived from $\color{red}\hat{q}(S'A,\mathbf{w})$ (e.g., $\epsilon$-greedy) \For {each step of e} \State Take action A, observe R, S' \If {$S'$ is terminal} \State $\color{red}\mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ R - \hat{q}(S, A, \mathbf{w}) \right] \nabla \hat{q}(S, A, \mathbf{w})$ \State Go to next episode \EndIf \State Choose A' as a function of $\color{red}\hat{q}(S'A,\mathbf{w})$ (e.g., $\epsilon$-greedy) \State $\color{red}\mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ R + \gamma \hat{q}(S', A', \mathbf{w}) - \hat{q}(S, A, \mathbf{w}) \right] \nabla \hat{q}(S, A, \mathbf{w})$ \State $S \leftarrow S'$; \State $A \leftarrow A'$ \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<p>This allows us to learn a good policy for the task. We will also need a step to update the weights.</p>

<div class="no-row-height column-margin column-container"><div id="fig-projected-bellman-error" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-projected-bellman-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/nFM5kLJl34c" title="Advances in Value Estimation in Reinforcement Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-projected-bellman-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Talk titled ‘Advances in Value Estimation in Reinforcement Learning’ at London Machine Learning Meetu by Martha White on Two Pieces of Research on Exploration in Reinforcement Learning. On work from [patterson2024generalizedprojectedbellmanerror]
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Over Thinking Semi-gradient SARSA {.unnumbered}:
</div>
</div>
<div class="callout-body-container callout-body">
<p>So you think you understand SARSA with function approximation?</p>
<ol type="1">
<li><p>Why is this a semi-gradient method?</p>
<p>Hints:</p>
<ul>
<li>what is the MSE td error of the Q-value the next state?</li>
</ul></li>
</ol>
<p>If we define the MSE of the Q-value of the next state as the projected Bellman error, then we can see that the update rule is a projected gradient descent on the projected Bellman error. This is a semi-gradient method because we are not using the true value of the next state in the update rule.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7B%7BQE%7D%7D(w)%20%5Cdoteq%20%5Csum_%7Bs%5Cin%20%5Cmathcal%7BS%7D,a%5Cin%20%5Cmathcal%7BA%7D%7D%20%5Cmu(s)%5Cleft%5B%20%20q%5E*(s,a)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%20%5Cright%5D%5E2%0A"></p>
<p>since we dont know the action value function we can’t compute the true value of the next state. Instead we use the value of the next state.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%5Coverline%7B%7BQE%7D%7D(w)%20&amp;%5Capprox%20%5Csum_%7Bs%5Cin%20%5Cmathcal%7BS%7D,a%5Cin%20%5Cmathcal%7BA%7D%7D%20%5Cmu(s)%5Cleft%5B%20r+%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%20%5Cright%5D%5E2%20%5C%5C%0A%20%20&amp;=%20%5Cmathbb%7BE%7D_%5Cmu%20%5Cleft%5B%20(r+%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D))%5E2%20%5Cright%5D%0A%5Cend%7Balign*%7D%0A"></p>
<p>then we have the following objective function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20J(w)%20=%20%5Cmathbb%7BE%7D_%5Cmu%20%5Cleft%5B%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%5Cgamma%5Et%20%20%5Cleft(%20r_t%20+%20%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%5E2%20%5Cright%5D%0A"></p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmu"> is the state visitation distribution</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)"> is the approximated action value function</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> is the weight vector</li>
<li><img src="https://latex.codecogs.com/png.latex?r_t"> is the reward at time step t</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the discount factor</li>
</ul></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla_%5Cmathbf%7Bw%7D%20J(w)%20&amp;=%20%5Cmathbb%7BE%7D_%5Cmu%20%5Cleft%5B%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%5Cgamma%5Et%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Cleft(%20r_t%20+%20%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%5E2%20%5Cright%5D%20%5C%5C%0A&amp;=%20%5Cmathbb%7BE%7D_%5Cmu%20%5Cleft%5B%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%5Cgamma%5Et%202%20%5Cleft(%20r_t%20+%20%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Cleft(%20r_t%20+%20%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%20%5Cright%5D%20%5Cnewline%0A&amp;=%20%5Cmathbb%7BE%7D_%5Cmu%20%5Cleft%5B%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%5Cgamma%5Et%202%20%5Cdelta%20%20%5Cleft(%20%5Cgamma%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%20%5Cright%5D%20%5C%5C%0A%5Cend%7Balign*%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20=%20r_t%20+%20%5Cgamma%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)"> is the TD error.</p>
<p>this is the projected gradient of the projected Bellman error.</p>
<p>which would give us the following update rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5Cdelta%20%5Cleft(%20%5Cgamma%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Chat%7Bq%7D(s',a',%5Cmathbf%7Bw%7D)%20-%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cright)%0A"></p>
<p>rather than the update rule we have in the algorithm:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5Cdelta%20%5Cnabla_%5Cmathbf%7Bw%7D%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bq%7D(S_%7Bt+1%7D,A_%7Bt+1%7D,%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(S_t,A_t,%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bq%7D(S_t,A_t,%5Cmathbf%7Bw%7D)%0A"></p>
<ol type="1">
<li>Can we do better than this?</li>
</ol>
<p>Hint: using a projected Bellman error, we may get a better algorithm than SARSA with function approximation.</p>
<ol type="1">
<li>Do we have any convergence guarantees for SARSA with function approximation?</li>
</ol>
<p>In the text book <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement sec. 10.1]</span> the authors state that for a constant policy <sup>1</sup>, this method converges in the same way that TD(0) does, with the same kind of error bound. Then they go on to say that for a non-constant policy, the convergence is a matter of ongoing research.</p>
<p>For the tabular setting, <span class="citation" data-cites="singh2000convergence">[@singh2000convergence]</span> the authors show that the algorithm has asymptotic converges to the optimal policy provided that the policies from the policy improvement operator is “greedy in the limit with infinite exploration”</p>
<p>For the function approximation setting, Empirical results show that the linear SARSA can chatter, i.e.&nbsp;the weight vector does not go to infinity (i.e., it does not diverge) but oscillates in a bounded region.</p>
<ol type="1">
<li>Since it is semi-gradient, we are not using the true value of the next state in the update rule.</li>
<li>Are we biased?</li>
<li>Does this algorithm have lower variance than the TD(0) algorithm?</li>
<li>When we choose the next action, are we on policy or off policy?</li>
<li>In the non terminal rule we use two samples, but they are highly correlated. Why is this a problem</li>
<li>How can we reduce this correlation?</li>
<li>What is the policy improvement operator?</li>
<li>What is the Lipschitz constant of this operator and why don’t we see this in the SARSA algorithm?</li>
</ol>
</div>
</div>
</section>
<section id="sec-l1g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g1">The update for Episodic SARSA with function approximation</h2>
<p>So far we have only been using function approximation to parametrize state value function,</p>
<p><span id="eq-parametize-state-value-function"><img src="https://latex.codecogs.com/png.latex?%0AV_%5Cpi(s)%20%E2%89%88%20%5Chat%7Bv%7D(s,w)%20%5Cdoteq%20%5Cmathbf%7Bw%7D%5ET%20%5Ccdot%20%5Cmathbf%7Bx%7D(S)%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<p>For SARSA we need a parametrized approximation <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D"> for the the action value function <img src="https://latex.codecogs.com/png.latex?q_pi">,</p>
<p><span id="eq-parametrized-action-value-function"><img src="https://latex.codecogs.com/png.latex?%0Aq_%5Cpi(s,a)%20%E2%89%88%20%5Chat%7Bq%7D(s,a,%5Cmathbf%7Bw%7D)%20%5Cdoteq%20%5Cmathbf%7Bw%7D%5ET%20%5Ccdot%20%5Cmathbf%7Bx%7D(s,a)%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>
</section>
<section id="episodic-sarsa-in-mountain-car-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="episodic-sarsa-in-mountain-car-video">Episodic SARSA in Mountain Car (Video)</h2>

<div class="no-row-height column-margin column-container"><div id="fig-mountain-car-environment" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-environment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/mountain-car-environment.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-environment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: The mountain car environment
</figcaption>
</figure>
</div></div></section>
<section id="feature-choices-in-episodic-sarsa-with-function-approximation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="feature-choices-in-episodic-sarsa-with-function-approximation">Feature Choices in Episodic SARSA with Function Approximation</h2>

<div class="no-row-height column-margin column-container"><div id="fig-mountain-car-feature-representations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-feature-representations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/mountain-car-feature-representations.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-feature-representations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: The feature representations for the mountain car problem
</figcaption>
</figure>
</div></div><p>What features do we use for the mountain car problem?</p>
<p>for the state:</p>
<ul>
<li>position</li>
<li>velocity</li>
</ul>
<p>for the action:</p>
<ul>
<li>accelerate left</li>
<li>accelerate right</li>
<li>do nothing</li>
</ul>
</section>
<section id="sec-l1g3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g3">Visualizing Value Function and Learning Curves</h2>

<div class="no-row-height column-margin column-container"><div id="fig-mountain-car-learned-values" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-learned-values-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/mountain-car-learned-values.png" class="img-fluid figure-img" data-group="slides">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-learned-values-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: The value function for the mountain car problem
</figcaption>
</figure>
</div><div id="fig-mountain-car-learned-values-trajectory" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-learned-values-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/mountain-car-learned-values-trajectory.png" class="img-fluid figure-img" data-group="slides">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-learned-values-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: The trajectory through the state space for the mountain car problem
</figcaption>
</figure>
</div><div id="fig-mountain-car-learning-curve" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mountain-car-learning-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/mountain-car-learning-curve.png" class="img-fluid figure-img" data-group="slides">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mountain-car-learning-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: The learning curve for the mountain car problem
</figcaption>
</figure>
</div></div>

<p>The first two figures show the learned value function for the mountain car problem. The first figure shows the value function for each state. The second shows a possible trajectory through the state space.</p>
<p>Then we look at the learning curve for the mountain car problem. This shows how the value function improves over time as the agent learns the optimal policy. We see the familiar exponential decay in the learning curves.</p>
<p>It worth noting that this is a very simple environment and that many more sophisticated deep learning techniques don’t do a very good job on this problem.</p>
</section>
<section id="expected-sarsa-with-function-approximation-video" class="level2">
<h2 class="anchored" data-anchor-id="expected-sarsa-with-function-approximation-video">Expected SARSA with Function Approximation (Video)</h2>
<p>Now we extend SARSA with under function approximation into Expected SARSA .</p>
<p>First, recall the update rule for Tabular SARSA:</p>
<p><span id="eq-tabular-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,A_t)%20%5Cleftarrow%20Q(S_t,A_t)%20+%20%5Calpha%20(R_%7Bt+1%7D%20+%20%5Cgamma%20Q(%20%5Ctextcolor%7Bred%7D%7B%20S_%7Bt+1%7D,%20A_%7Bt+1%7D%20%7D)%20-%20Q(S_t,A_t))%20%5Cqquad%0A%5Ctag%7B3%7D"></span></p>
<p>SARSA’s update target includes the action value for the next state in action.</p>
<p>Next, recall how Tabular Expected SARSA uses the expectation over its target policy instead.</p>
<p><span id="eq-tabular-expected-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,A_t)%20%5Cleftarrow%20Q(S_t,A_t)%20+%20%5Calpha%20(R_%7Bt+1%7D%20+%20%5Cgamma%20%5Ctextcolor%7Bred%7D%7B%20%5Csum_%7Ba'%7D%20%5Cpi%20(a'%20%5Cmid%20S_%7Bt+1%7D)%20Q(S_%7Bt+1%7D,a')%7D%20-%20Q(S_t,A_t))%20%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<p>We can compute the same expectation using function approximation.</p>
<p>First, recall the update for SARSA with function approximation. It looks similar to the tabular setting except the action value estimates are parameterized by the weight factor, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D">. i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?q_%5Cpi%20(s,%20a)%20%5Capprox%20%5Chat%7Bq%7D%20(s,%20a,%20%5Cmathbf%7Bw%7D)"></p>
<p><span id="eq-semi-gradient-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bq%7D(%5Ctextcolor%7Bred%7D%20%7B%20S_%7Bt+1%7D%20%7D%20,%20%5Ctextcolor%7Bred%7D%20%7B%20A_%7Bt+1%7D%7D,%20%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bq%7D(S_t,%20A_t,%20%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bq%7D%20(S_t,%20A_t,%20%5Cmathbf%7Bw%7D)%20%5Cqquad%0A%5Ctag%7B5%7D"></span></p>
<p>Expected Sarsa with function approximation follows a similar structure.</p>
<p><span id="eq-semi-gradient-expected-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%20%20%5Ctextcolor%7Bred%7D%7B%20%5Csum_%7Ba'%7D%20%5Cpi(%20a'%20%5Cmid%20S_%7Bt+1%7D)%20%5Chat%7Bq%7D%20(S_%7Bt+1%7D,%20a'%20,%20%5Cmathbf%7Bw%7D%20)%7D%20-%20%5Chat%7Bq%7D(S_t%20,%20A_t%20,%20%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bq%7D(S_t%20,%20A_t%20,%20%5Cmathbf%7Bw%7D)%20%5Cqquad%0A%5Ctag%7B6%7D"></span></p>
</section>
<section id="sec-l1g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g4">How this extends to Q-learning easily, since it is a subset of Expected SARSA</h2>
<p>Finally, for Q-learning with function approximation as Q-learning is a special case of expected SARSA in which we take the greedy. So next we replace the expectation over the target policy by argmax action to derive the Q-learning update rule.</p>
<p>The Q-learning update rule is:</p>
<p><span id="eq-semi-gradient-Q-learning-update"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bw%7D%20%5Cleftarrow%20%5Cmathbf%7Bw%7D%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%20%5Ctextcolor%7Bred%7D%7B%20%5Cmax_%7Ba'%7D%20%5Chat%7Bq%7D(S_%7Bt+1%7D,a',%5Cmathbf%7Bw%7D)%7D%20%E2%88%92%5Chat%7Bq%7D(S_t,A_t,%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bq%7D(S_t,A_t,%5Cmathbf%7Bw%7D)%20%5Cqquad%0A%5Ctag%7B7%7D"></span></p>
</section>
</section>
<section id="lesson-2-exploration-under-function-approximation" class="level1">
<h1>Lesson 2: Exploration under Function Approximation</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Understanding</em> optimistically initializing your value function as a form of exploration #</label></li>
</ul>
</div>
</div>
<section id="exploration-under-function-approximation-video" class="level2">
<h2 class="anchored" data-anchor-id="exploration-under-function-approximation-video">Exploration under Function Approximation (Video)</h2>
</section>
<section id="sec-l2g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g1">Optimistic Initialization as a Form of Exploration</h2>
<p>In the tabular setting, we saw that we could use optimistic initialization as a form of early exploration. The way this works is that we initialize the value function to be very high, this way the agent will be encouraged to try to exploit one of the states. It will discover that the value of the state is not as high as it thought and will try to exploit another and so on until it have visited all the states and learned more realistic values for them. Over time the effect of the initial values will diminish. This however assumes two things:</p>
<ol type="1">
<li>the number of states is finite</li>
<li>the values are independent of each other</li>
</ol>
<p>In the function approximation setting, we can try to do the same thing. This time we will want to initialize the weights so as to make the value function high. We face a number of issues in this setting:</p>
<ol type="1">
<li>the values are not independent of each other so each weight may loose its optimistic value long before the agent has explored many of the states within the features neighborhood in the state space.</li>
<li>Unlike values we can’t be certain that high weights will lead to high values. While this may work for linear function approximation, for a non-linear function approximation using Neural Networks with tanh activation functions, positive weights may lead to negative values.</li>
</ol>
<p>epsilon-greedy exploration is easy to implement even with non-linear function approximation. However it is not as effective in the function approximation setting. Because it relies on randomness to explore states near those followed by the current policy. This is not as systematic as the exploration due to optimistic initialization in the tabular setting.</p>
<p>Improving exploration with function approximation is an open research question</p>
<p>In this course we will stick to epsilon-greedy exploration.</p>
<p>Q. Why is epsilon-greedy exploration ineffective in the function approximation setting?</p>
<ol type="1">
<li>like in the bandit setting, the agent keeps exploring even after it has found the optimal policy.</li>
<li>like in environments with a changing maze multiple epsilon-greedy exploration steps may be required to explore states required by the optimal policy that are not near the current policy. The chance for such an exploration is <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)%5Cepsilon%5En"> where <img src="https://latex.codecogs.com/png.latex?mu(s)"> is importance of the nearest state and n is the number of steps required to reach the target state. This can be vanishingly small for large state spaces. Which means it can take too long to find the optimal policy using epsilon-greedy exploration. We need to think of better ways to organize exploration in the function approximation setting.</li>
<li>Prioritizing using count based on a coarse coding may be more effective. Even better if we track the uncertainty in the value function for each if these features. This is a form of intrinsic motivation.</li>
</ol>
<p>However the last video by Satinder Singh discusses using intrinsic motivation to improve exploration in reinforcement learning systems. And in it he shows a different paradigm of exploration. Rather than getting agents to explore systematic one wants to explore in a way that is interesting to the agent.</p>
</section>
</section>
<section id="lesson-3-average-reward" class="level1 page-columns page-full">
<h1>Lesson 3: Average Reward</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Describe</em> the average reward setting #</label></li>
<li><label><input type="checkbox" checked=""><em>Explain</em> when average reward optimal policies are different from discounted solutions #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how differential value functions are different from discounted value functions #</label></li>
</ul>
</div>
</div>
<section id="average-reward-a-new-way-of-formulating-control-problems-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="average-reward-a-new-way-of-formulating-control-problems-video">Average Reward: A New Way of Formulating Control Problems (Video)</h2>
<p>You probably never thought about it, since discounting is familiar like a geometric series, but it can really skew the value function.</p>

<div class="no-row-height column-margin column-container"><div id="fig-near-sighted-mdp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-near-sighted-mdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/near-sighted-mdp.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>The near sighted MDP</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-near-sighted-mdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: The near sighted MDP. The lower discounting causes the agent to prefer the smaller immediate reward over the larger delayed reward.
</figcaption>
</figure>
</div></div><p>In most states, there’s only one action, so there are no decisions to be made. There’s only one state were a decision can be made. In this state, the agent can decide which ring to traverse.</p>
<p>This means there are two deterministic policies, traversing the left ring or traversing the right ring. The reward is zero everywhere except for in one transition in each ring. In the left ring, the reward is +1 immediately after state S. In the right ring, the reward is +2 immediately before state S. Intuitively, you would pick the right action because you know you will get +2 reward. But what would the value function tell us to do?</p>
<p><strong>If we use discounting, what are the values of state S under these two different policies?</strong></p>
<p>The policy that chooses the left action has a value of <img src="https://latex.codecogs.com/png.latex?v_l(S)%20=%20%5Cfrac%7B1%7D%7B1-%5Cgamma%5E5%7D">. How do we figure this out? If you write out the infinite discounted return, you will see this is a fairly straightforward geometric series with a closed form solution.</p>
<p>The policy that chooses the right action has a value of <img src="https://latex.codecogs.com/png.latex?v_r(S)%20=%20%5Cfrac%7B2%20%5Cgamma%5E4%7D%7B1-%5Cgamma%5E5%7D">. Let’s think of the value of state S under these two part policies for particular values of <img src="https://latex.codecogs.com/png.latex?%5Cgamma">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cgamma=%200.5%20%5Cimplies%20V_L%20%5Capprox%201%20%5Cqquad%20V_R%20%5Capprox%200.1"></p>
<p>This means the policy that takes the left action is preferable under this more myopic discount.</p>
<p>Let’s try</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cgamma=%200.9%20%5Cimplies%20V_L%20%5Capprox%202.4%20%5Cqquad%20V_R%20%5Capprox%203.2"></p>
<p>So now we prefer the other policy.</p>
<p>In fact, we can figure out the minimum value of <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> so that the agent prefers the policy that goes right. <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> needs to be at least 0.841. So the problem here is that the discount magnitude depends on the problem.</p>
<p>For this example, 0.85 is sufficiently large. But if the rings had 100 states each, this discount factor would need to be over 0.99.</p>
<p>In general, the only way to ensure that the agents actions maximize reward over time is to keep increasing the discount factor towards 1.</p>
<p>Depending on the problem, we might need <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> to be quite large. And we can’t set it to 1 in a continuing setting as the return might be infinite.</p>
<p>Now, what’s wrong with having larger <img src="https://latex.codecogs.com/png.latex?%5Cgamma">? Larger values of <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> can also result in larger and more variables sums, which might be difficult to learn. So is there an alternative?</p>
</section>
<section id="sec-l3g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g1">The Average Reward Setting</h2>
<p><span id="eq-average-reward"><img src="https://latex.codecogs.com/png.latex?%0Ar(%5Cpi)%20%5Cdoteq%20%5Clim%20_%7Bh%20%5Cto%20%5Cinfty%7D%20%5Cfrac%7B1%7D%7Bh%7D%20%5Csum_%7Bt=1%7D%5E%7Bh%7D%20%5Cmathbb%7BE%7D%5BR_t%20S_0,A_%7B0:t%E2%88%921%7D%20%5Csim%20%5Cpi%5D%20%20%5Cqquad%0A%5Ctag%7B8%7D"></span></p>
<p>Let’s discuss a new objective called the average reward. Imagine the agent has interacted with the world for H steps. This is the reward it has received on average across those H steps. In other words, it’s rate of reward. If the agents goal is to maximize this average reward, then it cares equally about nearby and distant rewards. We denote the average reward of a policy with <img src="https://latex.codecogs.com/png.latex?R_%5Cpi">.</p>
<p><span id="eq-average-reward-visitation"><img src="https://latex.codecogs.com/png.latex?%0Ar(%5Cpi)%20=%20%5Csum_%7Bs%7D%20%5Cmu_%5Cpi%20(s)%20%5Csum_%7Ba%7D%20%5Cpi(a%20%5Cmid%20s)%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)%20r%0A%5Ctag%7B9%7D"></span></p>
<p>More generally, we can write the average reward using the state visitation, <img src="https://latex.codecogs.com/png.latex?%5Cmu">. This inner term is the expected reward in a state under policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">. The outer sum takes the expectation over how frequently the policy is in that state. Together, we get the expected reward across states. In other words, the average reward for a policy.</p>
<p>In the nearsighted example, the two deterministic possible policies visit either the left loop or the right loop indefinitely. In both cases, the five states in each loop are visited equally many times. In the left loop, the immediate expected reward is +0 for all states except one, which gets +1. This results in an average reward of 1 every 5 steps or 0.2.</p>
<p><img src="https://latex.codecogs.com/png.latex?r(%5Cpi_L)=1/5=0.2%20%5Cqquad%20r(%5Cpi_R)=2/5=0.4"></p>
<p>Most states in the right loop also have +0 <img src="https://latex.codecogs.com/png.latex?%5Cmu"> to expected reward. But this time, the last state gets +2. This gives an average reward of 2 every 5 steps or 0.4.</p>

<div class="no-row-height column-margin column-container"><div id="fig-near-sighted-mdp-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-near-sighted-mdp-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/near-sighted-mdp-2.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Near sighted MDP return for Average rewards</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-near-sighted-mdp-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Return for the nearsighted MDP for average rewards. The average reward for the left policy is 0.4 and for the right policy is 1.4.
</figcaption>
</figure>
</div><div id="fig-near-sighted-mdp-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-near-sighted-mdp-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/near-sighted-mdp-3.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Near sighted MDP return for Average rewards</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-near-sighted-mdp-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Return for the nearsighted MDP for average rewards. The average reward for the left policy is <img src="https://latex.codecogs.com/png.latex?G_t=-1.8"> and for the right policy is <img src="https://latex.codecogs.com/png.latex?G_t=0.8">.
</figcaption>
</figure>
</div></div>
<p>We can see the average reward puts preference on the policy that receives more reward in total without having to consider larger and larger discounts.</p>
<p>The average reward definition is intuitive for saying if one policy is better than another, but <strong>how can we decide which actions from a state are better?</strong></p>
<p>What we need are action values for this new setting. The first step is to figure out what the return is. In the average reward setting, returns are defined in terms of differences between rewards and the average reward <img src="https://latex.codecogs.com/png.latex?R_%5Cpi">. This is called <strong>the differential return</strong>.</p>
<p><span id="eq-differential-return"><img src="https://latex.codecogs.com/png.latex?%0AG_t%20=%20R_%7Bt+1%7D%20%E2%88%92r_%5Cpi%20+%20R_%7Bt+2%7D%20%E2%88%92r_%5Cpi%20+%20R_%7Bt+2%7D%20%E2%88%92r_%5Cpi%20%5Cldots%20%5Cqquad%0A%5Ctag%7B10%7D"></span></p>
<p>Let’s look at what the <strong>differential returns</strong> are in our nearsighted MDP.</p>
<p>The differential return represents how much more reward the agent will receive from the current state in action compared to the average reward of the policy. Let’s look at the differential return starting in state <img src="https://latex.codecogs.com/png.latex?s">, first choosing action L and then following <img src="https://latex.codecogs.com/png.latex?%5Cpi"> L afterwards.</p>
<p>The average reward for this policy is 0.2.</p>
<p>The differential return is the sum of rewards into the future with the average reward subtracted from each one. This sum starts in state S with the action L. We can compute it by summing to some finite horizon H. Then taking the limit as H goes to infinity. We are simplifying things slightly with this limit notation. While notation provider works in many cases, we need to use a different technique when the environment is periodic. In this case, we compute the return using a more general limit called the <a href="https://en.wikipedia.org/wiki/Ces%C3%A0ro_summation">Cesàro sum</a>, but this technical detail is not critical. The main point here is the intuition. We find that the differential return is 0.4. Now, let’s look at the other action. This time, we can break the differential return into two parts. First the sum for a single trajectory through the right loop. We can write the sum explicitly and it’s equal to 1. Then the sum corresponding to taking the left action indefinitely. This sum is the same as the differential return we just computed, 0.4. Adding the two parts together, we find that the differential return is 1.4.</p>
<p>We can write the average reward using the state visitation, <img src="https://latex.codecogs.com/png.latex?%5Cpi">. This inner term is the expected reward in a state under policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">. The outer sum takes the expectation over how frequently the policy is in that state. Together, we get the expected reward across states. In other words, the average reward for a policy</p>
<p>In the average reward setting, returns are defined in terms of differences between rewards and the average reward <img src="https://latex.codecogs.com/png.latex?r_%5Cpi">, which is called the differential return. The differential return represents how much more reward the agent will receive from the current state in action compared to the average reward of the policy.</p>
<p>The differential return represents how much better it is to take an action in a state then on average under a certain policy. The differential return can only be used to compare actions if the same policy is followed on subsequent time steps. To compare policies, their average reward should be used instead</p>
<p>This quantity captures how much more reward the agent will get by starting in a particular state than it would get on average over all states if it followed a fixed policy.</p>
<p><span id="eq-differential-value-function"><img src="https://latex.codecogs.com/png.latex?%0Aq_%5Cpi(s,a)%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BG_t%20%5Cmid%20S_t%20=%20s,A_t%20=%20a%5D%20%5Cqquad%0A%5Ctag%7B11%7D"></span></p>
<p>Like in the discounted setting, differential value functions can be written as Bellman equations. They only differ in that they subtract r() from the immediate reward and there is no discounting. <span id="eq-differential-bellman"><img src="https://latex.codecogs.com/png.latex?%0Aq_%5Cpi(s,a)%20=%20%5Csum_%7Bs',r%7D%20p(s',r%20%5Cmid%20s,a)(r%20%E2%88%92r(%5Cpi))%20+%20%5Csum_%7Ba'%7D%20%5Cpi(a'%20%5Cmid%20s')q_%5Cpi(s',a')%20%5Cqquad%0A%5Ctag%7B12%7D"></span></p>
<p>Many algorithms from the discounted settings can be rewritten to apply to the average reward case.</p>
</section>
<section id="sec-l3g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g2">When Average Reward Optimal Policies are Different from Discounted Solutions</h2>
</section>
<section id="sec-l3g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g3">Differential Value Functions v.s. Discounted Value Functions</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Episodic Semi-gradient SARSA
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-differential-sg-sarsa" class="pseudocode-container quarto-float" data-line-number-punc=":" data-pseudocode-number="3" data-no-end="false" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-comment-delimiter="#" data-line-number="true">
<div class="pseudocode">
\begin{algorithm} \caption{Differential Semi-gradient SARSA for estimating ($\hat{q} \approx q_*$)}\begin{algorithmic} \State $\textbf{Input:}$ \State $\qquad \text{a differentiable action-value fn parameterization } \hat{q}: \mathcal{S} \times A \times \mathbb{R}^d \to \mathbb{R}$ \State $\textbf{Algorithm parameters:}$ \State $\qquad \alpha, \beta &gt; 0$ \State $\textbf{Initialize:}$ \State $\qquad$ value function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily, (e.g., $\mathbf{w} = 0$) \State $\bar{R} source \in \mathbb{R}$ avg reward estimate \State Initialize $S$ and action $A$ \For {each step} \State Take action A, observe R, S' \State Choose A' as a function of $\hat{q}(S',\cdot,\mathbf{w})$ (e.g., $\epsilon$-greedy) \State $\delta \leftarrow R - \bar{R} + \hat{q}(S', A', \mathbf{w}) - \hat{q}(S, A, \mathbf{w}) $ \State $ \bar{R}\leftarrow \bar{R} + \beta \delta$ \State $\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta \nabla \hat{q}(S, A, \mathbf{w})$ \State $S \leftarrow S'$; $A \leftarrow A'$ \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
</section>
<section id="satinder-singh-on-intrinsic-rewards-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="satinder-singh-on-intrinsic-rewards-video">Satinder Singh on Intrinsic Rewards (Video)</h2>

<div class="no-row-height column-margin column-container"><div id="fig-subtasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/L-9Pbhpk7pQ" title="From Reinforcement Learning to Artificial Intelligence ?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: This is a high level talk by Satinder Singh on AI and RL
</figcaption>
</figure>
</div><div id="fig-subtasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/jn1NE8uIxgw" title="Steps Towards Continual Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: This talk titled ‘Steps Towards Continual Learning’ by Satinder Singh on Reinforcement Learning at DLSS &amp; RLSS 2017 - Montreal
</figcaption>
</figure>
</div><div id="fig-subtasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Z7JhZx3urEY" title="Discovery in Reinforcement Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Talk titled ‘Discovery in Reinforcement Learning’ at Beijing Academy of Artificial Intelligence by Satinder Singh on Two Pieces of Research on Exploration in Reinforcement Learning.
</figcaption>
</figure>
</div></div>

<p>Satinder Singh is a professor at the University of Michigan. He is a leading researcher in reinforcement learning and has made significant contributions to the field. In this video, he discusses intrinsic rewards and how they can be used to improve learning in reinforcement learning systems. It’s worth noting that he is one of the researchers who has worked on options with Doina Precup.</p>
<p>Now Satinder Singh is a good speaker and he has lots of interesting research results to share. Unfortunately, this video is not his finest hour. I would definitely recommend watching some of his other talks linked above.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discussion prompt
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>What are the issues with extending some of the exploration methods we learned about bandits and Dyna to the full RL problem? How can we do visitation counts or UCB with function approximation?</p>
</blockquote>
<blockquote class="blockquote">
<p>A control agent with function approximation has to explore to find the best policy, learn a good state representation, and try to get a lot of reward, all at the same time. How might an agent balance these potentially conflicting goals?</p>
</blockquote>
</div>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>e.g.&nbsp;epsilon greedy policy without decay↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w3.html</guid>
  <pubDate>Tue, 02 Apr 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Constructing Features for Prediction</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w2.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.4-9.5.0, pp. 204-210]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.7, pp. 223-228]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
</ul>
</div>
</div>
</div>
<p>We discussed methods for representing large, an possibly continuous state spaces. Ways to construct features. A representation is an agent’s internal encoding of the state, the agent constructs features to summarize the current input. Whenever we are talking about features and representation learning, we are in the land of function approximation.</p>
</section>
<section id="lesson-1-feature-construction-for-linear-methods" class="level1 page-columns page-full">
<h1>Lesson 1: Feature Construction for Linear Methods</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Define</em> the difference between <strong>coarse coding</strong> and tabular representations #</label></li>
<li><label><input type="checkbox" checked=""><em>Explain</em> the trade-off when designing representations between discrimination and generalization #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how different coarse coding schemes affect the functions that can be represented #</label></li>
<li><label><input type="checkbox" checked=""><em>Explain</em> how tile coding is a (computationally?) convenient case of coarse coding #</label></li>
<li><label><input type="checkbox" checked=""><em>Describe</em> how designing the tilings affects the resultant representation #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> that tile coding is a computationally efficient implementation of coarse coding #</label></li>
</ul>
</div>
</div>
<section id="coarse-coding-video" class="level2">
<h2 class="anchored" data-anchor-id="coarse-coding-video">Coarse Coding (Video)</h2>
<p>In this video, Adam White introduces the concept of <strong>coarse coding</strong>, covering the first learning objective of this lesson.</p>
<p>Coarse coding are a way to represent states in a more general way than tabular representations. This allows for generalization across states. The trade-off is that the representation is less discriminative.</p>
</section>
<section id="sec-l1g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g1">The difference between <strong>coarse coding</strong> and tabular representations</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-coding_states.png" class="img-fluid figure-img"></p>
<figcaption>approximation</figcaption>
</figure>
</div></div><p>Recall that linear function approximation are paramertized by a weight vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> and a feature vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D(s)">.</p>
<p>As we saw in the previous unit tabular representations associates one feature per state, this is called a one-hot encoding of the state space.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-tabular_coding.png" class="img-fluid figure-img"></p>
<figcaption>one hot coding</figcaption>
</figure>
</div></div><p>We associate one hot encoding with an indicator function <img src="https://latex.codecogs.com/png.latex?%5Cdelta_%7Bij%7D(s)">. This is a very discriminative representation but it does generalize.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-state-aggregation.png" class="img-fluid figure-img"></p>
<figcaption>state aggregation</figcaption>
</figure>
</div></div><p>We also discussed using <strong>state aggregation</strong> for the 1000 state random walk example. In state aggregation we break the continuous state space into discrete regions and associate a feature with each region. This is a more general representation than tabular representations but less discriminative.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-coarse-coding.png" class="img-fluid figure-img"></p>
<figcaption>coarse coding</figcaption>
</figure>
</div></div><p><strong>Coarse coding</strong> uses multiple overlapping shapes to represent states. This is a more General representation than state aggregation but less discriminative. Features are the circles they are in. If the circles overlap, we can have items that are in multiple circles. I.e. they are characterized by multiple features. In the example shown there can be from one to three active features.</p>
<p>So the difference is that tabular representations are one hot encodings while coarse coding uses membership in multiple overlapping shapes to represent states.</p>
<p>How does coarse coding relates to state aggregation?</p>
<p>Coarse coding is also a generalization of state aggregation. In state aggregation we break the state space into discrete regions and associate a feature with each region. But we don’t let these regions overlap. In coarse coding we allow the regions to overlap which can give greater generalization as regions can share features.</p>
<p>In this video the term Reception Field is used to describe the region of the state space that a feature is associated with. This is an idea that comes from CNNs.</p>
</section>
<section id="generalization-properties-of-coarse-coding-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="generalization-properties-of-coarse-coding-video">Generalization Properties of Coarse Coding (Video)</h2>
<p>In this video Martha White discusses the generalization properties of coarse coding.</p>
<p>She looks at using small overlapping 1-d intervals to represent a 1-d function.</p>
<p>We see that changing shape size and number of effects the generalization properties of the representation.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/rl-scale-generalization.png" class="img-fluid figure-img"></p>
<figcaption>scale</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/rl-shape-generalization.png" class="img-fluid figure-img"></p>
<figcaption>shape</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/rl-shape-discrimination.png" class="img-fluid figure-img"></p>
<figcaption>discrimination</figcaption>
</figure>
</div></div>

<p>Next we looked at using short interval vs longer intervals to approximate a 1-d function. We see that the longer intervals give a smoother approximation.</p>
</section>
<section id="sec-l1g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g2">The trade-off between discrimination and generalization</h2>
</section>
<section id="tile-coding-video" class="level2">
<h2 class="anchored" data-anchor-id="tile-coding-video">Tile Coding (Video)</h2>
<p>In this video, Martha White introduces the concept of <strong>tile coding</strong>. This is simply a implementation of coarse coding using multiple overlapping grids.</p>
</section>
<section id="sec-l1g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g4">Explain how tile coding is a (computationally?) convenient case of coarse coding</h2>
<p>Tile coding is a computationally efficient implementation of coarse coding. We use multiple overlapping tilings to represent states. Each tiling is a grid of tiles. Each tile is a feature.</p>
<p>If we use one tiling we get state aggregation. If we use multiple tilings we get coarse coding. One tiling means we don’t discriminate between states that are in the same tile. Multiple tilings means we can discriminate between states that are in the same tile in one tiling but not in another.</p>
</section>
<section id="sec-l1g5" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g5">Describe how designing the tilings affects the resultant representation</h2>
<p>The textbook goes into some more details about how we can generalize using tile coding - using regular tilings generates in a diagonal pattern. Using random tilings generates more spherical regions.</p>
<p>However we also saw that the number size and shape of the tiles affects the generalization properties of the representation. And that increasing the overlap between the tiles an increase the discrimination properties of the representation.</p>
</section>
<section id="sec-l1g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g6"><em>Understand</em> that tile coding is a computationally efficient implementation of coarse coding</h2>
<p>Tile coding is a computationally efficient implementation of coarse coding. Since grids are uniform it is easy to compute which cells a state is in. A second reason is that end up with a sparse representations thus the dot product is just the sum of the weights of the active features for each state.</p>
<p>One caveat is that in high dimensional spaces we end up an exponential number of features. This is called the curse of dimensionality.</p>
</section>
<section id="using-tile-coding-in-td-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="using-tile-coding-in-td-video">Using Tile Coding in TD (Video)</h2>
<p>In this video, Adam White shows how to use tile coding in TD learning. He goes back to the 1000 state random walk example and shows how to use tile coding to approximate the value function. We end up needing six tiles.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-tile-coding-performance.png" class="img-fluid figure-img"></p>
<figcaption>tile coding v.s. state aggregation</figcaption>
</figure>
</div></div></section>
<section id="feature-construction-for-linear-methods" class="level2">
<h2 class="anchored" data-anchor-id="feature-construction-for-linear-methods">Feature Construction for Linear Methods</h2>
<p>In the textbook we see two forms of features for linear methods that are not covered in the videos.</p>
<p>The first are polynomials. We might use polynomials features for the state to represent the state space. This seems to be a good for problems where RL is dealing to a greater extent with interpolation or regression.</p>
<p>The following is given as an example of a polynomial feature representation of the state space. It took a bit of time to understand what was going on here.</p>
<p>They explain about the different combination of two features <img src="https://latex.codecogs.com/png.latex?s_1"> and <img src="https://latex.codecogs.com/png.latex?s_2"> doesn’t cover some edge cases but using four <img src="https://latex.codecogs.com/png.latex?(1,s_1,s_2,s_1s_2)"> covers all the possible combinations of the two features. We might also want to include higher powers of the atoms and that is what the polynomial representation is doing.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_i(s)%20=%20%5Cprod_%7Bj=1%7D%5Ek%20s_j%5E%7Bc_%7Bij%7D%7D%0A"></p>
<p>It important to point out that we are not using the polynomials as a function approximation basis function. What we are talking about is a formulation of multinomial from a set of fixed numbers <img src="https://latex.codecogs.com/png.latex?s_1%20%5Clsots%20s_k"> I.e. we are talking about all the possible products product from powers of these atoms.</p>
<p>The second are Fourier bases.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_i(s)%20=%20%5Ccos%5Cleft(%5Cfrac%7B2%5Cpi%20s%5ET%20a_i%7D%7Bb%7D%5Cright)%0A"></p>
<p>The book mentions that the Fourier basis is particularly useful for periodic functions.</p>
<p>There are many other orthogonal bases used as functnio expansions that could be used, as features for linear function approximation.</p>
<ul>
<li>Walsh functions and Haar wavelets have discrete support and are used in signal processing.</li>
<li>Legendre polynomials are used in physics.</li>
<li>Chebyshev polynomials are used in numerical analysis.</li>
</ul>
</section>
<section id="other-forms-of-coarse-coding" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="other-forms-of-coarse-coding">Other Forms of Coarse Coding</h2>
<p>In the textbook we see that there are other forms of coarse coding.</p>
<p>For example in section 9.5.5 we see using radial basis functions.</p>
<dl>
<dt>An RBF</dt>
<dd>
is a real-valued function whose value depends only on the distance between the input and a fixed point (called the center). &nbsp;
</dd>
</dl>
<p>Visualizing - Imagine a hill or bump centered at a specific point. The height of the hill at any other point depends solely on its distance from the center. The hill gets flatter as you move away from the cente</p>
<div class="page-columns page-full"><p> <img src="https://latex.codecogs.com/png.latex?%0Ax_i(s)%20=%20%5Cexp%5Cleft(-%5Cfrac%7B%5C%7Cs-c_i%5C%7C%5E2%7D%7B2%5Csigma_i%5E2%7D%5Cright)%0A"></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/notes-rl/posts/coursera/rl-radial-basis-functions.png" class="img-fluid" alt="one dimensional radial basis functions"></div></div>
<ul>
<li>Where</li>
<li><img src="https://latex.codecogs.com/png.latex?c_i"> is the center of the radial basis function and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csigma_i"> is the width.</li>
</ul>
<p>This is a form of coarse coding where the features are the distance from a set of centers. This is a more general representation than tile coding but less discriminative. The advantage of RBFs over tiles is that they are approximate functions that vary smoothly and are differentiable. However it appears there is both a computational cost and no real advantage in having continuous/differential features according to the book.</p>
<div id="cell-fig-radial-basis-functions" class="cell page-columns page-full" data-fig.height="3" data-fig.width="3" data-execution_count="1">

<div class="no-row-height column-margin column-container"><div class="cell-output cell-output-display">
<div id="fig-radial-basis-functions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-radial-basis-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/c3-w2_files/figure-html/fig-radial-basis-functions-output-1.png" width="662" height="470" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-radial-basis-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: One-dimensional radial basis functions with centers at -2, 0, and 2.
</figcaption>
</figure>
</div>
</div></div></div>
<p>I find this a bit disappointing as it seems like a nice intermediate step between linear function approximation with its convergence guarantees and neural networks which have no such guarantees.</p>
</section>
</section>
<section id="lesson-2-neural-networks" class="level1 page-columns page-full">
<h1>Lesson 2: Neural Networks</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Define</em> a neural network #</label></li>
<li><label><input type="checkbox" checked=""><em>Define</em> activation functions #</label></li>
<li><label><input type="checkbox" checked=""><em>Define</em> a feed-forward architecture #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how neural networks are doing feature construction #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how neural networks are a non-linear function of state #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how deep networks are a composition of layers #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> the tradeoff between learning capacity and challenges presented by deeper networks #</label></li>
</ul>
</div>
</div>
<section id="what-is-a-neural-network-video" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-neural-network-video">What is a Neural Network? (Video)</h2>
<p>In this video, Martha White introduces the concept of a neural network. We look at a simple one layer feed forward neural network. Where the <img src="https://latex.codecogs.com/png.latex?output=f(sW)"> is a non-linear function of the input.</p>
</section>
<section id="sec-l2g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g1">Define a neural network</h2>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-feedforward-nn.png" class="img-fluid"></div></div>
<p>A Neural network consists of a network of nodes which process and pass on information.</p>
<ul>
<li>The circles are the noes</li>
<li>The lines are the connections</li>
<li>The nodes are organized in layers</li>
</ul>
<p>Data starts at the input layer. It is passed through the connections to the hidden layer. The hidden layer is preforms some computation on the data and passes it to the output layer. This process repeats until the last layer produces the output of the network.</p>
</section>
<section id="deep-neural-networks-video" class="level2">
<h2 class="anchored" data-anchor-id="deep-neural-networks-video">Deep Neural Networks (Video)</h2>
<p>In this video, Martha White introduces the concept of neural networks with multiple hidden layers and activation functions.</p>
</section>
<section id="neural-networks-mechanics" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks-mechanics">Neural Networks Mechanics</h2>
<p>A node in the network is a function</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aoutput%20=%20f%5B(w_1%20%5Ctimes%20input_1)%20+%20(w_2%20%5Ctimes%20input_2)%20+%20%5Cldots%20+%20(w_n%20%5Ctimes%20input_n)%20+%20b%5D%0A"></p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?w_i"> are the weights,</li>
<li><img src="https://latex.codecogs.com/png.latex?input_i"> are the inputs, and</li>
<li><img src="https://latex.codecogs.com/png.latex?b"> is the bias.</li>
<li><img src="https://latex.codecogs.com/png.latex?f"> is the activation function.</li>
</ul></li>
</ul>
<p>The sum of the product of the weights and inputs is a linear operation. The activation function <img src="https://latex.codecogs.com/png.latex?f"> is where a non-linearity is introduced into the network.</p>
</section>
<section id="sec-l2g2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g2">Define activation functions</h2>
<p>Activation functions are non-linear functions that are applied to the output of a node. They introduce non-linearity into the network.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-activation-functions-tanh.png" class="img-fluid figure-img"></p>
<figcaption>tanh activation</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-activation-functions-relu.png" class="img-fluid figure-img"></p>
<figcaption>rectified linear activation function</figcaption>
</figure>
</div></div>
<p>Martha White also mentions threshold activation functions. However these are not used in practice as they are not differentiable. There is some work since this course came out on compressing neural networks to use threshold activation functions which are easy to compute on a CPU as matrix multiplication becomes a series of comparisons. However these are trained with a differentiable approximation of the threshold function and then quantized to the threshold function.</p>
</section>
<section id="the-neural-network-implementation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-neural-network-implementation">The Neural Network Implementation</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-nn-implementation.png" class="img-fluid figure-img"></p>
<figcaption>Neural Network Implementation</figcaption>
</figure>
</div></div><p>A neural network is a parameterized function that is a composition of linear and non-linear functions. It is a function of the state. The linear functions are the weights and the non-linear functions are the activation functions. The weights are learned from data.</p>
</section>
<section id="sec-l2g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g3">Define a feed-forward architecture</h2>
<p>A feed forward architecture is a neural network where the connections between nodes do not form a cycle. The data flows from the input layer to the output layer.</p>
<p>An example of a non-feed forward architecture is a recurrent neural network where the connections between nodes form cycles.</p>
</section>
<section id="non-linear-approximation-with-neural-networks-video" class="level2">
<h2 class="anchored" data-anchor-id="non-linear-approximation-with-neural-networks-video">Non-linear Approximation with Neural Networks (video)</h2>
</section>
<section id="sec-l2g4" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g4">How Neural Networks are doing feature construction</h2>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/rl-activation-1.png" class="img-fluid figure-img"></p>
<figcaption>neural feature 1</figcaption>
</figure>
</div>
<p>darker means greater activation for the feature</p>
</div><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/rl-activation-2.png" class="img-fluid figure-img"></p>
<figcaption>neural feature 2</figcaption>
</figure>
</div>
<p>the one generalize differently</p>
</div></div>
<p>We construct a non-linear function of the state using a neural network.</p>
<p>recall A node takes the form</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aoutput%20=%20f%5B(w_1%20%5Ctimes%20input_1)%20+%20%5Cldots%20+%20(w_n%20%5Ctimes%20input_n)%20+%20b%5D%0A"></p>
<p>We call this output of the node a feature! We can see that these features are a non-linear function of the inputs. We repeat this process until we evaluate all the nodes of the final layer. And the output of this final layer is called the representation.</p>
<p>Note: This is not very different from tile coding where we pass input to a tile coder and get back a new representation of the state.</p>
<p>In both cases we are constructing a non-linear mapping of the input of the features. And we take a nonlinear function of the representation to form the output - a nonlinear approximation of the state.</p>
<p>Recall that in tile coding we had to set some hyper-parameters: size shape of tiles + number of tiling. These are fixed before training. In a neural network we also have hyperparameters for the size of the layers, the number of layers, the activation functions. These too are fixed before training.</p>
<p>The difference is that Neural networks have weights that get updated during training. But tile coding does not change during training.</p>
</section>
<section id="sec-l2g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g5">How neural networks are a non-linear function of state</h2>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/rl-activation-3.png" class="img-fluid figure-img"></p>
<figcaption>neural feature 3</figcaption>
</figure>
</div>
<p>there are no hard boundaries</p>
</div><div class="">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/rl-activation-4.png" class="img-fluid" alt="neural feature 4"> this shows how it generelises</p>
</div></div>
<p>Neural networks are non linear functions of the because of the non-linear nature of the activation functions. These are applied recursively as we move to the final layer.</p>
</section>
<section id="deep-neural-networks-video-1" class="level2">
<h2 class="anchored" data-anchor-id="deep-neural-networks-video-1">Deep Neural Networks (Video)</h2>
</section>
<section id="sec-l2g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g6">How deep networks are a composition of layers</h2>
<p>Neural networks are modular. We can add or remove layers. Each layer is a function of the previous layer. The output of the previous layer is the input to the next layer.</p>
<p>Depth allows composition of features. Each layer can learn a different representation of the input. The final layer can learn a representation of the input that is a composition of the representations learned by the previous layers</p>
<p>We can design the network to remove undesirable features. For example we can design a network with a bottleneck that has less features than the input. This forces the network to learn a compressed representation of the input.</p>
</section>
<section id="sec-l2g7" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g7">The tradeoff between learning capacity and challenges presented by deeper networks</h2>
<p>Depth can increase the learning capacity of the network by allowing the network to learn complex compositions and abstractions. However, deeper networks are harder to train.</p>
</section>
</section>
<section id="lesson-3-training-neural-networks" class="level1 page-columns page-full">
<h1>Lesson 3: Training Neural Networks</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Compute</em> the gradient for a single hidden layer neural network #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how to compute the gradient for arbitrarily deep networks #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> the importance of initialization for neural networks #</label></li>
<li><label><input type="checkbox" checked=""><em>Describe</em> strategies for initializing neural networks #</label></li>
<li><label><input type="checkbox" checked=""><em>Describe</em> optimization techniques for training neural networks #</label></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discussion prompt
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>What properties of the representation are important for our online setting? This contrasts the offline, batch setting.</p>
</blockquote>
</div>
</div>
<section id="gradient-descent-for-training-neural-networks-video" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="gradient-descent-for-training-neural-networks-video">Gradient Descent for Training Neural Networks (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/nn-notation.png" class="img-fluid"></p>
</div></div><p>If we use the square error loss then</p>
<p><span id="eq-loss"><img src="https://latex.codecogs.com/png.latex?%0AL(%5Chat%20y_k,y_k)%20=%20(%5Chat%20y_k-y_k)%5E2%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AA%20=%20A%20%E2%88%92%CE%B1%CE%B4%5EAs%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB%20=%20B%20%E2%88%92%CE%B1%CE%B4%5EBx%0A"></p>
<p>Let’s start at the output of the network and work backwards. Recall: <img src="https://latex.codecogs.com/png.latex?%0Ax%20=%20f_A(sA)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7By%7D%20=%20f_B(xB)%0A"></p>
<p>We start by taking the partial derivative of the loss function with respect to the first set of weights B.</p>
<p>We use the chain rule given the derivative of L with respect to <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D%20%5Ctimes%20%5Cfrac%7B%E2%88%82%5Chat%7By%7D%7D%7B%E2%88%82B%7D">. The next step is again to use the chain rule for this derivative.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82%5Chat%7By%7D_k%7D%20%5Cfrac%7B%E2%88%82%5Chat%7By%7D_k%7D%7B%E2%88%82B_%7Bjk%7D%7D%0A"></p>
<p>let’s introduce a new variable, θ where θ is the output of the hidden layer times the last set of weights.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%CE%B8%20%5Cdot%20=%20xB%0A"></p>
<p>Thus</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%20y%20%5Cdot%20=%20f_B(%CE%B8)%0A"></p>
<p>Rewriting we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82%5Chat%7By%7D_k%7D%20%5Cfrac%7B%E2%88%82f_B(%5Ctheta_k)%7D%7B%E2%88%82%5Ctheta%7D%20%5Cfrac%7B%E2%88%82%5Ctheta_k%7D%7B%E2%88%82B_%7Bjk%7D%7D%0A"></p>
<p>and since</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82%5Ctheta_k%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20x_j%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82%5Chat%7By%7D_k%7D%20%5Cfrac%7B%E2%88%82f_B(%5Ctheta_k)%7D%7B%E2%88%82%5Ctheta%7D%20x_j%0A"></p>
<p>now that we calculated the gradient for the last layer we can move to the previous layer.</p>
<p>we use</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi%20%5Cdot%20=%20%20sA%0A"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax%20%5Cdot%20=%20f_A(%5CPsi)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82A_%7Bij%7D%7D%20&amp;=%20%5Cdelta_k%5EB%20%5Cfrac%20%7B%E2%88%82%5Ctheta_k%7D%7B%E2%88%82A_%7Bij%7D%7D%20%5Cnewline%0A&amp;%20=%20%5Cdelta_k%5EB%20B_%7Bjk%7D%20%5Cfrac%20%7B%E2%88%82x_j%7D%7B%E2%88%82A_%7Bij%7D%7D%20%5Cnewline%0A%20%20&amp;%20=%20%5Cdelta_k%5EB%20B_%7Bjk%7D%20%5Cfrac%20%7B%E2%88%82f_A(%5CPsi_j)%7D%7B%E2%88%82%5CPsi_j%7D%20%5Cfrac%20%7B%E2%88%82%5CPsi_j%7D%7B%E2%88%82A_%7Bij%7D%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>since</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%20%7B%E2%88%82%5CPsi_j%7D%7B%E2%88%82A_%7Bij%7D%7D%20=%20s_%7Bij%7D%0A"></p>
<p>we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82A_%7Bij%7D%7D%20=%20%5Cdelta_k%5EB%20B_%7Bjk%7D%20%5Cfrac%20%7B%E2%88%82f_A(%5CPsi_j)%7D%7B%E2%88%82%5CPsi_j%7D%20s_%7Bij%7D%0A"></p>
<p>We can clean up this derivative by again, defining a term <img src="https://latex.codecogs.com/png.latex?%CE%B4_A">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%CE%B4%5EA_j%20=%20(B_%7Bjk%7D%CE%B4%5EB_k%20)%20%5Cfrac%7B%E2%88%82f_A(%CF%88_j)%7D%7B%E2%88%82%CF%88_j%7D%0A"></p>
<p>The final result will be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82A_%7Bij%7D%7D=%20%CE%B4%5EA_j%20s_i%0A"></p>
<p>Obtaining as a final result for both gradients the next expressions</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20%CE%B4%5EB_k%20x_j%0A"></p>
</section>
<section id="sec-l3g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g1">Computing the Gradient for a Single Hidden Layer Neural Network</h2>
<p>Let’s summerize the results:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82B_%7Bjk%7D%7D%20=%20%CE%B4%5EB_k%20x_j%20%5Cqquad%0A%5Cfrac%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82A_%7Bij%7D%7D=%20%CE%B4%5EA_j%20s_i%0A"></p>
<p>where:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%CE%B4%5EB_k%20=%20%5Cfrac%20%7B%E2%88%82L(%5Chat%7By%7D_k,y_k)%7D%7B%E2%88%82%5Chat%7By%7D_k%7D%20%5Cfrac%7B%E2%88%82f_B(%5Ctheta_k)%7D%7B%E2%88%82%5Ctheta%7D%20%5Cqquad%0A%CE%B4%5EA_j%20=%20(B_%7Bjk%7D%CE%B4%5EB_k%20)%20%5Cfrac%7B%E2%88%82f_A(%CF%88_j)%7D%7B%E2%88%82%CF%88_j%7D%0A"></p>
</section>
<section id="sec-l3g2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g2">Computing the Gradient for Arbitrarily Deep Networks</h2>

<div class="no-row-height column-margin column-container"><div id="fig-gradient-descent-pseudo-code" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-descent-pseudo-code-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/nn-backpop.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-descent-pseudo-code-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Gradient Descent Pseudo-code
</figcaption>
</figure>
</div><div id="fig-gradient-descent-RELU" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-descent-RELU-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/nn-backpop-relu.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-descent-RELU-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Gradient Descent Pseudo-code for RELU
</figcaption>
</figure>
</div></div>
<p>Now that we have estimated the gradient for a hidden layer neural network. We can use it to learn to optimize the weights of the network by updating the weights to minimize the error in the loss function in the direction of the negative gradient.</p>
<p>The pseudocode in the figure outlines how to implementing the backprop algorithm with Stochastic gradient descent.</p>
<p>For each data point s, y in our dataset, we first get our prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> from the network. This is the forward pass. Then we can estimate the loss using the actual value <img src="https://latex.codecogs.com/png.latex?y"></p>
<p>Next we compute the gradients starting from the output. We first compute <img src="https://latex.codecogs.com/png.latex?%CE%B4%5EB"> and the gradient for <img src="https://latex.codecogs.com/png.latex?B">, then we use this gradient to update the parameters <img src="https://latex.codecogs.com/png.latex?B">, with the step size <img src="https://latex.codecogs.com/png.latex?%CE%B1_B"> for the last layer.</p>
<p>Next, we update the parameters <img src="https://latex.codecogs.com/png.latex?A">. We compute <img src="https://latex.codecogs.com/png.latex?%CE%B4%5EA"> which reuses <img src="https://latex.codecogs.com/png.latex?%CE%B4%5EB">.</p>
<p>Notice, that by computing the gradients of the end of the network first, we avoid recomputing the same terms for A, that were already computed for <img src="https://latex.codecogs.com/png.latex?%CE%B4B">. We then compute the gradient for A and update A with this gradient using step size <img src="https://latex.codecogs.com/png.latex?%CE%B1_A">.</p>
<hr>
<p>Next we look at how we adapt the pseudocode to work with the ReLU activation on the hidden layer and a linear unit for the output.</p>
<p>First, we compute the error for the output layer, then we compute the derivative of the ReLU units with respect to <img src="https://latex.codecogs.com/png.latex?%5CPsi">, and finally, we use the aerial signal from the output layer along with you to compute the air signal for the hidden layer, the rest remains the same</p>
</section>
<section id="optimization-strategies-for-nns-video" class="level2">
<h2 class="anchored" data-anchor-id="optimization-strategies-for-nns-video">Optimization Strategies for NNs (Video)</h2>
</section>
<section id="sec-l3g3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g3">The Importance of Initialization for Neural Networks</h2>
<p>One simple yet effective initialization strategy for the weights, is to randomly sample the initial weights from a normal distribution with small variance Fig. 42. This way, each neuron has a different output from other neurons within its layer. This provides a more diverse set of potential features.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-feedforward-nn.png" class="img-fluid figure-img"></p>
<figcaption>Weights initialization</figcaption>
</figure>
</div></div><p>By keeping the variants small, we ensure that the output of each neuron is within the same range as its neighbors. One downside to this strategy is that, as we add more inputs to a neuron, the variance of the output grows.</p>
</section>
<section id="sec-l3g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g4">Strategies for Initializing Neural Networks</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0AW_%7Binit%7D%20~%20N(0,1)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AW_%7Binit%7D%20~%20%5Cfrac%7BN(0,1)%7D%7B%5Csqrt%7Bn_%7Bin%7D%7D%7D%0A"></p>
</section>
<section id="sec-l3g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g5">Optimization Techniques for Training Neural Networks</h2>
<ul>
<li>momentum update AKA heavy ball method <img src="https://latex.codecogs.com/png.latex?%0AW_%7Bt+1%7D%20%E2%86%90%20W_t%20%E2%88%92%CE%B1%E2%88%87_wL(W_t)%20+%20%CE%BBM_t%0A"></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AM_%7Bt+1%7D%20=%20%CE%BBM_t%20%E2%88%92%CE%B1%E2%88%87_wL%0A"></p>
<p>vector step size adaptation</p>
<ul>
<li>separate step size for each weight</li>
</ul>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/notes-rl/posts/coursera/nn-vector-step-size.png" class="img-fluid"></div></div>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w2.html</guid>
  <pubDate>Mon, 01 Apr 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Constructing Features for Prediction</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w2.1.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.4-9.5.0, pp. 204-210]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.7, pp. 223-228]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
</ul>
</div>
</div>
</div>
<p>This is not a video lecture or notes for a learning goal. This is however my attempts to cover some material from the readings from chapter 9 of <span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement]</span> mentioned above.</p>
<p>I’ve added this material about a year after completing the specialization as I have been taking a course on deep reinforcement learning. I had felt that the material in this course had been both challenging on occasion and rather basic on others.</p>
<p>I don’t think I would have would have gone ahead and created this unit without looking at the material by Shangtong Zhang at https://github.com/ShangtongZhang/reinforcement-learning-an-introduction In which he reproduced some of the figures from the book. However this is my own implementation and though similar to some of the course material - I follow the Coursera Honor code and this is not likely to be much help to anyone working on the programming assignments which is a different environment, uses RL-glue and an implementation of tile coding.</p>
<p>What I felt was that I was not happy about the basics in this chapter. This includes the parameterization of the value function, the convergence results regarding linear function approximation. The ideas about why TD is a semi-gradient method etc.</p>
<p>I am also having many idea about both creating algorithms for creating features for RL environments. As I get familiar with gridwolds, atari games, sokoban etc I had many good ideas for making progress in both these environments and for improving algorithms for more general cases.</p>
<p>For example it appears that in many papers it turns out the agents are not learning very basic abilities. My DQN agent for space invaders was very poor at shooting bullets. I had an few ideas that should make a big difference. Like adding features for bullets, the invaders and so on. This are kind of challenging to implement in the current gymnasium environments. However I soon had a much more interesting idea that seems to be good for many of the atari environments and quite possibly even more broadly to most cnn based agents.</p>
<ul>
<li>In brief this would combine
<ul>
<li>a multiframe YOLO,</li>
<li>a generelised value function to replace YOLO’s supervision</li>
<li>a ‘robust’ causal attention mechanism to decide which pixels are more or less important
<ul>
<li>dropping them would not impact performance. e.g.&nbsp;bullets</li>
<li>which affect survival e.g.&nbsp;certain bullets</li>
<li>for scoring e.g.&nbsp;mother ship</li>
<li>which ones we can influence e.g.&nbsp;standing under an invader gets it to shoot at us.</li>
</ul></li>
</ul></li>
</ul>
<p>Note this is not the causal attention mechanism from NLP where one censored the future inputs but rather a mechanism that decides which pixels represent features that are potentialy the cause of the future states.</p>
<p>Clearly this Algorithm and its parts need to be worked out in a very simple environment. The YOLO part is all about modeling features using bounding boxes via a single pass. The GVFs are to replace yolo supervision loss with a RL compatible loss and the causal attention to reduce the overfitting and speed up learning.</p>
<p>I decided that converting some of these simple environments to gymnasium environments would be a good way to kick start some of these ideas, more so as reviewing papers and talks by Adam and Martha White shows that most experiments in RL environments turn out to be too complicated and access to simple environments turns out to be the way to get the experiments started.</p>
<p>In this chapter we use a simple environment called the 1000 state Random walk. I implemented this independently in Python.</p>
<p>We also learned the MC prediction algorithm and the TD(0) algorithms for function approximation.</p>
<p>We will use these algorithms to learn the value function of the Random Walk 1000 environment. We will also use tile coding and coarse coding to create features for the Random Walk 1000 environment.</p>
</section>
<section id="feature-construction-linear-function-approximation" class="level1">
<h1>Feature Construction &amp; linear Function approximation</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this module we consider environments that allow to consider simple state aggregationI’d like to cover the following topics:</p>
<ol type="1">
<li><label><input type="checkbox" checked="">create gymnasium compatible environment for the 1000 step Random Walk environment.</label>
<ul>
<li>This will allow to use it with many RL agents built using many libraries.</li>
<li><label><input type="checkbox" checked="">I’d also want to extend this environment to include a neighborhood size parameter - this will set the bound how far a left or right step will move us.</label></li>
<li><label><input type="checkbox" checked="">A dimension parameter to control dimensions of the state space. This can allow us to consider state aggregation in one two and three dimensions.</label></li>
</ul></li>
<li><label><input type="checkbox" checked="">Plot the trajectory of the random walk.</label></li>
<li><label><input type="checkbox">Implement the function approximation environment for gymnasium.</label>
<ul>
<li><label><input type="checkbox">extend to looking at interpolation and regression problems in the context of RL. (How - using a different dataset loaded by pandas?) Time series, Tabular data, Clustering, Classification, Regression, Pricing with elasticity. Multi-dimensional data.</label></li>
<li>This environment can be a basis for looking at how Temporal Abstraction aggregation play with function approximation in a highly simplified form.</li>
<li>This is a fundamental issue that <a href="https://www.youtube.com/watch?v=GntIVgNKkCI">Doina Precup</a> has raised in her talks as an ongoing area of research.</li>
<li>So such an environment might be useful in testing how different approaches can handle these issues in an environment that is very close to supervised learning.</li>
</ul></li>
<li>implement Gradient MC agent</li>
<li>implement Semi Gradient TD(0) agent</li>
<li>use these agents with and without state aggregation to learn the value function of the Random Walk 1000 environment.</li>
<li>implement coarse coding via tile coding to create features for the Random Walk 1000 environment.</li>
<li>implement use of <strong>polynomial features</strong> to create features for the Random Walk 1000 environment.</li>
<li>implement use of <strong>radial basis functions</strong> to create features for the Random Walk 1000 environment.</li>
<li>implement use of <strong>Fourier basis functions</strong> to create features for the Random Walk 1000 environment.</li>
</ol>
</div>
</div>
<section id="the-1000-step-random-walk-environment" class="level2">
<h2 class="anchored" data-anchor-id="the-1000-step-random-walk-environment">The 1000 Step Random Walk Environment</h2>
<p>In this lesson we implement the 1000 Random Walk example as an environment. This is good to demonstrate how to construct features for linear methods. We will use tile coding and coarse coding to create features for the Random Walk 1000 environment.</p>
<div id="2ee026e5" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gymnasium <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> gym</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> gymnasium <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> spaces</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> RandomWalk1000(gym.Env):</span>
<span id="cb1-6">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, neighborhood_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb1-7">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb1-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_states <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> num_states</span>
<span id="cb1-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> neighborhood_size</span>
<span id="cb1-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.observation_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> spaces.Discrete(num_states <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># add two states 0 and num_states + 1 as terminal states</span></span>
<span id="cb1-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.action_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> spaces.Discrete(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 0 for left, 1 for right</span></span>
<span id="cb1-12">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># start in the middle</span></span>
<span id="cb1-13">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.np_random, seed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gym.utils.seeding.np_random(seed)</span>
<span id="cb1-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.trajectory <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>]</span>
<span id="cb1-15"></span>
<span id="cb1-16">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> reset(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, options<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb1-17">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().reset(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seed)</span>
<span id="cb1-18">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span></span>
<span id="cb1-19">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.trajectory <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>]</span>
<span id="cb1-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state, {}</span>
<span id="cb1-21"></span>
<span id="cb1-22">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, action):</span>
<span id="cb1-23"></span>
<span id="cb1-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> action <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># move left</span></span>
<span id="cb1-25">             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># left neighbours</span></span>
<span id="cb1-26">            left_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size)</span>
<span id="cb1-27">            left_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state</span>
<span id="cb1-28">            num_left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> left_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> left_start</span>
<span id="cb1-29"></span>
<span id="cb1-30">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> left_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb1-31">                prob_terminate_left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> num_left) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size</span>
<span id="cb1-32">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-33">                prob_terminate_left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-34">            </span>
<span id="cb1-35">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.np_random.random() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> prob_terminate_left:</span>
<span id="cb1-36">               </span>
<span id="cb1-37">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, {} <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># terminate left</span></span>
<span id="cb1-38"></span>
<span id="cb1-39">            next_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.np_random.integers(low<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>left_start, high<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>left_end)</span>
<span id="cb1-40"></span>
<span id="cb1-41"></span>
<span id="cb1-42">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> action <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># move right</span></span>
<span id="cb1-43">             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># right neighbours</span></span>
<span id="cb1-44">            right_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-45">            right_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_states <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-46">            num_right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> right_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> right_start</span>
<span id="cb1-47">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> right_end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_states <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb1-48">                 prob_terminate_right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> num_right) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.neighborhood_size</span>
<span id="cb1-49">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-50">                prob_terminate_right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-51">            </span>
<span id="cb1-52">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.np_random.random() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> prob_terminate_right:</span>
<span id="cb1-53"></span>
<span id="cb1-54">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_states <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, {} <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># terminate right</span></span>
<span id="cb1-55"></span>
<span id="cb1-56">            next_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.np_random.integers(low<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>right_start, high<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>right_end)</span>
<span id="cb1-57">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-58">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">ValueError</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Invalid action"</span>)</span>
<span id="cb1-59"></span>
<span id="cb1-60">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> next_state</span>
<span id="cb1-61"></span>
<span id="cb1-62">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.trajectory.append(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state)</span>
<span id="cb1-63">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_state, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, {} <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># not terminated or truncated</span></span>
<span id="cb1-64"></span>
<span id="cb1-65"></span>
<span id="cb1-66"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-67"></span>
<span id="cb1-68"></span>
<span id="cb1-69"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_trajectory(trajectory, num_states):</span>
<span id="cb1-70">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Plots the trajectory of the random walk."""</span></span>
<span id="cb1-71">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(trajectory))</span>
<span id="cb1-72">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(trajectory)</span>
<span id="cb1-73">    </span>
<span id="cb1-74">    plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb1-75">    plt.plot(x, y, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-'</span>, markersize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb1-76">    plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Time Step'</span>)</span>
<span id="cb1-77">    plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'State'</span>)</span>
<span id="cb1-78">    plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Random Walk Trajectory'</span>)</span>
<span id="cb1-79">    plt.yticks(np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>))</span>
<span id="cb1-80">    plt.grid(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'y'</span>)</span>
<span id="cb1-81"></span>
<span id="cb1-82">    plt.tight_layout()</span>
<span id="cb1-83">    plt.show()</span></code></pre></div>
</details>
</div>
<div id="eef7b364" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#import gymnasium as gym</span></span>
<span id="cb2-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#from random_walk_gym import RandomWalk1000</span></span>
<span id="cb2-3"></span>
<span id="cb2-4">env <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomWalk1000()</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Reset the env</span></span>
<span id="cb2-7">obs, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.reset()</span>
<span id="cb2-8">terminated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb2-9"></span>
<span id="cb2-10"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> terminated:</span>
<span id="cb2-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># For this environment, an action is not needed.</span></span>
<span id="cb2-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Here we pass in a dummy value</span></span>
<span id="cb2-13">    obs, reward, terminated, truncated, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.step(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb2-14">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"State: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, Reward: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>reward<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, Terminated: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>terminated<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-15"></span>
<span id="cb2-16">env.close()</span>
<span id="cb2-17"></span>
<span id="cb2-18">plot_trajectory(env.trajectory, num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>env.num_states)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>State: 456, Reward: 0, Terminated: False
State: 364, Reward: 0, Terminated: False
State: 309, Reward: 0, Terminated: False
State: 261, Reward: 0, Terminated: False
State: 198, Reward: 0, Terminated: False
State: 174, Reward: 0, Terminated: False
State: 115, Reward: 0, Terminated: False
State: 109, Reward: 0, Terminated: False
State: 21, Reward: 0, Terminated: False
State: 12, Reward: 0, Terminated: False
State: 1, Reward: -1, Terminated: True</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/c3-w2.1_files/figure-html/cell-3-output-2.png" width="1142" height="374" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="3b5f44d1" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">env <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomWalk1000(num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, neighborhood_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb4-2">obs, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.reset()</span>
<span id="cb4-3">terminated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb4-4">truncated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb4-5">trajectory <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb4-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> terminated <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> truncated:</span>
<span id="cb4-7">    action <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.action_space.sample()  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Replace with your policy</span></span>
<span id="cb4-8">    obs, reward, terminated, truncated, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.step(action)</span>
<span id="cb4-9">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>obs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>action<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>reward<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>terminated<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb4-10"></span>
<span id="cb4-11">plot_trajectory(env.trajectory, num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>env.num_states)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>obs=457, action=0, reward=0, terminated=False
obs=414, action=0, reward=0, terminated=False
obs=456, action=1, reward=0, terminated=False
obs=531, action=1, reward=0, terminated=False
obs=589, action=1, reward=0, terminated=False
obs=635, action=1, reward=0, terminated=False
obs=543, action=0, reward=0, terminated=False
obs=576, action=1, reward=0, terminated=False
obs=647, action=1, reward=0, terminated=False
obs=680, action=1, reward=0, terminated=False
obs=772, action=1, reward=0, terminated=False
obs=744, action=0, reward=0, terminated=False
obs=676, action=0, reward=0, terminated=False
obs=730, action=1, reward=0, terminated=False
obs=654, action=0, reward=0, terminated=False
obs=618, action=0, reward=0, terminated=False
obs=543, action=0, reward=0, terminated=False
obs=469, action=0, reward=0, terminated=False
obs=569, action=1, reward=0, terminated=False
obs=482, action=0, reward=0, terminated=False
obs=492, action=1, reward=0, terminated=False
obs=401, action=0, reward=0, terminated=False
obs=336, action=0, reward=0, terminated=False
obs=316, action=0, reward=0, terminated=False
obs=286, action=0, reward=0, terminated=False
obs=340, action=1, reward=0, terminated=False
obs=350, action=1, reward=0, terminated=False
obs=288, action=0, reward=0, terminated=False
obs=319, action=1, reward=0, terminated=False
obs=259, action=0, reward=0, terminated=False
obs=290, action=1, reward=0, terminated=False
obs=335, action=1, reward=0, terminated=False
obs=272, action=0, reward=0, terminated=False
obs=313, action=1, reward=0, terminated=False
obs=355, action=1, reward=0, terminated=False
obs=278, action=0, reward=0, terminated=False
obs=203, action=0, reward=0, terminated=False
obs=150, action=0, reward=0, terminated=False
obs=240, action=1, reward=0, terminated=False
obs=152, action=0, reward=0, terminated=False
obs=66, action=0, reward=0, terminated=False
obs=111, action=1, reward=0, terminated=False
obs=157, action=1, reward=0, terminated=False
obs=58, action=0, reward=0, terminated=False
obs=0, action=0, reward=-1, terminated=True</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/c3-w2.1_files/figure-html/cell-4-output-2.png" width="1142" height="374" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Lets simulate the random walk till success and plot its trajectory.</p>
<div id="3ad5d6ea" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">env <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomWalk1000(num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, neighborhood_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb6-2">obs, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.reset()</span>
<span id="cb6-3">terminated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb6-4">truncated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb6-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> terminated <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> truncated:</span>
<span id="cb6-6">    action <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.action_space.sample()  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Replace with your policy</span></span>
<span id="cb6-7">    obs, reward, terminated, truncated, info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.step(action)</span>
<span id="cb6-8"></span>
<span id="cb6-9"></span>
<span id="cb6-10">plot_trajectory(env.trajectory, num_states<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>env.num_states)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/c3-w2.1_files/figure-html/cell-5-output-1.png" width="1142" height="374" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="a-short-digression-on-einstein-tiling-for-rl" class="level2">
<h2 class="anchored" data-anchor-id="a-short-digression-on-einstein-tiling-for-rl">A short digression on Einstein Tiling for RL</h2>
<p>One (awful) Idea I keep returning to is to use Einstein Tiling for RL. I mentino that Einstein in this context is not the physicist but rather a pun on the word ‘Einstein’ which means ‘one stone’ in German.</p>
<p>Let’s quickly review why it is a bad idea, and then why it is also a fascinating idea.</p>
<ol type="1">
<li><p>Unlike A square tiling this is an aperiodic tiling so we need to generate it efficiently. Depending on the space it will may take some time to generate the tiling. We need to store the tiling in memory. For a square tiling we can generate the tiling in a few lines of code. We can access the tiling or tile using a simple formula.</p></li>
<li><p>We need a quick way to find which tile a point is in. This is not to hard for one tile. But as the number of tiles increases this becomes more difficult. It is trivial for a square tiling where again we have a formula to efficiently determine the tile a point belongs to.</p></li>
</ol>
<p>Some reasons why it is a fascinating idea.</p>
<ol type="1">
<li>We only need one tiling. If we have one we can map the first tile ton any other location it is in the same orientation and we will get a new tiling! This is due to the aperiodic nature of the tiling.</li>
<li>The hat tile is constructed by glueing together eight smaller kite tiles that are sixth of a hexagon. We can easily use larger kites that so we can use two such grids as coarse and coarser tilings.</li>
<li>Different can be similar locally but will tend to diverge. This suggest that we will get a good generalization.</li>
<li>There may be variant einsteins that are easier to generate and use</li>
<li>In https://www.ams.org/journals/proc/1995-123-11/S0002-9939-1995-1277129-X/ the authors show that for d&gt;=3 aperiodic tilings can naturally avoid more symmetries than just translations. I.e. we can have a periodic tilings in higher dimensions.</li>
</ol>
<p>I may be wrong but It may be possible to generate the tiling using a simple formula. Ok so far tiling generation is insanely complicated. Though this is not a judgment on the complexity of the tiling but rather the complexity of the code and mathematics to generate the tiling.</p>
<p>The hat tile allows one to create many different tilings of the state space in two dimensions. Each tiling is going to have a different set of features.</p>
<p>As the hat tile is constructed by glueing together eight smaller tiles. Tilings are created in a hierarchical manner. This suggests that we can will get fine and course feature in this process and that we can just keep going to increase discrimination.</p>
<p>Some issues - it is possible to get two tilings that are the same but for a ‘conway worm’ this is a curve in the tiling that is different. The problem here is that the features will be the same for every where except the worm. Not good for generalization.</p>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w2.1.html</guid>
  <pubDate>Mon, 01 Apr 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>On-Policy Prediction with Approximation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w1.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level1 page-columns page-full">
<h1>Introduction</h1>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<ul>
<li>We now start the third course in the reinforcement learning specialization.</li>
<li>In terms of the <strong>?@fig-rl-chart</strong> we are on the left branch of the tree.</li>
<li>This course is about prediction and control with function approximation.</li>
<li>The main difference in this course is that will start consider continuous state spaces and action spaces which cannot be represented as tables.
<ul>
<li>However many of the methods we will develop will be useful in handling large scale tabular problems as well.</li>
<li>We will use methods from supervised learning but only online methods that can handle non-stationary data.</li>
<li>The main differences are the use of weights to parameterize the value functions.</li>
<li>The use of function approximation to estimate value functions and policies in reinforcement learning.
<ul>
<li>Weights lead to using a loss function to estimate the value function.</li>
<li>Minimizing the continuous loss function leads to Gradient descent and</li>
<li>Using sampling leads to Stochastic gradient descent.</li>
</ul></li>
<li>The idea of learning weights rather than values is a key idea in this course.</li>
<li>The tradeoff between discrimination and generalization is also a key idea in this course.</li>
</ul></li>
<li>We will learn how to use function approximation to estimate value functions and policies in reinforcement learning.</li>
<li>We will also learn how to use function approximation to solve large-scale reinforcement learning problems.</li>
<li>We will see some simple linear function approximation methods and later</li>
<li>We will see modern nonlinear approximation methods using deep neural networks.</li>
</ul>
<p>I did not find the derivation of the SGD alg particularly enlightening and I have seen it several times. However the online setting is the best motivation for the use of SGD and makes perfect sense in the context of reinforcement learning. Minibatches are then a natural extension of this idea.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§9.1-9.4, pp. 194-209]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=194">book</a></label></li>
</ul>
</div>
</div>
</div>
</section>
<section id="lesson-1-estimating-value-functions-as-supervised-learning" class="level1 page-columns page-full">
<h1>Lesson 1: Estimating Value Functions as Supervised Learning</h1>
<p>In this lesson we will cover some important notation that will remain with us till the end of the course. Mathematically most of it is trivial, but it is important to understand the notation - otherwise the rest of the course will be hard to follow.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><p><label><input type="checkbox" checked=""><em>Understand</em> how we can use <strong>parameterized functions</strong> to approximate value functions #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Explain</em> the meaning of <strong>linear value function approximation</strong> #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Recognize</em> that the tabular case is a special case of linear value function approximation #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Understand</em> that there are many ways to parameterize an approximate value function #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Understand</em> what is meant by <strong>generalization</strong> and <strong>discrimination</strong> #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Understand</em> how generalization can be beneficial #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Explain</em> why we want both generalization and discrimination from our function approximation #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Understand</em> how value estimation can be framed as a <strong>supervised learning</strong> problem #</label></p></li>
<li><p><label><input type="checkbox" checked=""><em>Recognize</em> not all function approximation methods are well suited for reinforcement learning #</label></p></li>
</ul>
</div>
</div>
<section id="moving-to-parameterized-policies-video" class="level2">
<h2 class="anchored" data-anchor-id="moving-to-parameterized-policies-video">Moving to Parameterized Policies (video)</h2>
<p>This video covers the first four learning objectives.</p>
<p>This video covers parameterized policies and how they can be used to approximate value functions. The idea is that using a table has some limitations/ The first is that the tables can be very large. For continuous states they can become infinite.</p>
<p>The second is that the tables can be very sparse and in a table we don’t generalize between states.</p>
<p>We see that we don’t really want functions that directly approximate the value function. We want functions that have some structure that we can learn.</p>
<p>This is called a parameterized function. The ideas is to use a weighted sum of the features of the state. This allows us to learn the weights and use them to approximate the value function. This is called linear function approximation. The way to get around this which is two fold. We first represent the salient properties of a states into features.</p>
<p>Then we use weights to combine these features to approximate the value function. This is called linear function approximation.</p>
<p>More generally we can use non linear parameterized functions to approximate value functions.</p>
<p>Adam shows that is the features are not picked wisely we may not be able to discriminate between states - out function for one state will be the same as for another dissimilar state. Learning about one will make us forget what we learned about the other. This is called bias. On the other hand if we have too many features we may not be able to generalize between states. This is called variance. The goal is to balance between bias and variance.</p>
</section>
<section id="sec-l1g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g1">Understanding parameterized functions</h2>
<ul>
<li>In the previous courses we represented value functions as tables or arrays:
<ul>
<li>For <img src="https://latex.codecogs.com/png.latex?V(s)"> we had an array of size <img src="https://latex.codecogs.com/png.latex?%7CS%7C">,</li>
<li>For <img src="https://latex.codecogs.com/png.latex?Q(s,a)"> we had an array of size <img src="https://latex.codecogs.com/png.latex?%7CS%7C%20%5Ctimes%20%7CA%7C">. This becomes impractical as <img src="https://latex.codecogs.com/png.latex?%7CS%7C%20%5Crightarrow%20%5Cinfty">. We can use <strong>parameterized functions</strong> to approximate value functions. This is called <strong>function approximation</strong>.</li>
</ul></li>
<li><strong>Linear value function approximation</strong> is a simple and popular method.
<ul>
<li>We represent the value function as a linear combination of features:</li>
</ul></li>
</ul>
<p><span id="eq-fn-approx"><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(s,%20%5Cmathbb%7Bw%7D)%20%5Capprox%20v_%5Cpi(s)%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D()"> is the approximate value function</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> is a weight vector</li>
</ul></li>
<li>for example:</li>
</ul>
</section>
<section id="sec-l1g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g2">Linear value function approximation</h2>
<ul>
<li>We can write the approximate value function as a linear combination of features:</li>
</ul>
<p><span id="eq-lin-fn-approx"><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(s,%20%5Cmathbb%7Bw%7D)%20%5Cdot%20=%20w_1%20X%20+%20w_2%20+%20Y%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are features of the state <img src="https://latex.codecogs.com/png.latex?s"></li>
<li><img src="https://latex.codecogs.com/png.latex?w_1"> and <img src="https://latex.codecogs.com/png.latex?w_2"> are the weights of the features</li>
</ul></li>
<li>now learning becomes finding better weights that parameterize the value function.</li>
</ul>
<p>finding the weights that minimize the error between the approximate value function and the true value function:</p>
</section>
<section id="sec-l1g3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g3">Tabular case is a special case of linear value function approximation</h2>
<div class="page-columns page-full"><p> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Chat%7Bv%7D(s,%20%5Cmathbb%7Bw%7D)%20&amp;%20%5Cdot%20=%20%5Csum%20w_i%20x_i(s)%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%3C%5Cmathbf%7Bw%7D,%20%5Cmathbf%7Bx%7D(s)%3E%20%5Cqquad%0A%5Cend%7Balign*%7D%0A"></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/c3-w2-parametrized_functions.png" id="fig-rl-linear-approximation" class="img-fluid" alt="Linear value function generalize the tabular case"></div></div>
<ul>
<li>here:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> is a weight vector</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D(s)"> is a feature vector that is 1 in the <img src="https://latex.codecogs.com/png.latex?i">-th position and 0 elsewhere.</li>
</ul></li>
<li>linear value function approximation is a generalization of the tabular case.</li>
<li>limitations of linear value function approximation:
<ul>
<li>the choice of features limits the expressiveness of the value function.</li>
<li>it can only represent linear relationships between the features and the value function.</li>
<li>it can only represent a limited number of features.</li>
</ul></li>
<li>so how are tabular functions a special case of linear value function approximation?
<ul>
<li>we can see from the figure that all we need is use one hot encoding for the features. Then the weighted vector will be the same as the value function in the table.</li>
</ul></li>
</ul>

<div class="no-row-height column-margin column-container"><div id="fig-rl-failure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-failure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-linear-fn-fail.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-failure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Linear value function approximation failure
</figcaption>
</figure>
</div></div></section>
<section id="sec-l1g4" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g4">There are many ways to parameterize an approximate value function</h2>

<div class="no-row-height column-margin column-container"><div id="fig-neural-networks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neural-networks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-non-linear-fn-approximation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neural-networks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: neural networks are non-linear fn approximators
</figcaption>
</figure>
</div></div><ul>
<li>We can use different types of functions to approximate the value function:
<ul>
<li>one hot encoding</li>
<li>linear functions</li>
<li>tile coding</li>
<li>neural networks</li>
</ul></li>
</ul>
</section>
<section id="generalization-and-discrimination-video" class="level2">
<h2 class="anchored" data-anchor-id="generalization-and-discrimination-video">Generalization and Discrimination (video)</h2>
<p>In this video Martha covers the next three learning objectives. The video is about:</p>
<ul>
<li>Generalization - using knowledge (V,Q,Pi) about similar states.</li>
<li>Discrimination - being able to distinguish between different states.</li>
</ul>
</section>
<section id="sec-l1g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g5">Understanding generalization and discrimination</h2>

<div class="no-row-height column-margin column-container"><div id="fig-generalization-discrimination-chart" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-generalization-discrimination-chart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-generalization-discrimination-matrix.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generalization-discrimination-chart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: generalization and discrimination
</figcaption>
</figure>
</div></div><ul>
<li>Generalization:
<ul>
<li>the ability to estimate the value of states that were not seen during training.</li>
<li>in the case of policy evaluation, generalization is the ability of updates of value functions in one state to affect the value of other states.</li>
<li>in the tabular case, generalization is not possible because we only update the value of the state we are in.</li>
<li>in the case of function approximation, we can think of generalization as corresponding to an embedding of the state space into a lower-dimensional space.</li>
</ul></li>
<li>Discrimination: the ability to distinguish between different states.</li>
</ul>
</section>
<section id="sec-l1g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g6">How generalization can be beneficial</h2>
<ul>
<li>Generalization can be beneficial because:
<ul>
<li>It allows us to estimate the value of states that are similar to states seen during training.</li>
<li>This includes states that were not seen during training.</li>
<li>It allows us to estimate the value of states that are far from states seen during training. (So long as they are similar in terms of the features we are using to approximate the value function)</li>
</ul></li>
</ul>
</section>
<section id="sec-l1g7" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g7">Why we want both generalization and discrimination from our function approximation</h2>
<ul>
<li>We want both generalization and discrimination from our function approximation because:
<ul>
<li>generalization allows us to estimate the value of states that were not seen during training.</li>
<li>discrimination allows us to distinguish between different states.</li>
<li>generalization allows us to estimate the value of states that are similar to states seen during training.</li>
<li>discrimination allows us to estimate the value of states that are far from states seen during training.</li>
</ul></li>
</ul>
<p>We hear a lot about function approximation and gradient methods having a bias or high variance. I tracked this from wikipedia and statistical learning. While it makes sense for a Bayesian regression I’m not sure that it is quite correct for RL. Unfortunately I don’t have a better explanation, though reviewing <a href="https://www.youtube.com/watch?v=y3oqOjHilio">this policy gradient lecture might be helpful</a></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bias-variance tradeoff
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>an important result called the <a href="https://en.wikipedia.org/wiki/Bias-variance_tradeoff"><strong>bias-variance tradeoff</strong></a>:
<ul>
<li>Bias is the error introduced by approximating a real-world problem, which may be extremely complicated, by a much simpler model. This means that since we cannot discriminate between different states that share weights for the same feature vector we have errors we characterize as bias.</li>
<li>High bias corresponds to underfitting in our model.</li>
<li>Variance is the opposite issue arising from having more features than we need to discriminate between states. This means that updating certain weights will affect only some of these related states and not others. This type of error is called variance and is also undesirable.</li>
<li>High variance corresponds to overfitting in our model which can be due to our model fitting the noise in the data rather than the underlying signal.</li>
<li>In general for a model there is some optimal point where the bias and variance are balanced. Going forward from that point we observe a trade off between bias and variance so we need to choose one or the other.</li>
<li>This choice is usually governed by business realities and the nature of the data or the problem we are trying to solve.</li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="framing-value-estimation-as-supervised-learning-video" class="level2">
<h2 class="anchored" data-anchor-id="framing-value-estimation-as-supervised-learning-video">Framing Value Estimation as Supervised Learning (video)</h2>
<p>In this video we cover the next two learning objectives. Martha has a good background in supervised learning and she explains how many parts of RL can be framed as supervised learning problems.</p>
<p>Things we like to learn in RL in this course:</p>
<ul>
<li>Value fn approximation (V)</li>
<li>Action fn values (Q)</li>
<li>Policies (Pi)</li>
</ul>
<p>In reality we may want to learn other things as well which can be framed as supervised learning problems:</p>
<ul>
<li>State representations - i.e.&nbsp;better features (CNNs, RNNs, etc)</li>
<li>Models of Dynamics i.e.&nbsp;Transition probabilities (P)</li>
<li>Reward precesses (R) What is a good reward function? How do we learn it? This is an inverse reinforcement learning problem. It is ill posed because there are many reward functions that can explain the data. We need to find the simplest one that explains the data. This is intertwined with learning internal motivations and goals. c.f. <a href="https://www.youtube.com/watch?v=jn1NE8uIxgw">Satinder Singh</a>’s work on intrinsic motivation.</li>
<li>Generalized Value functions (Gvfs) c.f. <a href="https://www.youtube.com/watch?v=GmGL9cVfJG4&amp;t=1889s">Martha and Adam White’s work on GVFs</a>.</li>
<li>Options (Spatial or Temporal Aggregations of actions) c.f. Doina Precup’s work on options and semi-markov decision processes.</li>
</ul>
<p>Even more things that we might want to learn in RL that might be framed as supervised learning problems:</p>
<ul>
<li>Approximate/Compressed policies for sub goals AKA heuristics
<ul>
<li>Fully Pooled policies (all states are the same) - Think random uniform policy</li>
<li>Partial pooled policies, (some states are the same)</li>
<li>Unpooled policies (all states are different - tabular setting)</li>
<li>Priors for Policies</li>
<li>Hierarchies of options - Think Nethack</li>
</ul></li>
<li>Beliefs about policies</li>
<li>Beliefs about other agents (theory of mind)</li>
<li>Beliefs about the environment.</li>
<li>Causal models of the environment
<ul>
<li>what can we influence and what can’t we influence.</li>
</ul></li>
<li>Coordination and communication with other agents c.f. work by <a href="https://www.jakobfoerster.com/">Jakob Foerster</a>, <a href="https://natashajaques.ai/">Natasha Jacques</a>, and <a href="">Marco Baroni</a> on emergent communication.
<ul>
<li>What part of communication is cheap talk</li>
<li>What part of communication is credible</li>
</ul></li>
</ul>
</section>
<section id="sec-l1g8" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g8">How value estimation can be framed as a supervised learning problem</h2>
<ul>
<li><p>The problem of policy evaluation in reinforcement learning can be framed as supervised learning problem</p>
<ul>
<li>In the case of Monte Carlo methods,
<ul>
<li>the inputs are the states and</li>
<li>the outputs are the returns <img src="https://latex.codecogs.com/png.latex?G">.</li>
</ul></li>
<li>In the case of TD methods,
<ul>
<li>the inputs are the states and</li>
<li>the outputs are the one step bootstrapped returns. <img src="https://latex.codecogs.com/png.latex?U_t%20%5Cdot=R_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)"></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="sec-l1g9" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g9">Not all function approximation methods are well suited for reinforcement learning</h2>
<blockquote class="blockquote">
<p>In principle, any function approximation technique from supervised learning can be applied to the <strong>policy evaluation task</strong>. However, not all are equally well-suited. – Martha White</p>
</blockquote>
<ul>
<li>in RL the agent interacts with the environment and generates data, which corresponds to the online setting in supervised learning.</li>
<li>When we want to use supervised learning we need to choose a method that is well suited for the online setting which can handle
<ul>
<li>non-stationary data.</li>
<li>non-stationary and correlated data (which is the case in RL).</li>
</ul></li>
</ul>
<p>In fact much of the learning in RL is about learning such correlations and quickly adapting to non-stationary in the environment.</p>
<p>In TD learning the target depends on <img src="https://latex.codecogs.com/png.latex?w"> but in supervised learning the target is fixed and given.</p>
</section>
</section>
<section id="lesson-2-the-objective-for-on-policy-prediction" class="level1 page-columns page-full">
<h1>Lesson 2: The Objective for On-policy Prediction</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Understand</em> the <strong>mean-squared value error objective</strong> for policy evaluation #</label></li>
<li><label><input type="checkbox" checked=""><em>Explain</em> the role of the <strong>State distribution</strong> in the objective #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> the idea behind <strong>Gradient descent</strong> and <strong>Stochastic gradient descent</strong> #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> that gradient descent converges to stationary points #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how to use <strong>Gradient descent</strong> and <strong>Stochastic gradient descent</strong> to minimize the value error #</label></li>
<li><label><input type="checkbox" checked=""><em>Outline</em> the <strong>Gradient Monte Carlo</strong> algorithm for value estimation #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> how <strong>state aggregation</strong> can be used to approximate the value function #</label></li>
<li><label><input type="checkbox" checked=""><em>Apply</em> <strong>Gradient Monte-Carlo</strong> with state aggregation #</label></li>
</ul>
</div>
</div>
<section id="the-value-error-objective-video" class="level2">
<h2 class="anchored" data-anchor-id="the-value-error-objective-video">The Value Error Objective (Video)</h2>
<p>In this video Adam White covers the first two learning objectives of the unit.<br>
The main subject about using the mean squared error as a loss for the approximate value function.</p>
<p>we get a sequence of <img src="https://latex.codecogs.com/png.latex?(S_1,v_%7B%5Cpi%7D(S_1)%20),(S_2,v_%7B%5Cpi%7D(S_2)%20),(S_3,v_%7B%5Cpi%7D(S_3)%20),%20%5Cldots"> and we want to approximate the value function . We can track how well we are approximating <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D(s)"> by using <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D(s,%5Cmathbb%7Bw%7D)">. The difference can be positive or negative so if we average it the sum will tend to cancel out. If we square the error we get a positive number we have a much better estimate of the error. And if normalize it by taking the mean we can get use it to compare runs of different lengths. This is called the mean squared error.</p>
<p>It turns out that this is not enough for RL and we need to take a weighted average using the state distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)">. This is because we care more about some states than others. The state distribution is the long run probability of visiting the state <img src="https://latex.codecogs.com/png.latex?s"> under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">. This weighted average is called <em>the mean squared <strong>value</strong> error</em>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Your Objective is My Loss
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In the optimization literature the <strong>loss</strong> function is called the <strong>objective</strong> function. This is because we are trying to optimize the weights of the function to minimize the loss. So we will often hear the term objective function or just objective. Life is simpler if we recall that this is just a loss function for a supervised learning problem.</p>
<p>A related point is that if we want to optimize our approximate value we can swap the with a different loss function or with a different approximation function and the outcome should remain the same, at least under certain conditions. This is how we can switch from the mean squared value error objective to the Monte Carlo objective and then to the TD learning objective.</p>
</div>
</div>
</div>
</section>
<section id="sec-l2g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g1">Understanding the mean-squared value error objective for policy evaluation</h2>

<div class="no-row-height column-margin column-container"><div id="fig-mse" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-mean-squared-value-error.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: mean-squared value error objective
</figcaption>
</figure>
</div></div><ul>
<li>An idealized Scenario:
<ul>
<li>input: <img src="https://latex.codecogs.com/png.latex?%5C%7B(S_1,%20v_%5Cpi(S_1)),%20(S_2,%20v_%5Cpi(S_2)),%20%5Cldots,%20(S_n,%20v_%5Cpi(S_n))%5C%7D"></li>
<li>output: <img src="https://latex.codecogs.com/png.latex?%5Chat%20v(s,w)%20%5Capprox%20v_%5Cpi(s)"></li>
<li>however in reality we may get some some error in the approximation.
<ul>
<li>this could be due to our choice of the approximation.</li>
<li>but initially we just don’t have good weights - to fit the data.</li>
</ul></li>
<li>What we need is a way to measure the error in the approximation.</li>
<li>Also we may care more about some states than others and we can encode this using the state distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)">.</li>
</ul></li>
<li>The mean-squared value error objective for policy evaluation is to minimize the mean-squared error between the true value function and the approximate value function:</li>
</ul>
<p><span id="eq-msve"><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7BVE%7D%20=%20%5Csum_%7Bs%5Cin%20S%7D%5Cmu(s)%5Bv_%5Cpi(S)%20-%20%5Chat%7Bv%7D(S,%20%5Cmathbf%7Bw%7D)%5D%5E2%0A%5Ctag%7B3%7D"></span></p>
<ul>
<li>where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverline%7BVE%7D"> is the mean-squared value error</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmu(s)"> is the state distribution</li>
<li><img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> is the true value of state <img src="https://latex.codecogs.com/png.latex?s"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)"> is the approximate value of state <img src="https://latex.codecogs.com/png.latex?s"> with weights <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"></li>
</ul></li>
<li>the goal is to find the weights that minimize the mean-squared value error.</li>
</ul>
</section>
<section id="sec-l2g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g2">Explaining the role of the state distribution in the objective</h2>
<ul>
<li>The state distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)"> is the long run probability of visiting the state <img src="https://latex.codecogs.com/png.latex?s"> under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</li>
<li>This makes more sense if our markov chain is ergodic - i.e.&nbsp;we can reach any state from any other state by following some transition trajectory.</li>
<li>The state distribution is important because it determines how much we care about the error in each state.</li>
<li>The state distribution is usually unknown, and hard to estimate as it has complex dependencies on the policy and the environment.</li>
<li>We will later see a result that shows how we can avoid the need to know the state distribution.</li>
<li>In the diagram we see that the state distribution is a probability distribution over the states of the MDP and that there is little probability mass of visiting states at the edges of the state space.</li>
<li>The mean square error has less impact in these low probability states.</li>
</ul>
</section>
<section id="sec-l2g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g3">The idea behind gradient descent and stochastic gradient descent</h2>
<ul>
<li>Gradient descent is an optimization algorithm that uses the gradient to find a local minimum of a function.</li>
<li>The gradients points in the direction of the steepest ascent of the function and our objective is to minimize the mean squared error we move in the opposite direction.</li>
<li>Hence the name gradient descent.</li>
<li>The gradient of the mean-squared value error with respect to the weights <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> is given by:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20w%20%5Cdot%20=%20%5Cleft%20%5B%20%5Cbegin%7Bmatrix%7D%20w_1%20%5C%5C%20%5Cvdots%20%5C%5C%20w_d%20%20%5Cend%7Bmatrix%7D%20%5Cright%20%5D%20%5Cqquad%20%5Cnabla%20f%20=%20%5Cleft%20%5B%20%5Cbegin%7Bmatrix%7D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20w_1%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20w_d%7D%20%20%5Cend%7Bmatrix%7D%20%5Cright%20%5D%20%5Cqquad%0A"></p>
<ul>
<li>for a linear function:</li>
</ul>
<p><span id="eq-grad-lin-fn"><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20=%20%5Csum%20%5Cmathbf%7Bw%7D%5ET%20%5Cmathbf%7Bx%7D(s)%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%7D%7B%5Cpartial%20w_i%7D%20=%20%5Cmathbf%7Bx_i%7D(s)%20%5C%5C%0A%5Cnabla%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20=%20%5Cmathbf%7Bx%7D(s)%20%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<ul>
<li>we can write the update rule for the weights as:</li>
</ul>
<p><span id="eq-grad-descent"><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bt+1%7D%20%5Cdot=%20w_t%20-%20%5Calpha%20%5Cnabla%20J(%5Cmathbf%7Bw_t%7D)%20%5Cqquad%0A%5Ctag%7B5%7D"></span></p>
<ul>
<li>Stochastic gradient descent is a variant of gradient descent that uses a random sample of the data to estimate the gradient.</li>
<li>Stochastic gradient descent uses mini-batches of data to estimate the gradient, which makes it computationally efficient and reduces the variance of the gradient estimate.</li>
<li>In practice we will use variants like:
<ul>
<li>Adam - which adapts the learning rate based on the gradient.</li>
<li>RMSProp - which uses a moving average of the squared gradient.</li>
<li>Adagrad - which uses a different learning rate for each parameter.</li>
<li>SGD - which uses a fixed learning rate.</li>
</ul></li>
</ul>
</section>
<section id="sec-l2g7" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g7">Gradient descent converges to stationary points</h2>

<div class="no-row-height column-margin column-container"><div id="fig-gradient-descent" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-sgd-convergence.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: gradient descent
</figcaption>
</figure>
</div></div><ul>
<li>Gradient descent converges to stationary points because the gradient of the mean-squared value error is zero at the minimum.</li>
<li>Gradient descent can get stuck in a local minima, so it is important to use a good initialization and learning rate.</li>
<li>Stochastic gradient descent can escape a local minima because it uses a random sample of the data to estimate the gradient.</li>
<li>In general the optimizer is not guaranteed to find the global minimum of the function - just a local minima</li>
</ul>
</section>
<section id="sec-l2g8" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g8">How to use Gradient descent and Stochastic gradient descent to minimize the value error</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla%20J(%5Cmathbf%7Bw%7D)%20&amp;%20=%20%5Cnabla%20%5Csum_%7Bs%5Cin%20S%7D%20%5Cmu(s)%5Bv_%5Cpi(s)%20-%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%5D%5E2%20%5Cqquad%20%20%5Cqquad%20%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20=%20%3C%5Cmathbf%7Bw%7D,%5Cmathbf%7Bx%7D(s)%3E%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%20%5Csum_%7Bs%5Cin%20S%7D%20%20%5Cmu(s)%20%5Cnabla%20%5Bv_%5Cpi(s)%20-%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%5D%5E2%20%5Cqquad%20%20%5Cqquad%20%5Cnabla%20%20%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20=%20%5Cmathbf%7Bx%7D(s)%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%20%5Csum_%7Bs%5Cin%20S%7D%20%5Cmu(s)%202%20%5Bv_%5Cpi(s)%20-%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%5D%5Cnabla%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%0A%5Cend%7Balign*%7D%20%5Cqquad%0A"></p>
<section id="stochastic-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>If we have a sample of states <img src="https://latex.codecogs.com/png.latex?s_1,%20s_2,%20%5Cldots,%20s_n"> observed by following <img src="https://latex.codecogs.com/png.latex?pi"><br>
we can write the update rule for a pair of weights as:</p>
<p><span id="eq-sgd"><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bt+1%7D%20%5Cdot=%20w_t%20+%20%5Calpha%20%5Bv_%5Cpi(S_1)%20-%20%5Chat%7Bv%7D(s_1,%20%5Cmathbf%7Bw_1%7D)%5D%5Cnabla%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20%5Cqquad%0A%5Ctag%7B6%7D"></span></p>
<p>This allows us to decrease the error in the value function by making updates for one state at a time and moving the weights in the direction of the negative gradient. By making this type of update we might increase the error occasionally but in the long run we will decrease the error.</p>
<blockquote class="blockquote">
<p>This updating approach is called stochastic gradient descent, because it only uses a stochastic estimate of the gradient. In fact, the expectation of each stochastic gradient equals the gradient of the objective. You can think of this stochastic gradient as a noisy approximation to the gradient that is much cheaper to compute, but can nonetheless make steady progress to a minimum – Martha White</p>
</blockquote>
<p>we have here one issue - we don’t know the true value of the policy <img src="https://latex.codecogs.com/png.latex?v_pi(s_1)">, how do we get around this?</p>
<p>one option is to replace the true value with an estimate, one option is to use the return from the state <img src="https://latex.codecogs.com/png.latex?s_1">.</p>
<p>recall that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Av_%5Cpi(s)%20=%20%5Cmathbb%7BE%7D%5BG_t%20%5Cmid%20S_t%20=%20s%5D%20%5Cqquad%0A"></p>
<p>so we can substitute the true value with the return from the state <img src="https://latex.codecogs.com/png.latex?s_1">.</p>
<p><span id="eq-gradient-mc-update"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Aw_%7Bt+1%7D%20&amp;%20%5Cdot=%20w_t%20+%20%5Calpha%20%5Bv_%5Cpi(S_1)%20-%20%5Chat%7Bv%7D(s_1,%20%5Cmathbf%7Bw_1%7D)%5D%5Cnabla%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20%5Cqquad%20%5C%5C%0A%20%20%20%20%20%20%20%20&amp;%20%5Cdot%20=%20w_t%20+%20%5Calpha%20%5BG_1%20-%20%5Chat%7Bv%7D(s_1,%20%5Cmathbf%7Bw_1%7D)%5D%5Cnabla%20%5Chat%7Bv%7D(s,%20%5Cmathbf%7Bw%7D)%20%5Cqquad%0A%5Cend%7Balign*%7D%0A%5Ctag%7B7%7D"></span></p>
</section>
</section>
<section id="sec-l2g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g4">The gradient Monte Carlo algorithm for value estimation</h2>
<p>We now have a way to update the weights of the value function using the gradient of the mean-squared value error. Which allows us to present the gradient Monte Carlo algorithm for value estimation.</p>
<div id="nte--gradient-mc" class="alg callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;1: MC prediction fist visit for estimating <img src="https://latex.codecogs.com/png.latex?V%20%5Capprox%20v_%5Cpi">
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-gradient-mc" class="pseudocode-container quarto-float" data-no-end="false" data-pseudocode-number="1" data-caption-prefix="Algorithm" data-line-number="true" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{GradientMC($\pi$)}\begin{algorithmic} \State Input \State $\qquad \pi$ to be evaluated \State $\qquad \text{a differentiable function } \hat{v}: \mathcal{S} \times \mathbb{R}^d \rightarrow \mathbb{R}$ \State Algorithm parameters: \State $\qquad \alpha \in (0, 1]$ step size \State Initialize: \State $\qquad \mathbf{w} \leftarrow x \in \mathbb{R^d} \text{ arbiterly}$ (e.g. w=0) \For {each episode:} \State Generate an episode by following $\pi: S_0, A_0, R_1, S_1, A_1, R_2,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0$ \For {each step of episode, $t = 0, 1, \ldots, T-1$:} \State $\mathbf{w} \leftarrow w_t + \alpha [G_t - \hat{v}(S_t , \mathbf{w_1})] \nabla \hat{v}(S_t , \mathbf{w})$ \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<ul>
<li>The Gradient Monte Carlo algorithm is a policy evaluation algorithm that uses stochastic gradient descent to minimize the mean-squared value error.</li>
</ul>
</section>
<section id="sec-l2g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g5">How state aggregation can be used to approximate the value function</h2>

<div class="no-row-height column-margin column-container"><div id="fig-gradient-mc-state-agg" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-mc-state-agg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-gradient-mc-with-state-agg.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-mc-state-agg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: gradient mc with state aggregation
</figcaption>
</figure>
</div></div><ul>
<li>State aggregation</li>
<li>is a method for reducing the dimensionality of the state space by grouping similar states together.</li>
<li>can be used to approximate the value function by representing each group of states as a single state.</li>
<li>can be used to reduce the number of parameters in the value function and improve generalization.</li>
<li>the example used a 1000 state MDP with 10 groups of 100 states each.</li>
<li>left and right jump left 1-100 states and right 1-100 states.</li>
<li>if they pass the terminal state they get to the terminal state.</li>
<li>state aggregation is a way to reduce the number of parameters in the value function by grouping similar states together.</li>
<li>it is an example of linear function approximation.</li>
<li>there is one feature for each group of states.</li>
<li>the weights are updated using the gradient of the mean-squared value error.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw%20%5Cleftarrow%20w%20+%20%5Calpha%20%5BG_t%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%0A"></p>
<p>and the gradient of the approximate value function is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%20=%20%5Cmathbf%7Bx%7D(S_t)%0A"></p>
<p>which is either 1 or 0 depending on the group of states.</p>
</section>
<section id="sec-l2g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g6">Applying Gradient Monte-Carlo with state aggregation</h2>
<ul>
<li>Gradient Monte Carlo with state aggregation is a policy evaluation algorithm that uses state aggregation to approximate the value function.</li>
<li>The algorithm works as follows:</li>
</ul>
</section>
</section>
<section id="lesson-3-the-objective-for-td" class="level1 page-columns page-full">
<h1>Lesson 3: The Objective for TD</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Understand</em> the <strong>TD-update</strong> for function approximation #</label></li>
<li><label><input type="checkbox" checked=""><em>Highlight</em> the advantages of TD compared to Monte-Carlo #</label></li>
<li><label><input type="checkbox" checked=""><em>Outline</em> the <strong>Semi-gradient TD(0)</strong> algorithm for value estimation #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> that TD converges to a <strong>biased</strong> value estimate #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> that TD converges much <strong>faster</strong> than Gradient Monte Carlo #</label></li>
</ul>
</div>
</div>
<section id="sec-l3g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g1">The TD-update for function approximation</h2>
<ul>
<li>recall the Monte Carlo update rule:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw%20%5Cleftarrow%20w%20+%20%5Calpha%20%5BG_t%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%0A"></p>
<ul>
<li>we can use other targets for the update rule than the return <img src="https://latex.codecogs.com/png.latex?G_t">.</li>
<li>we can replace the return with any estimate of the value of the next state.</li>
<li>we can call this target <img src="https://latex.codecogs.com/png.latex?U_t"> and if it is unbiased it converge to a local minimum of the mean squared value error.</li>
<li>we can use the one step bootstrapped return:</li>
</ul>
<p><span id="eq-td-target"><img src="https://latex.codecogs.com/png.latex?%0AU_t%20%5Cdot=R_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)%0A%5Ctag%7B8%7D"></span></p>
<ul>
<li>but this is not unbiased because it essential approximating the expected return using the current value of the state.</li>
<li>there is no guarantee that the the update will converge to a local minimum of the mean squared value error.</li>
<li>however the update has many advantages over the Monte Carlo update:
<ul>
<li>it has lower variance because it uses a single sample.</li>
<li>it can update the value function after every step.</li>
<li>it can learn online.</li>
<li>it can learn from incomplete episodes.</li>
<li>it can learn from non-episodic tasks.</li>
</ul></li>
<li>the TD-update is nor a true gradient update because the target is not the true value of the state. we call it a semi-gradient update.
<ul>
<li>let’s estimate the gradient of the mean squared value error with respect to the weights <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D">:</li>
</ul></li>
</ul>
<p><span id="eq-td-grad"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla%20J(%5Cmathbf%7Bw%7D)%20&amp;%20=%20%5Cnabla%20%5Cfrac%7B1%7D%7B2%7D%5BU_t%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%5D%5E2%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20(U_t%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D))%20(%5Cnabla%20U_t%20-%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D))%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20%5Cne%20-%20(U_t%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D))%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%20%5Cquad%20%5Ctext%7Bunless%7D%20%5Cquad%20%5Cnabla%20%20U_t%20=%200%0A%5Cend%7Balign*%7D%0A%5Ctag%7B9%7D"></span></p>
<ul>
<li>but for TD we have:</li>
</ul>
<p><span id="eq-td-grad-ut"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cnabla%20U_t%20=%20&amp;%20=%20%5Cnabla%20(R_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D))%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cgamma%20%5Cnabla%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20%5Cne%200%0A%5Cend%7Balign*%7D%0A%5Ctag%7B10%7D"></span></p>
<ul>
<li>So the TD-update isn’t a true gradient update. However TD often converge in many cases we care updates.</li>
</ul>
</section>
<section id="sec-l3g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g2">Advantages of TD compared to Monte-Carlo</h2>
<ul>
<li>Adam point out that in Gradient Monte Carlo we need to run the alg for a long time and decay the step size to get convergence. But that in practice we don’t decay the step size and we use a fixed step size.<sup>1</sup></li>
<li>TD has several advantages over Monte-Carlo:
<ul>
<li>TD can update the value function after every step, while Monte-Carlo can only update the value function after the episode is complete.</li>
<li>TD can learn online, while Monte-Carlo can only learn offline.</li>
<li>TD can learn from incomplete episodes, while Monte-Carlo requires complete episodes.</li>
<li>TD can learn from non-episodic tasks, while Monte-Carlo can only learn from episodic tasks.</li>
</ul></li>
</ul>
</section>
<section id="sec-l3g3" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g3">The Semi-gradient TD(0) algorithm for value estimation</h2>
<ul>
<li>The Semi-gradient TD(0) algorithm is a policy evaluation algorithm that uses the TD-update for function approximation.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Semi-gradient TD(0) algorithm for estimating <img src="https://latex.codecogs.com/png.latex?v_%5Cpi">
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-td-zero" class="pseudocode-container quarto-float" data-no-end="false" data-pseudocode-number="2" data-caption-prefix="Algorithm" data-line-number="true" data-comment-delimiter="#" data-indent-size="1.2em" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{Semi-gradient TD(0) for estimating $v_\pi$}\begin{algorithmic} \State Input: \State $\qquad \pi \leftarrow \text{policy to be evaluated}$ \State $\qquad \text{a differentiable function } \hat{v}: \mathcal{S} \times \mathbb{R}^d \rightarrow \mathbb{R}$ \State Algorithm parameters: \State $\qquad \alpha \in (0, 1]$ step size \State $\qquad \gamma \in [0, 1]$ discount factor \State Initialize: \State $\qquad value function weights w \leftarrow x \in \mathbb{R}^d \quad \forall s \in \mathcal{S}$ (e.g. w=0) \FORALL {episode $e$:} \State $Initialize S$ \FORALL {step $S \in e$:} \State $\text{Choose } A \sim \pi(\cdot \mid S)$ \State Take action $A$, observe $R, S'$ \State $w \leftarrow w + \alpha [R + \gamma \hat{v}(S', \mathbf{w}) - \hat{v}(S, \mathbf{w})] \nabla \hat{v}(S, \mathbf{w})$ \State $S \leftarrow S'$ \State until $S$ is terminal \ENDFOR \ENDFOR \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
</section>
<section id="sec-l3g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l3g4">TD converges to a biased value estimate</h2>
<ul>
<li>TD converges to a biased value estimate because it updates the value function using an estimate of the next state.</li>
<li>The bias of TD can be reduced by using a smaller step size or by using a more accurate estimate of the next state.</li>
</ul>
</section>
<section id="sec-l3g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g5">TD converges much faster than Gradient Monte Carlo</h2>

<div class="no-row-height column-margin column-container"><div id="fig-early-learning-mc-vs-td" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-early-learning-mc-vs-td-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-early-learning-mc-vs-td.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-early-learning-mc-vs-td-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: early learning experiment
</figcaption>
</figure>
</div></div><ul>
<li>We run the same random walk experiment for the 1000 episodes 1000 step random walk and we see that TD has a worse fit than MC on most of the range.</li>
<li>We run a second experiments with only 30 episodes to see early learning performance and we see that TD has a better fit than MC on most of the range. In this case we used the best alpha for each method. MC needed a much smaller alpha to get a good fit.</li>
<li>TD converges much faster than Gradient Monte Carlo because it updates the value function after every step.</li>
<li>Gradient Monte Carlo can only update the value function after the episode is complete, which can be slow for long episodes.</li>
<li>TD can learn online, while Gradient Monte Carlo can only learn offline.</li>
<li>TD can learn from incomplete episodes, while Gradient Monte Carlo requires complete episodes.</li>
<li>TD can learn from non-episodic tasks, while Gradient Monte Carlo can only learn from episodic tasks.</li>
</ul>
</section>
<section id="sec-l3g6" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l3g6">Doina Precup’s talk on Building Knowledge for AI Agents with Reinforcement Learning</h2>

<div class="no-row-height column-margin column-container"><div id="fig-generelization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-generelization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-dorina-precup.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-generelization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Dorina Precup
</figcaption>
</figure>
</div><div id="fig-options" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-options-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-dorina-options.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-options-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Options are a temporal generalization
</figcaption>
</figure>
</div></div>
<p>In this talk, <a href="https://rl.cs.mcgill.ca/people/doina-precup/">Dorina Precup</a> discusses the challenges of building knowledge for AI agents using reinforcement learning.</p>
<ul>
<li>Dorina Precup is a professor at McGill University and a research team lead at DeepMind.</li>
<li>She is an expert in reinforcement learning and machine learning.</li>
<li>Her interests are in the areas of abstractions.</li>
<li>When I think about generalization in RL I think about:
<ul>
<li>Learning a parameterized value function that can be used to estimate the value of any state.</li>
<li>Learning a parameterized policy that can be used to select actions in any state.</li>
<li>Being able to transfer this policy to a similar task</li>
<li>Being able to learn using less interaction with the environment and more from replaying past experiences.</li>
<li>Being able to learn from a small number of examples.</li>
</ul></li>
<li>Dorina talks about two other aspects of generalization:
<ul>
<li>Action duration are one time step in an MDP, yet in reality some actions like traveling from one city to another require sticking to the action over an extended period of time.</li>
</ul></li>
<li>This might be happen through planning but idealy, agents should be able to learn skills which are sequences of actions that are executed over an extended period of time.</li>
<li>This has been formalized in the literature as options.</li>
<li>She references two sources
<ul>
<li><span class="citation" data-cites="Sutton1999BetweenMA">[@Sutton1999BetweenMA]</span> a paper from 1999 on options in reinforcement learning.</li>
<li><span class="citation" data-cites="precup2000temporal">[@precup2000temporal]</span> her doctoral thesis from 2000 on temporal abstraction in reinforcement learning.</li>
</ul></li>
<li>Options consists of
<ul>
<li>an initiation set <img src="https://latex.codecogs.com/png.latex?%5Ciota_%5Comega(s)"> the precondition which is a probability of starting the option in state <img src="https://latex.codecogs.com/png.latex?s">.</li>
<li>a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Comega(a%5Cmid%20s)"> that is executed in the option</li>
<li>a termination condition <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%5Comega(s)">. the termination condition is a probability of terminating the option in state <img src="https://latex.codecogs.com/png.latex?s">.</li>
</ul></li>
<li>Options are “chunks of behavior” that can be executed over an extended period of time.</li>
<li>The model will need to learn options and work with them.</li>
<li>IT needs expected reward over the option.</li>
<li>A transition model over the option.</li>
<li>These models are predictive models about outcomes conditioned on the model being executed.</li>
<li>Adding options to the model weakens the MDP assumption, because the option duration is not fixed so state now have a longer dependence is a sequence of actions that are not Markovian <sup>2</sup>.</li>
<li>Precup’s point out that combining temporal and spatial abstraction is an ongoing research challenge.</li>
<li>She also points out that the model needs to learn the options and the value function at the same time.</li>
<li>According to her profile Precup has a number of students working on this problem. Some additional references are:
<ul>
<li><span class="citation" data-cites="Bacon2016TheOA">[@Bacon2016TheOA]</span> a paper from 2016 on option-critic architecture which extends actor-critic algorithms to work with options.</li>
</ul></li>
<li>Earlier work uses the term macro-actions to refer to options.
<ul>
<li><span class="citation" data-cites="bradtke1994reinforcement">[@bradtke1994reinforcement]</span> a paper from 1994 on reinforcement learning with hierarchies of machines.</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Options &amp; CI
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>This type of formulation seems very similar to that used by <a href="https://www.youtube.com/watch?v=JxzjdTc15A0">Judea Pearl</a> in his structureal graphical model of Causality. If we can express options as a graph of states we can use his algorithms to infer the best options to take in a given state.</li>
<li>options are like do operations (interventions)</li>
<li>choosing between options is like conterfactual reasoning.</li>
</ul>
</div>
</div>
</section>
</section>
<section id="lesson-4-linear-td" class="level1">
<h1>Lesson 4: Linear TD</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><em>Derive</em> the TD-update with linear function approximation #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> that tabular TD(0) is a special case of linear semi-gradient TD(0) #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> why we care about linear TD as a special case #</label></li>
<li><label><input type="checkbox" checked=""><em>Highlight</em> the advantages of linear value function approximation over nonlinear #</label></li>
<li><label><input type="checkbox" checked=""><em>Understand</em> the <strong>fixed point</strong> of linear TD learning #</label></li>
<li><label><input type="checkbox" checked=""><em>Describe</em> a theoretical guarantee on the mean squared value error at the <strong>TD fixed point</strong> #</label></li>
</ul>
</div>
</div>
<section id="sec-l4g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g1">Deriving the TD-update with linear function approximation</h2>
<ul>
<li>Linear function is both:</li>
<li>simple enough to be understood, yet</li>
<li>powerful enough that with TD to be useful to create agents that are stornger than human Atari games.</li>
<li>The TD-update with linear function approximation is a way to update the weights of the value function using the TD-error.</li>
<li>The TD-update with linear function approximation works as follows:
<ul>
<li>Compute the TD-error <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> as the difference between the one-step bootstrapped return and the approximate value of the next state.</li>
<li>Update the weights <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> in the direction of the TD-error.</li>
</ul></li>
</ul>
<p>recall the TD-update rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdelta%20%5Cdot=%20R_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%20%5C%5C%0Aw%20%5Cleftarrow%20w%20+%20%5Calpha%20%5Cdelta_t%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%0A"> in the linear case we can write the value function as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%20%5Cdot%20=%20%5Csum%20%5Cmathbf%7Bw%7D%5ET%20%5Cmathbf%7Bx%7D(S_t)%20%5C%5C%0A%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%20=%20%5Cmathbf%7Bx%7D(S_t)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw%20%5Cleftarrow%20w%20+%20%5Calpha%20%5Cdelta_t%20%5Cmathbf%7Bx%7D(S_t)%0A"></p>
</section>
<section id="sec-l4g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g2">Tabular TD(0) is a special case of linear semi-gradient TD(0)</h2>
<ul>
<li>Tabular TD(0) is a special case of linear semi-gradient TD(0) where the features are one-hot encoded.</li>
<li>In the tabular case, the weights are the same as the value function in the table.</li>
<li>In the linear case, the weights are the parameters of the value function.</li>
<li>Tabular TD(0) can be seen as a special case of linear semi-gradient TD(0) where the features are one-hot encoded.</li>
</ul>
</section>
<section id="sec-l4g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g4">Advantages of linear value function approximation over nonlinear</h2>
<ul>
<li>Linear value function approximation has several advantages over nonlinear value function approximation:
<ul>
<li>Linear value function approximation is computationally efficient and easy to implement.</li>
<li>Linear value function approximation is easy to interpret and understand.</li>
<li>Linear value function approximation is less prone to overfitting than nonlinear value function approximation.</li>
<li>Linear value function approximation can be used to approximate any function, while nonlinear value function approximation is limited by the choice of features.</li>
</ul></li>
<li>If we have access to expert knowledge we can use it to define good features and use linear value function approximation to learn the value function quickly.</li>
<li>Most of the theory of function approximation in reinforcement learning is based on linear value function approximation.</li>
</ul>
</section>
<section id="sec-l4g5" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g5">The fixed point of linear TD learning</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bt+1%7D%20%5Cdot=%20w_t%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)%20-%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%5D%20%5Cnabla%20%5Chat%7Bv%7D(S_t,%20%5Cmathbf%7Bw%7D)%0A"> recall that in the linear case we defined the approximate value function as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bv%7D(S_%7Bt+1%7D,%20%5Cmathbf%7Bw%7D)%20%5Ccdot=%20%20%5Cmathbf%7Bw%7D%5ET%20%5Cmathbf%7Bx%7D(S_%7Bt+1%7D)%0A"> with :</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> the weights of the value function</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D(S_%7Bt+1%7D)"> the features of the next state</li>
</ul>
<p>using this definition we can write the update rule as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Aw_%7Bt+1%7D%20&amp;%20=%20w_t%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Cmathbf%7Bw%7D%5ET%20%5Cmathbf%7Bx%7D_%7Bt+1%7D%20-%20%5Cmathbf%7Bw%7D%5ET%20%5Cmathbf%7Bx%7D%5D%20%5Cmathbf%7Bx%7D_t%20%5Cnewline%0A%20%20%20%20%20%20%20%20&amp;=%20%20w_t%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20%5Cmathbf%7Bx%7D_t%20-%20%20%5Cmathbf%7Bx%7D_t(%20%5Cmathbf%7Bx%7D_t%20-%20%5Cgamma%20%5Cmathbf%7Bx_%7Bt+1%7D%7D)%5ET%20%5Cmathbf%7Bw_t%7D%5D%20%5Cnewline%0A%5Cend%7Balign*%7D%0A"></p>
<p>let us now consider what this update looks like in expectation:</p>
<p>we can think about it as an expected update plus a noise term but the noise term is dominated by the behaviour of the expected update.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5B%5CDelta%20w_%7Bt%7D%5D%20%20=%20%5Calpha(b-Aw_t)%0A"> where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?b%20=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20%5Cmathbf%7Bx%7D_t%5D"> - expectation over the features and the rewards</li>
<li><img src="https://latex.codecogs.com/png.latex?A%20=%20%5Cmathbb%7BE%7D%5B%5Cmathbf%7Bx%7D_t(%20%5Cmathbf%7Bx%7D_t%20-%20%5Cgamma%20%5Cmathbf%7Bx_%7Bt+1%7D%7D)%5ET%5D"> - an expectation over the rewards</li>
</ul>
<p>note: this is a linear system of equations that looks like a <strong>linear regression problem</strong>.</p>
<p>when the weights do not change we have a fixed point:</p>
<p><span id="eq-fixed-point"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BE%7D%5B%5CDelta%20w_%7BTD%7D%5D%20&amp;%20=%20%5Calpha(b-Aw_%7BTD%7D)%20=%200%20%5Cnewline%0A%5Cimplies%20&amp;%20w_%7BTD%7D%20=%20A%5E%7B-1%7Db%0A%5Cend%7Balign*%7D%0A%5Ctag%7B11%7D"></span> more generally <img src="https://latex.codecogs.com/png.latex?w_%7BTD%7D"> is the solution to this equation and we could show that it minimises</p>
<p><span id="eq-min-lin-td"><img src="https://latex.codecogs.com/png.latex?%0A(b-Aw)%5ET(b-Aw)%0A%5Ctag%7B12%7D"></span></p>
<p>this is related to Bellman equations via the projected Bellman error.</p>
</section>
<section id="sec-l4g6" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g6">Theoretical guarantee on the mean squared value error at the TD fixed point</h2>
<p><span id="eq-td-fixed-point-and-minimum-ve"><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7BVE%7D(w_%7BTD%7D)%20%5Cleq%20%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%20%5Cmin_%7Bw%7D%20%5Coverline%7BVE%7D(w)%0A%5Ctag%7B13%7D"></span></p>
<ul>
<li>if <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Capprox%201"> then the mean squared value error at the TD fixed point can be large</li>
<li>if <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Capprox%200"> then the mean squared value error at the TD fixed point can be small</li>
<li>if the features representation is good then the two will be equal regardless of <img src="https://latex.codecogs.com/png.latex?%5Cgamma">. since both will be almost zero.</li>
</ul>
</section>
<section id="sec-l4g7" class="level2">
<h2 class="anchored" data-anchor-id="sec-l4g7">Semi-gradient TD(0) algorithm</h2>
<p>In the assignment I implemented the Semi-gradient TD(0) algorithm for value estimation.</p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>why not?↩︎</p></li>
<li id="fn2"><p>the property that the future is independent of the past given the present↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w1.html</guid>
  <pubDate>Sun, 31 Mar 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>On-Policy Prediction with Approximation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w1.1.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<p>Some of the notes I made in this course became a bit too long. Rather than break the flow of the lesson I decided to move them to a separate file. This is one of those notes.</p>
<section id="a-few-thought-on-generalization-and-discrimination-in-rl" class="level1">
<h1>A few Thought on Generalization and discrimination in RL</h1>
<p>There are a couple of issues on generalization.</p>
<section id="how-are-generalization-in-ml-is-closely-related-to-transfer-learning-in-rl." class="level2">
<h2 class="anchored" data-anchor-id="how-are-generalization-in-ml-is-closely-related-to-transfer-learning-in-rl.">How are generalization in ML is closely related to transfer learning in RL.</h2>
<p>In ML we have a rather clear understanding of generalization. We have a training set and a test set. We train on the training set and then test on the test set. The goal is to do well on the test set. The test set is a sample from the same distribution as the training.</p>
<p>Geometrically, for classification we want to a decision boundary that separates the classes with the least error and with a fewest number of parameters. This is the essence of the bias-variance tradeoff.</p>
<p>In RL we tend to think of generalization as the ability of an agent to perform well on a task that is different from the one it was trained on. The algorithms can take decades of CPU compute to solve a simple video game. But change a few pixels in the game and the agent can’t play at all. This suggest these agents are severely overfitting.</p>
<p>Would we be able to learn much faster if we could avoid overfitting i.e.&nbsp;if we could generalize better?</p>
<p>One point worth considering here is that solving a general problem is harder than a specific one.</p>
<p>E.g. To solve a maze we need a policy matrix. To solve all mazes we need an algorithm.</p>
<p>So in one sense it is harder to generalize than to solve a specific problem. However it also should allow us to discard most of the irrelevant information that take up most of the model’s capacity and end up slowing down learning.</p>
<ul>
<li>Agents learn a policy that is only suitable to a specific task. The policy doesn’t generalize to even small changes in the task, e.g.&nbsp;moving the start and goal in the same maze tasks.</li>
<li>Learned representation for features are not abstract and thus can’t be mapped to a slightly different task (e.g.&nbsp;changing a few pixels in a game)</li>
<li>We definitely can’t map the representation to different tasks.</li>
<li>Ideally, we would like to deal with challenging problems by reusing knowledge from agents trained on other problems.</li>
<li>One direction called options lays in decomposing learning a policy for a goal into reframing it into learning sub-goals, strategies and tactics and basic moves.</li>
<li>Another direction I call heuristics concerns finding minimal policies that are just strong enough to get the agent to the goal a high percentage of the time.</li>
<li>Learning should be aggregational and compositional. However, these terms require reinterpretation for each problem and at many levels of abstraction.</li>
</ul>
</section>
<section id="human-like-to-use-heuristic-which-are-are" class="level2">
<h2 class="anchored" data-anchor-id="human-like-to-use-heuristic-which-are-are">Human like to use Heuristic, which are are:</h2>
<pre><code>- A minimal sub-optimal policy that is suffiecnt to get the agent to its goal with high probability.
- In an MDP with lots of sub-goals, we may have benefit in learning learning heuristic style policy for each sub-goal and then compose them into a policy for the goal. 
- Composing heuristics is vague so let try make it clear.
    - We want to follow the heuristic policy until we reach a sub-goal.
    - We then switch to the policy for the next sub-goal. 
    - If we have well established entry and exit points for each heuristic we can have two benefits one is generalization and the other is discrimination.
        - Generalization is due to using the same heuristic from different starting points.
        - Discrimination is due to having different heuristics for different sub-goals.
        - A third advantage is that the heuristic policy is for a smaller state space and can be learned faster.
        - Third advantage is may be that of mapping different sub-problem to the same heuristic may allow us to discard some of the features of the state space that are not required for the heuristic to work.
    - Thus composing heuristics in this case is just about switching between heuristics at the right time.
    - Another direction is to use the heuristics as a form of  priors for the policy we want to learn.  
    - Simple models are often a good fit for more problems than complex models.
    - If we are good at learning to decompose problems into simpler sub problems and then we might be able to leverage the power of heuristics.

-   Heuristics don't always work but overall they capture the essence of the solution to the problem.
-   Heuristics are usually more general than an optimal policy.
-   A heuristic might be a very good behavior policy for off policy learning the optimal policy.
-   I don't see RL algorithms for heuristics.</code></pre>
</section>
<section id="models-in-rl-try-to-approximate-mdp-dynamics-using-its-transition-and-rewards" class="level2">
<h2 class="anchored" data-anchor-id="models-in-rl-try-to-approximate-mdp-dynamics-using-its-transition-and-rewards">Models in RL try to approximate MDP dynamics using its transition and rewards</h2>
<pre><code>-   In ML we often use boosting and bagging to aggregate very simple models.
-   In RL we often replace the model by sampling from a replay buffer of the agent's past experiences. </code></pre>
</section>
<section id="the-problem-for-a-general-ai-is-very-much-the-problem-of-transfer-learning-in-rl." class="level2">
<h2 class="anchored" data-anchor-id="the-problem-for-a-general-ai-is-very-much-the-problem-of-transfer-learning-in-rl.">The problem for a general ai is very much the problem of transfer learning in RL.</h2>
<ul>
<li>agents learn a very specific policy for a very specific task - the learned representation cannot be mapped to other tasks or even other states in the same task.</li>
<li>if agents learning was decomposed into
<ul>
<li>learning very general policies that solved more abstract problems and then</li>
<li>learning a good composition of these policies to solve the specific problem.</li>
<li>only after getting to this point would the agent try to optimize the policy for the specific task.</li>
<li>e.g.&nbsp;chess
<ul>
<li>learn the basic moves and average value of pieces</li>
<li>learning tactics - short term goals</li>
<li>learning about end game
<ul>
<li>update the value of pieces based on the ending</li>
</ul></li>
<li>learning about strategy
<ul>
<li>positional play
<ul>
<li>learn about pawn formations and weak square
<ul>
<li>value of pawn formations</li>
<li>how they can be used with learned tactics.</li>
</ul></li>
<li>the center
<ul>
<li>add value to pieces based on their position on the board</li>
</ul></li>
<li>open files and diagonals</li>
</ul></li>
<li>long term plans
<ul>
<li>minority attack, king side attack, central breakthrough</li>
<li>creating a passed pawn</li>
<li>exchanging to win in the end game</li>
<li>sacrificing material to get a better position</li>
<li>attacking the king</li>
</ul></li>
<li>castling</li>
<li>piece development and the center</li>
<li>tempo</li>
</ul></li>
<li>localize value of pieces in different positions on the board using the learned tactics and strategy.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="bayesian-models-and-hierarchical-model-encode-knowledge-using-priors-which-can-pool-or-bias-beliefs-towards-a-certain-outcome." class="level2">
<h2 class="anchored" data-anchor-id="bayesian-models-and-hierarchical-model-encode-knowledge-using-priors-which-can-pool-or-bias-beliefs-towards-a-certain-outcome.">Bayesian models and hierarchical model encode knowledge using priors which can pool or bias beliefs towards a certain outcome.</h2>
<pre><code>-   learning in Bayesian models is about updating the initial beliefs based on incoming evidence.</code></pre>
</section>
<section id="ci-may-be-useful-here" class="level2">
<h2 class="anchored" data-anchor-id="ci-may-be-useful-here">CI may be useful here</h2>
<ul>
<li>Is in a big way about mapping knowledge into
<ul>
<li>Statistical joint probabilities,</li>
<li>Casual concepts that are not in the joint distributions like interventions and Contrafactuals, latent, missing, mediators, confounders, etc.</li>
<li>Hypothesizing a causal structural model, deriving a statistical model and Testing it against the data.</li>
<li>Interventions in the form of actions and options -</li>
</ul></li>
<li>Many key ideas in RL are counterfactual reasoning
<ul>
<li>Off-policy learning is about learning from data generated by a different policy.</li>
<li>Options are like do operations (interventions)</li>
<li>Choosing between actions and options is like contrafactual reasoning.</li>
</ul></li>
<li>Using and verifying CI models could be the way to unify the spatial and temporal abstraction in RL.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Prediction and Control with Function Approximation</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c3-w1.1.html</guid>
  <pubDate>Sun, 31 Mar 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Sample-based Learning Methods</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c2-w4.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="lesson-1-what-is-a-model" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-1-what-is-a-model">Lesson 1: What is a model?</h2>
<section id="lesson-learning-goals" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Describe what a model is and how they can be used #</label></li>
<li><label><input type="checkbox">Classify models as <strong>distribution models</strong> or <strong>sample models</strong> #</label></li>
<li><label><input type="checkbox">Identify when to use a distribution model or sample model #</label></li>
<li><label><input type="checkbox">Describe the advantages and disadvantages of sample models and distribution models #</label></li>
<li><label><input type="checkbox">Explain why sample models can be represented more compactly than distribution models #</label></li>
</ul>
</section>
<section id="sec-l1g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g1">What is a model and how can it be used?</h3>
<ul>
<li><p>A model is a simplified representation of the environment dynamics</p></li>
<li><p>Models can be used to simulate the environment</p></li>
<li><p>In this course a Model is a function that predicts the next state and reward given the current state and action</p></li>
<li><p>a transition model <img src="https://latex.codecogs.com/png.latex?s_%7Bt_+1%7D%20=%20f(s_t,%20a_t)"> predicts the next state given the current state and action</p></li>
<li><p>a reward model <img src="https://latex.codecogs.com/png.latex?r_%7Bt_+1%7D%20=%20f(s_t,%20a_t)"> predicts the reward given the current state and action</p></li>
</ul>
<p>three other model types are mentioned in the <a href="https://sites.google.com/view/mbrl-tutorial">ICML Tutorial on Model-Based Reinforcement Learning</a></p>
<ul>
<li><strong>Inverse models</strong> predict the action given the current state and next state <img src="https://latex.codecogs.com/png.latex?a_%7Bt+1%7D%20=%20f_s%5E%7B-1%7D(s_t,%20s_%7Bt+1%7D)"></li>
<li><strong>Distances models</strong> predict the distance between the current state and the goal state <img src="https://latex.codecogs.com/png.latex?d_%7Bij%7D%20=d(s,%20s')"></li>
<li><strong>Future return models</strong> predict the future return given the current state and action <img src="https://latex.codecogs.com/png.latex?G_t=Q(s_t,%20a_t)"> or <img src="https://latex.codecogs.com/png.latex?G_t=V(s_t)"></li>
</ul>
<p>Why do we want to use models?</p>
<ul>
<li>model allow us to simulate the environment without interacting with it.</li>
<li>this can increase sample efficiency - e.g.&nbsp;by replaying past experiences to propergate learning from goal to all predecessor states we have visited</li>
<li>this can reduce risks - e.g.&nbsp;by simulating dangerous situations instead of actually experiencing them.</li>
<li>this can reduce costs - e.g.&nbsp;by simulating costly actions in a simulated environment instead of paying the cost in the real environment.</li>
<li>this could be much faster than real-time interaction with the environment. Often in robotics simulation is orders of magnitude faster than real-time interaction.</li>
</ul>
</section>
<section id="sec-l1g2" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l1g2">Types of models</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-models.png" class="img-fluid figure-img"></p>
<figcaption>models</figcaption>
</figure>
</div></div><ul>
<li><strong>Distribution models</strong> predict the probability distribution of the next state and reward</li>
<li><strong>Sample models</strong> predict a single next state and reward</li>
</ul>
<p>Also there are:Environment simulator</p>
<ul>
<li>Chess Programs typicaly can simulate all possible movers and evaluate the board position (tacticaly and strategically). The difference between the current board position and the board position after a move is the reward.</li>
<li><span class="citation" data-cites="Silver2016MasteringTG">[@Silver2016MasteringTG]</span> mentions an <strong>Environment simulator</strong> for the game of go</li>
<li><span class="citation" data-cites="Agostinelli2019SolvingTR">[@Agostinelli2019SolvingTR]</span> used a simulator of the rubik’s cube to train a reinforcement learning agent to solve the cube.</li>
<li><span class="citation" data-cites="Bellemare2012TheAL">[@Bellemare2012TheAL]</span> used a simulator of the game of atari to train a reinforcement learning agent to play atari games.</li>
<li><span class="citation" data-cites="Todorov2012MuJoCoAP">[@Todorov2012MuJoCoAP]</span> used a simulator of the physics of the real world to train a reinforcement learning agent to control a robot.</li>
<li><span class="citation" data-cites="Shen2018MWalkLT">[@Shen2018MWalkLT]</span> used a simulator to train agents to navigate a graph using MCTS.</li>
<li><span class="citation" data-cites="Ellis2019WriteEA">[@Ellis2019WriteEA]</span> used a REPL environment to train a reinforcement learning agent to write code.</li>
</ul>
</section>
<section id="sec-l1g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g3">When to use a distribution model or sample model</h3>
<ul>
<li><strong>Distribution models</strong> are useful when we need to know the probability of different outcomes</li>
<li><strong>Sample models</strong> are useful when we need to simulate the environment</li>
</ul>
</section>
<section id="sec-l1g4" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g4">Advantages and disadvantages of sample models and distribution models</h3>
<ul>
<li><p><strong>Sample models</strong> can be represented more compactly than distribution models</p></li>
<li><p><strong>Distribution models</strong> can be more accurate than sample models</p></li>
<li><p>exact expectations can be computed from distribution models</p></li>
<li><p>assessing risks and uncertainties is easier with distribution models</p></li>
</ul>
</section>
<section id="sec-l1g5" class="level3">
<h3 class="anchored" data-anchor-id="sec-l1g5">Why sample models can be represented more compactly than distribution models</h3>
<ul>
<li><p><strong>Sample models</strong> can be represented more compactly than distribution models because they only need to store a single next state and reward</p></li>
<li><p><strong>Distribution models</strong> need to store the joint probability of each possible next state and reward pair</p></li>
<li><p><strong>Sample models</strong> can be more efficient when we only need to simulate the environment</p></li>
</ul>
</section>
</section>
<section id="lesson-2-planning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-2-planning">Lesson 2: Planning</h2>
<section id="lesson-learning-goals-1" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-1">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Explain how planning is used to improve policies #</label></li>
<li><label><input type="checkbox">Describe random-sample one-step <strong>tabular Q-planning</strong> #</label></li>
</ul>
</section>
<section id="sec-l2g1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l2g1">How planning is used to improve policies</h3>
<ul>
<li><strong>Planning</strong> is the process of using a model to improve a policy or value function</li>
<li><strong>Planning</strong> can be used to improve a policy or value function without interacting with the environment</li>
<li><strong>Planning</strong> can be used to improve a policy or value function more efficiently than direct RL updates</li>
</ul>
<p>Random-sample one-step <strong>tabular Q-planning</strong> {#sec-l2g2}</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-wk5-random-sample-one-step-tabular-Q-learning.png" class="img-fluid figure-img"></p>
<figcaption>Q-planning alg overview</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-wk5-Q-planning.png" class="img-fluid figure-img"></p>
<figcaption>random sample one step tabular Q-planning</figcaption>
</figure>
</div></div>
<!-- replace with latex version -->
<ul>
<li><strong>Tabular Q-planning</strong> is a planning algorithm that uses a sample model to improve a policy or value function</li>
<li><strong>Tabular Q-planning</strong> uses a sample model to simulate the environment</li>
<li><strong>Tabular Q-planning</strong> uses the simulated experience to improve a policy or value function</li>
</ul>
<p>advantages of planning</p>
<ul>
<li><strong>Planning</strong> can be more efficient than direct RL updates</li>
<li><strong>Planning</strong> can be used to improve a policy or value function without interacting with the environment</li>
<li><strong>Planning</strong> can be used to improve a policy or value function more efficiently than direct RL updates</li>
</ul>
</section>
</section>
<section id="lesson-3-dyna-as-a-formalism-for-planning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-3-dyna-as-a-formalism-for-planning">Lesson 3: Dyna as a formalism for planning</h2>
<section id="lesson-learning-goals-2" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-2">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Recognize that direct RL updates use experience from the environment to improve a policy or value function #</label></li>
<li><label><input type="checkbox">Recognize that planning updates use experience from a model to improve a policy or value function #</label></li>
<li><label><input type="checkbox">Describe how both direct RL and planning updates can be combined through the <strong>Dyna architecture</strong> #</label></li>
<li><label><input type="checkbox">Describe the <strong>Tabular Dyna-Q algorithm</strong> #</label></li>
<li><label><input type="checkbox">Identify the direct-RL and planning updates in <strong>Tabular Dyna-Q</strong> #</label></li>
<li><label><input type="checkbox">Identify the model learning and search control components of <strong>Tabular Dyna-Q</strong> #</label></li>
<li><label><input type="checkbox">Describe how learning from both direct and simulated experience impacts performance #</label></li>
<li><label><input type="checkbox">Describe how simulated experience can be useful when the model is accurate #</label></li>
</ul>
</section>
<section id="sec-l3g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g1">Direct RL updates use experience from the environment to improve a policy or value function</h3>
<ul>
<li><strong>Direct RL updates</strong> use experience from the environment to improve a policy or value function</li>
<li><strong>Direct RL updates</strong> can be used to improve a policy or value function by interacting with the environment</li>
</ul>
</section>
<section id="sec-l3g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g2">Planning updates use experience from a model to improve a policy or value function</h3>
<ul>
<li><strong>Planning updates</strong> use experience from a model to improve a policy or value function</li>
<li><strong>Planning updates</strong> can be used to improve a policy or value function without interacting with the environment</li>
</ul>
</section>
<section id="sec-l3g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g3">Both direct RL and planning updates can be combined through the <strong>Dyna architecture</strong></h3>
<ul>
<li><strong>Dyna architecture</strong> combines direct RL updates and planning updates to improve a policy or value function</li>
<li><strong>Dyna architecture</strong> uses a model to simulate the environment</li>
<li><strong>Dyna architecture</strong> uses the simulated experience to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l3g4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l3g4">The <strong>Tabular Dyna-Q algorithm</strong></h3>
<ul>
<li><strong>Tabular Dyna-Q</strong> is a planning algorithm that uses a sample model to improve a policy or value function</li>
<li><strong>Tabular Dyna-Q</strong> uses a sample model to simulate the environment</li>
<li><strong>Tabular Dyna-Q</strong> uses the simulated experience to improve a policy or value function</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-wk5-tabular-dyna-Q.png" class="img-fluid figure-img"></p>
<figcaption>The Tabular Dyna-Q algorithm</figcaption>
</figure>
</div></div><p>Exercise 8.2 Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiments?</p>
<p>Dyna-Q+ is like a generalized UCB while Dyna-Q+ is like a generalized epsilon greedy alg. Dyna-Q+ is doing more efficent exploration. It will revisits will be more spread out more over time but it scheme also tends to increases in non independent way - probabilities for unvisited regions keep growing so if it starts exploring it may like doing an extended sequence till it gets to a dead end.<br>
Dyna Q exploration is independent for each state,action combo so retrying sequences get asymptotically less likely with time.</p>
<p>Exercise 8.3 Careful inspection of Figure 8.5 reveals that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?</p>
<p>Dyna-Q+ is more efficient at exploring so it learned a better policy, but since the environment was static Dyna-Q got to catch up, but it never reached the same policy.</p>
<p>Exercise 8.5 How might the tabular Dyna-Q algorithm shown on page 164 be modified to handle stochastic environments? How might this modification perform poorly on changing environments such as considered in this section? How could the algorithm be modified to handle stochastic environments and changing environments?</p>
<p>to hadle a stochastic environment one would need to to model probabilities of stochastic dynamics. One way to do this is to use Bayesian updating with a dericlet prior and a multinomial posterior.<br>
This modification would likely fare much worse since learning low probability transitions would require many visits to discover.<br>
In the case of changing environment it would also take much longer for new state to be reflected in the model (if a state was visited 10 with just one transition and then the transition changed to another state then it would take many more than 10 vistis to quash the old probability and get the new one correct to 10%<br>
This means that we adding a forgetting rule might be better then the plain derichlet-multinomial model.<br>
To handle both stochastic and changing updates we may want to<br>
1. track the recency of the last visit and reward this option like in dyna-q plus.<br>
2. decay old probabilities - would require storing the time for each visit - i.e.&nbsp;path dependent model.<br>
3. A better idea is to use a hirachial model with parial pooling representing short term and long term transitions - this could fix the problem of decay by simply giving greater weight to the smaller more recent model.<br>
The short term would track the last k visits in each state and the long term all the visits. We could then do partial pooling between these two estimators with much greater emphasis on the recent one!<br>
<br>
</p>
</section>
<section id="sec-l3g5" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g5">Direct-RL and planning updates in <strong>Tabular Dyna-Q</strong></h3>
<ul>
<li><strong>Tabular Dyna-Q</strong> uses direct RL updates to improve a policy or value function</li>
<li><strong>Tabular Dyna-Q</strong> uses planning updates to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l3g6" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g6">Model learning and search control components of <strong>Tabular Dyna-Q</strong></h3>
<ul>
<li><strong>Tabular Dyna-Q</strong> uses a sample model to simulate the environment</li>
<li><strong>Tabular Dyna-Q</strong> uses the simulated experience to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l3g7" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g7">Learning from both direct and simulated experience impacts performance</h3>
<ul>
<li>Learning from both direct and simulated experience can improve performance</li>
<li>Learning from both direct and simulated experience can be more efficient than direct RL updates</li>
</ul>
</section>
<section id="sec-l3g8" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g8">Simulated experience can be useful when the model is accurate</h3>
<ul>
<li>Simulated experience can be useful when the model is accurate</li>
<li>Simulated experience can be used to improve a policy or value function without interacting with the environment</li>
</ul>
</section>
</section>
<section id="lesson-4-dealing-with-inaccurate-models" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-4-dealing-with-inaccurate-models">Lesson 4: Dealing with inaccurate models</h2>
<section id="lesson-learning-goals-3" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-3">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Identify ways in which models can be inaccurate #</label></li>
<li><label><input type="checkbox">Explain the effects of planning with an inaccurate model #</label></li>
<li><label><input type="checkbox">Describe how <strong>Dyna</strong> can plan successfully with a partially inaccurate model #</label></li>
<li><label><input type="checkbox">Explain how model inaccuracies produce another exploration-exploitation trade-off #</label></li>
<li><label><input type="checkbox">Describe how <strong>Dyna-Q+</strong> proposes a way to address this trade-off #</label></li>
</ul>
</section>
<section id="sec-l4g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g1">Ways in which models can be inaccurate</h3>
<ul>
<li>Models can be inaccurate for many reasons</li>
<li>because they have not sampled all actions in all states</li>
<li>because the environment is has changed since the model was learned</li>
<li>if the environment is stochastic</li>
</ul>
</section>
<section id="sec-l4g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g2">Effects of planning with an inaccurate model</h3>
<ul>
<li>Planning with an inaccurate model can cause the value function to become worse</li>
<li>Planning with an inaccurate model can lead to sub-optimal policies</li>
</ul>
</section>
<section id="sec-l4g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g3"><strong>Dyna</strong> can plan successfully with a partially inaccurate model</h3>
<ul>
<li><strong>Dyna</strong> can plan successfully with a partially inaccurate model</li>
<li><strong>Dyna</strong> can use direct RL updates to improve a policy or value function as well as the model</li>
<li><strong>Dyna</strong> can use planning updates to improve a policy or value function</li>
</ul>
</section>
<section id="sec-l4g4" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g4">Model inaccuracies produce another exploration-exploitation trade-off</h3>
<ul>
<li><p>Model inaccuracies produce another exploration-exploitation trade-off</p></li>
<li><p>exploit an inaccurate model to improve the policy</p></li>
<li><p>revisit states/actions with low value to update the model</p></li>
<li><p>Can we use an inverse sort of planning to identify states for which the model is inaccurate?</p></li>
<li><p>Model inaccuracies can lead to suboptimal policies</p></li>
<li><p>Model inaccuracies can lead to poor performance</p></li>
</ul>
</section>
<section id="sec-l4g5" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l4g5"><strong>Dyna-Q+</strong> proposes a way to address this trade-off</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-wk5-bonus-rewards-for-exploration.png" class="img-fluid figure-img"></p>
<figcaption>Dyna-Q+ solution</figcaption>
</figure>
</div></div><ul>
<li><strong>Dyna-Q+</strong> proposes a way to address this trade-off</li>
<li><strong>Dyna-Q+</strong> uses a bonus reward to encourage exploration</li>
<li><strong>Dyna-Q+</strong> can improve performance when the model is inaccurate</li>
</ul>
</section>
<section id="drew-bagnell-on-self-driving-cars-robotics-and-model-based-reinforcement-learning" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="drew-bagnell-on-self-driving-cars-robotics-and-model-based-reinforcement-learning">Drew Bagnell on self-driving cars robotics and model-based reinforcement learning</h3>
<p>Drew Bagnell is a professor at Carnegie Mellon University and the CTO at Aurora innovation.</p>
<p>He has worked on self-driving cars and robotics. He has also worked on model-based reinforcement learning. He point out a dirty little secret that model-based reinforcement learning is a key technology for robotics.</p>
<p>He points out that the real world is expensive and dangerous. Using model based reinforcement learning can reduce the number of interactions with the real world and along learning about risky actions in the simulated world to improve performance in the real world. Also as we pointer out before this can usually be done much faster than real-time interaction with the environment.</p>
<p>Sample complexity: how many real-world samples are required to achieve high performance? It takes exponentially fewer interactions with a model than without. Not really sure what exponentially fewer means here - but it’s a lot fewer.</p>
<p>Quadratic value function approximation goes back to optimal control in the 1960s. It’s continuous in states and actions. This is a method that should be part of the next course but isn’t covered there either</p>
<p>For linear transition dynamics with quadratic costs/rewards, it’s exact. For local convex / concave points, it is a good approximation of the true action-value function.</p>
<p>Here is the math from his slide:</p>
<p>Quadratic value function approximation</p>
<p><span id="eq-quadratic-value-function-approximation"><img src="https://latex.codecogs.com/png.latex?%0AQ_t(x,a)%20=%20%5Cbegin%7Bbmatrix%7D%20%20%20%20%20x%20%20%20%5C%5C%20a%20%20%20%5C%5C%20%5Cend%7Bbmatrix%7D%5ET%0A%5Cbegin%7Bbmatrix%7D%20%20%20%20%20Q_%7Bxx%7D%20%20%20&amp;&amp;%20%20Q_%7Bxa%7D%20%20%20%5C%5C%20Q_%7Bxa%7D%20%20%20&amp;&amp;%20%20Q_%7Buu%7D%20%20%20%5C%5C%20%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%20x%20%20%20%5C%5C%20a%20%20%20%5C%5C%20%5Cend%7Bbmatrix%7D%5ET%20+%0A%5Cbegin%7Bbmatrix%7D%20%20%20%20q_x%20%20%20%5C%5C%20q_a%20%5C%5C%20%5Cend%7Bbmatrix%7D%5ET%0A%5Cbegin%7Bbmatrix%7D%20%20%20%20%20x%20%20%20%5C%5C%20a%20%20%20%5C%5C%20%5Cend%7Bbmatrix%7D%20+%20const%0A%5Ctag%7B1%7D"></span></p>
<p>The approximation allows for calculating the optimal action-value in closed form (finite number of standard operations) even with continuous actions.</p>
<p>Differential dynamic programming takes advantage of the technique above.<br>
<br>
So this seems complicated - because of matrix maths. But intuitively this is something we like to do in physics - add the term for the second derivative<br>
in the taylor series approximation of our function.</p>
<p>The 2nd paper is particularly clear and easy to work through for the approach just described.</p>
<div class="column-page">
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">timeline

    title Bandit Algorithms Timeline

    
        1952 : Thompson Sampling
        1955 : Upper Confidence Bound (UCB)
        1963 : Epsilon-Greedy
        2002 : Bayesian UCB
        2011 : Bayesian Bandits
        2012 : Contextual Bandits
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
<div class="column-page">
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">%%{init: {'theme': 'base', 'themeVariables': { 'timeline': { 'nodeSpacing': 50, 'sectionSpacing': 100, 'verticalStartPosition': 50, 'verticalSectionStartPosition': 50 }}}}%%
timeline
    direction TD
    title Reinforcement Learning Algorithms Timeline


    
        1948 : Monte Carlo Methods
        1950 : Bellman Optimality Equations
        1957 : Dynamic Programming
        1959 : Temporal Difference Learning (TD)
        1960 : Policy Iteration
        1963 : Value Iteration
        1983 : Q-Learning
        1984 : Expected SARSA
        1990 : Dyna-Q : Dyna-Q+
        1992 : SARSA
        1994 : Monte Carlo with E-Soft
        1995 : Monte Carlo with Exploring Starts
             : Generalized Policy Iteration (GPI)
        1998 : Semi-Gradient TD
        2000 : Differential Semi-Gradient SARSA
        2001 : Gradient Monte Carlo (Gradient MC)
        2003 : Gaussian Actor-Critic
             : Softmax Actor-Critic
             : Deep Q-Network (DQN)


</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<p>Materials from <a href="https://sites.google.com/view/mbrl-tutorial">ICML Tutorial on Model-Based Reinforcement Learning</a>:</p>
<p>the page above contains the following materials as well as an extensive bibliography.</p>
<ul>
<li><a href="https://docs.google.com/presentation/d/1f-DIrIvh44-jmTIKdKcue0Hx2RqQSw52t4k8HEdn5-c/edit?usp=sharing">Slides</a></li>
<li><a href="https://slideslive.com/38930488/modelbased-methods-in-reinforcement-learning-part-1-introduction-learning-models">Part 1: Introduction and Learning Models</a></li>
<li><a href="https://slideslive.com/38930486/modelbased-methods-in-reinforcement-learning-part-2-modelbased-control">Part 2: Model-Based Control</a></li>
<li><a href="https://slideslive.com/38930487/modelbased-methods-in-reinforcement-learning-part-3-modelbased-control-in-the-loop">Part 3: Model-Based Control in the Loop</a></li>
<li><a href="https://slideslive.com/38930489/modelbased-methods-in-reinforcement-learning-part-4-beyond-vanilla-mbrl">Part 4: Beyond Vanilla MBRL</a></li>
</ul>
<p>From Bagnell’s talk:</p>
<ul>
<li><a href="https://macrl-book.github.io/">Modern Adaptive Control and Reinforcement Learning</a></li>
<li><a href="https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf">Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization</a></li>
<li><a href="https://katefvision.github.io/katefSlides/trajectoryoptimization_katef.pdf">Optimal Control, Trajectory Optimization, Learning Dynamics</a></li>
</ul>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Sample-based Learning Methods</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c2-w4.html</guid>
  <pubDate>Sun, 03 Mar 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Temporal Difference Learning Methods for Control</title>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c2-w3.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="module-3-temporal-difference-learning-methods-for-control" class="level1 page-columns page-full">
<h1>Module 3: Temporal Difference Learning Methods for Control</h1>
<section id="lesson-1-td-for-control" class="level2">
<h2 class="anchored" data-anchor-id="lesson-1-td-for-control">Lesson 1: TD for Control</h2>
<section id="lesson-learning-goals" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox" checked="">Explain how generalized policy iteration can be used with TD to find improved policies #</label></li>
<li><label><input type="checkbox" checked="">Describe the Sarsa Control algorithm #</label></li>
<li><label><input type="checkbox" checked="">Understand how the Sarsa control algorithm operates in an example MDP #</label></li>
<li><label><input type="checkbox" checked="">Analyze the performance of a learning algorithm in an MDP #</label></li>
</ul>
</section>
</section>
<section id="sec-l1g1" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g1">Generalized Policy Iteration with TD</h2>
<p>we would like now to combine TD with a planning algorithm to use TD for control. We This will be a GPI algorithm.</p>
<section id="generalized-policy-iteration---recap" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="generalized-policy-iteration---recap">Generalized Policy Iteration - Recap</h3>
<p>lets recap the Generalized Policy Iteration (GPI) algorithm:</p>
<ul>
<li><strong>Policy Evaluation</strong>: Update the value function V to be closer to the true value function of the current policy</li>
<li><strong>Policy Improvement</strong>: Improve the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> based on the current value function V</li>
<li><strong>Generalized Policy Iteration</strong>: Repeated steps of policy evaluation and policy improvement</li>
<li>GPI does not require full evaluation of the value function, just an improvement can be used to update the policy.</li>
<li>policy iteration
<ul>
<li>run policy evaluation to convergence</li>
<li>greedifing the policy</li>
</ul></li>
<li>GPI MC
<ul>
<li>each episode:
<ul>
<li>policy evaluation (nota full evaluation)</li>
<li>improvement per episode</li>
</ul></li>
</ul></li>
<li>GPI TD
<ul>
<li>each step:
<ul>
<li>policy evaluation (for just one action)</li>
<li>improvement pi after the single time step.</li>
</ul></li>
</ul></li>
</ul>
</section>
<p>Recall how in the first course we saw DP methods for solving MDPs using the four part dynamic function and its variants. We used the Bellman equation to write down a system of linear equations for the value function and solve them exactly. We then used the value function to find the optimal policy. So in DP we don’t need to interact with the environment or to learn. We can compute the value function and the optimal policy exactly.</p>
<p>In these course we relaxed the assumption of knowing the transition dynamics or the expected returns. This creates a new challange of learning V or Q from experience.</p>
<p>In the first lesson we saw how MC methods can help us learn the value function but with the caveat that we need to wait until the end of the episode to update the value function.</p>
<p>However we have now seen how the TD(0) algorithm uses recursive nature of the Bellman equation for the value function to make approximate updates to the value function. This allows us to learn Values of states directly from experience.</p>
<p>Once we are able to approximate the value function, we can use it to create new generalized policy iteration algorithms. This part of the GPI remains the same, we still evaluate the policy and improve it. But now we can do this in an online fashion, updating the value function after each step.</p>
<p>In SARSA we are making updates to the policy after a single step - this may lead to much faster convergence to the optimal policy. It also allows us to improve our plans during an episode or in a continuing task.</p>
<p>The advantage of TD methods is that they can be used in continuing tasks, where the agent interacts with the environment indefinitely. This is because the value function is updated after each step, and the agent can continue to learn and improve its policy as it interacts with the environment. But this advantage is better understood by considering the episodic tasks, where the agent can learn during an episode that consequences of its actions are sub optimal. This allows TD(0) based GPI to make more frequent updates to the policy within one episode.</p>
</section>
<section id="sec-l1g2" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g2">Sarsa: On-policy TD Control</h2>
<p>Next we consider how we can derive and use a similar approximate updating of the action-value function to learn the action-value function directly from experience.</p>
<p>lets recap the Bellman equation for the action-value function:</p>
<p><span id="eq-belman-action-value"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Aq_%5Cpi(s,a)%20&amp;%20%5Cdot%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BG_t%20%7C%20S_t%20=%20s,%20A_t%20=%20a%5D%20%5Cqquad%20%5Cnewline%0A&amp;%20=%20%5Csum_%7Bs',%20r%7D%20p(s',%20r%20%7C%20s,%20a)%20%5Br%20+%20%5Cgamma%20%5Csum_%7Ba'%7D%5Cpi(a'%20%5Cmid%20s')%20q_%5Cpi(s',%20a')%5D%20%5Cqquad%0A%5Cend%7Baligned%7D%0A%5Ctag%7B1%7D"></span></p>
<p>We can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.</p>
<p><span id="eq-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,%20A_t)%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20Q(S_%7Bt+1%7D,%20A_%7Bt+1%7D)%20-%20Q(S_t,%20A_t)%5D%0A%5Ctag%7B2%7D"></span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SARSA
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-sarsa" class="pseudocode-container quarto-float" data-no-end="false" data-pseudocode-number="1" data-comment-delimiter="#" data-line-number="true" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{SARSA($\alpha,\epsilon$)}\begin{algorithmic} \State Initialize: \State $\qquad Q(s,a) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S^+}, a\in\mathcal{A}(s)\ except\ for\ Q(terminal,\cdot)=0$ \For {each episode e:} \State Initialize $S$ \State Choose A from S using policy derived from Q (e.g., $\epsilon$-greedy) \For {each step of e} \State Take action A, observe R, S' \State Choose A' from S' using policy derived from Q (e.g., $\epsilon$-greedy) \State $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma Q(S',A') - Q(S, A)]$ \State $S \leftarrow S'$; $A \leftarrow A'$ \EndFor \State until $S$ is terminal \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<p>The SARSA algorithm is due to Rummery, Gavin Adrian, and Mahesan Niranjan. The name Sarsa is due to Rich Sutton and comes from the fact that the algorithm uses the tuple <img src="https://latex.codecogs.com/png.latex?(S_t,%20A_t,%20R_%7Bt+1%7D,%20S_%7Bt+1%7D,%20A_%7Bt+1%7D)"> to update the action-value function. <span class="citation" data-cites="Rummery1994OnlineQU">[@Rummery1994OnlineQU]</span></p>
<p>SARSA is a sample-based algorithm to solve the Bellman equation for action-values. - It picks an action based on the current policy and then - It policy evaluation by a TD updates of Q the action-value function based on the reward and the next action. - Then it does a policy improvement.</p>
</section>
<section id="sec-l1g3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l1g3">SARSA in an Example MDP</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-sarsa-windy-gridworld.png" class="img-fluid figure-img"></p>
<figcaption>windy gridworld</figcaption>
</figure>
</div></div><p>In this grid world isn’t a good fit for MC methods as most policies never terminate. This is because the agent is pushed up by the wind and has to learn to navigate to the goal. Anyhow if the episode never terminates MC wont be able to update the value function.</p>
<p>But Sarsa can learn the optimal policy in this environment by abandoning the epsilon greedy policy in mid episode and finding better policies. We can see that early episodes take longer to terminate after the e-greedy policy stops peaks.</p>
</section>
<section id="sec-l1g4" class="level2">
<h2 class="anchored" data-anchor-id="sec-l1g4">Performance of Learning Algorithms in an MDP</h2>
<p>On the right side of the figure we see the performance of the learning algorithms in the windy grid world. We see that in this chart the Sarsa algorithm learns the optimal policy at Around step 7000 where the gradient becomes constant.</p>
<p>Q. why is SARSA called an on-policy algorithm?</p>
<p>this is because it learns by sampling from the policy induced by Q while following the same policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
</section>
<section id="lesson-2-off-policy-td-control-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="lesson-2-off-policy-td-control-q-learning">Lesson 2: Off-policy TD Control: Q-learning</h2>
<section id="lesson-learning-goals-1" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-1">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox" checked="">Describe the Q-learning algorithm #</label></li>
<li><label><input type="checkbox" checked="">Explain the relationship between q-learning and the Bellman optimality equations. #</label></li>
<li><label><input type="checkbox">Apply q-learning to an MDP to find the optimal policy #</label></li>
<li><label><input type="checkbox">Understand how Q-learning performs in an example MDP #</label></li>
<li><label><input type="checkbox">Understand the differences between Q-learning and Sarsa #</label></li>
<li><label><input type="checkbox">Understand how Q-learning can be off-policy without using importance sampling #</label></li>
<li><label><input type="checkbox">Describe how the on-policy nature of SARSA and the off-policy nature of Q-learning affect their relative performance #</label></li>
</ul>
</section>
<p>lets recap the Bellman optimality equation for the action-value function:</p>
<p><span id="eq-belman-optimality-action-value"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Aq_%7B%5Cstar%7D(s,a)%20=%20%5Csum_%7Bs',%20r%7D%20p(s',%20r%20%7C%20s,%20a)%20%5Br%20+%20%5Cgamma%20%5Cmax_%7Ba'%7D%20q_%7B%5Cstar%7D(s',%20a')%5D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B3%7D"></span></p>
<p>The following is an update rule for Q-learning:</p>
<p>We can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.</p>
<p><span id="eq-q-learning-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,%20A_t)%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Cmax_a%20Q(S_%7Bt+1%7D,%20a')%20-%20Q(S_t,%20A_t)%5D%0A%5Ctag%7B4%7D"></span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q-learning (off-policy TD control)
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-q-learning" class="pseudocode-container quarto-float" data-no-end="false" data-pseudocode-number="2" data-comment-delimiter="#" data-line-number="true" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{Q-learning Off-policy TD control}\begin{algorithmic} \State Initialize: \State $\qquad Q(s,a) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S^+}, a\in\mathcal{A}(s)\ except\ for\ Q(terminal,\cdot)=0$ \For {each episode e:} \State Initialize $S$ \For {each step of e} \State Choose A from $A_B(S)$ using any ergodic Behavioural policy $B$ - perhaps the $\epsilon$-greedy induced by Q. \State Take action A, observe R, S' \State $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \max_a Q(S',a) - Q(S, A)]$ \State $S \leftarrow S'$ \EndFor \State until $S$ is terminal \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
<p>Note: I made some cosmetic changes to the psuedo code in the book to resolve the confusion I had about nature the behavioral policy.</p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The behavioral policy in Q-learning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Q-learning has a subtle issue I found confusing at first.</p>
<p>Here I first state the issue:</p>
<p>What is the behavioral policy we follow in these three td-learning algorithms when we sample the next action to follow?</p>
<p>We are not writing here that Q function is <img src="https://latex.codecogs.com/png.latex?Q_%5Cpi"> but the value functions are by definitions expectations under some policy. In these algorithms we keep updating the Q function using TD(0) updates. If we update the Q function in a sense that the best action changes at a given step then the updated function now uses a new policy. (In the case of Sarsa we can actually get a worse policy after the update.) I figured this out very quickly.</p>
<p>A fully specified Q-functions isn’t just defined by following a policy. It also <strong>induces a policy</strong>. This in generaly is a stochastic policy. But if we take the greedy action with arbitrary tie breaks we get one or more deterministic policies. So it seems that off policy algorithms like Q-learning and Expected Sarsa are following a sequence of policies that are induced by the Q function that is being learned.</p>
<p>In general off policy learning may be using S,A,R sequences that have been sampled like we clearly did in MC. So the question which arises is can sample from any ergodic policy as our behavioral policy in these off-policy algorithms or are we supposed to learn from experience and sample using the policy induced by latest and greatest Q function that we are learning?</p>
<p><strong>Luckily Martha White is very clear about this</strong>:</p>
<ul>
<li>The target policy is the easy part - we are targeting <img src="https://latex.codecogs.com/png.latex?Q_%7B%5Cpi_%5Cstar%7D">.</li>
<li>The behavior policy is the policy can be any policy so long as it is ergodic.
<ul>
<li>Using an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy policy derived from Q is very logical choice but we could use any other policy.</li>
</ul></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Do these algorithms converge?
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Is Q-learning guaranteed to converge ?</li>
<li>The course glossed over this in lectures perhaps referencing the text book – I will have to go back and check this.</li>
<li>However as this is introductory CS and not Mathematics I will try to suspend my disbelief that the algs are guarenteed to converge and return to the point.</li>
</ul>
</div>
</div>
<ol type="1">
<li>It is an off-policy algorithm.</li>
</ol>
<ul>
<li>The <strong>target policy</strong> is the easy part - we are targeting <img src="https://latex.codecogs.com/png.latex?Q_%7B%5Cpi_%5Cstar%7D">.</li>
<li>The <strong>behavior policy</strong> is the policy that we are following but what is that ?
<ul>
<li>it is clearly not <img src="https://latex.codecogs.com/png.latex?Q_%7B%5Cpi_%5Cstar%7D"> as we don’t know it yet.</li>
<li>we initialized Q(s,a) arbitrarily - so we may have a uniform random policy.</li>
<li>bu we actual have any random policy.
<ul>
<li>any action that is a legit transition from the current state is a valid action.</li>
<li>so long as their probabilities add up to 1.</li>
</ul></li>
<li>later Martha keeps saying that we need the ergodicity of the MDP to ensure that out policy will visit all states and actions with non-zero probability.</li>
<li>this anyhow is one source of confusion.</li>
<li>however an epsilon greedy policy of the induced policy from Q seems like a very good choice. Can we do better ?</li>
<li>another point to consider here is that this is a value iteration algorithm.
<ul>
<li>what can we say about the inermediate Q functions that we are learning ?</li>
<li>are they even a valid action value function ?</li>
<li>is the policy they induce a coherent probability distribution over actions ?</li>
</ul></li>
</ul></li>
</ul>
<ol start="2" type="1">
<li>When we select the action A’ what policy are we using ?</li>
</ol>
<ul>
<li><p>we are clearly not using the policy that we are learning <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Cstar"> - we dont know it yet.</p></li>
<li><p>we are could use an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy policy by greedifying Q. this seems the most logical</p></li>
<li><p>but we could pretty much use any other policy.</p></li>
<li><p>this is because Q-learning is an off-policy algorithm.</p></li>
<li><p>the confusion arises because it is not clear what “any policy derived from Q” means in the algorithm.</p></li>
<li><p>q-learning</p>
<ul>
<li>is a <span class="marked">value iteration algorithm</span></li>
<li>uses <span class="marked">the Bellman optimality equation</span> to update the action-value function.</li>
<li>selects the action based on greedyfing the current q-values and then</li>
</ul></li>
<li><p>it policy evaluation by a TD updates of Q the action-value function based on the reward and the next action.</p></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
why can’t q-learning account for the consequences of exploration in its policy ?
</div>
</div>
<div class="callout-body-container callout-body">
<p>q-learning learns the optimal policy but follows a some other policy. Let suppose the optimal policy is deterministic. And let’s suppose that the behavior policy is epsilon greedy based on that.</p>
<p>The alg does not follow the optimal policy - it follows the behavior policy and this will perform much worse because of exploration.</p>
<p>If we need to account for the consequences of exploration in the policy we need to use a different algorithm!</p>
</div>
</div>
</section>
<section id="sec-l2g4" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g4">Q-learning in an Example MDP</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-sarsa-windy-gridworld.png" class="img-fluid figure-img"></p>
<figcaption>windy gridworld</figcaption>
</figure>
</div></div><p>In this grid world isn’t a good fit for MC methods as most policies never terminate.</p>
<p>Q-learning can learn the optimal policy in this environment by abandoning the epsilon greedy policy in mid episode and finding better policies.</p>
<p>We can see that early episodes take longer to terminate after the e-greedy policy stops peaks.</p>
<p>However Q-learning does not take seem to factor in the consequences of exploration in its policy.</p>
<p>This is because it is learning the optimal policy and not the policy that it follows.</p>
<p>Q-learning does not need Importance sampling to learn off-policy. This is because it is learning action values.</p>
</section>
<section id="sec-l2g5" class="level2">
<h2 class="anchored" data-anchor-id="sec-l2g5">Comparing Sarsa and Q-learning</h2>
<p>Q-learning is an off-policy algorithm:</p>
<ul>
<li>the target policy is the optimal policy since the update rule approximates the Bellman optimality equation.</li>
<li>the behavior policy is initially given updated at each step from the inital get updated a bit towards the optimal policy at each step.</li>
</ul>
<p>because it is learning <img src="https://latex.codecogs.com/png.latex?%5Cpi_*"> (the optimal policy) but it samples a different policy.</p>
<p>This is in contrast to Sarsa, which is an on-policy algorithm because it learns the policy that it follows.</p>
</section>
<section id="lesson-3-expected-sarsa" class="level2">
<h2 class="anchored" data-anchor-id="lesson-3-expected-sarsa">Lesson 3: Expected SARSA</h2>
<section id="lesson-learning-goals-2" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-2">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Describe the Expected SARSA algorithm #</label></li>
<li><label><input type="checkbox">Describe Expected SARSA’s behavior in an example MDP #</label></li>
<li><label><input type="checkbox">Understand how Expected SARSA compares to SARSA control #</label></li>
<li><label><input type="checkbox">Understand how Expected SARSA can do off-policy learning without using importance sampling #</label></li>
<li><label><input type="checkbox">Explain how Expected SARSA generalizes Q-learning #</label></li>
</ul>
</section>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AQ(S_t,%20A_t)%20&amp;%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Cmathbb%7BE%7D%5BQ(S_%7Bt+1%7D,%20A_%7Bt+1%7D)%5D%20-%20Q(S_t,%20A_t)%5D%20%5Cnewline%0A&amp;%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Csum_a%20%5Cpi(a%7CS_%7Bt+1%7D)%20%5Ccdot%20Q(S_%7Bt+1%7D,%20a)%20-%20Q(S_t,%20A_t)%5D%0A%5Cend%7Baligned%7D%0A"></p>
<section id="sec-l3g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g1">Understanding Expected Sarsa</h3>
<p>lets recap the Bellman equation for the action-value function:</p>
<p><span id="eq-belman-action-value"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Aq_%5Cpi(s,a)%20&amp;%20%5Cdot%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BG_t%20%7C%20S_t%20=%20s,%20A_t%20=%20a%5D%20%5Cqquad%20%5Cnewline%0A&amp;%20=%20%5Csum_%7Bs',%20r%7D%20p(s',%20r%20%7C%20s,%20a)%20%5Br%20+%20%5Cgamma%20%5Csum_%7Ba'%7D%5Cpi(a'%20%5Cmid%20s')%20q_%5Cpi(s',%20a')%5D%20%5Cqquad%0A%5Cend%7Baligned%7D%0A%5Ctag%7B5%7D"></span></p>
<p>We can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.</p>
<p>in the sarsa update rule:</p>
<p><span id="eq-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,%20A_t)%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20Q(S_%7Bt+1%7D,%20A_%7Bt+1%7D)%20-%20Q(S_t,%20A_t)%5D%0A%5Ctag%7B6%7D"></span></p>
<p>we knows the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> so we can make a better update by replacing the sampled next action with the expected value of the next action under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>This is the basis of Expected Sarsa.</p>
<p>which uses the update rule:</p>
<p><span id="eq-expected-sarsa-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,%20A_t)%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%20%5Calpha%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Csum_a%20%5Cpi(a%7CS_%7Bt+1%7D)%20%5Ccdot%20Q(S_%7Bt+1%7D,%20a)%20-%20Q(S_t,%20A_t)%5D%0A%5Ctag%7B7%7D"></span></p>
<p>Otherwise the algorithm is the same as Sarsa.</p>
<ul>
<li>This target is more stable than the Sarsa target because it is less noisy.</li>
<li>This makes it converge faster than Sarsa.</li>
</ul>
<p>this has a has a down side - it has more computation than Sarsa due to avaraging over many actions for every step.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expected Sarsa
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-q-learning" class="pseudocode-container quarto-float" data-no-end="false" data-pseudocode-number="3" data-comment-delimiter="#" data-line-number="true" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{expected SARSA}\begin{algorithmic} \State Initialize: \State $\qquad Q(s,a) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S^+}, a\in\mathcal{A}(s)\ except\ for\ Q(terminal,\cdot)=0$ \For {each episode e:} \State Initialize $S$ \State Choose A from S using policy derived from Q (e.g., $\epsilon$-greedy) \For {each step of e} \State Take action A, observe R, S' \State Choose A' from S' using policy derived from Q (e.g., $\epsilon$-greedy) \State $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \sum_a \pi(a|S') Q(S',A') - Q(S, A)]$ \State $S \leftarrow S'$; $A \leftarrow A'$ \State $S$ is terminal \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summary - Connecting the dots
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Expected Sarsa is a generalization of Q-learning and Sarsa.</li>
<li>There are many RL algorithms in our specialization flowchart.</li>
<li>We would like to find a few or even one algorithm that may be widely applicable to many different settings, carrying over the insights we learned from each new algorithm.</li>
<li>The first step in this direction was introducing the <img src="https://latex.codecogs.com/png.latex?epsilon"> parameter to the bandit algorithms, which allowed us to treat the exploration-exploitation trade-off. We have seen additional strategies for exploration but we have been using either an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy strategy or a epsilon-soft strategy in most algorithms.
<ul>
<li>We also got the powerful idea of using confidence intervals as tie breakers in the case of multiple actions with the same expected reward.</li>
</ul></li>
<li>Another step in this direction was to introduce discounting of rewards which let us parameter discounting with <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> and thus treat episodic and continuing tasks in the same way.</li>
<li>The MC algorithms showed that this is not enough to fully generalize to episodic and continuing tasks. However we got a powerful new ideas of inverse sampling and doubly robust estimators. For use with off-policy learning.</li>
<li>Next we introduced GPI in which we combined policy evaluation and policy improvement algorithms to iteratively approximate the optimal policy in a single algorithm.</li>
<li>Another lesson was to using the TD error to bootstrap the value function. This let us update value functions after each step, rather than waiting until the end of the episode, increasing the data efficiency of the algorithms.
<ul>
<li>We also saw that we can use this idea with action-value functions, which is more fine grained than the value function and can lead to more efficient learning.</li>
</ul></li>
<li>Next we saw that Expected Sarsa is one such algorithm that can be used in many different settings.
<ul>
<li>It can be used in episodic and continuing tasks,</li>
<li>It can be used for on-policy and off-policy learning.</li>
<li>It is a GPI algorithm that uses the TD error to update the action-value function. And it the <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy strategy implicit in its action-value function.</li>
</ul></li>
</ul>
</div>
</div>


</section>
</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Sample-based Learning Methods</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c2-w3.html</guid>
  <pubDate>Sat, 02 Mar 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Temporal Difference Learning Methods for Prediction</title>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c2-w2.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reading
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=91">RL Book§5.0-5.5 (pp.91-104)</a></label></li>
</ul>
</div>
</div>
</div>
<section id="lesson-1-introduction-to-temporal-difference-learning" class="level2">
<h2 class="anchored" data-anchor-id="lesson-1-introduction-to-temporal-difference-learning">Lesson 1: Introduction to Temporal Difference Learning</h2>
<section id="lesson-learning-goals" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Define temporal-difference learning #</label></li>
<li><label><input type="checkbox">Define the temporal-difference error #</label></li>
<li><label><input type="checkbox">Understand the TD(0) algorithm #</label></li>
</ul>
</section>
<section id="temporal-difference-learning-definition" class="level3">
<h3 class="anchored" data-anchor-id="temporal-difference-learning-definition">Temporal Difference Learning definition</h3>
<p>Now we turn to a new class of methods called Temporal Difference (TD) learning. According to Bass, TD learning is one of the key innovations in RL. TD learning is a method that combines the sampling of Monte Carlo methods with the bootstrapping of Dynamic Programming methods. The term <strong>temporal</strong> in the name references learning from two subsequent time steps and the term <strong>difference</strong> refers to using the difference between the values of each state.</p>
<p>Let us now derive the TD update rule for the value function:</p>
<p>Recall the definition of the value function from the previous course, is the expected return when starting in a particular state and following a particular policy.<br>
We write this as:</p>
<p><span id="eq-value-function"><img src="https://latex.codecogs.com/png.latex?%0Av_%5Cpi(s_t)%20%5Cdot%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BG_t%20%7C%20S_t%20=%20s%5D%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<p>We can motivate td update rule by considering the DP and MC update rules.</p>
<p>In The MC update rule a sample update based on the return for the entire episode. Which means that we can only update our value function at the end of the episode.</p>
<p><span id="eq-mc-online-update"><img src="https://latex.codecogs.com/png.latex?%0AV(S_t)%20%5Cleftarrow%20V(S_t)%20+%20%5Calpha%20%5B%5Cunderbrace%7BG_t%7D_%7B%5Ctext%7BMC%20target%7D%7D%20-V(S_t)%5D%20%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>
<p>where:</p>
<p><img src="https://latex.codecogs.com/png.latex?G_t"> is the return at time step <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?S_t"> is the state at time step <img src="https://latex.codecogs.com/png.latex?t">.</p>
<p>This is the actual return at time t.</p>
<p>In the DP update rule, the target is a full backup based on the entire distribution of possible next states which we can solve for exactly since we have the full dynamics of the environment.</p>
<p><span id="eq-dp-update"><img src="https://latex.codecogs.com/png.latex?%0AV(S_t)%20%5Cleftarrow%20V(S_t)%20+%20%5Csum_a%5Cpi(a%7CS_t)%20(%5Br(s,a)%20+%20%5Cgamma%20p(s'%5Cmid%20s,a)%20V(s')%5D)%20%5Cqquad%0A%5Ctag%7B3%7D"></span></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D"> is the reward at time step <img src="https://latex.codecogs.com/png.latex?t+1"></li>
<li><img src="https://latex.codecogs.com/png.latex?V(S_%7Bt+1%7D)"> is the approximate value of the state at time step <img src="https://latex.codecogs.com/png.latex?t+1"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the discount factor</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha"> is the learning rate</li>
</ul>
<p>How can we make updates at each time step?</p>
<p><span id="eq-return-definition"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AG_t%20&amp;%20%5Cdot=%20R_%7Bt+1%7D%20+%20%5Cgamma%20R_%7Bt+2%7D%20+%20%5Cgamma%5E2%20R_%7Bt+3%7D%20+%20%5Cldots%20%5Cqquad%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20&amp;=%20R_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%20%5Cqquad%0A%5Cend%7Baligned%7D%0A%5Ctag%7B4%7D"></span></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Av_%5Cpi(s_t)%20%20&amp;%20=%20%5Cmathbb%7BE%7D_%5Cpi%20%5BG_t%20%5Cmid%20S_t%20=%20s%5D%20&amp;%20%5Ctext%7Bdefinition%7D%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cmathbb%7BE%7D_%5Cpi%20%5BR_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%20%5Cmid%20S_t%20=%20s%5D%20&amp;%20%5Ctext%7B(subst.%20Recursive%20return)%7D%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BR_%7Bt+1%7D%20+%20%5Cgamma%20v_%5Cpi%20(S_%7Bt+1%7D)%20%7CS_t=s%5D%20%20%20%20&amp;%20%5Ctext%7B(subst.%20value%20function)%7D%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cmathbb%7BE%7D_%5Cpi%5BR_%7Bt+1%7D%7CS_t=s%5D%20%20+%20%5Cgamma%20%5Cmathbb%7BE%7D_%5Cpi%5Bv_%5Cpi%20(S_%7Bt+1%7D)%20%7CS_t=s%5D%20&amp;%20%5Ctext%7B(by%20linearity%20of%20Expectation)%7D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20&amp;%20=%20R_%7Bt+1%7D%20+%20%5Cgamma%20v_%5Cpi%20(S_%7Bt+1%7D)%20&amp;%20%5Ctext%7B(by%20Expectation%20of%20constant%20RV)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>In this formula we have replaced <img src="https://latex.codecogs.com/png.latex?G_t"> with <img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D%20+%20%5Cgamma%20V(S_%7Bt+1%7D)">. We now have a recursive formula for the value functions in terms of the next value function. The next value function is a stand in for the value of the Return <img src="https://latex.codecogs.com/png.latex?G_%7Bt+1%7D"> which we don’t know.</p>
<p>we can use this formula to make an online update to our value function using an MC estimate of the return, without saving the full list of rewards.</p>
<p>since <img src="https://latex.codecogs.com/png.latex?G_t">, the MC update target is the return for the entire episode, we can only update our value function at the end of the episode. We will soon look at TD learning, which allows us to make updates at each time step.</p>
<p>the motivation for TD learning is based on the MC update rule. The MC update rule is a sample update based on the return for the entire episode. This means that we can only update our value function at the end of the episode. We will soon look at TD learning, which allows us to make updates at each time step.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AV_%5Cpi(S_t)%20%5Cleftarrow%20V(S_t)%20+%20%5Calpha%20%5B%0A%5Cunderbrace%7BR_%7Bt+1%7D%20+%20%5Cgamma%20V(S_%7Bt+1%7D)%7D_%7B%5Ctext%7BTD%20target%7D%7D%20-%20V(S_t)%5D%0A"></p>
<p>We can use the following to make online TD updates like we were able to update our value function without saving the full list of rewards. here:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?V(S_t)"> is the value of the state at time <img src="https://latex.codecogs.com/png.latex?t"></li>
<li><img src="https://latex.codecogs.com/png.latex?V(S_%7Bt+1%7D)"> is the value of the state at time <img src="https://latex.codecogs.com/png.latex?t+1"></li>
<li><img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D"> is the reward at time <img src="https://latex.codecogs.com/png.latex?t+1"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha"> is a constant the learning rate</li>
<li>the target is an estimate of the return</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AG_t%20&amp;%20%5Cdot%20=%20R_%7Bt+1%7D%20+%20%5Cgamma%20R_%7Bt+2%7D%20+%20%5Cgamma%5E2%20R_%7Bt+3%7D%20+%20%5Cldots%20%5Cnewline%0A&amp;=%20R_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%20%5Cnewline%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>The MC target is an estimate of the expected value (average of sampled return) - The DP target is an estimate because the true value of the state is not known - The TD target is an estimate for both reasons: a sample of the expected value of the reward, and the current estimate of the value of the state rather than the true value.</li>
</ul>
<p>Like with MC, the target is a sample updates based on a single observed transition. This is in stark contrast to DP, where the target is a full backup based on the entire distribution of possible next states which we can solve for exactly since we have the full dynamics of the environment.</p>
</section>
<section id="temporal-difference-error" class="level3">
<h3 class="anchored" data-anchor-id="temporal-difference-error">Temporal Difference Error</h3>
<ul>
<li>The TD error is the difference between the estimated value of a state and the value of the state at the next time step.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdelta_t%20%5Cdot%20=%20R_%7Bt+1%7D%20+%20%5Cgamma%20V(S_%7Bt+1%7D)%20-%20V(S_t)%0A"> this is one of the most important equations in reinforcement learning - we will see it again and again.</p>
</section>
<section id="td0-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="td0-algorithm">TD(0) Algorithm</h3>
<ul>
<li>The TD(0) algorithm is a TD learning algorithm that uses a bootstrapping method to estimate the value of a state.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The TD(0) algorithm for estimating <img src="https://latex.codecogs.com/png.latex?v_%5Cpi">
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-td-zero" class="pseudocode-container quarto-float" data-comment-delimiter="#" data-line-number-punc=":" data-no-end="false" data-caption-prefix="Algorithm" data-pseudocode-number="1" data-line-number="true" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{TD(0) for estimating $v_\pi$}\begin{algorithmic} \State Input: \State $\qquad \pi \leftarrow \text{policy to be evaluated}$ \State Initialize: \State $\qquad V(s) \leftarrow x \in \mathbb{R} \quad \forall s \in \mathcal{S}$ \FORALL {episode $e$:} \State $S \leftarrow \text{initial state of episode}$ \State $\text{Choose } A \sim \pi(\cdot \mid S)$ \FORALL {step $S \in e$:} \State Take action $A$, observe $R, S'$ \State $V(S) \leftarrow V(S) + \alpha [R + \gamma V(S') - V(S)]$ \State $S \leftarrow S'$ \State $\text{Choose} A' \sim \pi(\cdot \mid S)$ \State $S$ is terminal \ENDFOR \ENDFOR \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
</section>
</section>
<section id="lesson-2-advantages-of-td" class="level2">
<h2 class="anchored" data-anchor-id="lesson-2-advantages-of-td">Lesson 2: Advantages of TD</h2>
<section id="lesson-learning-goals-1" class="level3 unnumbered callout-info">
<h3 class="unnumbered anchored" data-anchor-id="lesson-learning-goals-1">Lesson Learning Goals</h3>
<ul class="task-list">
<li><label><input type="checkbox">Understand the benefits of learning online with TD #</label></li>
<li><label><input type="checkbox">Identify key advantages of TD methods over Dynamic Programming and Monte Carlo methods #</label></li>
<li><label><input type="checkbox">Identify the empirical benefits of TD learning #</label></li>
</ul>
</section>


</section>

 ]]></description>
  <category>Coursera</category>
  <category>Sample-based Learning Methods</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c2-w2.html</guid>
  <pubDate>Fri, 01 Mar 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Monte-Carlo Methods for Prediction &amp; Control</title>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c2-w1.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<div class="tldr callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR This lesson in a nutshell
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/in_a_nutshell.jpg" class="img-fluid figure-img"></p>
<figcaption>in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>In this module we will embrace the paradigm of “learning from experience”.</li>
<li>This is called Sample based Reinforcement Learning and it we will let us relax some strong of the requirements of dynamic programming, namely knowing the table of MDP dynamics.</li>
<li>We will first use efficient Monte-Carlo ⚅🃁 methods for 🔮 prediction problem of estimating <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(S)"> value functions and action–value functions <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(a)"> from sampled episodes.</li>
<li>We will revise our algorithm to better handle exploration using exploring starts and <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">–soft policies.</li>
<li>We will adapt GPI algorithms for use with Mote-Carlo to solve the 🎮 control problem of policy improvement.</li>
<li>With off policy learning learn a policy using samples from another policy, by corrected using importance sampling.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reading
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=91">RL Book§5.0-5.5 (pp.91-104)</a></label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definitions
</div>
</div>
<div class="callout-body-container callout-body">
<p>Here are most of the definitions we need for this module. Let us review them before we start.</p>
<div id="dfn-value">
<dl>
<dt>Value Function <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"></dt>
<dd>
<p>a state’s value is its expected return</p>
</dd>
<dd>
<p><img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)%20%5Cdoteq%20%5Cmathbb%7BE%7D%5BG_t%7CS_t=s%5D"></p>
</dd>
</dl>
</div>
<div id="dfn-action-value">
<dl>
<dt>Action Value Function</dt>
<dd>
<p>is the expected return for taking action <img src="https://latex.codecogs.com/png.latex?a"> in state <img src="https://latex.codecogs.com/png.latex?s"> if we follow policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"></p>
</dd>
<dd>
<p><img src="https://latex.codecogs.com/png.latex?q_%5Cpi(a)%20%5Cdoteq%20%5Cmathbb%7BE%7D%5BG_t%20%5Cvert%20A_t=a%5D%20%5Cspace%20%5Cforall%20a%20%5Cin%20%5C%7Ba_1%20...%20a_k%5C%7D"></p>
</dd>
</dl>
</div>
<div id="dfn-bootstrap">
<dl>
<dt>Bootstrapping</dt>
<dd>
<p>“learning by guessing from a guess” or more formally</p>
</dd>
<dd>
<p>the process of updating an estimate of the value or action-value function based on other estimated values. It involves using the current estimate of the value function to update and improve the estimate itself.</p>
</dd>
</dl>
</div>
<div id="dfn-control">
<dl>
<dt>Control</dt>
<dd>
<p>to approximate optimal policies using the DP approach of GPI</p>
</dd>
</dl>
</div>
<div id="dfn-epsilon-soft">
<dl>
<dt>ϵ-Soft Policy</dt>
<dd>
<p>A policy in which each possible action is assigned at least <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20/%20%7CA%7C"> probability.</p>
</dd>
</dl>
</div>
<div id="dfn-exploring-starts">
<dl>
<dt>Exploring Starts</dt>
<dd>
<p>Learning the value or action values of a policy by trying each action starting in each state at least once and then following the policy.</p>
</dd>
<dd>
<p>This can include taking actions that are not part of the policy.</p>
</dd>
</dl>
</div>
<div id="dfn-mc">
<dl>
<dt>Monte-Carlo Methods</dt>
<dd>
<p>Estimation methods which relies on repeated random sampling. Also see <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte-Carlo methods <i class="bi bi-wikipedia"></i></a></p>
</dd>
</dl>
</div>
<div id="dfn-on-policy-learning">
<dl>
<dt>On-policy learning</dt>
<dd>
<p>learning a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> by sampling from <img src="https://latex.codecogs.com/png.latex?%5Cpi"></p>
</dd>
</dl>
</div>
<div id="dfn-off-policy-learning">
<dl>
<dt>Off-policy learning</dt>
<dd>
<p>learning a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> by sampling from some other policy <img src="https://latex.codecogs.com/png.latex?%5Cpi'"></p>
</dd>
</dl>
</div>
<div id="dfn-prediction">
<dl>
<dt>Prediction</dt>
<dd>
<p>Estimating <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> is called policy evaluation in the DP literature.</p>
<p>We also refer to it as the Prediction problem <sup>1</sup></p>
</dd>
</dl>
</div>
<div id="dfn-return">
<dl>
<dt>Return (<img src="https://latex.codecogs.com/png.latex?G_t">)</dt>
<dd>
<p><img src="https://latex.codecogs.com/png.latex?G_0%20%5Cdoteq%20R_1+%20%5Cgamma%5E1%20R_2%20+%20%5Ccdots+%20%5Cgamma%5En%20R_n"></p>
<p>i.e.&nbsp;the discounted sum of future rewards</p>
</dd>
</dl>
</div>
<div id="dfn-tqabular">
<dl>
<dt>Tabular methods</dt>
<dd>
<p>RL methods for which the action-values can be represented by a table</p>
</dd>
</dl>
</div>
</div>
</div>
<ul>
<li>Sample based methods learning from experience, without having prior knowledge of the underlying MDP model.</li>
<li>We will cover tabular methods in which the action-values can be represented by a table.</li>
</ul>
<section id="lesson-1-introduction-to-monte-carlo-methods" class="level1 page-columns page-full">
<h1>Lesson 1: Introduction to Monte-Carlo Methods</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand how Monte-Carlo can be used to estimate <img src="https://latex.codecogs.com/png.latex?v(s)"> value functions from sampled interaction #</label></li>
<li><label><input type="checkbox" checked="">Identify problems that can be solved using Monte-Carlo methods #</label></li>
<li><label><input type="checkbox" checked="">Use Monte-Carlo prediction to estimate the value function for a given policy. #</label></li>
</ul>
</div>
</div>
<ul>
<li>After completing the introduction we all think that MDPs and DP are the best?</li>
<li>Alas, Martha burst this bubble, introducing some shortcomings of DP, namely they require us to know a model of the dynamics <img src="https://latex.codecogs.com/png.latex?p(s,a%7Cs',r)"> and rewards <img src="https://latex.codecogs.com/png.latex?r"> of the MDP to estimate <img src="https://latex.codecogs.com/png.latex?v(s)"> or <img src="https://latex.codecogs.com/png.latex?q(a)">.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-mc-methods.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>MC methods for Policy evaluation</figcaption>
</figure>
</div></div><p>let us now try to understand how Monte-Carlo can be used to estimate <img src="https://latex.codecogs.com/png.latex?v(s)"> value functions from sampled interaction.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-mc-12-dice.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>12 dice</figcaption>
</figure>
</div></div><div id="exm-dp-dice" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Rolling 12 Dice)</strong></span> &nbsp;</p>
<ul>
<li>Say our MDP requires rolling 12 dice.
<ul>
<li>this is probably intractable to estimate theoretically using DP.</li>
<li>this is likely to be error prone (particularly and constitutionally).</li>
<li>this will be easy to estimate using MC methods</li>
</ul></li>
</ul>
</div>
<ul>
<li>For most MDPs knowing the dynamics and rewards is an unreasonably strong requirement.</li>
<li>If we can treat this like a bandit problem we can try to use the long term averages rewards to estimate value of a state</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-mc-bandit.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>MC bandits</figcaption>
</figure>
</div></div><p>more formally we can use the MC value prediction algorithm.</p>
<hr>
<p>Next we present an algorithm for estimating the value function of a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> using MC methods.</p>
<div id="nte-mc-value-prediction-any-visit" class="alg callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;1: MC prediction any visit for estimating <img src="https://latex.codecogs.com/png.latex?V%20%5Capprox%20v_%5Cpi">
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-prediction-any-visit" class="pseudocode-container quarto-float" data-line-number="true" data-line-number-punc=":" data-caption-prefix="Algorithm" data-pseudocode-number="1" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{MCFirstVisitValuePrediction($\pi$)}\begin{algorithmic} \State Input \State $\qquad \pi$ to be evaluated \State Initialize: \State $\qquad V(s) \leftarrow x \in \mathbb{R}$, arbitrarily, $\forall s \in \mathcal{S}$ \State $\qquad Returns(s)$ an empty list, $\forall s \in \mathcal{S}$ \For {each episode:} \State Generate an episode by following $\pi: S_0, A_0, R_1, S_1, A_1, R_2,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0$ \For {each step of episode, $t = T-1, T-2,\ldots, 0$:} \State $G \leftarrow \gamma G + R_{t+1}$ \State Append $G$ to $Returns(S_t)$ \State $V(S_t) \leftarrow average(Returns(S_t))$ \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<div id="nte-mc-value-prediction-first-visit" class="alg callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;2: MC prediction fist visit for estimating <img src="https://latex.codecogs.com/png.latex?V%20%5Capprox%20v_%5Cpi">
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-prediction-first-visit" class="pseudocode-container quarto-float" data-line-number="true" data-line-number-punc=":" data-caption-prefix="Algorithm" data-pseudocode-number="2" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{MCAnyVisitValuePrediction($\pi$)}\begin{algorithmic} \State Input \State $\qquad \pi$ to be evaluated \State Initialize: \State $\qquad V(s) \in \mathbb{R}$, arbitrarily, $\forall s \in \mathcal{S}$ \State $\qquad Returns(s)$ an empty list, $\forall s \in \mathcal{S}$ \For {each episode:} \State Generate an episode by following $\pi: S_0, A_0, R_1, S_1, A_1, R_2,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0$ \For {each step of episode, $t = T-1, T-2,\ldots, 0$:} \State $G \leftarrow \gamma G + R_{t+1}$ \If{$S_t \not \in S_0, S_1,\ldots,S_{t-1}$} \State Append $G$ to $Returns(S_t)$ \State $V(S_t) \leftarrow average(Returns(S_t))$ \EndIf \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Any visit / First-visit
</div>
</div>
<div class="callout-body-container callout-body">
<p>The book uses presents a small variation called the <em>first visit MC method</em>, We considered the any-visit case. This estimates <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> using the average of the returns following an episode’s first visit to <img src="https://latex.codecogs.com/png.latex?s">, whereas this the every-visit MC algorithms averages the returns following all visits to <img src="https://latex.codecogs.com/png.latex?s"></p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuition for the Algorithm
</div>
</div>
<div class="callout-body-container callout-body">

<p>The main idea is to use the recursive nature of the returns which is embodied in the following formula:</p>
<p><span id="eq-incremental-update-rule"><img src="https://latex.codecogs.com/png.latex?%0ANewEstimate%20%5Cleftarrow%20OldEstimate%20+%20StepSize%5BTarget%20-%20OldEstimate%5D%20%5Cqquad%20%5Ctext%7B(incremental%20update%20rule)%7D%0A%5Ctag%7B1%7D"></span></p>
<p>The key to understanding this algorithm is represented in the following diagram. At the top is a backup diagram for an discounted episode.</p>
<p>Martha explain that the MC uses the recursive nature of the returns to efficiently compute the average returns for each state by starting at the end of the episode and working backwards.</p>
<p>We can see that the returns from a series of equations that can be solved by substitution. Each return is the current reward and the discounted previous return that has been computed.</p>
<p>Thus we can compute all the returns for a state in a single pass through the episode. by solving this series of the full “telescoping” equations.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center callout-margin-content">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-mc-calc.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Efficient returns calculations</figcaption>
</figure>
</div></div><p>this bring us to our second example:</p>
<div id="exm-black-jack-mdp" class="theorem example page-columns page-full">
<p><span class="theorem-title"><strong>Example 2 (Blackjack MDP)</strong></span> &nbsp;</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-bj-example.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Blackjack example</figcaption>
</figure>
</div></div><ul>
<li><strong>Undiscounted</strong> MDP where each game of blackjack corresponds to an episode with
<ul>
<li>Rewards:
<ul>
<li>r= -1 for a loss</li>
<li>r= 0 for a draw</li>
<li>r= 1 for a win</li>
</ul></li>
<li>Actions : <img src="https://latex.codecogs.com/png.latex?a%5Cin%20%5C%7B%5Ctext%7BHit%7D,%20%5Ctext%7BStick%7D%5C%7D"></li>
<li>States S:
<ul>
<li>player has a usable ace (Yes/No) <sup>2</sup></li>
<li>sum of cards (12-21)<sup>3</sup></li>
<li>The card the dealer’s card shows (Ace-10)</li>
</ul></li>
<li>Cards are dealt with replacement<sup>4</sup></li>
<li>Policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">:
<ul>
<li>if sum &lt; 20, stick</li>
<li>otherwise, hit</li>
</ul></li>
</ul></li>
</ul>
</div>
<p>In the programming assignment we will produce the following graphs</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-bj-outcomes.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>Blackjack outcomes</figcaption>
</figure>
</div></div><ul>
<li>In real world settings we typical don’t know theoretical functions like values, action values or rewards. Out best option is to sample reality in trial and error experiment of testing different interventions.</li>
<li>However under certain conditions such samples may be enough to perform the prediction task learn a value function or the action value function .</li>
<li>We can these function to learn better policies from this experience.</li>
<li>A second scenario involves historical samples collected from past interactions. We can use probabilistic methods like MCMC to estimate <img src="https://latex.codecogs.com/png.latex?q(a)">.</li>
</ul>
<p>we can use the MC prediction alg to estimate the expected returns for a state given a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The key limitations of <em>MC value estimation algorithm</em> is its requirement for episodic tasks and for completing such an episode before it starts. In some games an episode can be very long.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="emoji" data-emoji="bulb">💡</span> Is this really so? <span class="emoji" data-emoji="thinking">🤔</span>
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>If we work in the Bayesian paradigm with some prior and use Bayesian updating.</li>
<li>At every step we should have well defined means.</li>
<li>So it seems one can perhaps do sample based on non-episodic tasks</li>
<li>One more idea is to treat n_steps as an episode.</li>
<li>Without episodic we most likely lose the efficient updating. <span class="emoji" data-emoji="thinking">🤔</span></li>
<li>Perhaps we can use the online update rule for the mean.</li>
</ul>
</div>
</div>
<ul>
<li><p><label><input type="checkbox">TODO - try to implement this as an algorithm.</label></p></li>
<li><p>To ensure well-defined average sample returns, we define Monte Carlo methods only on episodic tasks that all eventually terminate - only on termination are value estimates and policies updated.</p></li>
</ul>
<p>Implications of MC Learning</p>
<ul>
<li>We don’t need to keep a large mode of the environment.</li>
<li>We estimate the values of each state independently of other states</li>
<li>Computation for updating values or each state is independent of the size of the MDP<sup>5</sup></li>
</ul>
</section>
<section id="sec-mc-control" class="level1 page-columns page-full">
<h1>Lesson 2: Monte Carlo for Control</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox">Estimate action-value functions using Monte Carlo #</label></li>
<li><label><input type="checkbox">Understand the importance of maintaining exploration in Monte Carlo algorithms #</label></li>
<li><label><input type="checkbox" checked="">Understand how to use Monte Carlo methods to implement a GPI algorithm. #</label></li>
<li><label><input type="checkbox" checked="">Apply Monte Carlo with exploring starts to solve an MDP #</label></li>
</ul>
</div>
</div>
<section id="sec-l2g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g1">MC Action-Value Functions</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-mc-action-values.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>action values</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-mc-backoff.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>back off</figcaption>
</figure>
</div></div>
<p>This back off diagram indicates that the value of a state S depends on the values of its actions.</p>
<ul>
<li>Recall that control is simply improving a policy using our action values estimate.</li>
<li>Policy improvement is done by <strong>Greedyfying</strong> a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> at a state <img src="https://latex.codecogs.com/png.latex?s"> by selecting the action <img src="https://latex.codecogs.com/png.latex?a"> with the highest action value.</li>
<li>If we are missing some action values we can make the policy worse!</li>
<li>We need to ensure that our RL algorithm engages the different actions of a state. There are two strategies:
<ul>
<li>Exploring starts</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-Soft strategies</li>
</ul></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-exploring-starts.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>exploring starts</figcaption>
</figure>
</div></div><p>The following is the MC alg with exploring start for estimation.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-monte-carlo-GPI-01.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>exploring starts pseudocode</figcaption>
</figure>
</div></div><p>Let’s recap how GPI looks:</p>
<ul>
<li>Keeping <img src="https://latex.codecogs.com/png.latex?%5Cpi_0"> fixed we do evaluation of <img src="https://latex.codecogs.com/png.latex?q_%5Cpi"> using MC–ES</li>
<li>We improve <img src="https://latex.codecogs.com/png.latex?%5Cpi_0"> by picking the actions with the highest values</li>
<li>We stop when we don’t improve <img src="https://latex.codecogs.com/png.latex?%5Cpi"></li>
</ul>
<p>Here, in the evaluation step, we estimate the action-values using MC prediction, with exploration driven by exploring Starts or an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-soft policy</p>
</section>
</section>
<section id="lesson-3-exploration-methods-for-monte-carlo" class="level1 page-columns page-full">
<h1>Lesson 3: Exploration Methods for Monte Carlo</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand why Exploring Starts can be problematic in real problems #</label></li>
<li><label><input type="checkbox" checked="">Describe an alternative exploration method for Monte Carlo control #</label></li>
</ul>
</div>
</div>
<p>Next we look into using exploring starts to learn action values using MC sampling.</p>
<p>Recall that we like action values over state values since they allow us to find optimal policies by quickly picking the best action in a state.</p>
<p>Mr White explains that we can’t use a deterministic policy to learn action values since we need to explore multiple actions in a state to pick the best one. To do this with a deterministic policy we use exploring starts - this means we start each simulation with a random state and action then follow the policy. This should eventually allow us to learn the action values for all actions in all states.</p>
<p>So here is how can use exploring starts or some other exploration strategy to ensure that we can learn the action values for all actions in all states.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Monte Carlo Exploring Start
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-es-control" class="pseudocode-container quarto-float" data-line-number="true" data-line-number-punc=":" data-caption-prefix="Algorithm" data-pseudocode-number="3" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{MonteCarloExploringStartsGPI()}\begin{algorithmic} \State Initialize: \State $\qquad \pi(s) \in A(s) \quad \forall s\in \mathcal{S}$ \State $\qquad Q(s,a) \leftarrow x \in \mathbb{R} \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)$ \State $\qquad Returns(s,a) \leftarrow \text {an empty list} \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)$ \For {each episode:} \State Choose $S_0 \in \mathcal{S} and A_0 \in \mathcal{A}(S_0) \text{randomly :} p(s,a)&gt;0 \forall s,a$ \comment{$\textcolor{blue}{Exploring Starts}$} \State Generate an episode e from $S_0, A_0$ by following $\pi: S_0, A_0, R_1,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0$ \For {each step of $e, t \in T-1, T-2,..., 0:} \comment{$\textcolor{blue}{Backward\ pass}$} \State $G \leftarrow \gamma G + R_{t+1}$ \IF {$S_t , A_t \not \in S_0 , A_0 , S_1 , A_1 \ldots , S_{t-1} , A_{t-1}$} \State Append $G$ to $Returns(S_t,A_t)$ \State $Q(S_t,A_t) \leftarrow average(Returns(S_t,A_t))\quad$ \comment{$\textcolor{blue}{\text{MC action-value estimate}}$} \State $\pi(S_t) \leftarrow \arg\max_a Q(S_t,a)\quad$ \comment{$\textcolor{blue}{\text{greedy policy improvement}}$} \EndIf \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Connecting the dots: MC vs DP
</div>
</div>
<div class="callout-body-container callout-body">
<p>Adam White points out how this is so much simpler than the DP methods we learned earlier. We don’t need to solve a set of simultaneous equations. We can just use the MC method to estimate the action values and then use GPI to improve the policy. The only thing we need to do is to ensure that we explore all the actions in all the states.</p>
</div>
</div>
<section id="sec-l3g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g1">Challenges for exploring starts</h3>
<p>Exploring start can be problematic as we may not able to say try all actions on all states.</p>
<ul>
<li>there may be too many states actions to try</li>
<li>testing certain actions in certain states it could be unethical <sup>6</sup> or risky <sup>7</sup></li>
<li>it could cost too much - we need too many experiments.</li>
</ul>
<p>Note: The Blackjack MDP can be improved using Exploring Starts since each initial state can be sampled. Recall there were 200 states.</p>
</section>
<section id="sec-l3g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g2">ϵ-Soft Policies</h3>
<p>AN ALTERNATIVE approach to policy improvement is an generalization of both the <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">- greedy policy and the random uniform, which we first learned in the contexts of the multi-armed bandits problem in the fundamentals course.</p>
<dl>
<dt>ϵ-soft policy</dt>
<dd>
<p>An ϵ-soft policy is one for which in each state, all actions have a probability of at least <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cepsilon%7D%7B%7CA%7C%7D"></p>
</dd>
</dl>
<p>The advantages of using an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-soft policy are: - we get a never ending exploration</p>
<p>However this means we can never reach a deterministic optimal policy - but we can get to a stochastic policy where the best choice is <img src="https://latex.codecogs.com/png.latex?1-%5Cepsilon+%5Cfrac%7B%5Cepsilon%7D%7B%7CA%7C%7D"></p>
<p>We can get to a deterministic policy if we use a decaying <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy policy or if we greedify the policy breaking ties randomly.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MC control with ϵ-soft policies
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-ϵ-soft-control" class="pseudocode-container quarto-float" data-line-number="true" data-line-number-punc=":" data-caption-prefix="Algorithm" data-pseudocode-number="4" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{MonteCarloϵ-SoftControl()}\begin{algorithmic} \State Initialize \State $\qquad \epsilon \in (0,1)$ \Comment{ $\textcolor{blue}{\ algorithm\ parameter}$} \State $\qquad \pi \leftarrow \epsilon$-soft policy \Comment{ $\textcolor{blue}{\ Initialize: policy}$} \State $\qquad Q(s) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S}$ \State $\qquad Returns(s)$ an empty list, $\quad \forall s \in \mathcal{S}$ \For {each episode:} \State Generate an episode by following $\pi: S_0, A_0, R_1,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0$ \For {each step of episode, $t \in T-1, T-2,..., 0$:} \State $G \leftarrow \gamma G + R_{t+1}$ \State Append $G$ to $Returns(S_t,A_t)$ \State $Q(S_t,A_t) \leftarrow average(Returns(S_t,A_t))$ \State $A^* \leftarrow \arg\max_a Q(S_t,a) \qquad \textcolor{blue}{\text{(ties broken arbitrarily)}}$ \For {each $a \in \mathcal{A}$} \If{$a = A^*$} \State $\pi(a \mid S_t) \leftarrow 1 - \epsilon + \frac{\epsilon}{|A(S_t)|}$ \Else \State $\pi(a \mid S_t) \leftarrow \frac{\epsilon}{|A(S_T)|}$ \EndIf \EndFor \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<p>The Highlights indicate modification of the Exploring Starts alg</p>
<ol type="1">
<li>We can start with Uniform-random as its epsilon-soft.</li>
<li>Episode generation uses the current <img src="https://latex.codecogs.com/png.latex?%5Cpi"> (<img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-soft policy) <em>before</em> it is improved.</li>
<li>We drop the first-visit check - this is an every-visit MC algorithm.</li>
<li>The new policy generated in each iteration is <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-greedy w.r.t. the current action-value estimate, which is improved prior.</li>
<li>The optimal <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-soft policy is an <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">-soft policy.</li>
</ol>
</section>
<section id="lesson-4-off-policy-learning-for-prediction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lesson-4-off-policy-learning-for-prediction">Lesson 4: Off-policy learning for prediction</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox">Understand how off-policy learning can help deal with the exploration problem #</label></li>
<li><label><input type="checkbox">Produce examples of target policies and examples of behavior policies. #</label></li>
<li><label><input type="checkbox">Understand importance sampling #</label></li>
<li><label><input type="checkbox">Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution. #</label></li>
<li><label><input type="checkbox">Understand how to use importance sampling to correct returns #</label></li>
<li><label><input type="checkbox">Understand how to modify the Monte Carlo prediction algorithm for off-policy learning. #</label></li>
</ul>
</div>
</div>
<section id="sec-l4g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g1">Off-policy learning</h3>
<ul>
<li>Off-policy learning is a way to learn a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> using samples from another policy <img src="https://latex.codecogs.com/png.latex?%5Cpi'">.</li>
<li>This is useful when we have a policy that is easier to sample from than the policy we want to learn.</li>
<li>A key idea is to correct the returns using importance sampling.</li>
</ul>
<p>For example suppose we can use a rule based model to generate samples of agent state, action and rewards - but we don’t really have an MDP, value function or policy. We could start with a uniform random policy and then use the samples to learn a better policy. However this would require us to interact with the environment and our agents may not be able to do this. In the case of Sugarscape model the agents are not really making decisions, they are following rules.</p>
<p>If we wished to develop agent that learn using RL with different rules on or off and other settings and use those to learn a policy using many samples. One advantage of the Sugarscape model is that it is highly heterogeneous so we get a rich set of samples to work with. A second advantage is that the rule based model can be fast to sample from and we can generate many samples by running it using hyper-parameters optimized test-bed.</p>
<p>So if we have lots of samples we may not need to explore as much initially, but rather learn to exploit the samples we have. Once we learn a near optimal policy for the samples we can use our agent to explore new vistas in our environment.</p>
</section>
<section id="sec-l4g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g2">Target and behavior policies</h3>
<ul>
<li>The target policy is the policy we want to learn.</li>
<li>The behavior policy is the policy we sample from.</li>
</ul>
</section>
<section id="sec-l4g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g3">Importance sampling</h3>
<ul>
<li>Importance sampling is a technique to estimate the expected value of a target distribution using samples from a different distribution.</li>
<li>Why cant we just use the samples from the behavior policy to estimate the target policy?</li>
<li>The answer is that the samples from the behavior policy are biased towards the behavior policy.</li>
<li>In the target policy we may have states that are never visited by the behavior policy.</li>
<li>For example we might want to learn a policy that focuses on trade rather than combat or Vica-versa. This extreme idea of introducing/eliminating some action would significantly change behavioral trajectories. Sample based methods could be able to handle these changes - if we can restrict them to each subset of actions but clearly the expected return of states will be diverge in the long run.</li>
<li>So what we want is someway to correct the returns from the behavior policy to the target policy.</li>
<li>It is used to correct returns from the behavior policy to the target policy.</li>
</ul>
<p>The probability of a trajectory under <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is:</p>
<p><span id="eq-trajectory-probability"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20P(A_t,%20S_%7Bt+1%7D,%20&amp;%20A_%7Bt+1%7D,%20...%20,S_T%20%7C%20S_t,%20A_%7Bt:T-1%7D%20%5Csim%20%5Cpi)%20%5Cnewline%0A%20%20&amp;%20=%20%5Cpi(A_t%7CS_t)p(S_%7Bt+1%7D%7CS_t,%20A_t)%5Cpi(A_%7Bt+1%7D,%20S_%7Bt+1%7D)%20%5Ccdot%5Ccdot%5Ccdot%20p(S_T%7CS_%7BT-1%7D,%20A_%7BT-1%7D)%20%5Cnewline%0A%20%20&amp;%20=%20%5Cprod_%7Bk=t%7D%5E%7BT-1%7D%20%5Cpi(A_k%7CS_k)p(S_%7Bk+1%7D%7CS_k,%20A_k)%0A%5Cend%7Balign*%7D%0A%5Ctag%7B2%7D"></span></p>
</section>
<section id="sec-l4g4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l4g4">Importance sampling ratio</h3>
<p><strong>Definition:</strong> The importance sampling ratio (rho, <img src="https://latex.codecogs.com/png.latex?%5Crho">) is the relative probability of the trajectory under the target vs behavior policy:</p>
<p><span id="eq-importance-sampling"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Crho_%7Bt:T-1%7D%20&amp;%20%5Cdoteq%20%5Cfrac%7B%5Cprod_%7Bk=t%7D%5E%7BT-1%7D%20%5Cpi(A_k%20%5Cmid%20S_k)%20%5Ccancel%7B%20p(S_%7Bk+1%7D%20%5Cmid%20S_k,%20A_k)%7D%7D%7B%5Cprod_%7Bk=t%7D%5E%7BT-1%7D%20b(A_k%20%5Cmid%20S_k)%20%5Ccancel%7B%20p(S_%7Bk+1%7D%20%5Cmid%20S_k,%20A_k)%7D%20%7D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cprod_%7Bk=t%7D%5E%7BT-1%7D%20%5Cfrac%7B%5Cpi(A_k%20%5Cmid%20S_k)%7D%7Bb(A_k%20%5Cmid%20S_k)%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B3%7D"></span></p>
<p><span id="eq-value1"><img src="https://latex.codecogs.com/png.latex?%0Av_%5Cpi(s)%20=%20%5Cmathbb%7BE%7D_b%5B%5Crho_%7Bt:T-1%7D%20%5Ccdot%20G_t%20%5Cmid%20S_t%20=%20s%5D%20%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<p><span id="eq-weighted-importance-sampling"><img src="https://latex.codecogs.com/png.latex?%0AV(s)%20%5Cdoteq%20%5Cfrac%7B%5Cdisplaystyle%20%5Csum_%7Bt%5Cin%20%5Cmathscr%20T(s)%7D%5Crho_%7Bt:T(t)%20-%201%7D%20%5Ccdot%20G_t%7D%7B%7C%5Cmathscr%20T%20(s)%7C%7D%20%5Cqquad%0A%5Ctag%7B5%7D"></span></p>
<p><span id="eq-weighted-importance-sampling2"><img src="https://latex.codecogs.com/png.latex?%0AV(s)%20%5Cdoteq%20%5Cfrac%7B%5Cdisplaystyle%20%5Csum_%7Bt%5Cin%20%5Cmathscr%20T(s)%7D%20%5CBig(%5Crho_%7Bt:T(t)%20-%201%7D%20%5Ccdot%20G_t%5CBig)%7D%7B%5Cdisplaystyle%20%5Csum_%7Bt%5Cin%20%5Cmathscr%20T(s)%7D%5Crho_%7Bt:T(t)%20-%201%7D%7D%20%5Cqquad%0A%5Ctag%7B6%7D"></span></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/wk2-importance-sampling-example.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>importance sampling example</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/wk2-off-policy-trajectories.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>off policy trajectories</figcaption>
</figure>
</div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Off-policy every visit MC prediction
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-off-policy-prediction" class="pseudocode-container quarto-float" data-line-number="true" data-line-number-punc=":" data-caption-prefix="Algorithm" data-pseudocode-number="5" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{OffPolicyMonteCarloPrediction()}\begin{algorithmic} \State Input: \State $\qquad \pi \leftarrow \text{policy to be evaluated}$ \State Initialize: \State $\qquad V(s) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S}$ \State $\qquad Returns(s) \text{ an empty list,} \quad \forall s \in \mathcal{S}$ \For {each episode:} \State Generate an episode by following $\pi: S_0, A_0, R_1,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0, W \leftarrow 1$ \For {each step of episode, $t \in T-1, T-2,..., 0$:} \State $G \leftarrow \gamma WG + R_{t+1}$ \State Append $G$ to $Returns(S_t)$ \State $V(S_t) \leftarrow average(Returns(S_t))$ \State $W \leftarrow W \frac{\pi(A_t|S_t)}{b(A_t|S_t)}$ \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Off-policy every visit MC control
</div>
</div>
<div class="callout-body-container callout-body">
<div id="alg-mc-off-control-prediction" class="pseudocode-container quarto-float" data-line-number="true" data-line-number-punc=":" data-caption-prefix="Algorithm" data-pseudocode-number="6" data-no-end="false" data-comment-delimiter="#" data-indent-size="1.2em">
<div class="pseudocode">
\begin{algorithm} \caption{OffPolicyMonteCarloPrediction()}\begin{algorithmic} \State Input: \State $\qquad \pi \leftarrow \text{policy to be evaluated}$ \State Initialize: \State $\qquad V(s) \leftarrow x \in \mathbb{R} \forall s \in \mathcal{S}$ \State $\qquad Returns(s) \text{ an empty list,} \quad \forall s \in \mathcal{S}$ \For {each episode:} \State Generate an episode by following $b: S_0, A_0, R_1,\ldots, S_{T-1}, A_{T-1}, R_T$ \State $G \leftarrow 0, W \leftarrow 1$ \For {each step of episode, $t = T-1, T-2,\ldots, 0$:} \State $G \leftarrow \gamma WG + R_{t+1}$ \State Append $G$ to $Returns(S_t)$ \State $V(S_t) \leftarrow average(Returns(S_t))$ \State $W \leftarrow W \frac{\pi(A_t \mid S_t)}{b(A_t \mid S_t)}$ \EndFor \EndFor \end{algorithmic} \end{algorithm}
</div>
</div>
</div>
</div>
</section>
<section id="emma-brunskill-batch-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="emma-brunskill-batch-reinforcement-learning">Emma Brunskill: Batch Reinforcement Learning</h3>
<p>These guest talks have a dual purpose:</p>
<ol type="1">
<li>to let the speakers share their passion for the field and introduce us to their research. this can be a good start for reading more about our own interests or for looking how to solve real problems that we are facing.</li>
<li>to show us how the concepts we are learning are being used in the real world.</li>
</ol>
<ul>
<li><a href="https://cs.stanford.edu/people/ebrun/">Emma Brunskill</a> is a professor at Stanford University.</li>
<li>Burnskill motivated her approach with an edutainment app in which the goal is to maximize student engagement in game based on historical data.</li>
<li>In batch RL we have a fixed dataset of samples and we want to learn a policy from this data.</li>
<li>This is useful when we have a fixed dataset of samples and we want to learn a policy from this data.</li>
<li>The key idea is to use importance sampling to correct the returns from the behavior policy to the target policy. We learned that the challenge this poses is primarily due to the bias of the behavior policy.</li>
<li><span class="marked">Importance sampling provides us with an unbiased estimate of the value function yet can have high variance</span>. These may can be exponentially large in the number of steps. So these results in very poor estimates for the value function if there are many steps in the trajectory.</li>
<li>Brunskill suggest that <span class="marked">the real challenge posed by batch RL is a sparsity of trajectories with actions leading to optimal next states under the target policy</span> in the historical data.<sup>8</sup></li>
<li>One point we learned about this is that we should seek algorithms that are more data efficient. However</li>
<li>A send idea is to use parametric models which are biased by can learn the transition dynamics and the reward function more efficiently.</li>
<li>Brunskill points out that since we have few samples we may need a better approach to get robust estimates of the value function.</li>
<li>This approach which comes from statistic is called <a href="">doubly robust stimators</a> and has been used in bandits and RL</li>
<li>She presents a chart from a 2019 paper with a comparison of different methods for RL in the cart-pole environment.
<ul>
<li>Off policy policy gradient with state Distribution Correction - dominates the other methods. And has a significantly narrower confidence interval for the value, if I understand the figure correctly.</li>
</ul></li>
<li>She also presents results from many papers on Generalization Guarantees for RL, which show that we can learn a policy that is close to the optimal policy with a small number of samples from another policy. However I cannot make much sense of the result in the slide.</li>
<li>An example of this is the Sugarscape model where we have a fixed dataset of samples from the rule-based model.</li>
<li>More generally, we can use batch RL to learn from historical data how to make better decisions in the future.</li>
</ul>
<dl>
<dt>Counterfactual</dt>
<dd>
<p>You don’t know what your life would be like if you weren’t reading this right now.</p>
</dd>
</dl>
<ul>
<li>Causal reasoning based on counterfactuals is a key idea to tackling this problem.</li>
</ul>
<dl>
<dt>Counterfactual or Batch Reinforcement Learning</dt>
<dd>
<p>In batch RL we have a fixed dataset of samples and we want to learn a new policy from this data.</p>
</dd>
</dl>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Doubly Robust Estimators
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://en.wikipedia.org/wiki/Inverse_probability_weighting#Interpretation_and_%22double_robustness%22">Doubly robust estimators</a> is a technique from statistics that and causal inference that allows us to combine to do importance sampling and model based learning and a propensity score to estimate the value function. combine the best of both worlds - they are robust to errors in the model and the policy.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%20V_%7BDR%7D%20=%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20%5Cleft%5B%20%5Crho_i%20(R_i%20+%20%5Cgamma%20Q(s_%7Bi+1%7D,%20%5Cpi(s_%7Bi+1%7D)))%20-%20%5Crho_i%20%5Chat%20Q_%7B%5Cpi%7D(s_i,%20a_i)%20+%20%5Chat%20Q_%7B%5Cpi%7D(s_i,%20a_i)%20%5Cright%5D%0A"> where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Crho_i"> is the importance sampling ratio for the <img src="https://latex.codecogs.com/png.latex?i">-th sample</li>
<li><img src="https://latex.codecogs.com/png.latex?R_i"> is the reward - <img src="https://latex.codecogs.com/png.latex?Q(s_%7Bi+1%7D,%20%5Cpi(s_%7Bi+1%7D))"> is the value of the next state under the target policy</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%20Q_%7B%5Cpi%7D(s_i,%20a_i)"> is the model based Q-function estimate</li>
<li><img src="https://latex.codecogs.com/png.latex?Q(s_%7Bi+1%7D,%20%5Cpi(s_%7Bi+1%7D))"> is the value of the next state under the target policy</li>
</ul>
</div>
</div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/ADD4B0bOZi4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>and paper</p>
<ul>
<li><a href="https://arxiv.org/abs/2007.08202">Provably Good Batch Reinforcement Learning Without Great Exploration</a></li>
<li><a href="https://arxiv.org/abs/1604.00923">Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning</a></li>
</ul>


</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><strong>Prediction</strong> in the sense that we want to predict for <img src="https://latex.codecogs.com/png.latex?%5Cpi"> how well it will preforms i.e.&nbsp;its expected returns for a state↩︎</p></li>
<li id="fn2"><p>worth either 1 or 11↩︎</p></li>
<li id="fn3"><p>face card are worth 10↩︎</p></li>
<li id="fn4"><p>this is a big simplifying assumption↩︎</p></li>
<li id="fn5"><p>in DP we had to solve <img src="https://latex.codecogs.com/png.latex?n%5Ctimes%20n"> - simultaneous equations↩︎</p></li>
<li id="fn6"><p>think of a medical trial↩︎</p></li>
<li id="fn7"><p>think of a self driving car↩︎</p></li>
<li id="fn8"><ul>
<li>Can we learn form one or two examples by sampling ?</li>
<li>what if the good actions are never sampled by our algorithm?</li>
</ul>
↩︎</li>
</ol>
</section></div> ]]></description>
  <category>Coursera</category>
  <category>Sample-based Learning Methods</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c2-w1.html</guid>
  <pubDate>Thu, 29 Feb 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Dynamic Programming</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c1-w4.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="week-4-dynamic-programming" class="level1">
<h1>Week 4: Dynamic Programming</h1>
<p>In this week, we learn how to compute value functions and optimal policies, assuming you have the MDP model. You will implement dynamic programming to compute value functions and optimal policies and understand the utility of dynamic programming.</p>
<p>The ‘programming’ in dynamic programming really means solving an optimization problem. We have learned about using the Bellman equations as update rules. Now we look at some basic applications of this idea to solve MDP.</p>
<p>The intuition is pretty simple we have two tasks - one is to decide how good a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is - think <mark>discounted summation of the rewards from the best actions over the <img src="https://latex.codecogs.com/png.latex?s_ta_tr_t"> tree</mark>. This policy evaluation step is named <strong>prediction</strong>, as we don’t really know what the actual rewards of stochastic actions will be, only their expectation. But what we really want is to find near optimal policy which is called ‘control’. We have a strong theoretical result on how to go about this by iteratively improving a policy by picking its the actions with highest value at each steps.</p>
<p>What is surprising at first is that even starting with a uniform random policy we don’t need to explore the tree too deeply in the prediction step to be able to pick better actions. Also we can see from the maze like grid world that we really need to update one or two states every iteration. Which suggest that there is great room for improvement with smarter algorithms.</p>
</section>
<section id="lesson-1-policy-evaluation-prediction" class="level1">
<h1>Lesson 1: Policy Evaluation (Prediction)</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Read
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=72">RL Book§4.1-5,6-7, pp73-88</a></label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand the distinction between <strong>policy evaluation</strong> and <strong>control</strong>. #</label></li>
<li><label><input type="checkbox" checked="">Explain the setting in which dynamic programming can be applied, as well as its limitations. #</label></li>
<li><label><input type="checkbox" checked="">Outline the <strong>iterative policy evaluation algorithm</strong> for estimating state values under a given policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">. #</label></li>
<li><label><input type="checkbox" checked="">Apply iterative policy evaluation to compute value functions. #</label></li>
</ul>
</div>
</div>
<section id="sec-policy-evaluation-control" class="level2">
<h2 class="anchored" data-anchor-id="sec-policy-evaluation-control">Policy Evaluation and Control</h2>
<p>The distinction between policy evaluation and control:</p>
<dl>
<dt>policy evaluation (prediction)</dt>
<dd>
is the task of evaluating the future, i.e.&nbsp;the value function given some specific policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.
</dd>
<dt>control</dt>
<dd>
is the task of finding the optimal policy, given some specific value function <img src="https://latex.codecogs.com/png.latex?v">.
</dd>
<dt>planning</dt>
<dd>
is the task of finding the optimal policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cstar%7D"> and value function <img src="https://latex.codecogs.com/png.latex?v">, given a model of the environment. this is typically done by dynamic programming methods.
</dd>
</dl>
<p>Typically we need to solve the prediction problem before we can solve the control problem. This is because we need to know the value of the states under the policy to be able to pick the best actions.</p>
</section>
<section id="sec-dynamic-programming" class="level2">
<h2 class="anchored" data-anchor-id="sec-dynamic-programming">Dynamic Programming</h2>
<ul>
<li>Dynamic programming is a method for solving complex problems by breaking them down into simpler sub-problems.</li>
<li>It is a general approach to solving problems that can be formulated as a sequence of decisions.</li>
<li>Dynamic programming can be applied to problems that have the following properties:
<ul>
<li>Optimal substructure: The optimal solution to a problem can be obtained by combining the optimal solutions to its sub-problems.</li>
<li>Overlapping sub-problems: The same sub-problems are solved multiple times.</li>
</ul></li>
</ul>
</section>
<section id="sec-iterative-policy-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="sec-iterative-policy-evaluation">Iterative Policy Evaluation Algorithm</h2>
<p>Continuing with our goal of finding the optimal policy, we now turn to the an algorithms that will allow us to predict the value all the state starting with even the most naive policy.</p>
<p>The iterative policy evaluation algorithm is a simple iterative algorithm that estimates the value function for a given policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>We start with no knowledge of the value function or the policy. We set all the values to zero and we may even assume all actions are equally likely and all states are equally good. This is the uniform random policy. Alternatively we can start with some other policy.</p>
<p>These two assumptions are implemented in the initialization step of the algorithm.</p>
<p>The crux of the algorithm is the update step which is based on the recursive bellman equation for the value function under a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Av_%7B%5Cpi%7D(s)%20=%20%5Csum_%7Bs',r%7D%20p(s',r%7Cs,a)%5Br%20+%20%5Cgamma%20V(s')%5D%20%5Csum_%7Ba%7D%20%5Cpi(a%7Cs)%0A"> I rearranged the terms to make it clear that we are iterating over the states we use this equation to update the value of each state using</p>
<ol type="1">
<li>the four part dynamics function <img src="https://latex.codecogs.com/png.latex?p(s',r%7Cs,a)"> to get the probability of receiving a reward <img src="https://latex.codecogs.com/png.latex?r"> at a successor state <img src="https://latex.codecogs.com/png.latex?s'"> given the current state <img src="https://latex.codecogs.com/png.latex?s"> and action <img src="https://latex.codecogs.com/png.latex?a">.</li>
<li>the value of the next state <img src="https://latex.codecogs.com/png.latex?V(s')">. which we initially assumed is 0 and may have already updated</li>
<li>the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%7Cs)"> which we use to weigh the previous term</li>
</ol>
<p>Al this will give us the expected value of the state under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>The final part of the algorithm is the stopping condition. We stop when the change in the value function is less than a small threshold <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<p>The algorithm is guaranteed to converge to the value function for the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>Here is the concise statement of the algorithm with just one array in pseudo code:</p>
<div id="alg-Iterative-Policy-Evaluation" class="pseudocode-container quarto-float" data-line-number="true" data-indent-size="1.2em" data-pseudocode-number="1" data-comment-delimiter="#" data-no-end="false" data-caption-prefix="Algorithm" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{Iterative Policy Evaluation, for estimating $V \approx v_{\pi}$}\begin{algorithmic} \State Input: $\pi$, the policy to be evaluated, default to uniform random policy \State Algorithm parameter: a small threshold $\theta &gt; 0$ determining accuracy of estimation \State $V(s) \leftarrow \leftarrow \vec 0 \forall s \in S$ \REPEAT \STATE $\Delta \leftarrow 0$ \FORALL { $s\in S$} \STATE $v \leftarrow V(s)$ \STATE $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]\quad$ \comment{ Bellman equation} \STATE $\Delta \leftarrow \max(\Delta, |v - V(s)|)$ \ENDFOR \UNTIL{$\Delta &lt; \theta$} \State Output: $V \approx v_{\pi}$ \end{algorithmic} \end{algorithm}
</div>
</div>
<p>note: the algorithm makes a couple of assumptions that are omitted in the pseudo code.</p>
<ol type="1">
<li>that we have access to the dynamics function <img src="https://latex.codecogs.com/png.latex?p(s',r%7Cs,a)"></li>
<li>that we have access to the reward function <img src="https://latex.codecogs.com/png.latex?r(s,a,s')"></li>
</ol>
</section>
<section id="sec-applying-iterative-policy-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="sec-applying-iterative-policy-evaluation">Applying Iterative Policy Evaluation</h2>
<p>The iterative policy evaluation algorithm can be applied to compute the value function for a given policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
</section>
</section>
<section id="lesson-2-policy-iteration-control" class="level1 page-columns page-full">
<h1>Lesson 2: Policy Iteration (Control)</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand the <strong>policy improvement theorem</strong>. #</label></li>
<li><label><input type="checkbox" checked="">Use a value function for a policy to produce a better policy for a given MDP. #</label></li>
<li><label><input type="checkbox" checked="">Outline the <strong>policy iteration algorithm for finding the optimal policy</strong>. #</label></li>
<li><label><input type="checkbox" checked="">Understand <strong>the dance of policy and value</strong>. #</label></li>
<li><label><input type="checkbox" checked="">Apply policy iteration to compute <strong>optimal policies</strong> and optimal <strong>value functions</strong>. #</label></li>
</ul>
</div>
</div>
<section id="sec-l2g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l2g1">Policy Improvement Theorem</h3>
<p>The policy improvement theorem states that given a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and the value function <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D">, we can construct a new policy <img src="https://latex.codecogs.com/png.latex?%5Cpi'"> that is as good as or better than <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
</section>
<section id="sec-l2g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l2g2">Value Function for a Policy</h3>
<p>The value function for a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s"> and following policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> thereafter.</p>
<p>The value function for a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is denoted by <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D(s)">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Av_%7B%5Cpi%7D(s)%20=%20%5Cmathbb%7BE%7D%5BG_t%20%5Cvert%20S_t%20=%20s%5D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?G_t"> is the return at time <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?S_t"> is the state at time <img src="https://latex.codecogs.com/png.latex?t">.</p>
</section>
<section id="sec-l2g3" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l2g3">Policy Iteration Algorithm</h3>
<p>The policy iteration algorithm is a simple iterative algorithm that alternates between policy evaluation and policy improvement.</p>
<p>The algorithm starts with an initial policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and iteratively evaluates the policy to get the value function <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D"> and then improves the policy to get a new policy <img src="https://latex.codecogs.com/png.latex?%5Cpi'">.</p>
<p>The algorithm continues this process until the policy no longer changes, which indicates that the optimal policy has been found.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi_0%20%5Cxrightarrow%7B%5Ctext%7BEvaluation%7D%7D%20v_%7B%5Cpi_0%7D%20%5Cxrightarrow%7B%5Ctext%7BImprovement%7D%7D%20%5Cpi_1%20%5Cxrightarrow%7B%5Ctext%7BEvaluation%7D%7D%20v_%7B%5Cpi_1%7D%20%5Cxrightarrow%7B%5Ctext%7BImprovement%7D%7D%20%5Cldots%20%5Cpi_*%20%5Cxrightarrow%7B%5Ctext%7BEvaluation%7D%7D%20v_%7B%5Cpi_*%7D%0A"></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-iterative-pl-eval1.png" class="img-fluid figure-img"></p>
<figcaption>starting with the uniform random policy</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-iterative-pl-eval2.png" class="img-fluid figure-img"></p>
<figcaption>we iterate to an optimal policy</figcaption>
</figure>
</div></div>
<p>Suppose we have computed for a deterministic policy <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D">, the value function for a deterministic policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>Now when would it be better to prefer some action ? <img src="https://latex.codecogs.com/png.latex?a%20%E2%89%A0%20%5Cpi(s)?"> in some state s?</p>
<p>It is better to switch to action a for state s if and only if: <span id="eq-action-switching-criterion"><img src="https://latex.codecogs.com/png.latex?%0Aq_%7B%5Cpi%7D(s,a)%20%3E%20v_%7B%5Cpi%7D(s)%0A%5Ctag%7B1%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?q_%7B%5Cpi%7D(s,a)"> is the value of taking action a in state s and then following policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>And, we can compute <img src="https://latex.codecogs.com/png.latex?q_%CF%80%20(s,a)"> from <img src="https://latex.codecogs.com/png.latex?v_%CF%80"> by:</p>
<p><span id="eq-action-value-function"><img src="https://latex.codecogs.com/png.latex?%0Aq_%7B%5Cpi%7D(s,a)%20=%20%5Csum_%7Bs',r%7D%20p(s',r%7Cs,a)%5Br%20+%20%5Cgamma%20v_%7B%5Cpi%7D(s')%5D%0A%5Ctag%7B2%7D"></span></p>
<p>this is the the key step the policy improvement step of the policy iteration algorithm.</p>
<div id="alg-Policy-Iteration" class="pseudocode-container quarto-float" data-line-number="true" data-indent-size="1.2em" data-pseudocode-number="2" data-comment-delimiter="#" data-no-end="false" data-caption-prefix="Algorithm" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{Policy Iteration, for estimating $\pi \approx \pi_{\star}$}\begin{algorithmic} \State Input: $\pi$, the policy to be evaluated \State Algorithm parameter: a small threshold $\theta &gt; 0$ determining accuracy of estimation \State Initialize $V(s) \in \mathbb{R}, \quad \pi(s) \in A(s)\ \forall s\in S,\quad V(terminal)= 0$ \State \State {Policy Evaluation} \REPEAT \STATE $\Delta \leftarrow 0$ \FORALL { $s\in S$} \STATE $v \leftarrow V(s)$ \STATE $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]$ \STATE $\Delta \leftarrow \max(\Delta, |v - V(s)|)$ \ENDFOR \UNTIL{$\Delta &lt; \theta$} \State \State {Policy Improvement} \ForAll { $s\in S$} \State old-action $\leftarrow \pi(s)$ \State $\pi(s) \leftarrow \arg\max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]\quad$ \comment{ greedyfication} \If {old-action $\neq \pi(s)$} \State policy-stable $\leftarrow$ false \EndIf \EndFor \If {policy-stable} \Return {$V \approx v_\star,\ \pi \approx \pi_\star$} \Else \State go to Policy Evaluation \EndIf \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
<section id="sec-l2g5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g5">Value Iteration</h2>
<p>Value iteration is an important example of Generalized Policy Iteration. It is an iterative algorithm that computes the optimal value function and the optimal policy for a given MDP but it does not directly referrence a particular policy.</p>
<p>In value iteration, the algorithm starts with an initial estimate of the value function and iteratively runs a single step of greedy polict evaluation per step, using the greedy value to update the state-value function.</p>
<p>updates the value function until it converges to the optimal value function.</p>
<div id="alg-Value-Iteration" class="pseudocode-container quarto-float" data-line-number="true" data-indent-size="1.2em" data-pseudocode-number="3" data-comment-delimiter="#" data-no-end="false" data-caption-prefix="Algorithm" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{Value Iteration, for estimating $\pi \approx \pi_{\star}$}\begin{algorithmic} \State Input: $\pi$, the policy to be evaluated \State Algorithm parameter: a small threshold $\theta &gt; 0$ determining accuracy of estimation \State Initialize $V(s) \leftarrow \vec{0} \forall s \in \mathbb{R}$ \REPEAT \STATE $\Delta \leftarrow 0$ \FORALL { $s\in S$} \STATE $v \leftarrow V(s)$ \STATE $V(s) \leftarrow max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]$ \STATE $\Delta \leftarrow \max(\Delta, |v - V(s)|)$ \ENDFOR \UNTIL{$\Delta &lt; \theta$} \State Output: $V \approx v_{\pi}$ such that \State $\pi(s) = \arg\max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V(s')]$ \end{algorithmic} \end{algorithm}
</div>
</div>
<section id="sec-l2g4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-l2g4">The Dance of Policy and Value</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-dance.png" class="img-fluid figure-img"></p>
<figcaption>Dance of Policy and Value</figcaption>
</figure>
</div></div><p>The policy iteration algorithm is called the dance of policy and value because it alternates between policy evaluation and policy improvement. The policy evaluation step computes the value function for the current policy, and the policy improvement step constructs a new better greedyfied policy based on the value function.</p>
<p>This is also true for other generalized policy iteration algorithms, such as value iteration, which alternates between policy evaluation and policy.</p>
</section>
</section>
</section>
<section id="lesson-3-generalized-policy-iteration" class="level1">
<h1>Lesson 3: Generalized Policy Iteration</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand the framework of <strong>generalized policy iteration</strong>. #</label></li>
<li><label><input type="checkbox" checked="">Outline <strong>value iteration</strong>, an important example of generalized policy iteration. #</label></li>
<li><label><input type="checkbox" checked="">Understand the distinction between <strong>synchronous</strong> and <strong>asynchronous</strong> dynamic programming methods. #</label></li>
<li><label><input type="checkbox" checked="">Describe brute force search as an alternative method for searching for an optimal policy. #</label></li>
<li><label><input type="checkbox" checked="">Describe <strong>Monte Carlo</strong> as an alternative method for learning a value function. #</label></li>
<li><label><input type="checkbox" checked="">Understand the advantage of Dynamic programming and <strong>bootstrapping</strong> over these alternative strategies for finding the optimal policy. #</label></li>
</ul>
</div>
</div>
<section id="sec-l3g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g1">Generalized Policy Iteration</h3>
<p>Generalized policy iteration is a framework for solving reinforcement learning problems that combines policy evaluation and policy improvement in a single loop. The idea is to alternate between evaluating the policy and improving the policy until the policy converges to the optimal policy.</p>
</section>
<section id="sec-l3g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g2">Value Iteration</h3>
<p>Value iteration is an important example of generalized policy iteration. It is an iterative algorithm that computes the optimal value function and the optimal policy for a given MDP. The algorithm starts with an initial estimate of the value function and iteratively updates the value function until it converges to the optimal value function.</p>
</section>
<section id="sec-l3g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g3">Synchronous and Asynchronous Dynamic Programming</h3>
<p>Synchronous dynamic programming methods update all states in the MDP in each iteration, while asynchronous dynamic programming methods update only a subset of states in each iteration. Synchronous dynamic programming methods are typically slower than asynchronous dynamic programming methods, but they are guaranteed to converge to the optimal policy.</p>
</section>
<section id="sec-l3g4" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g4">Brute Force Search</h3>
<p>Brute force search is an alternative method for searching for an optimal policy. It involves exploring all possible policies and selecting the policy that maximizes the expected return. Brute force search is computationally expensive and is not practical for large MDPs.</p>
</section>
<section id="sec-l3g5" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g5">Monte Carlo</h3>
<p>Monte Carlo is an alternative method for learning a value function. It involves estimating the value function by sampling returns from the environment. Monte Carlo is computationally expensive and is not practical for large MDPs.</p>
</section>
<section id="sec-l3g6" class="level3">
<h3 class="anchored" data-anchor-id="sec-l3g6">Advantage of Dynamic Programming</h3>
<p>Dynamic programming and bootstrapping are more efficient than brute force search and Monte Carlo for finding the optimal policy. Dynamic programming and bootstrapping exploit the structure of the MDP to update the value function iteratively, while brute force search and Monte Carlo do not.</p>
</section>
<section id="warren-powell-approximate-dynamic-programming-for-fleet-management" class="level3">
<h3 class="anchored" data-anchor-id="warren-powell-approximate-dynamic-programming-for-fleet-management">Warren Powell: Approximate dynamic programming for fleet management</h3>
<p>In this lecture Warren Powell talks about the application of dynamic programming to fleet management.</p>
<ul>
<li><p>We want to calculate the marginal value of a single driver.</p></li>
<li><p>This is a linear programming problem, solvable by <a href="https://www.gurobi.com/">Gurobi</a> and <a href="https://en.wikipedia.org/wiki/CPLEX">cplux</a>.</p></li>
<li><p>For each driver, we drop them out of the system and calculate the system’s new value.</p></li>
<li><p>The difference in values between the original and driver dropped value is the value of the driver.</p></li>
</ul>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Fundamentals</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c1-w4.html</guid>
  <pubDate>Wed, 04 May 2022 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Value Functions &amp; Bellman Equations</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-rl/posts/coursera/c1-w3.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<p><a href="https://en.wikipedia.org/wiki/Decision_theory">Decision theory</a> is the branch of Mathematics dealing with the analysis of decisions by a single agent. <a href="https://en.wikipedia.org/wiki/Game_theory">Game theory</a> is the branch of Mathematics dealing with the analysis of decisions by multiple agents. The introduction of a second agent makes the problem more complex and introduces the notion of strategic behavior. Decision theory is in many ways a simplification of game theory. In <span class="citation" data-cites="silver2015">[@silver2015]</span>, Dave Silver responded to a question that a simple way of viewing MARL is that each agents are an independent decision maker.</p>
<p>Once the problem is formulated as an MDP, finding the optimal policy is more efficient when using value functions.</p>
<p>This week, we learn the definition of <em>policies</em> and <em>value functions</em>, as well as <em>Bellman equations</em>, which are the key technology behind all the algorithms we will learn.</p>
<p>For someone with a background in game theory, the concept of a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is not new in game theory, we call this a strategy and it is a mapping from states to actions. i.e.&nbsp;an assignment of some action to each state representing the best action that an agent should take in that state.</p>
<p>A second familiar concept is the value function. In game theory, we call this the payoff for an action. The payoffs are typically assigned to the terminal states of the game and can be backpropagated to non-terminal states using the laws of probability. Here we are interested in the expected value of the rewards that an agent can expect to receive when following a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> from a given state <img src="https://latex.codecogs.com/png.latex?s">.</p>
<p>I found the Policy and values functions somewhat families due to some background in game theory and markov processes.</p>
<p>I found the Bellman equations more of a challenge. I think the main issue is the unfamiliarity with the notation which make the material look like gibberish. However, the more I made myself more familiar with the notation, I came to see that these equations express a rather simple idea.</p>
<p>We describe a MDP as a linear process in time. However, it is really a tree of possible actions. What the Bellman equations express is that if we want to estimate the value <img src="https://latex.codecogs.com/png.latex?v_%5Cpi(s)"> of a state or more specifically the value of an action <img src="https://latex.codecogs.com/png.latex?q_%5Cpi(s,a)"> what we do is consider the immediate rewards and then we have have a copy of pretty much the same tree. As we move forward in time we will end up making ever smaller (discounted) corrections to our best assessment.</p>
<section id="lesson-1-policies-and-value-functions" class="level1 page-columns page-full">
<h1>Lesson 1: Policies and Value Functions</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Read
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=58">@sutton2018reinforcement§3.5-7, pp.&nbsp;58-67</a></label></li>
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=68">@sutton2018reinforcement§3.8, pp.&nbsp;68-69</a></label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Recognize that a policy is a distribution over actions for each possible state #</label></li>
<li><label><input type="checkbox" checked="">Describe the similarities and differences between stochastic and deterministic policies #</label></li>
<li><label><input type="checkbox" checked="">Identify the characteristics of a well-defined policy #</label></li>
<li><label><input type="checkbox" checked="">Generate examples of valid policies for a given MDP #</label></li>
<li><label><input type="checkbox" checked="">Describe the roles of state-value and action-value functions in reinforcement learning #</label></li>
<li><label><input type="checkbox" checked="">Describe the relationship between value functions and policies #</label></li>
<li><label><input type="checkbox" checked="">Create examples of valid value functions for a given MDP #</label></li>
</ul>
</div>
</div>
<div class="cell">
<details class="code-fold hidden">
<summary>Code</summary>
<div class="sourceCode cell-code hidden" id="cb1" data-startfrom="66" data-source-offset="0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 65;"><span id="cb1-66">dotStyles <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ({ <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">defaults</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span></span>
<span id="cb1-67"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  graph [</span></span>
<span id="cb1-68"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    labelloc = "b",</span></span>
<span id="cb1-69"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontname="Times",</span></span>
<span id="cb1-70"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontsize=12,</span></span>
<span id="cb1-71"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    overlap=false ];</span></span>
<span id="cb1-72"></span>
<span id="cb1-73"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  node [</span></span>
<span id="cb1-74"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    style=filled,</span></span>
<span id="cb1-75"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    shape=circle,</span></span>
<span id="cb1-76"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    colorscheme=accent5,</span></span>
<span id="cb1-77"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    color=1,</span></span>
<span id="cb1-78"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontname="Times",</span></span>
<span id="cb1-79"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontsize=12];</span></span>
<span id="cb1-80"></span>
<span id="cb1-81"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  edge [</span></span>
<span id="cb1-82"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontname="Times",</span></span>
<span id="cb1-83"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    fontsize=12,</span></span>
<span id="cb1-84"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    margin=2,</span></span>
<span id="cb1-85"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    overlap=false ];</span></span>
<span id="cb1-86"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span> })</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="ojs-cell-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell">
<details class="code-fold hidden">
<summary>Code</summary>
<div class="sourceCode cell-code hidden" id="cb2" data-startfrom="93" data-source-offset="0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 92;"><span id="cb2-93">{ </span>
<span id="cb2-94">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">flatten</span>(ll) {</span>
<span id="cb2-95">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> ll<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">reduce</span>((acc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> l) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">...</span>acc]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">concat</span>(l)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> [])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-96">  }</span>
<span id="cb2-97">  </span>
<span id="cb2-98">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> numActions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-99">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> numNextStates <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-100">  </span>
<span id="cb2-101">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> actions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> numActions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-102">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> nextStates <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">flatten</span>(actions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> numNextStates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`s</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}${</span>j<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span>)))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-103">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> nextStateTransitions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">flatten</span>(actions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> d3<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> numNextStates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`a</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;"> -&gt; s</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}${</span>j<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  [label="p, r"]`</span>)))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-104">    </span>
<span id="cb2-105">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dot</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span></span>
<span id="cb2-106"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">digraph G {</span></span>
<span id="cb2-107"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>dotStyles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">defaults</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb2-108"></span>
<span id="cb2-109"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph root {</span></span>
<span id="cb2-110"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb2-111"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=1 ];</span></span>
<span id="cb2-112"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s ;</span></span>
<span id="cb2-113"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb2-114"></span>
<span id="cb2-115"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph env1 {</span></span>
<span id="cb2-116"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb2-117"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=2 ];</span></span>
<span id="cb2-118"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>actions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`a</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;"> [label="s, a</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">"]`</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">join</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">';</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-119"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb2-120"></span>
<span id="cb2-121"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph sp {</span></span>
<span id="cb2-122"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb2-123"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=1, label="s'" ];</span></span>
<span id="cb2-124"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>nextStates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">join</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'; '</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-125"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb2-126"></span>
<span id="cb2-127"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>actions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">map</span>(a <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">=&gt;</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`s -&gt; a</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>a<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;"> [label="&amp;pi;"];`</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">join</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb2-128"></span>
<span id="cb2-129"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>nextStateTransitions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">join</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb2-130"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">}`</span>     </span>
<span id="cb2-131">}</span></code></pre></div>
</details>
<div id="fig-ojs-cell-2" class="cell-output cell-output-display quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ojs-cell-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="ojs-cell-2" data-nodetype="expression">

</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ojs-cell-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Backup diagram for the V(s) - state value function
</figcaption>
</figure>
</div>
</div>
<div class="cell">
<details class="code-fold hidden">
<summary>Code</summary>
<div class="sourceCode cell-code hidden" id="cb3" data-startfrom="139" data-source-offset="0" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 138;"><span id="cb3-139"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dot</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span></span>
<span id="cb3-140"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">digraph G {</span></span>
<span id="cb3-141"></span>
<span id="cb3-142"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">${</span>dotStyles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">defaults</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb3-143"></span>
<span id="cb3-144"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph root {</span></span>
<span id="cb3-145"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb3-146"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=2,label="s, a" ];</span></span>
<span id="cb3-147"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa ;</span></span>
<span id="cb3-148"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  } </span></span>
<span id="cb3-149"></span>
<span id="cb3-150"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph s1 {</span></span>
<span id="cb3-151"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb3-152"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=1, label="s'"];</span></span>
<span id="cb3-153"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s1;</span></span>
<span id="cb3-154"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s2;</span></span>
<span id="cb3-155"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s3;</span></span>
<span id="cb3-156"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb3-157"></span>
<span id="cb3-158"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  subgraph sp {</span></span>
<span id="cb3-159"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank=same;</span></span>
<span id="cb3-160"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    node [ color=2, label="s', a'" ];</span></span>
<span id="cb3-161"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa11; </span></span>
<span id="cb3-162"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa12; </span></span>
<span id="cb3-163"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa21; </span></span>
<span id="cb3-164"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa22; </span></span>
<span id="cb3-165"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa31; </span></span>
<span id="cb3-166"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa32;</span></span>
<span id="cb3-167"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">  } </span></span>
<span id="cb3-168"></span>
<span id="cb3-169"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    sa -&gt; s1, s2, s3 [label="p, r"];</span></span>
<span id="cb3-170"></span>
<span id="cb3-171"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s1 -&gt; sa11, sa12 [label="&amp;pi;"];</span></span>
<span id="cb3-172"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s2 -&gt; sa21, sa22 [label="&amp;pi;"];</span></span>
<span id="cb3-173"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">    s3 -&gt; sa31, sa32 [label="&amp;pi;"];</span></span>
<span id="cb3-174"><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">}`</span></span></code></pre></div>
</details>
<div id="fig-ojs-cell-3" class="cell-output cell-output-display quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ojs-cell-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="ojs-cell-3" data-nodetype="expression">

</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ojs-cell-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Backup diagram for the Q(a,s) action value function
</figcaption>
</figure>
</div>
</div>
<section id="sec-policy-definition" class="level2">
<h2 class="anchored" data-anchor-id="sec-policy-definition">Policy Definition</h2>
<ul>
<li>A policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a distribution over actions for each possible state.</li>
<li>It is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%7Cs)">, which is the probability of taking action <img src="https://latex.codecogs.com/png.latex?a"> in state <img src="https://latex.codecogs.com/png.latex?s"> under policy <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</li>
</ul>
</section>
<section id="sec-stochastic-vs-deterministic" class="level2">
<h2 class="anchored" data-anchor-id="sec-stochastic-vs-deterministic">Stochastic vs Deterministic Policies</h2>
<ul>
<li>A policy can be deterministic or stochastic.</li>
<li>A deterministic policy is a policy that selects a single action in each state.
<ul>
<li>For example, the greedy policy selects the action with the highest value</li>
</ul></li>
<li>A stochastic policy is a policy that selects actions with some probability that can be conditioned on the state.
<ul>
<li>For example the uniform policy selects each action with equal probability.</li>
</ul></li>
</ul>
</section>
<section id="sec-value-functions" class="level2">
<h2 class="anchored" data-anchor-id="sec-value-functions">Value Functions</h2>
<ul>
<li>We generally want to evaluate the value of each state or better yet the value of each action in each state before we create the policy. To do this we define two types of value functions:</li>
</ul>
<section id="sec-state-value-functions" class="level3">
<h3 class="anchored" data-anchor-id="sec-state-value-functions">State-value functions <img src="https://latex.codecogs.com/png.latex?V_%5Cpi"></h3>
<p>The state-value function <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi%7D(s)"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s"> and following policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> thereafter.</p>
<p><span id="eq-state-value-function"><img src="https://latex.codecogs.com/png.latex?%0Av_%5Cpi(s)%20%5Cdot%20=%20%5Cmathbb%7BE_%5Cpi%7D%5BG_t%7CS_t%20=%20s%5D%20%5Cquad%20%5Ctext%7Bfor%20policy%7D%20%5Cquad%20%5Cpi%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
</section>
<section id="sec-action-value-functions" class="level3">
<h3 class="anchored" data-anchor-id="sec-action-value-functions">Action-value functions <img src="https://latex.codecogs.com/png.latex?Q_%5Cpi"></h3>
<p>The action-value function <img src="https://latex.codecogs.com/png.latex?q_%7B%5Cpi%7D(s,a)"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s">, taking action <img src="https://latex.codecogs.com/png.latex?a">, and following policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> thereafter.</p>
<p><span id="eq-action-value-function"><img src="https://latex.codecogs.com/png.latex?%0Aq_%5Cpi(s,a)%20%5Cdot%20=%20%5Cmathbb%7BE_%5Cpi%7D%5BG_t%7CS_t%20=%20s,%20A_t%20=%20a%5D%20%5Cquad%20%5Ctext%7Bfor%20policy%7D%20%5Cquad%20%5Cpi%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>
</section>
<section id="relationship-between-value-functions-and-policies" class="level3">
<h3 class="anchored" data-anchor-id="relationship-between-value-functions-and-policies">Relationship between Value Functions and Policies</h3>
<p>In the short term, the value functions are more useful than the return G</p>
<ul>
<li>The return G is not immediately available</li>
<li>The return G can be non-deterministic.</li>
</ul>
<p>The value functions are deterministic and can be computed from the MDP.</p>
</section>
</section>
<section id="lesson-2-bellman-equations" class="level2">
<h2 class="anchored" data-anchor-id="lesson-2-bellman-equations">Lesson 2: Bellman Equations</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Derive the Bellman equation for state-value functions #</label></li>
<li><label><input type="checkbox" checked="">Derive the Bellman equation for action-value functions #</label></li>
<li><label><input type="checkbox" checked="">Understand how Bellman equations relate current and future values #</label></li>
<li><label><input type="checkbox" checked="">Use the Bellman equations to compute value functions the state value function is <img src="https://latex.codecogs.com/png.latex?v(s)"> #</label></li>
</ul>
</div>
</div>
</section>
<section id="sec-bellman-equation-state-value-functions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-bellman-equation-state-value-functions">Bellman Equation for State-Value Functions</h2>

<div class="no-row-height column-margin column-container"><div id="fig-backup-v" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-backup-v-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-backup-v.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-backup-v-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: backup diagram for <img src="https://latex.codecogs.com/png.latex?v_%5Cpi">
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bellman Equation intuition
</div>
</div>
<div class="callout-body-container callout-body">
<p>Richard Bellman was a uniquely gifted mathematician who worked on dynamic programming. The Bellman equations is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state. These equations form the basis of Dynamic Programming which is used in disparate problems including</p>
<ul>
<li>Schedule optimization</li>
<li>String algorithms (e.g.&nbsp;sequence alignment)</li>
<li>Graph algorithms (e.g.&nbsp;the shortest path problem)</li>
<li>Graphical algorithms (e.g.&nbsp;the Vitrebi algorithm)</li>
<li>Bioinformatics (e.g.&nbsp;lattice models)</li>
</ul>
<p>Although Bellman was one of the greatest problem solvers of the 20th century, he was not a great communicator. He was known for his terse and cryptic writing. The Bellman equations are a case in point. They are simple to understand once you get the hang of them but they are not easy to read for the first time. However the key to understanding the Bellman equations is to understand that they are a recursive equation based on some physical process</p>
<blockquote class="blockquote">
<p>The trick that one learns over time, a basic part of mathematical methodology, is to sidestep the equation and focus instead on the structure of the underlying physical process – Richard Bellman</p>
</blockquote>
<p>in the case of RL the recursive physical process is: <span id="eq-bellman-equation-intuition"><img src="https://latex.codecogs.com/png.latex?%0A%20%20S%20%5Crightarrow%20A%20%5Crightarrow%20R.%0A%5Ctag%7B3%7D"></span></p>
<p>and we can diagram it using a backup diagram as shown in the Figure&nbsp;3 above.</p>
<p>The name backup diagram comes from the idea that we are backing up the value of the state <img src="https://latex.codecogs.com/png.latex?v(s)"> from the successor state <img src="https://latex.codecogs.com/png.latex?v(s')">. I.e. we are going back up the tree of possible effects of some action <img src="https://latex.codecogs.com/png.latex?a"> starting from the state <img src="https://latex.codecogs.com/png.latex?s">.</p>
<p>While the Bellman equation are difficult to read, remember and to derive, the backup diagram are very easy to sketch even if you don’t remember the equations. Once you have sketch the backup diagram, you should be able to easily derive the Bellman equations.</p>
<p>This same intuition can be used for working through all the above dynamic programming algorithms!</p>
</div>
</div>
<p>The Bellman equation for state-value functions is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state.</p>
<p><span id="eq-bellman-state-value-function"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20v_%5Cpi(s)%20&amp;=%20%5Cmathbb%7BE_%5Cpi%7D%5BG_t%7CS_t=s%5D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Cmathbb%7BE_%5Cpi%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20G_%7Bt+1%7D%7CS_t=s%5D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Cmathbb%7BE_%5Cpi%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20v_%5Cpi(S_%7Bt+1%7D)%7CS_t=s%5D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csum_a%20%5Cpi(a%7Cs)%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20%5Cmathbb%7BE_%5Cpi%7D%5BG_%7Bt+1%7D%7CS_%7Bt+1%7D=s'%5D)%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csum_a%20%5Cpi(a%7Cs)%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20v_%5Cpi(s'))%0A%5Cend%7Balign%7D%0A%5Ctag%7B4%7D"></span></p>
</section>
<section id="sec-bellman-equation-action-value-functions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-bellman-equation-action-value-functions">Bellman Equation for Action-Value Functions</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-backup-q.png" class="img-fluid figure-img"></p>
<figcaption>backup diagram for q(s,a) function</figcaption>
</figure>
</div></div><p>The Bellman equation for action-value functions is a recursive equation that decomposes the value of a state-action pair into the immediate reward and the discounted value of the successor state-action pair.</p>
<p><span id="eq-bellman-action-value-function"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20q_%5Cpi(s,a)%20&amp;%20%5Cdot%20=%20%5Cmathbb%7BE_%5Cpi%7D%5BG_t%7CS_t=s,%20A_t=a%5D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20%5Cmathbb%7BE_%5Cpi%7D%5BG_%7Bt+1%7D%7CS_%7Bt+1%7D=s'%5D)%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20%5Csum_%7Ba'%7D%20%5Cpi(a'%7Cs')%20%5Cmathbb%7BE_%5Cpi%7D%5BG_%7Bt+1%7D%7CS_%7Bt+1%7D=s',%20A_%7Bt+1%7D=a'%5D)%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20%5Csum_%7Ba'%7D%20%5Cpi(a'%7Cs')%20q_%5Cpi(s',a'))%0A%5Cend%7Balign%7D%0A%5Ctag%7B5%7D"></span></p>
</section>
<section id="bellman-equations" class="level2">
<h2 class="anchored" data-anchor-id="bellman-equations">Bellman Equations</h2>
<p>the bellman equations capture the relationship between the current value and the future value. The Bellman equations are a set of equations that express the relationship between the value of a state and the value of its successor states. The Bellman equations are used to compute the value functions of a Markov Decision Process (MDP).</p>
</section>
<section id="example-gridworld" class="level2">
<h2 class="anchored" data-anchor-id="example-gridworld">Example: Gridworld</h2>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td>A</td>
<td>B</td>
</tr>
<tr class="even">
<td>C</td>
<td>D</td>
</tr>
</tbody>
</table>
<p>In the 2x2 gridworld example, the agent can move up, down, left, or right. The agent receives a reward of 0 for each step taken unless it gets to location B for which it gets +5. The agent receives will return to the current cell if it bumping into the wall.</p>
<p>We will use the uniform random policy where the agent selects each action with equal probability.</p>
<p>gamma = 0.7</p>
<p>lets calculate the value of each state using the Bellman equation.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Av_%5Cpi(A)%20&amp;=%20%5Csum_a%20%5Cpi(a%7CA)%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7CA,a)%20(r%20+%20%5Cgamma%20v(s'))%20%5Cnewline%0A%20%20%20%20%20&amp;=%20%5Csum_a%20%5Cpi(a%7CA)%20(r%20+%200.7%20v_%5Cpi(s'))%20%5Cnewline%0A%20%20%20%20%20&amp;=%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(A)%20+%200.25%20%5Ctimes%20(5%20+%200.7%20%5Ctimes%20v(B))%20+%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(C)%20+%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(A)%20%5Cnewline%0Av_%5Cpi(B)%20&amp;=%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(A)%20+%200.5%20%5Ctimes%20(5%20+%200.7%20%5Ctimes%20v(B))%20+%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(D)%20%5Cnewline%0Av_%5Cpi(C)%20&amp;=%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(A)%20+%200.5%20%5Ctimes%20(0.7%20%5Ctimes%20v(B))%20+%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(C)%20%5Cnewline%0Av_%5Cpi(D)%20&amp;=%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(B)%20+%200.5%20%5Ctimes%200.7%20%5Ctimes%20v(C)%20+%200.25%20%5Ctimes%200.7%20%5Ctimes%20v(D)%0A%5Cend%7Balign%7D%0A"> we can solve these equations to get the value of each state.</p>
<p>theses are</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Av_%5Cpi(A)%20&amp;=%204.2%20%5Cnewline%0Av_%5Cpi(B)%20&amp;=%206.1%20%5Cnewline%0Av_%5Cpi(C)%20&amp;=%202.2%20%5Cnewline%0Av_%5Cpi(D)%20&amp;=%204.2%20%5Cnewline%0A%5Cend%7Balign*%7D%0A"></p>
<p>We can use the Bellman equation to calculate the value of each state in the Gridworld. The value of each state is the expected return when starting in that state and following the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> thereafter. The value of each state is calculated by summing the immediate reward and the discounted value of the successor states.</p>
<p>For larger MDP the Bellman equations are not practical method to calculate the value of each state. Instead, we will use algorithms based on the Bellman equations to estimate the value of each state.</p>
<section id="lesson-3-optimality-optimal-policies-value-functions" class="level3">
<h3 class="anchored" data-anchor-id="lesson-3-optimality-optimal-policies-value-functions">Lesson 3: Optimality (Optimal Policies &amp; Value Functions)</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Define an optimal policy #</label></li>
<li><label><input type="checkbox" checked="">Understand how a policy can be at least as good as every other policy in every state. #</label></li>
<li><label><input type="checkbox" checked="">Identify an optimal policy for given MDPs.</label></li>
<li><label><input type="checkbox" checked="">Derive the Bellman optimality equation for state-value functions</label></li>
<li><label><input type="checkbox" checked="">Derive the Bellman optimality equation for action-value functions</label></li>
<li><label><input type="checkbox" checked="">Understand how the Bellman optimality equations relate to the previously introduced Bellman equations</label></li>
<li><label><input type="checkbox" checked="">Understand the connection between the optimal value function and optimal policies</label></li>
<li><label><input type="checkbox" checked="">Verify the optimal value function for given MDPs</label></li>
</ul>
</div>
</div>
</section>
</section>
<section id="sec-optimal-policy" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-optimal-policy">Optimal Policy</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-optimal-policy.png" class="img-fluid figure-img"></p>
<figcaption>Bellman Optimality Equation</figcaption>
</figure>
</div></div><ul>
<li>A policy <img src="https://latex.codecogs.com/png.latex?pi_1"> is better than a policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_2"> if <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi_1%7D(s)%20%5Cgeq%20v_%7B%5Cpi_2%7D(s)"> for all states <img src="https://latex.codecogs.com/png.latex?s">.</li>
<li>Given any two policies <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi'">, if we pick the action that maximizes the value function, from either at every state, we will get a new policy that is at least as good as both <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi'">.</li>
<li>There is always at least one deterministic optimal policy for any MDP.</li>
<li>An optimal policy <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E*"> is a policy that is at least as good as every other policy in every state.</li>
<li>The optimal policy is denoted by <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E*"> and is defined as:</li>
</ul>
<p><span id="eq-optimal-policy"><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%5E*%20%5Cdot%20=%20%5Carg%20%5Cmax_%7B%5Cpi%7D%20v_%7B%5Cpi%7D(s)%20%5Cquad%20%5Cforall%20s%5Cin%20S%0A%5Ctag%7B6%7D"></span></p>
</section>
<section id="sec-bellman-optimality-state-value-function" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-bellman-optimality-state-value-function">Bellman Optimality Equation for State-Value Functions</h2>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-backup-v-star.png" class="img-fluid"></div></div>
<p>The Bellman optimality equation for state-value functions is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state under the optimal policy.</p>
<p><span id="eq-bellman-optimality-state-value-function"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Av_*(s)%20&amp;%20%5Cdot%20=%20%5Cmax_%7B%5Cpi%7D%20v_%7B%5Cpi%7D(s)%20%5Cquad%20%5Cforall%20s%20%5Cin%20S%5Cnewline%0A%20%20%20%20%20%20%20&amp;%20=%20%5Cmax_a%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20v_*(S_%7Bt+1%7D)%7CS_t=s,%20A_t=a%5D%20%5Cnewline%0A%20%20%20%20%20%20%20&amp;%20=%20%5Cmax_a%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20v_*(s'))%0A%5Cend%7Balign%7D%0A%5Ctag%7B7%7D"></span></p>
</section>
<section id="sec-bellman-optimality-action-value-function" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-bellman-optimality-action-value-function">Bellman Optimality Equation for Action-Value Functions</h2>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/notes-rl/posts/coursera/img/rl-backup-q-star.png" class="img-fluid"></div></div>
<p>The Bellman optimality equation for action-value functions is a recursive equation that decomposes the value of a state-action pair into the immediate reward and the discounted value of the successor state-action pair under the optimal policy.</p>
<p><span id="eq-bellman-optimality-action-value-function"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Aq_*(s,a)%20&amp;%20%5Cdot%20=%20%5Cmax_%7B%5Cpi%7D%20q_%7B%5Cpi%7D(s,a)%20%5Cquad%20%5Cforall%20s%20%5Cin%20S,%20%5Cforall%20a%20%5Cin%20A%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Cmathbb%7BE%7D%5BR_%7Bt+1%7D%20+%20%5Cgamma%20%5Cmax_%7Ba'%7D%20q_*(S_%7Bt+1%7D,%20a')%7CS_t=s,%20A_t=a%5D%20%5Cnewline%0A%20%20%20%20%20%20%20%20%20&amp;%20=%20%5Csum_%7Bs'%7D%20%5Csum_r%20p(s',r%7Cs,a)%20(r%20+%20%5Cgamma%20%5Cmax_%7Ba'%7D%20q_*(s',a'))%0A%5Cend%7Balign%7D%0A%5Ctag%7B8%7D"></span></p>
<p>Martha White asks the question: “How can <img src="https://latex.codecogs.com/png.latex?%5CPi_3"> have better strictly better values than both <img src="https://latex.codecogs.com/png.latex?%5CPi_1"> and <img src="https://latex.codecogs.com/png.latex?%5CPi_2"> in all states if all we did is take the best action in each state from either <img src="https://latex.codecogs.com/png.latex?%5CPi_1"> or <img src="https://latex.codecogs.com/png.latex?%5CPi_2">?”</p>
<p>This is because if for example we found a fast path through a bottleneck for any state that is before the bottleneck will have a higher value in the other policies which may have had longer paths through the bottleneck.</p>
</section>
<section id="sec-optimal-value-functions" class="level2">
<h2 class="anchored" data-anchor-id="sec-optimal-value-functions">Optimal Value Functions</h2>
<ul>
<li>The optimal value function <img src="https://latex.codecogs.com/png.latex?v_*(s)"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s"> and following the optimal policy thereafter.</li>
<li>The optimal action-value function <img src="https://latex.codecogs.com/png.latex?q_*(s,a)"> is the expected return when starting in state <img src="https://latex.codecogs.com/png.latex?s">, taking action <img src="https://latex.codecogs.com/png.latex?a">, and following the optimal policy thereafter.</li>
<li>An optimal policy can be obtained from the optimal action-value function by selecting the action with the highest value.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Coursera</category>
  <category>Fundamentals</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/notes-rl/posts/coursera/c1-w3.html</guid>
  <pubDate>Tue, 03 May 2022 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-rl/posts/coursera/img/nlp-brain-wordcloud.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
