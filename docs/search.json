[
  {
    "objectID": "posts/coursera/c1-w2.html",
    "href": "posts/coursera/c1-w2.html",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/coursera/c1-w2.html#sec-Markov-Process",
    "href": "posts/coursera/c1-w2.html#sec-Markov-Process",
    "title": "Markov Decision Processes",
    "section": "Markov Process",
    "text": "Markov Process\nSilver goes into some detail on what we mean by state in RL:\n\nIn th abstract state can be any function of the history.\nThe state should summarize the information on the previous actions, and rewards.\nHe points out that the history in RL can be very long, for Atari games it can include actions plus all the pixels for every screen in many plays of the game. In contrast the state tries to capture the bare essentials of the history for decision making at each time step. For Atari games they used the last 4 screens as the state.\nA second point is that there is always a state. The full history is also a state, but not a very useful one. The internal representation of the ram in the Atari game is also a state, much smaller but this is the representation used by the environment and contains more information than is available to the agent. Ideally the agent would want to model this state, but again it contains lots more information than is available would need to male a decision.\nAnother useful property of the state is that it should have the Markov Property for a state space which is when the future is independent of the past given the present.\n\nA state S_t is Markov if and only if:\n\n\\mathbb{P}[S_{t+1}  \\vert  S_{t}] =  \\mathbb{P}[S_{t+1}  \\vert  S_1,...,S_t] \\qquad \\text{(Markov Property)}\n\\tag{1}\nThe state captures all relevant information from the history Once the state is known, the history may be thrown away i.e. The state is a sufficient statistic of the future\nRecall:\n\na statistic satisfies the criterion for sufficency when no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter”. — [@doi:10.1098/rsta.1922.0009]\n\nFor a Markov state s and successor state s', the state transition probability is defined by:\n\n\\mathbb{P}_{ss'} = \\mathbb{P}[S_{t+1}=s'  \\vert  S_t=s]\n\n\n\n\n\n\n\nMarkov Process or Chain Definion\n\n\n\n\nA Markov Process is\n\na tuple ⟨S,P⟩\n\n\nwhere:\n\nS is a (finite) set of states\nP is a state transition probability matrix, P_{ss'} = P[S_{t+1} = s'  \\vert S_t=s] State transition matrix P_{ss'} defines transition probabilities from all states s to all successor states s',\n\n\n\\begin{align*}\n  P=\\left( \\begin{array}{cc}\n      p_{11} & \\cdots & p_{1n} \\newline\n      \\vdots & \\ddots & \\vdots \\newline\n      p_{n1} & \\cdots & p_{nn} \\end{array} \\right)\n\\end{align*}\n\\tag{2}"
  },
  {
    "objectID": "posts/coursera/c1-w2.html#sec-MRP",
    "href": "posts/coursera/c1-w2.html#sec-MRP",
    "title": "Markov Decision Processes",
    "section": "Markov Reward Process",
    "text": "Markov Reward Process\nA Markov Reward Process (MRP) is a Markov chain with values.\n\n\n\n\n\n\nMarkov Reward Process Definition\n\n\n\n\nA Markov Reward Process is\n\na tuple ⟨S, P, R, \\gamma⟩\n\n\nwhere:\n\nS is a finite set of states\nP is a state transition probability matrix, where P_{ss'} =  \\mathbb{P}[S_{t+1} = s' \\vert  S_t = s]\nR is a reward function, R_s = \\mathbb{E}[R_{t+1} \\vert S_t = s]\n\\gamma is a discount factor, \\gamma \\in [0, 1]\n\n\n\n\n\n\n\n\n\nthe return definition\n\n\n\n\nThe return G_t\n\nis the total discounted reward from time-step t.\n\n\n\nG_t =R_{t+1}+\\gamma R_{t+2}+...=\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\tag{3}\nwhere:\n\nR_t is the reward at time-step t\n\\gamma the discount factor \\gamma \\in [0, 1] is the present value of future rewards.\nThe value of receiving reward R after k+1 time-steps is \\gamma^k R\nThis values immediate reward above delayed reward.\n\n\\gamma = 0 makes the agent short-sighted.\n\\gamma = 1 makes the agent far-sighted.\n\n\n\n\n\n\n\n\n\n\nThe value function\n\n\n\n\nThe value function:\n\nThe state value function v(s) of an MRP is the expected return starting from state s\n\n\n\nv(s) =\\mathbb{E} [G_t  \\vert  S_t = s]\n\n\n\n\nBellman equations for MRP\nThe value function can be decomposed into two parts:\n\nan immediate reward R_{t+1} and\na discounted value of successor state \\gamma v(S_{t+1})\n\nThe Bellman equation for MRPs expresses this relationship:\n\n\\begin{align*}\nv(s) &= \\mathbb{E}[G_t  \\vert  S_t=s] \\newline\n& = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2}+\\gamma^2 R_{t+3} + ...  \\vert S_t = s]   \\newline\n& = \\mathbb{E}[R_{t+1} + \\gamma( R_{t+2}+\\gamma^2 R_{t+3} + ... )  \\vert S_t = s]   \\newline\n& = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1}  \\vert  S_t = s]   \\newline\n& = \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1})  \\vert S_t = s]\n\\end{align*} \\qquad \\text{(Bellman Equation)}\n\\tag{4}\nThe Bellman equation can also be expressed in terms of the dynamics matrix for state transitions:\n\nv(s) = R_s + γ \\sum_{s'\\in S} P_{ss'}v(s) \\qquad \\text{value with dynamics}\n\\tag{5}\nwhere we use the dynamics matrix P to express the expected value of the successor state."
  },
  {
    "objectID": "posts/coursera/c1-w2.html#sec-MDP",
    "href": "posts/coursera/c1-w2.html#sec-MDP",
    "title": "Markov Decision Processes",
    "section": "Markov Decision Processes",
    "text": "Markov Decision Processes\n\nThe k-Armed Bandit problem does not account for the fact that different situations call for different actions.\nBecause it the problem is limited to a single state agents can only make decisions based on immediate reward so they fail to consider the long-term impact of their decisions - this is an inability to make plan.\n\n\n\n\n\nThe agent–environment interaction in a Markov decision process.\n\n\n\n\n\n\n\nMDP definition\n\n\n\n\nA Markov Decision Process is a Markov Reward Process with decisions.\n\na tuple ⟨S, A, P, R, \\gamma⟩\n\n\nwhere:\n\nS is a finite set of states\nA is a finite set of actions\nP is a state transition probability matrix, P_{ss'}^a = \\mathbb{P}[S_{t+1} = s' \\vert S_t = s, A_t = a]\nR is a reward function, R_s^a = \\mathbb{E}[R_{t+1} \\vert S_t = s, A_t = a]\n\\gamma is a discount factor, \\gamma \\in [0, 1]\n\n\n\n\nThe dynamics of an MDP\nIn a finite MDP, the sets of states, actions, and rewards (S, A and R) all have a finite number of elements. In this case, the random variables S_t and R_t have well defined discrete probability distributions dependent only on the preceding state and action.\nThe dynamics of an MDP are defined by the four argument dynamics function:\n\np(s',r \\vert s,a)\\ \\dot =\\ Pr\\{S_t = s', R_t = r \\vert S_{t-1} = s, A_{t-1} = a\\}\\qquad  \\forall s',s \\in S\\ \\forall r\\in R\\ \\forall a\\in A\n\\tag{6}\nwhere the sum of the probabilities over fixed set of s,a is 1:\n\n\\sum_{s' \\in S} \\sum_{r \\in R} p(s',r \\vert s,a) = 1 \\qquad \\forall s \\in S, \\forall a \\in A \\qquad \\text{(Dynamics function)}\n\\tag{7}\nThis is just a regular function that takes a state and action and returns a probability distribution over the next state and reward.\nIn a tabular setting, we can also express this function as a table. Here is my solution for ex 3.4, a table for the recycling robot\n\n\n\nTable 1: Dynamics function for a recycling robot MDP\n\n\n\n\n\ns\na\ns'\nr\np(s',r \\mid s,a)\n\n\n\n\nhigh\nsearch\nhigh\nr_{search}\n\\alpha\n\n\nhigh\nsearch\nlow\nr_{search}\n1-\\alpha\n\n\nlow\nsearch\nhigh\n-3\n1-\\beta\n\n\nlow\nsearch\nlow\nr_{search}\n\\beta\n\n\nhigh\nwait\nhigh\nr_{wait}\n1\n\n\nlow\nwait\nlow\nr_{wait}\n1\n\n\nlow\nrecharge\nhigh\n0\n1\n\n\n\n\n\n\na couple of takeaways from this exercise are:\n\nrewards are uniquely assigned for action at s resulting in s’ so we don’t need to make r another factor (i.e. list all possible rewards for (s,a,s’) tuple.\nthere are (s,a,s’,r) tuples for which we don’t have a non-zero probabilities - since there are no transition possible.\ne.g. the robot wont charge when high, so there isn’t a transition from that state, nor a reward, nor a probability.\n\n\n\nGraphical representation of an MDP\nThe graphical representation of an MDP is a directed graph where the nodes represent states and the edges represent actions. The graph is labeled with the probabilities of transitioning from one state to another given an action.\n\n\n\n\ngraphical MDP for cleaning robot\n\nIn an MDP, the probabilities given by four part dynamics function completely characterize the environment’s dynamics.\nThat is, the probability of each possible value for S_{t+1} and R_{t+1} depends only on the immediately preceding state S_t and action A_t.\nThis is best viewed a restriction not on the decision process, but on the state.\nThe state must include information about all aspects of the past agent–environment interaction that make a difference for the future.\nIf it does, then the state is said to have the Markov property.\nWe can use the four-part dynamics function to compute the state transition probabilities for a given state and action:\n\n\\begin{align}\n  p(s' \\vert s,a) &= \\mathbb{P}[S_t = s'\\vert S_{t-1} = s, A_{t-1} = a] \\newline\n  &= \\sum_{r \\in R} p(s',r \\vert s,a) \\qquad \\text{(state transition p)}\n\\end{align}\n\\tag{8}\nwhere we summed over all possible rewards to get the state transition probability.\nWe can use the four-part dynamics function to compute the expected rewards for a given state and action:\n\n\\begin{align}\n  r(s,a) &= \\mathbb{E}[R_t \\vert S_{t-1} = s, A_{t-1} = a] \\newline\n  &= \\sum_{r \\in R} r  \\times \\sum_{s' \\in S}  p(s', r, \\vert s, a)\n\\end{align}\n\\tag{9}\nwhere we summed over all possible rewards and all successor state to get the expected reward.\nWe can also get the expected reward for a given state, action, and successor state using the four-part dynamics function:\n\n\\begin{align}\n  r(s,a,s') &= \\mathbb{E}[R_t \\vert S_{t-1} = s, A_{t-1} = a, S_t = s'] \\newline\n  &= \\sum_{r \\in R} r \\times\n  \\frac {  p(s', r \\vert s, a) } {  p(s' \\vert s, a) }\n\\end{align}\n\\tag{10}\nwhere we summed over all possible rewards to get the expected reward.\nNote: perhaps implementing these in python might be further clarify the math for a given state transition graph."
  },
  {
    "objectID": "posts/coursera/c1-w2.html#l2g1",
    "href": "posts/coursera/c1-w2.html#l2g1",
    "title": "Markov Decision Processes",
    "section": "Rewards and the Goal of an Agent",
    "text": "Rewards and the Goal of an Agent\nThe agent interacts with the environment by taking actions and receiving rewards.\nIn the bandit setting, it is enough to maximize immediate rewards. In the MDP setting, the agent must consider the long-term consequences of its actions.\nreturn G_t is the total future reward from time-step t.\n\nG_t \\dot= R_{t+1} + R_{t+2} +  R_{t+3} + \\ldots\n\n\nG_t is a random variable because both transition and the rewards can be stochastic.\n\nThe goal of an agent is to maximize the expected cumulative reward.\n\n\n\n\n\n\nThe reward hypothesis\n\n\n\nThat all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\nsee [@SILVER2021103535]\n\n\nsome basic formulations of the goal of an agent:\n\nMaze runner: -1 for each time step until the goal is reached, then 0.\nRecycling robot: +1 per can, 0 otherwise.\nChess: 1 for a win, 0 for a draw, -1 for a loss.\n\nNote there is more material on this subject in the guest lecture by Michael Littman."
  },
  {
    "objectID": "posts/coursera/c1-w2.html#l2g2",
    "href": "posts/coursera/c1-w2.html#l2g2",
    "title": "Markov Decision Processes",
    "section": "Episodes and Episodic Tasks",
    "text": "Episodes and Episodic Tasks\nAn episode is a sequence of states, actions, and rewards that ends in a terminal state. An episodic task is a task with a well-defined terminal state.\nAn example of an episodic task is a game of chess, where the game ends when one player wins or the game is a draw. The opposite setting of an episodic task is a continuing task, where the agent interacts with the environment indefinitely."
  },
  {
    "objectID": "posts/coursera/c1-w2.html#guest-lecture-with-michael-littman-on-the-reward-hypothesis",
    "href": "posts/coursera/c1-w2.html#guest-lecture-with-michael-littman-on-the-reward-hypothesis",
    "title": "Markov Decision Processes",
    "section": "Guest Lecture with Michael Littman on The Reward Hypothesis",
    "text": "Guest Lecture with Michael Littman on The Reward Hypothesis\n\n\n\n\n\n\nMichael Littman\n\n\n\n\nHis website\n\n\nLittman is a professor at Brown University and a leading researcher in reinforcement learning. He is known for his work on the reward hypothesis and the exploration-exploitation trade-off. He motivates the reward hypothesis with a humorous take on the old adage:\n\nGive a man a fish and he’ll eat for a day - traditional programming\nTeach a man to fish and he’ll eat for a lifetime - supervised learning\nGive a man a need for fish and he’ll figure it out - reinforcement learning\n\nI felt that the guest lecture was a bit of a let down. I was expecting more from a leading researcher in the field. The reward hypothesis is a fundamental concept and the lecture seemed all over the place. It raised many questions but didn’t answer them.\nIf we accept the hypothesis, then there are two areas need to be addressed:\n\nWhat rewards should agents optimize?\nDesigning algorithms to maximize them.\n\nSome rewards are easy to define, like winning a game, but others are more complex, like driving a car. His example was of air conditioning in a car, where the reward is not just the temperature but also the comfort of the passengers. Running the air conditioning has a cost in terms of fuel, but the comfort of the passengers is much harder to quantify, particularly since each passenger may have different preferences.\nNext he covered the two main approaches to setting up rewards in RL:\n\nRewards can be expressed as a final goal, or no goal yet:\n\nThe goal based representation: Goal achieved = +1, and everything else is 0. This has a downside of not signaling, to the agent, the urgency of getting to the goal.\nThe action-penalty representation: a -1 could be awarded every step that the goal is not yet achieved. This can cause problems if there is a small probability of getting stuck and never reaching the goal.\nIt seems that there are many ways to set up rewards. If we take a lesson from game theory, we can see that the value of rewards might be important or the relative value or order of rewards might be important. In rl values are often more important than the ‘order’ of rewards. However it might be interesting to consider if we can encode preferences into the rewards and if this formulation would still make sense in the context of the reward hypothesis and the bellman equations.\n\n\nLittleman then asked “Where do rewards come from?” and answered that they can come from - Programming - Human feedback - Examples - Mimic the rewards a human would give - Inverse reinforcement learning - learn the reward function from examples - Optimization - Evolutionary optimization like population dynamics. - The reward is the objective function - The reward is the gradient of the objective function\nNext he discusses some challenges to the reward hypothesis:\n\nTarget is something other than cumulative reward:\n\ncannot capture risk averse behavior\ncannot capture diversity of behavior\n\nis it a good match for a high level human behavior?\n\nsingle minded pursuit of a goal isn’t characteristic of good people.\nThe goals we “should” be pursuing may not be immediately evident to us - we might need time to understand making good decisions.\n\n\nThe big elephant in the room is that we can reject the reward hypothesis and have agents that peruse multiple goals. The main challenge is that it becomes harder to decide when there is a conflict between goals. However, the field of multi-objective optimization has been around for a long time and there are many ways to deal with this problem. Some are more similar to the reward hypothesis but others can lead to more complex behavior based on preferences and pareto optimality.\n\nLesson 3: Continuing Tasks\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nFormulate returns for continuing tasks using discounting. #\nDescribe how returns at successive time steps are related to each other. #\nUnderstand when to formalize a task as episodic or continuing. #\n\n\n\n\nReturns for Continuing Tasks\n\nIn continuing tasks, the agent interacts with the environment indefinitely.\nThe return at time t is the sum of the rewards from time t to the end of the episode.\nThe return can be formulated using discounting, where the rewards are discounted by a factor \\gamma.\n\n\n\\begin{align}\nG_t&=R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\newline\n&= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\end{align}\n\\tag{11}\n\nreturns have a recursive structure, where the return at time t is related to the return at time t+1 by the discount factor \\gamma.\n\n  \n\\begin{align}\nG_t &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\newline\n&= R_{t+1} + \\gamma ( R_{t+2} + \\gamma R_{t+3} + \\ldots ) \\newline\n&= R_{t+1} + \\gamma G_{t+1}\n\\end{align}\n\\tag{12}\nthis form of the return is called the recursive form of the return and is usefull in developing algorithms for reinforcement learning.\n\n\nReturns at Successive Time Steps\n\nThe return at time t is related to the return at time t+1 by the discount factor \\gamma.\nThe return at time t is the sum of the reward at time t and the discounted return at time $t+1.\n\n\n\nEpisodic vs. Continuing Tasks\n\nAn episodic task has a well-defined terminal state, and the episode ends when the terminal state is reached.\nA continuing task does not have a terminal state, and the agent interacts with the environment indefinitely.\nTo avoid infinite returns in continuing tasks, we use discounting to ensure that the return is finite.\nThe discount factor \\gamma\\in(0,1) is the present value of future rewards.\n\n@sutton2018reinforcement emphasizes that we can use the discount factor of [0,1] to unify both episodic and continuing tasks Here = 0 corresponds to myopic view of optimizing immediate rewards like in the k-armed bandit problem. The discount factor = 1 corresponds to the long-term view of optimizing undiscounted expected cumulative reward. into a single framework. This is a powerful idea that allows us to use the same algorithms for both types of tasks.\nI think it is a good place to consider a couple of ideas raised by Littman in the guest lecture:\nThe first is hyperbolic discounting and the second risk aversion.\nBehavioral economics has considered a notion of hyperbolic discounting where the discount factor is not constant but changes over time. This is a more realistic model of human behavior but is harder to work with mathematically. This idea is not covered in the course perhaps because it is a departure from the more rational exponential discounting model which we use.\ntwo forms of hyperbolic discounting are:\n\nG(D) = \\frac{1}{1 + \\gamma D}\n\n\nφh(τ) = (1+ ατ)−γ/α\n\\tag{13} where:\n\nφh(τ) is the hyperbolic discount factor at time τ\nα is the rate of discounting\nγ is the delay parameter\n\nthere is also quasi-hyperbolic discounting which is a combination of exponential and hyperbolic discounting.\n\nG(D) = \\begin{cases}\n  1 & \\text{if } t = 0  \\newline\n  \\beta^k \\delta^{D} & \\text{if } t &gt; 0\n  \\end{cases}\n\nThe notation for both terminal and non-terminal states is S^+\nExercise 2.10 proof of Equation 3.10\n\n\\begin{align*}\nG_t &= \\sum_{k=0}^\\infty \\gamma^k = lim_{n \\rightarrow \\infty} (1 + \\gamma + \\gamma^2 + ... + \\gamma^n) \\newline\n&= lim_{n \\rightarrow \\infty} \\frac{(1 + \\gamma + \\gamma^2 + ... + \\gamma^n) (1 - \\gamma)}{(1 - \\gamma)} \\newline\n&= lim_{n \\rightarrow \\infty} \\frac{1 - \\gamma^{n+1}}{1 - \\gamma} \\newline\n&= \\frac{1}{1 - \\gamma}\n\\end{align*}"
  },
  {
    "objectID": "posts/coursera/c1-w2.html#l3g1",
    "href": "posts/coursera/c1-w2.html#l3g1",
    "title": "Markov Decision Processes",
    "section": "Returns for Continuing Tasks",
    "text": "Returns for Continuing Tasks\n\nIn continuing tasks, the agent interacts with the environment indefinitely.\nThe return at time t is the sum of the rewards from time t to the end of the episode.\nThe return can be formulated using discounting, where the rewards are discounted by a factor \\gamma.\n\n\n\\begin{align}\nG_t&=R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\newline\n&= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\end{align}\n\\tag{11}\n\nreturns have a recursive structure, where the return at time t is related to the return at time t+1 by the discount factor \\gamma.\n\n  \n\\begin{align}\nG_t &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\newline\n&= R_{t+1} + \\gamma ( R_{t+2} + \\gamma R_{t+3} + \\ldots ) \\newline\n&= R_{t+1} + \\gamma G_{t+1}\n\\end{align}\n\\tag{12}\nthis form of the return is called the recursive form of the return and is usefull in developing algorithms for reinforcement learning."
  },
  {
    "objectID": "posts/coursera/c1-w2.html#l3g2",
    "href": "posts/coursera/c1-w2.html#l3g2",
    "title": "Markov Decision Processes",
    "section": "Returns at Successive Time Steps",
    "text": "Returns at Successive Time Steps\n\nThe return at time t is related to the return at time t+1 by the discount factor \\gamma.\nThe return at time t is the sum of the reward at time t and the discounted return at time $t+1."
  },
  {
    "objectID": "posts/coursera/c1-w2.html#l3g3",
    "href": "posts/coursera/c1-w2.html#l3g3",
    "title": "Markov Decision Processes",
    "section": "Episodic vs. Continuing Tasks",
    "text": "Episodic vs. Continuing Tasks\n\nAn episodic task has a well-defined terminal state, and the episode ends when the terminal state is reached.\nA continuing task does not have a terminal state, and the agent interacts with the environment indefinitely.\nTo avoid infinite returns in continuing tasks, we use discounting to ensure that the return is finite.\nThe discount factor \\gamma\\in(0,1) is the present value of future rewards.\n\n@sutton2018reinforcement emphasizes that we can use the discount factor of [0,1] to unify both episodic and continuing tasks Here = 0 corresponds to myopic view of optimizing immediate rewards like in the k-armed bandit problem. The discount factor = 1 corresponds to the long-term view of optimizing undiscounted expected cumulative reward. into a single framework. This is a powerful idea that allows us to use the same algorithms for both types of tasks.\nI think it is a good place to consider a couple of ideas raised by Littman in the guest lecture:\nThe first is hyperbolic discounting and the second risk aversion.\nBehavioral economics has considered a notion of hyperbolic discounting where the discount factor is not constant but changes over time. This is a more realistic model of human behavior but is harder to work with mathematically. This idea is not covered in the course perhaps because it is a departure from the more rational exponential discounting model which we use.\ntwo forms of hyperbolic discounting are:\n\nG(D) = \\frac{1}{1 + \\gamma D}\n\n\nφh(τ) = (1+ ατ)−γ/α\n\\tag{13} where:\n\nφh(τ) is the hyperbolic discount factor at time τ\nα is the rate of discounting\nγ is the delay parameter\n\nthere is also quasi-hyperbolic discounting which is a combination of exponential and hyperbolic discounting.\n\nG(D) = \\begin{cases}\n  1 & \\text{if } t = 0  \\newline\n  \\beta^k \\delta^{D} & \\text{if } t &gt; 0\n  \\end{cases}\n\nThe notation for both terminal and non-terminal states is S^+\nExercise 2.10 proof of Equation 3.10\n\n\\begin{align*}\nG_t &= \\sum_{k=0}^\\infty \\gamma^k = lim_{n \\rightarrow \\infty} (1 + \\gamma + \\gamma^2 + ... + \\gamma^n) \\newline\n&= lim_{n \\rightarrow \\infty} \\frac{(1 + \\gamma + \\gamma^2 + ... + \\gamma^n) (1 - \\gamma)}{(1 - \\gamma)} \\newline\n&= lim_{n \\rightarrow \\infty} \\frac{1 - \\gamma^{n+1}}{1 - \\gamma} \\newline\n&= \\frac{1}{1 - \\gamma}\n\\end{align*}"
  },
  {
    "objectID": "posts/coursera/c2-w4.html",
    "href": "posts/coursera/c2-w4.html",
    "title": "Sample-based Learning Methods",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/coursera/c2-w4.html#lesson-1-what-is-a-model",
    "href": "posts/coursera/c2-w4.html#lesson-1-what-is-a-model",
    "title": "Sample-based Learning Methods",
    "section": "Lesson 1: What is a model?",
    "text": "Lesson 1: What is a model?\n\nLesson Learning Goals\n\nDescribe what a model is and how they can be used #\nClassify models as distribution models or sample models #\nIdentify when to use a distribution model or sample model #\nDescribe the advantages and disadvantages of sample models and distribution models #\nExplain why sample models can be represented more compactly than distribution models #\n\n\n\nWhat is a model and how can it be used?\n\nA model is a simplified representation of the environment dynamics\nModels can be used to simulate the environment\nIn this course a Model is a function that predicts the next state and reward given the current state and action\na transition model s_{t_+1} = f(s_t, a_t) predicts the next state given the current state and action\na reward model r_{t_+1} = f(s_t, a_t) predicts the reward given the current state and action\n\nthree other model types are mentioned in the ICML Tutorial on Model-Based Reinforcement Learning\n\nInverse models predict the action given the current state and next state a_{t+1} = f_s^{-1}(s_t, s_{t+1})\nDistances models predict the distance between the current state and the goal state d_{ij} =d(s, s')\nFuture return models predict the future return given the current state and action G_t=Q(s_t, a_t) or G_t=V(s_t)\n\nWhy do we want to use models?\n\nmodel allow us to simulate the environment without interacting with it.\nthis can increase sample efficiency - e.g. by replaying past experiences to propergate learning from goal to all predecessor states we have visited\nthis can reduce risks - e.g. by simulating dangerous situations instead of actually experiencing them.\nthis can reduce costs - e.g. by simulating costly actions in a simulated environment instead of paying the cost in the real environment.\nthis could be much faster than real-time interaction with the environment. Often in robotics simulation is orders of magnitude faster than real-time interaction.\n\n\n\nTypes of models\n\n\n\n\nmodels\n\n\nDistribution models predict the probability distribution of the next state and reward\nSample models predict a single next state and reward\n\nAlso there are:Environment simulator\n\nChess Programs typicaly can simulate all possible movers and evaluate the board position (tacticaly and strategically). The difference between the current board position and the board position after a move is the reward.\n[@Silver2016MasteringTG] mentions an Environment simulator for the game of go\n[@Agostinelli2019SolvingTR] used a simulator of the rubik’s cube to train a reinforcement learning agent to solve the cube.\n[@Bellemare2012TheAL] used a simulator of the game of atari to train a reinforcement learning agent to play atari games.\n[@Todorov2012MuJoCoAP] used a simulator of the physics of the real world to train a reinforcement learning agent to control a robot.\n[@Shen2018MWalkLT] used a simulator to train agents to navigate a graph using MCTS.\n[@Ellis2019WriteEA] used a REPL environment to train a reinforcement learning agent to write code.\n\n\n\nWhen to use a distribution model or sample model\n\nDistribution models are useful when we need to know the probability of different outcomes\nSample models are useful when we need to simulate the environment\n\n\n\nAdvantages and disadvantages of sample models and distribution models\n\nSample models can be represented more compactly than distribution models\nDistribution models can be more accurate than sample models\nexact expectations can be computed from distribution models\nassessing risks and uncertainties is easier with distribution models\n\n\n\nWhy sample models can be represented more compactly than distribution models\n\nSample models can be represented more compactly than distribution models because they only need to store a single next state and reward\nDistribution models need to store the joint probability of each possible next state and reward pair\nSample models can be more efficient when we only need to simulate the environment"
  },
  {
    "objectID": "posts/coursera/c2-w4.html#lesson-2-planning",
    "href": "posts/coursera/c2-w4.html#lesson-2-planning",
    "title": "Sample-based Learning Methods",
    "section": "Lesson 2: Planning",
    "text": "Lesson 2: Planning\n\nLesson Learning Goals\n\nExplain how planning is used to improve policies #\nDescribe random-sample one-step tabular Q-planning #\n\n\n\nHow planning is used to improve policies\n\nPlanning is the process of using a model to improve a policy or value function\nPlanning can be used to improve a policy or value function without interacting with the environment\nPlanning can be used to improve a policy or value function more efficiently than direct RL updates\n\nRandom-sample one-step tabular Q-planning {#sec-l2g2}\n\n\n\n\nQ-planning alg overview\n\n\n\n\nrandom sample one step tabular Q-planning\n\n\n\n\nTabular Q-planning is a planning algorithm that uses a sample model to improve a policy or value function\nTabular Q-planning uses a sample model to simulate the environment\nTabular Q-planning uses the simulated experience to improve a policy or value function\n\nadvantages of planning\n\nPlanning can be more efficient than direct RL updates\nPlanning can be used to improve a policy or value function without interacting with the environment\nPlanning can be used to improve a policy or value function more efficiently than direct RL updates"
  },
  {
    "objectID": "posts/coursera/c2-w4.html#lesson-3-dyna-as-a-formalism-for-planning",
    "href": "posts/coursera/c2-w4.html#lesson-3-dyna-as-a-formalism-for-planning",
    "title": "Sample-based Learning Methods",
    "section": "Lesson 3: Dyna as a formalism for planning",
    "text": "Lesson 3: Dyna as a formalism for planning\n\nLesson Learning Goals\n\nRecognize that direct RL updates use experience from the environment to improve a policy or value function #\nRecognize that planning updates use experience from a model to improve a policy or value function #\nDescribe how both direct RL and planning updates can be combined through the Dyna architecture #\nDescribe the Tabular Dyna-Q algorithm #\nIdentify the direct-RL and planning updates in Tabular Dyna-Q #\nIdentify the model learning and search control components of Tabular Dyna-Q #\nDescribe how learning from both direct and simulated experience impacts performance #\nDescribe how simulated experience can be useful when the model is accurate #\n\n\n\nDirect RL updates use experience from the environment to improve a policy or value function\n\nDirect RL updates use experience from the environment to improve a policy or value function\nDirect RL updates can be used to improve a policy or value function by interacting with the environment\n\n\n\nPlanning updates use experience from a model to improve a policy or value function\n\nPlanning updates use experience from a model to improve a policy or value function\nPlanning updates can be used to improve a policy or value function without interacting with the environment\n\n\n\nBoth direct RL and planning updates can be combined through the Dyna architecture\n\nDyna architecture combines direct RL updates and planning updates to improve a policy or value function\nDyna architecture uses a model to simulate the environment\nDyna architecture uses the simulated experience to improve a policy or value function\n\n\n\nThe Tabular Dyna-Q algorithm\n\nTabular Dyna-Q is a planning algorithm that uses a sample model to improve a policy or value function\nTabular Dyna-Q uses a sample model to simulate the environment\nTabular Dyna-Q uses the simulated experience to improve a policy or value function\n\n\n\n\n\nThe Tabular Dyna-Q algorithm\n\nExercise 8.2 Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiments?\nDyna-Q+ is like a generalized UCB while Dyna-Q+ is like a generalized epsilon greedy alg. Dyna-Q+ is doing more efficent exploration. It will revisits will be more spread out more over time but it scheme also tends to increases in non independent way - probabilities for unvisited regions keep growing so if it starts exploring it may like doing an extended sequence till it gets to a dead end.\nDyna Q exploration is independent for each state,action combo so retrying sequences get asymptotically less likely with time.\nExercise 8.3 Careful inspection of Figure 8.5 reveals that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?\nDyna-Q+ is more efficient at exploring so it learned a better policy, but since the environment was static Dyna-Q got to catch up, but it never reached the same policy.\nExercise 8.5 How might the tabular Dyna-Q algorithm shown on page 164 be modified to handle stochastic environments? How might this modification perform poorly on changing environments such as considered in this section? How could the algorithm be modified to handle stochastic environments and changing environments?\nto hadle a stochastic environment one would need to to model probabilities of stochastic dynamics. One way to do this is to use Bayesian updating with a dericlet prior and a multinomial posterior.\nThis modification would likely fare much worse since learning low probability transitions would require many visits to discover.\nIn the case of changing environment it would also take much longer for new state to be reflected in the model (if a state was visited 10 with just one transition and then the transition changed to another state then it would take many more than 10 vistis to quash the old probability and get the new one correct to 10%\nThis means that we adding a forgetting rule might be better then the plain derichlet-multinomial model.\nTo handle both stochastic and changing updates we may want to\n1. track the recency of the last visit and reward this option like in dyna-q plus.\n2. decay old probabilities - would require storing the time for each visit - i.e. path dependent model.\n3. A better idea is to use a hirachial model with parial pooling representing short term and long term transitions - this could fix the problem of decay by simply giving greater weight to the smaller more recent model.\nThe short term would track the last k visits in each state and the long term all the visits. We could then do partial pooling between these two estimators with much greater emphasis on the recent one!\n\n\n\n\nDirect-RL and planning updates in Tabular Dyna-Q\n\nTabular Dyna-Q uses direct RL updates to improve a policy or value function\nTabular Dyna-Q uses planning updates to improve a policy or value function\n\n\n\nModel learning and search control components of Tabular Dyna-Q\n\nTabular Dyna-Q uses a sample model to simulate the environment\nTabular Dyna-Q uses the simulated experience to improve a policy or value function\n\n\n\nLearning from both direct and simulated experience impacts performance\n\nLearning from both direct and simulated experience can improve performance\nLearning from both direct and simulated experience can be more efficient than direct RL updates\n\n\n\nSimulated experience can be useful when the model is accurate\n\nSimulated experience can be useful when the model is accurate\nSimulated experience can be used to improve a policy or value function without interacting with the environment"
  },
  {
    "objectID": "posts/coursera/c2-w4.html#lesson-4-dealing-with-inaccurate-models",
    "href": "posts/coursera/c2-w4.html#lesson-4-dealing-with-inaccurate-models",
    "title": "Sample-based Learning Methods",
    "section": "Lesson 4: Dealing with inaccurate models",
    "text": "Lesson 4: Dealing with inaccurate models\n\nLesson Learning Goals\n\nIdentify ways in which models can be inaccurate #\nExplain the effects of planning with an inaccurate model #\nDescribe how Dyna can plan successfully with a partially inaccurate model #\nExplain how model inaccuracies produce another exploration-exploitation trade-off #\nDescribe how Dyna-Q+ proposes a way to address this trade-off #\n\n\n\nWays in which models can be inaccurate\n\nModels can be inaccurate for many reasons\nbecause they have not sampled all actions in all states\nbecause the environment is has changed since the model was learned\nif the environment is stochastic\n\n\n\nEffects of planning with an inaccurate model\n\nPlanning with an inaccurate model can cause the value function to become worse\nPlanning with an inaccurate model can lead to sub-optimal policies\n\n\n\nDyna can plan successfully with a partially inaccurate model\n\nDyna can plan successfully with a partially inaccurate model\nDyna can use direct RL updates to improve a policy or value function as well as the model\nDyna can use planning updates to improve a policy or value function\n\n\n\nModel inaccuracies produce another exploration-exploitation trade-off\n\nModel inaccuracies produce another exploration-exploitation trade-off\nexploit an inaccurate model to improve the policy\nrevisit states/actions with low value to update the model\nCan we use an inverse sort of planning to identify states for which the model is inaccurate?\nModel inaccuracies can lead to suboptimal policies\nModel inaccuracies can lead to poor performance\n\n\n\nDyna-Q+ proposes a way to address this trade-off\n\n\n\n\nDyna-Q+ solution\n\n\nDyna-Q+ proposes a way to address this trade-off\nDyna-Q+ uses a bonus reward to encourage exploration\nDyna-Q+ can improve performance when the model is inaccurate\n\n\n\nDrew Bagnell on self-driving cars robotics and model-based reinforcement learning\nDrew Bagnell is a professor at Carnegie Mellon University and the CTO at Aurora innovation.\nHe has worked on self-driving cars and robotics. He has also worked on model-based reinforcement learning. He point out a dirty little secret that model-based reinforcement learning is a key technology for robotics.\nHe points out that the real world is expensive and dangerous. Using model based reinforcement learning can reduce the number of interactions with the real world and along learning about risky actions in the simulated world to improve performance in the real world. Also as we pointer out before this can usually be done much faster than real-time interaction with the environment.\nSample complexity: how many real-world samples are required to achieve high performance? It takes exponentially fewer interactions with a model than without. Not really sure what exponentially fewer means here - but it’s a lot fewer.\nQuadratic value function approximation goes back to optimal control in the 1960s. It’s continuous in states and actions. This is a method that should be part of the next course but isn’t covered there either\nFor linear transition dynamics with quadratic costs/rewards, it’s exact. For local convex / concave points, it is a good approximation of the true action-value function.\nHere is the math from his slide:\nQuadratic value function approximation\n\nQ_t(x,a) = \\begin{bmatrix}     x   \\\\ a   \\\\ \\end{bmatrix}^T\n\\begin{bmatrix}     Q_{xx}   &&  Q_{xa}   \\\\ Q_{xa}   &&  Q_{uu}   \\\\ \\end{bmatrix}\n\\begin{bmatrix} x   \\\\ a   \\\\ \\end{bmatrix}^T +\n\\begin{bmatrix}    q_x   \\\\ q_a \\\\ \\end{bmatrix}^T\n\\begin{bmatrix}     x   \\\\ a   \\\\ \\end{bmatrix} + const\n\\tag{1}\nThe approximation allows for calculating the optimal action-value in closed form (finite number of standard operations) even with continuous actions.\nDifferential dynamic programming takes advantage of the technique above.\n\nSo this seems complicated - because of matrix maths. But intuitively this is something we like to do in physics - add the term for the second derivative\nin the taylor series approximation of our function.\nThe 2nd paper is particularly clear and easy to work through for the approach just described.\n\n\n\n\n\n\ntimeline\n\n    title Bandit Algorithms Timeline\n\n    \n        1952 : Thompson Sampling\n        1955 : Upper Confidence Bound (UCB)\n        1963 : Epsilon-Greedy\n        2002 : Bayesian UCB\n        2011 : Bayesian Bandits\n        2012 : Contextual Bandits\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'timeline': { 'nodeSpacing': 50, 'sectionSpacing': 100, 'verticalStartPosition': 50, 'verticalSectionStartPosition': 50 }}}}%%\ntimeline\n    direction TD\n    title Reinforcement Learning Algorithms Timeline\n\n\n    \n        1948 : Monte Carlo Methods\n        1950 : Bellman Optimality Equations\n        1957 : Dynamic Programming\n        1959 : Temporal Difference Learning (TD)\n        1960 : Policy Iteration\n        1963 : Value Iteration\n        1983 : Q-Learning\n        1984 : Expected SARSA\n        1990 : Dyna-Q : Dyna-Q+\n        1992 : SARSA\n        1994 : Monte Carlo with E-Soft\n        1995 : Monte Carlo with Exploring Starts\n             : Generalized Policy Iteration (GPI)\n        1998 : Semi-Gradient TD\n        2000 : Differential Semi-Gradient SARSA\n        2001 : Gradient Monte Carlo (Gradient MC)\n        2003 : Gaussian Actor-Critic\n             : Softmax Actor-Critic\n             : Deep Q-Network (DQN)\n\n\n\n\n\n\n\n\n\n\n\nReferences\nMaterials from ICML Tutorial on Model-Based Reinforcement Learning:\nthe page above contains the following materials as well as an extensive bibliography.\n\nSlides\nPart 1: Introduction and Learning Models\nPart 2: Model-Based Control\nPart 3: Model-Based Control in the Loop\nPart 4: Beyond Vanilla MBRL\n\nFrom Bagnell’s talk:\n\nModern Adaptive Control and Reinforcement Learning\nSynthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization\nOptimal Control, Trajectory Optimization, Learning Dynamics"
  },
  {
    "objectID": "posts/coursera/c1-w1.html",
    "href": "posts/coursera/c1-w1.html",
    "title": "The K-Armed Bandit Problem",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/coursera/c1-w1.html#sec-k-armed-bandit",
    "href": "posts/coursera/c1-w1.html#sec-k-armed-bandit",
    "title": "The K-Armed Bandit Problem",
    "section": "K-armed bandits 🐙",
    "text": "K-armed bandits 🐙\nIn the k-armed bandit problem there is an agent who is assigned a state s by the environment and must learn which action a from the possible set of actions A leads to the goal state through a signal based on the greatest expected reward.\nOne way this can be achieved is using a Bayesian updating scheme starting from a uniform prior."
  },
  {
    "objectID": "posts/coursera/c1-w1.html#sec-l1g1",
    "href": "posts/coursera/c1-w1.html#sec-l1g1",
    "title": "The K-Armed Bandit Problem",
    "section": "Temporal nature of the bandit problem",
    "text": "Temporal nature of the bandit problem\nThe bandit problem cam be static problem with a fixed reward distribution. However, more generally it is a temporal problem when the rewards distribution changes over time and agent must learn to adapt to these changes.\n\n\n\n\n\n\nDifference between bandits and RL\n\n\n\nIn the typical bandit setting there is only one state. So after we pull the arm nothing in the problem changes.\nBandits problems where agents can discriminate between states are called contextual bandits.\nHowever, bandits embody one of the main themes of RL - that of estimating an expected reward for different actions.\nIn the more general RL setting we will be interested in more general problems where actions will lead the agent to new states and the goal is some specific state we need to reach.\n\n\n\n\n\nVideo\nbandit\n\n\nExample 1 (Using Multi-armed bandit to randomize a medical trial)  \n\nagent is the doctor\nactions {blue, yellow, red} treatment\nk = 3\nthe rewards are the health of the patients’ blood pressure.\na random trial in which a doctor need to pick one of three treatments.\nq(a) is the mean of the blood pressure for the patient.\n\n\n\n\n\n\nclinical trial\n\n\nAction Values and Greedy Action Selection\nThe value of an action is its expected reward which can be expressed mathematically as:\n\n\\begin{align}\nq_{\\star}(a) & \\doteq \\mathbb{E}[R_t  \\vert  A_t=a] \\space \\forall a \\in \\{a_1 ... a_k\\} \\newline\n             & = \\sum_r p(r|a)r \\qquad \\text{(action value)}\n\\end{align}\n\\tag{1}\nwhere:\n\n\\doteq means definition\n\\mathbb{E}[r \\vert a] means expectation of a reward given some action a Since agents want to maximize rewards, recalling the definition of expectations we can write this as:\n\nThe goal of the agent is to maximize the expected reward which we can express mathematically as:\n\n\\arg\\max_a q(a)=\\sum_r p(r \\vert a) \\times r \\qquad \\text{(Greedification)}\n\\tag{2}\nwhere:\n\n\\arg \\max_a means the argument a maximizes - so the agent is looking for the action that maximizes the expected reward and the outcome is an action.\n\n\n\nReward, Return, and Value Functions\nThe reward r is the immediate feedback from the environment after the agent takes an action.\nThe return G_t is the total discounted reward from time-step t.\nThe value function v(s) of an MRP is the expected return starting from state s.\n\n\n\n\ndecisions\n\nexample of decisions under uncertainty:\n\nmovie recommendation.\nclinical trials.\nmusic recommendation.\nfood ordering at a restaurant.\n\n\n\n\n\nwhy discuss bandits\n\nIt best to consider issues and algorithms design choices in the simplest setting first. The bandit problem is the simplest setting for RL. More advanced algorithms will incorporate parts we use to solve this simple settings.\n\nmaximizing rewards.\nbalancing exploration and exploitation.\nestimating expected rewards for different actions.\n\nare all problems we will encounter in both the bandit and the more general RL setting."
  },
  {
    "objectID": "posts/coursera/c1-w1.html#sec-epsilon-greedy-policies",
    "href": "posts/coursera/c1-w1.html#sec-epsilon-greedy-policies",
    "title": "The K-Armed Bandit Problem",
    "section": "Ɛ-Greedy Policies",
    "text": "Ɛ-Greedy Policies\nThe Ɛ-greedy policy uses a simple heuristic to balance exploration with exploitation. The idea is to choose the best action with probability 1-\\epsilon and to choose a random action with probability \\epsilon.\n\n\n\\begin{algorithm} \\caption{EpsilonGreedy(K, α, β)}\\begin{algorithmic} \\For{$t = 1, 2, \\ldots $} \\State p = random() \\If {$p &lt; \\epsilon$} \\State select radom action $x_t \\qquad$ \\Comment{explore} \\Else \\State select $x_t = \\arg\\max_k \\hat{\\theta}_k \\qquad$ \\Comment{exploit} \\EndIf \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nThe problem with Ɛ-greedy policies\n\n\n\n\nA problem with Ɛ-greedy is that it is not optimal in the long run.\nEven after it has found the best course of action it will continue to explore with probability \\epsilon.\nThis is because the policy is not adaptive.\nOne method is too reduce \\epsilon over time. However unless there is a feedback from the environment this will likely stop exploring too soon or too late thus providing sub-optimal returns.\n\n\n\nThe following is a simple implementation of the Ɛ-greedy algorithm in Python from geeksforgeeks.org\n\n\nCode\n# Import required libraries \nimport numpy as np \nimport matplotlib.pyplot as plt \n  \n# Define Action class \nclass Actions: \n  def __init__(self, m): \n    self.m = m \n    self.mean = 0\n    self.N = 0\n  \n  # Choose a random action \n  def choose(self):  \n    return np.random.randn() + self.m \n  \n  # Update the action-value estimate \n  def update(self, x): \n    self.N += 1\n    self.mean = (1 - 1.0 / self.N)*self.mean + 1.0 / self.N * x \n  \n  \ndef run_experiment(m1, m2, m3, eps, N): \n      \n  actions = [Actions(m1), Actions(m2), Actions(m3)] \n  \n  data = np.empty(N) \n    \n  for i in range(N): \n    # epsilon greedy \n    p = np.random.random() \n    if p &lt; eps: \n      j = np.random.choice(3) \n    else: \n      j = np.argmax([a.mean for a in actions]) \n    x = actions[j].choose() \n    actions[j].update(x) \n  \n    # for the plot \n    data[i] = x \n  cumulative_average = np.cumsum(data) / (np.arange(N) + 1) \n  \n  # plot moving average ctr \n  plt.plot(cumulative_average) \n  plt.plot(np.ones(N)*m1) \n  plt.plot(np.ones(N)*m2) \n  plt.plot(np.ones(N)*m3) \n  plt.xscale('log') \n  plt.show() \n  \n  for a in actions: \n    print(a.mean) \n  \n  return cumulative_average \n\n\n\n\nCode\nc_1 = run_experiment(1.0, 2.0, 3.0, 0.1, 100000) \n#print(c_1)\n\n\n\n\n\n\n\n\n\n1.0533891142612195\n2.02556827747505\n2.9967725891863877\n\n\n\n\nCode\nc_05 = run_experiment(1.0, 2.0, 3.0, 0.05, 100000) \n#print(c_05)\n\n\n\n\n\n\n\n\n\n1.0039926459413782\n2.00700337763791\n3.003536310561855\n\n\n\n\nCode\nc_01 = run_experiment(1.0, 2.0, 3.0, 0.01, 100000) \n#print(c_01)\n\n\n\n\n\n\n\n\n\n0.9611950383032279\n2.026899073102919\n2.9958268510720685\n\n\n\n\nCode\n# log scale plot \nplt.plot(c_1, label ='eps = 0.1') \nplt.plot(c_05, label ='eps = 0.05') \nplt.plot(c_01, label ='eps = 0.01') \nplt.legend() \nplt.xscale('log') \nplt.show()"
  },
  {
    "objectID": "posts/coursera/c1-w1.html#sec-benefits-of-exploitation-and-exploration",
    "href": "posts/coursera/c1-w1.html#sec-benefits-of-exploitation-and-exploration",
    "title": "The K-Armed Bandit Problem",
    "section": "Benefits of Exploitation & Exploration",
    "text": "Benefits of Exploitation & Exploration\n\nIn the short term we may maximize rewards following the best known course of action. However this may represent a local maximum.\nIn the long term agents that explore different options and keep uncovering better options until they find the best course of action corresponding to the global maximum.\n\nTo get the best of both worlds we need to balance exploration and exploitation ideally using a policy that uses feedback to adapt to its environment."
  },
  {
    "objectID": "posts/coursera/c1-w1.html#sec-optimistic-initial-values",
    "href": "posts/coursera/c1-w1.html#sec-optimistic-initial-values",
    "title": "The K-Armed Bandit Problem",
    "section": "Optimistic initial values",
    "text": "Optimistic initial values\n\nOptimistic initial values\n\nSetting all initially action values greater than the algorithmically available values in [0,1]\n\n\nThe methods we have discussed are dependent on the initial action-value estimates, Q_1(a). In the language of statistics, we call these methods biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once. For methods with constant \\alpha, the bias is permanent, though decreasing over time.\n\n\n\\begin{algorithm} \\caption{OptimisticBernGreedy(K, α, β)}\\begin{algorithmic} \\For{$t = 1, 2, . . .$} \\State \\State \\Comment{ estimate model} \\For{$k = 1, . . . , K$} \\State $\\hat\\theta_k \\leftarrow 1 \\qquad$ \\Comment{optimistic initial value} \\EndFor \\State \\Comment{ select and apply action:} \\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$ \\State Apply $x_t$ and observe $r_t$ \\State \\Comment{ update distribution:} \\State $(α_{x_t}, β_{x_t}) \\leftarrow (α_{x_t} + r_t, β_{x_t} + 1 − r_t)$ \\EndFor \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "posts/coursera/c1-w1.html#sec-benefits-of-optimistic-initial-values-for-early-exploration",
    "href": "posts/coursera/c1-w1.html#sec-benefits-of-optimistic-initial-values-for-early-exploration",
    "title": "The K-Armed Bandit Problem",
    "section": "Benefits of optimistic initial values for early exploration",
    "text": "Benefits of optimistic initial values for early exploration\nSetting the initial action values to be higher than the true values has the effect of causing various bandit algorithm to try to exploit them - only to find out that most values are not as rewarding as it was led to expect.\nWhat happens is that the algorithm will initially explore more than it would have otherwise. Possibly even trying all the actions at least once.\nIn the short-term it will perform worse than Ɛ- greedy which tend to exploit. But as more of the state space is explored at least once the algorithm will beat an Ɛ-greedy policy which can take far longer to explore the space and find the optimal options.\n\n\n\nThe effect of optimistic initial action-value estimates\n\n\n\n\n\n\n\n\nCriticisms of optimistic initial values\n\n\n\n\nOptimistic initial values only drive early exploration. The agent will stop exploring once this is done.\nFor a non-stationary problems - this is inadequate.\nIn a real world problems the maximum reward is an unknown quantity."
  },
  {
    "objectID": "posts/coursera/c1-w1.html#sec-the-ucb-action-selection-method",
    "href": "posts/coursera/c1-w1.html#sec-the-ucb-action-selection-method",
    "title": "The K-Armed Bandit Problem",
    "section": "The UCB action selection method",
    "text": "The UCB action selection method\nUCB is an acronym for Upper Confidence Bound. The idea behind it is to select the action that has the highest upper confidence bound. This has the advantage over epsilon greedy that it will explore more in the beginning and then exploit more as the algorithm progresses.\nthe upper confidence bound is defined as:\n\nA_t = \\arg\\max\\_a \\Bigg[\n  \\underbrace{Q_t(a)}_{exploitation} +\n  \\underbrace{c \\sqrt{\\frac{\\ln t}{N_t(a)} }}_{exploration}\n\\Bigg] \\qquad\n\\tag{6}\nwhere:\n\nQ_t(a) is the action value\nc is a constant that determines the degree of exploration\nN_t(a) is the number of times action a has been selected prior to time t\n\n\n\n\n\nUCB intuition\n\nThe idea is we the action for which the action value plus the highest possible uncertainty give the highest sum. We are being optimistic in assuming this choice will give the highest reward. In reality any value in the confidence interval could be the true value. Each time we select an action we reduce the uncertainty in the exploration term and we also temper our optimism of the upper confidence bound by the number of times we have selected the action. This means that we will prefer to visit the actions that have not been visited as often.\nThe main advantage of UCB is that it is more efficient than epsilon greedy in the long run. If we measure the cost of learning in terms of the regret - the difference between the expected reward of the optimal action and the expected reward of the action we choose. UCB has a lower regret than epsilon greedy. The downside is that it is more complex and requires more computation.\n\n\n\\begin{algorithm} \\caption{UCB(K, α, β)}\\begin{algorithmic} \\For{$t = 1, 2, . . .$} \\For { $k = 1, . . . , K$ } \\State \\Comment{ $\\textcolor{blue}{compute\\ UCBs}$} \\State $U_k = \\hat\\theta_k + c \\sqrt{\\frac{\\ln t}{N_k}}$ \\EndFor \\State \\Comment{ $\\textcolor{blue}{select\\ and\\ apply\\ action}$} \\State $x_t \\leftarrow \\arg\\max_k h(x,U_x)$ \\State Apply xt and observe $y_t$ and $r_t$ \\State \\Comment{ $\\textcolor{blue}{estimate\\ model}$} \\For{$k = 1, . . . , K$} \\State $\\hat\\theta_k \\leftarrow a_k / (α_k + β_k)$ \\EndFor \\State \\Comment{ select and apply action:} \\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$ \\State Apply $x_t$ and observe $r_t$ \\State \\Comment{ update distribution:} \\State $(α_{x_t}, β_{x_t}) \\leftarrow (α_{x_t} + r_t, β_{x_t} + 1 − r_t)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nNote we can model UCB using an urn model."
  },
  {
    "objectID": "posts/coursera/c1-w1.html#Sec-Thompson-Sampling",
    "href": "posts/coursera/c1-w1.html#Sec-Thompson-Sampling",
    "title": "The K-Armed Bandit Problem",
    "section": "Thompson Sampling",
    "text": "Thompson Sampling\nThompson sampling is basically like UCB but taking the Bayesian approach to the bandit problem. We start with a prior distribution over the action values and then update this distribution as we take actions. The action we choose is then sampled from the posterior distribution. This has the advantage that it is more robust to non-stationary problems than UCB. The downside is that it is more computationally expensive.\n\nThompson Sampling Algorithm\nThe algorithm is as follows:\n\n\n\\begin{algorithm} \\caption{BernTS(K, α, β)}\\begin{algorithmic} \\For{$t = 1, 2, . . .$} \\State \\State \\Comment{ sample model} \\For{$k = 1, . . . , K$} \\State Sample $\\hat\\theta_k \\sim beta(α_k, β_k)$ \\EndFor \\State \\Comment{ select and apply action:} \\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$ \\State Apply $x_t$ and observe $r_t$ \\State \\Comment{ update distribution:} \\State $(α_{x_t}, β_{x_t}) \\leftarrow (α_{x_t} + r_t, β_{x_t} + 1 − r_t)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\nthis is a tutorial on Thompson Sampling"
  },
  {
    "objectID": "posts/coursera/c1-w1.html#L3G7",
    "href": "posts/coursera/c1-w1.html#L3G7",
    "title": "The K-Armed Bandit Problem",
    "section": "Optimism in the face of uncertainty",
    "text": "Optimism in the face of uncertainty\n\nOptimism in the face of uncertainty\n\nThis is a heuristic to ensure initial exploration of all actions by assuming that untried actions have a high expected reward. We then try to exploit them but end up successively downgrading their expected reward when they do not match our initial optimistic assessment.\n\n\nThe downside to this approach is when the space of action is continuous so we can never get to the benefits of exploration."
  },
  {
    "objectID": "posts/coursera/c1-w1.html#coding-bandits-with-mesa",
    "href": "posts/coursera/c1-w1.html#coding-bandits-with-mesa",
    "title": "The K-Armed Bandit Problem",
    "section": "Coding Bandits with MESA",
    "text": "Coding Bandits with MESA\n\n\nCode\nfrom tqdm import tqdm\nfrom mesa import Model, Agent\nfrom mesa.time import RandomActivation\nimport numpy as np\n\n\n\nclass EpsilonGreedyAgent(Agent):\n    \"\"\"\n    This agent implements the epsilon-greedy \n    \"\"\"\n\n    def __init__(self, unique_id, model, num_arms, epsilon=0.1):\n        super().__init__(unique_id,model)\n        self.num_arms = num_arms\n        self.epsilon = epsilon\n        self.q_values = np.zeros(num_arms)  # Initialize Q-value estimates\n        self.action_counts = np.zeros(num_arms)  # Track action counts\n\n    def choose_action(self):\n        if np.random.rand() &lt; self.epsilon:\n            # Exploration: Choose random arm\n            return np.random.randint(0, self.num_arms)\n        else:\n            # Exploitation: Choose arm with highest Q-value\n            return np.argmax(self.q_values)\n\n    def step(self, model):\n        chosen_arm = self.choose_action()\n        reward = model.get_reward(chosen_arm)\n        assert reward is not None, \"Reward is not provided by the model\"\n        self.action_counts[chosen_arm] += 1\n        self.q_values[chosen_arm] = (self.q_values[chosen_arm] * self.action_counts[chosen_arm] + reward) / (self.action_counts[chosen_arm] + 1)\n\n\nclass TestbedModel(Model):\n    \"\"\"\n    This model represents the 10-armed bandit testbed environment.\n    \"\"\"\n\n    def __init__(self, num_arms, mean_reward, std_dev,num_agents=1):\n        super().__init__()\n        self.num_agents = num_agents\n        self.num_arms = num_arms\n        self.mean_reward = mean_reward\n        self.std_dev = std_dev\n        self.env_init()\n        #self.arms = [None] * num_arms  # List to store arm rewards\n        self.schedule = RandomActivation(self)\n        for i in range(self.num_agents):\n          self.create_agent(EpsilonGreedyAgent, i, 0.1) \n\n    def env_init(self,env_info={}):\n        self.arms = np.random.randn(self.num_arms)  # Initialize arm rewards\n\n    def create_agent(self, agent_class, agent_id, epsilon):\n        \"\"\"\n        Create an RL agent instance with the specified class and parameters.\n        \"\"\"\n        agent = agent_class(agent_id, self, self.num_arms, epsilon)\n        self.schedule.add(agent)\n        return agent\n\n    def step(self):\n        for agent in self.schedule.agents:\n            chosen_arm = agent.choose_action()\n            reward = np.random.normal(self.mean_reward, self.std_dev)\n            self.arms[chosen_arm] = reward  # Update arm reward in the model\n            agent.step(self)  # Pass the model instance to the agent for reward access\n\n    def get_reward(self, arm_id):\n        # Access reward from the stored list\n        return self.arms[arm_id]\n\n\n# Example usage\nmodel = TestbedModel(10, 0, 1)  # Create model with 10 arms\nnum_runs = 200                  # The number of times we run the experiment\nnum_steps = 1000                # The number of pulls of each arm the agent takes\n\n\n# Run simulation for multiple steps\nfor _ in tqdm(range(num_runs)):\n    for _ in range(num_steps):\n        model.step()\n    model.step()\n\n\n/home/oren/.local/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:\n\nThe AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.\nWe would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919\n\n  0%|          | 0/200 [00:00&lt;?, ?it/s]  4%|▍         | 9/200 [00:00&lt;00:02, 81.16it/s]  9%|▉         | 18/200 [00:00&lt;00:02, 78.80it/s] 13%|█▎        | 26/200 [00:00&lt;00:02, 77.88it/s] 17%|█▋        | 34/200 [00:00&lt;00:02, 78.48it/s] 21%|██        | 42/200 [00:00&lt;00:02, 78.66it/s] 26%|██▌       | 51/200 [00:00&lt;00:01, 79.00it/s] 30%|██▉       | 59/200 [00:00&lt;00:01, 78.95it/s] 34%|███▎      | 67/200 [00:00&lt;00:01, 78.94it/s] 38%|███▊      | 75/200 [00:00&lt;00:01, 79.14it/s] 42%|████▏     | 83/200 [00:01&lt;00:01, 79.37it/s] 46%|████▌     | 91/200 [00:01&lt;00:01, 79.55it/s] 50%|████▉     | 99/200 [00:01&lt;00:01, 79.04it/s] 54%|█████▍    | 108/200 [00:01&lt;00:01, 79.46it/s] 58%|█████▊    | 116/200 [00:01&lt;00:01, 78.05it/s] 62%|██████▏   | 124/200 [00:01&lt;00:00, 77.45it/s] 66%|██████▌   | 132/200 [00:01&lt;00:00, 75.88it/s] 70%|███████   | 140/200 [00:01&lt;00:00, 74.49it/s] 74%|███████▍  | 148/200 [00:01&lt;00:00, 73.76it/s] 78%|███████▊  | 156/200 [00:02&lt;00:00, 73.19it/s] 82%|████████▏ | 164/200 [00:02&lt;00:00, 72.36it/s] 86%|████████▌ | 172/200 [00:02&lt;00:00, 72.04it/s] 90%|█████████ | 180/200 [00:02&lt;00:00, 72.29it/s] 94%|█████████▍| 188/200 [00:02&lt;00:00, 72.33it/s] 98%|█████████▊| 196/200 [00:02&lt;00:00, 72.68it/s]100%|██████████| 200/200 [00:02&lt;00:00, 75.97it/s]"
  },
  {
    "objectID": "posts/coursera/c3-w2.1.html",
    "href": "posts/coursera/c3-w2.1.html",
    "title": "Constructing Features for Prediction",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/coursera/c3-w2.1.html#the-1000-step-random-walk-environment",
    "href": "posts/coursera/c3-w2.1.html#the-1000-step-random-walk-environment",
    "title": "Constructing Features for Prediction",
    "section": "The 1000 Step Random Walk Environment",
    "text": "The 1000 Step Random Walk Environment\nIn this lesson we implement the 1000 Random Walk example as an environment. This is good to demonstrate how to construct features for linear methods. We will use tile coding and coarse coding to create features for the Random Walk 1000 environment.\n\n\nCode\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass RandomWalk1000(gym.Env):\n    def __init__(self, num_states=1000, neighborhood_size=100, seed=None):\n        super().__init__()\n        self.num_states = num_states\n        self.neighborhood_size = neighborhood_size\n        self.observation_space = spaces.Discrete(num_states + 2) # add two states 0 and num_states + 1 as terminal states\n        self.action_space = spaces.Discrete(2)  # 0 for left, 1 for right\n        self.current_state = 500 # start in the middle\n        self.np_random, seed = gym.utils.seeding.np_random(seed)\n        self.trajectory = [500]\n\n    def reset(self, *, seed=None, options=None):\n        super().reset(seed=seed)\n        self.current_state = 500\n        self.trajectory = [500]\n        return self.current_state, {}\n\n    def step(self, action):\n\n        if action == 0: # move left\n             # left neighbours\n            left_start = max(1, self.current_state - self.neighborhood_size)\n            left_end = self.current_state\n            num_left = left_end - left_start\n\n            if left_start == 1:\n                prob_terminate_left = (self.neighborhood_size - num_left) / self.neighborhood_size\n            else:\n                prob_terminate_left = 0\n            \n            if self.np_random.random() &lt; prob_terminate_left:\n               \n                return 0, -1, True, False, {} # terminate left\n\n            next_state = self.np_random.integers(low=left_start, high=left_end)\n\n\n        elif action == 1: # move right\n             # right neighbours\n            right_start = self.current_state + 1\n            right_end = min(self.num_states + 1, self.current_state + self.neighborhood_size + 1)\n            num_right = right_end - right_start\n            if right_end == self.num_states + 1:\n                 prob_terminate_right = (self.neighborhood_size - num_right) / self.neighborhood_size\n            else:\n                prob_terminate_right = 0\n            \n            if self.np_random.random() &lt; prob_terminate_right:\n\n                return self.num_states + 1, 1, True, False, {} # terminate right\n\n            next_state = self.np_random.integers(low=right_start, high=right_end)\n        else:\n            raise ValueError(\"Invalid action\")\n\n        self.current_state = next_state\n\n        self.trajectory.append(self.current_state)\n        return self.current_state, 0, False, False, {} # not terminated or truncated\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_trajectory(trajectory, num_states):\n    \"\"\"Plots the trajectory of the random walk.\"\"\"\n    x = np.arange(len(trajectory))\n    y = np.array(trajectory)\n    \n    plt.figure(figsize=(12, 4))\n    plt.plot(x, y, marker='o', linestyle='-', markersize=3)\n    plt.xlabel('Time Step')\n    plt.ylabel('State')\n    plt.title('Random Walk Trajectory')\n    plt.yticks(np.arange(0, num_states+2, 100))\n    plt.grid(axis='y')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nCode\n#import gymnasium as gym\n#from random_walk_gym import RandomWalk1000\n\nenv = RandomWalk1000()\n\n# Reset the env\nobs, info = env.reset()\nterminated = False\n\nwhile not terminated:\n    # For this environment, an action is not needed.\n    # Here we pass in a dummy value\n    obs, reward, terminated, truncated, info = env.step(0)\n    print(f\"State: {obs + 1}, Reward: {reward}, Terminated: {terminated}\")\n\nenv.close()\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n\n\nState: 456, Reward: 0, Terminated: False\nState: 364, Reward: 0, Terminated: False\nState: 309, Reward: 0, Terminated: False\nState: 261, Reward: 0, Terminated: False\nState: 198, Reward: 0, Terminated: False\nState: 174, Reward: 0, Terminated: False\nState: 115, Reward: 0, Terminated: False\nState: 109, Reward: 0, Terminated: False\nState: 21, Reward: 0, Terminated: False\nState: 12, Reward: 0, Terminated: False\nState: 1, Reward: -1, Terminated: True\n\n\n\n\n\n\n\n\n\n\n\nCode\nenv = RandomWalk1000(num_states=1000, neighborhood_size=100)\nobs, info = env.reset()\nterminated = False\ntruncated = False\ntrajectory = []\nwhile not terminated and not truncated:\n    action = env.action_space.sample()  # Replace with your policy\n    obs, reward, terminated, truncated, info = env.step(action)\n    print(f'{obs=}, {action=}, {reward=}, {terminated=}')\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n\n\nobs=457, action=0, reward=0, terminated=False\nobs=414, action=0, reward=0, terminated=False\nobs=456, action=1, reward=0, terminated=False\nobs=531, action=1, reward=0, terminated=False\nobs=589, action=1, reward=0, terminated=False\nobs=635, action=1, reward=0, terminated=False\nobs=543, action=0, reward=0, terminated=False\nobs=576, action=1, reward=0, terminated=False\nobs=647, action=1, reward=0, terminated=False\nobs=680, action=1, reward=0, terminated=False\nobs=772, action=1, reward=0, terminated=False\nobs=744, action=0, reward=0, terminated=False\nobs=676, action=0, reward=0, terminated=False\nobs=730, action=1, reward=0, terminated=False\nobs=654, action=0, reward=0, terminated=False\nobs=618, action=0, reward=0, terminated=False\nobs=543, action=0, reward=0, terminated=False\nobs=469, action=0, reward=0, terminated=False\nobs=569, action=1, reward=0, terminated=False\nobs=482, action=0, reward=0, terminated=False\nobs=492, action=1, reward=0, terminated=False\nobs=401, action=0, reward=0, terminated=False\nobs=336, action=0, reward=0, terminated=False\nobs=316, action=0, reward=0, terminated=False\nobs=286, action=0, reward=0, terminated=False\nobs=340, action=1, reward=0, terminated=False\nobs=350, action=1, reward=0, terminated=False\nobs=288, action=0, reward=0, terminated=False\nobs=319, action=1, reward=0, terminated=False\nobs=259, action=0, reward=0, terminated=False\nobs=290, action=1, reward=0, terminated=False\nobs=335, action=1, reward=0, terminated=False\nobs=272, action=0, reward=0, terminated=False\nobs=313, action=1, reward=0, terminated=False\nobs=355, action=1, reward=0, terminated=False\nobs=278, action=0, reward=0, terminated=False\nobs=203, action=0, reward=0, terminated=False\nobs=150, action=0, reward=0, terminated=False\nobs=240, action=1, reward=0, terminated=False\nobs=152, action=0, reward=0, terminated=False\nobs=66, action=0, reward=0, terminated=False\nobs=111, action=1, reward=0, terminated=False\nobs=157, action=1, reward=0, terminated=False\nobs=58, action=0, reward=0, terminated=False\nobs=0, action=0, reward=-1, terminated=True\n\n\n\n\n\n\n\n\n\nLets simulate the random walk till success and plot its trajectory.\n\n\nCode\nenv = RandomWalk1000(num_states=1000, neighborhood_size=100)\nobs, info = env.reset()\nterminated = False\ntruncated = False\nwhile not terminated and not truncated:\n    action = env.action_space.sample()  # Replace with your policy\n    obs, reward, terminated, truncated, info = env.step(action)\n\n\nplot_trajectory(env.trajectory, num_states=env.num_states)"
  },
  {
    "objectID": "posts/coursera/c3-w2.1.html#a-short-digression-on-einstein-tiling-for-rl",
    "href": "posts/coursera/c3-w2.1.html#a-short-digression-on-einstein-tiling-for-rl",
    "title": "Constructing Features for Prediction",
    "section": "A short digression on Einstein Tiling for RL",
    "text": "A short digression on Einstein Tiling for RL\nOne (awful) Idea I keep returning to is to use Einstein Tiling for RL. I mentino that Einstein in this context is not the physicist but rather a pun on the word ‘Einstein’ which means ‘one stone’ in German.\nLet’s quickly review why it is a bad idea, and then why it is also a fascinating idea.\n\nUnlike A square tiling this is an aperiodic tiling so we need to generate it efficiently. Depending on the space it will may take some time to generate the tiling. We need to store the tiling in memory. For a square tiling we can generate the tiling in a few lines of code. We can access the tiling or tile using a simple formula.\nWe need a quick way to find which tile a point is in. This is not to hard for one tile. But as the number of tiles increases this becomes more difficult. It is trivial for a square tiling where again we have a formula to efficiently determine the tile a point belongs to.\n\nSome reasons why it is a fascinating idea.\n\nWe only need one tiling. If we have one we can map the first tile ton any other location it is in the same orientation and we will get a new tiling! This is due to the aperiodic nature of the tiling.\nThe hat tile is constructed by glueing together eight smaller kite tiles that are sixth of a hexagon. We can easily use larger kites that so we can use two such grids as coarse and coarser tilings.\nDifferent can be similar locally but will tend to diverge. This suggest that we will get a good generalization.\nThere may be variant einsteins that are easier to generate and use\nIn https://www.ams.org/journals/proc/1995-123-11/S0002-9939-1995-1277129-X/ the authors show that for d&gt;=3 aperiodic tilings can naturally avoid more symmetries than just translations. I.e. we can have a periodic tilings in higher dimensions.\n\nI may be wrong but It may be possible to generate the tiling using a simple formula. Ok so far tiling generation is insanely complicated. Though this is not a judgment on the complexity of the tiling but rather the complexity of the code and mathematics to generate the tiling.\nThe hat tile allows one to create many different tilings of the state space in two dimensions. Each tiling is going to have a different set of features.\nAs the hat tile is constructed by glueing together eight smaller tiles. Tilings are created in a hierarchical manner. This suggests that we can will get fine and course feature in this process and that we can just keep going to increase discrimination.\nSome issues - it is possible to get two tilings that are the same but for a ‘conway worm’ this is a curve in the tiling that is different. The problem here is that the features will be the same for every where except the worm. Not good for generalization."
  },
  {
    "objectID": "posts/coursera/c2-w3.html",
    "href": "posts/coursera/c2-w3.html",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/coursera/c2-w3.html#lesson-1-td-for-control",
    "href": "posts/coursera/c2-w3.html#lesson-1-td-for-control",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Lesson 1: TD for Control",
    "text": "Lesson 1: TD for Control\n\nLesson Learning Goals\n\nExplain how generalized policy iteration can be used with TD to find improved policies #\nDescribe the Sarsa Control algorithm #\nUnderstand how the Sarsa control algorithm operates in an example MDP #\nAnalyze the performance of a learning algorithm in an MDP #"
  },
  {
    "objectID": "posts/coursera/c2-w3.html#sec-l1g1",
    "href": "posts/coursera/c2-w3.html#sec-l1g1",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Generalized Policy Iteration with TD",
    "text": "Generalized Policy Iteration with TD\nwe would like now to combine TD with a planning algorithm to use TD for control. We This will be a GPI algorithm.\n\nGeneralized Policy Iteration - Recap\nlets recap the Generalized Policy Iteration (GPI) algorithm:\n\nPolicy Evaluation: Update the value function V to be closer to the true value function of the current policy\nPolicy Improvement: Improve the policy \\pi based on the current value function V\nGeneralized Policy Iteration: Repeated steps of policy evaluation and policy improvement\nGPI does not require full evaluation of the value function, just an improvement can be used to update the policy.\npolicy iteration\n\nrun policy evaluation to convergence\ngreedifing the policy\n\nGPI MC\n\neach episode:\n\npolicy evaluation (nota full evaluation)\nimprovement per episode\n\n\nGPI TD\n\neach step:\n\npolicy evaluation (for just one action)\nimprovement pi after the single time step.\n\n\n\n\nRecall how in the first course we saw DP methods for solving MDPs using the four part dynamic function and its variants. We used the Bellman equation to write down a system of linear equations for the value function and solve them exactly. We then used the value function to find the optimal policy. So in DP we don’t need to interact with the environment or to learn. We can compute the value function and the optimal policy exactly.\nIn these course we relaxed the assumption of knowing the transition dynamics or the expected returns. This creates a new challange of learning V or Q from experience.\nIn the first lesson we saw how MC methods can help us learn the value function but with the caveat that we need to wait until the end of the episode to update the value function.\nHowever we have now seen how the TD(0) algorithm uses recursive nature of the Bellman equation for the value function to make approximate updates to the value function. This allows us to learn Values of states directly from experience.\nOnce we are able to approximate the value function, we can use it to create new generalized policy iteration algorithms. This part of the GPI remains the same, we still evaluate the policy and improve it. But now we can do this in an online fashion, updating the value function after each step.\nIn SARSA we are making updates to the policy after a single step - this may lead to much faster convergence to the optimal policy. It also allows us to improve our plans during an episode or in a continuing task.\nThe advantage of TD methods is that they can be used in continuing tasks, where the agent interacts with the environment indefinitely. This is because the value function is updated after each step, and the agent can continue to learn and improve its policy as it interacts with the environment. But this advantage is better understood by considering the episodic tasks, where the agent can learn during an episode that consequences of its actions are sub optimal. This allows TD(0) based GPI to make more frequent updates to the policy within one episode."
  },
  {
    "objectID": "posts/coursera/c2-w3.html#sec-l1g2",
    "href": "posts/coursera/c2-w3.html#sec-l1g2",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Sarsa: On-policy TD Control",
    "text": "Sarsa: On-policy TD Control\nNext we consider how we can derive and use a similar approximate updating of the action-value function to learn the action-value function directly from experience.\nlets recap the Bellman equation for the action-value function:\n\n\\begin{aligned}\nq_\\pi(s,a) & \\dot = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\qquad \\newline\n& = \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\sum_{a'}\\pi(a' \\mid s') q_\\pi(s', a')] \\qquad\n\\end{aligned}\n\\tag{1}\nWe can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.\n\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n\\tag{2}\n\n\n\n\n\n\nSARSA\n\n\n\n\n\n\\begin{algorithm} \\caption{SARSA($\\alpha,\\epsilon$)}\\begin{algorithmic} \\State Initialize: \\State $\\qquad Q(s,a) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S^+}, a\\in\\mathcal{A}(s)\\ except\\ for\\ Q(terminal,\\cdot)=0$ \\For {each episode e:} \\State Initialize $S$ \\State Choose A from S using policy derived from Q (e.g., $\\epsilon$-greedy) \\For {each step of e} \\State Take action A, observe R, S' \\State Choose A' from S' using policy derived from Q (e.g., $\\epsilon$-greedy) \\State $Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma Q(S',A') - Q(S, A)]$ \\State $S \\leftarrow S'$; $A \\leftarrow A'$ \\EndFor \\State until $S$ is terminal \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\nThe SARSA algorithm is due to Rummery, Gavin Adrian, and Mahesan Niranjan. The name Sarsa is due to Rich Sutton and comes from the fact that the algorithm uses the tuple (S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}) to update the action-value function. [@Rummery1994OnlineQU]\nSARSA is a sample-based algorithm to solve the Bellman equation for action-values. - It picks an action based on the current policy and then - It policy evaluation by a TD updates of Q the action-value function based on the reward and the next action. - Then it does a policy improvement."
  },
  {
    "objectID": "posts/coursera/c2-w3.html#sec-l1g3",
    "href": "posts/coursera/c2-w3.html#sec-l1g3",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "SARSA in an Example MDP",
    "text": "SARSA in an Example MDP\n\n\n\n\nwindy gridworld\n\nIn this grid world isn’t a good fit for MC methods as most policies never terminate. This is because the agent is pushed up by the wind and has to learn to navigate to the goal. Anyhow if the episode never terminates MC wont be able to update the value function.\nBut Sarsa can learn the optimal policy in this environment by abandoning the epsilon greedy policy in mid episode and finding better policies. We can see that early episodes take longer to terminate after the e-greedy policy stops peaks."
  },
  {
    "objectID": "posts/coursera/c2-w3.html#sec-l1g4",
    "href": "posts/coursera/c2-w3.html#sec-l1g4",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Performance of Learning Algorithms in an MDP",
    "text": "Performance of Learning Algorithms in an MDP\nOn the right side of the figure we see the performance of the learning algorithms in the windy grid world. We see that in this chart the Sarsa algorithm learns the optimal policy at Around step 7000 where the gradient becomes constant.\nQ. why is SARSA called an on-policy algorithm?\nthis is because it learns by sampling from the policy induced by Q while following the same policy \\pi."
  },
  {
    "objectID": "posts/coursera/c2-w3.html#lesson-2-off-policy-td-control-q-learning",
    "href": "posts/coursera/c2-w3.html#lesson-2-off-policy-td-control-q-learning",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Lesson 2: Off-policy TD Control: Q-learning",
    "text": "Lesson 2: Off-policy TD Control: Q-learning\n\nLesson Learning Goals\n\nDescribe the Q-learning algorithm #\nExplain the relationship between q-learning and the Bellman optimality equations. #\nApply q-learning to an MDP to find the optimal policy #\nUnderstand how Q-learning performs in an example MDP #\nUnderstand the differences between Q-learning and Sarsa #\nUnderstand how Q-learning can be off-policy without using importance sampling #\nDescribe how the on-policy nature of SARSA and the off-policy nature of Q-learning affect their relative performance #\n\n\nlets recap the Bellman optimality equation for the action-value function:\n\n\\begin{aligned}\nq_{\\star}(s,a) = \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\max_{a'} q_{\\star}(s', a')]\n\\end{aligned}\n\\tag{3}\nThe following is an update rule for Q-learning:\nWe can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.\n\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a') - Q(S_t, A_t)]\n\\tag{4}\n\n\n\n\n\n\nQ-learning (off-policy TD control)\n\n\n\n\n\n\\begin{algorithm} \\caption{Q-learning Off-policy TD control}\\begin{algorithmic} \\State Initialize: \\State $\\qquad Q(s,a) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S^+}, a\\in\\mathcal{A}(s)\\ except\\ for\\ Q(terminal,\\cdot)=0$ \\For {each episode e:} \\State Initialize $S$ \\For {each step of e} \\State Choose A from $A_B(S)$ using any ergodic Behavioural policy $B$ - perhaps the $\\epsilon$-greedy induced by Q. \\State Take action A, observe R, S' \\State $Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\max_a Q(S',a) - Q(S, A)]$ \\State $S \\leftarrow S'$ \\EndFor \\State until $S$ is terminal \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nNote: I made some cosmetic changes to the psuedo code in the book to resolve the confusion I had about nature the behavioral policy.\n\n\n\n\n\n\n\n\nThe behavioral policy in Q-learning\n\n\n\nQ-learning has a subtle issue I found confusing at first.\nHere I first state the issue:\nWhat is the behavioral policy we follow in these three td-learning algorithms when we sample the next action to follow?\nWe are not writing here that Q function is Q_\\pi but the value functions are by definitions expectations under some policy. In these algorithms we keep updating the Q function using TD(0) updates. If we update the Q function in a sense that the best action changes at a given step then the updated function now uses a new policy. (In the case of Sarsa we can actually get a worse policy after the update.) I figured this out very quickly.\nA fully specified Q-functions isn’t just defined by following a policy. It also induces a policy. This in generaly is a stochastic policy. But if we take the greedy action with arbitrary tie breaks we get one or more deterministic policies. So it seems that off policy algorithms like Q-learning and Expected Sarsa are following a sequence of policies that are induced by the Q function that is being learned.\nIn general off policy learning may be using S,A,R sequences that have been sampled like we clearly did in MC. So the question which arises is can sample from any ergodic policy as our behavioral policy in these off-policy algorithms or are we supposed to learn from experience and sample using the policy induced by latest and greatest Q function that we are learning?\nLuckily Martha White is very clear about this:\n\nThe target policy is the easy part - we are targeting Q_{\\pi_\\star}.\nThe behavior policy is the policy can be any policy so long as it is ergodic.\n\nUsing an \\epsilon-greedy policy derived from Q is very logical choice but we could use any other policy.\n\n\n\n\n\n\n\n\n\n\nDo these algorithms converge?\n\n\n\n\nIs Q-learning guaranteed to converge ?\nThe course glossed over this in lectures perhaps referencing the text book – I will have to go back and check this.\nHowever as this is introductory CS and not Mathematics I will try to suspend my disbelief that the algs are guarenteed to converge and return to the point.\n\n\n\n\nIt is an off-policy algorithm.\n\n\nThe target policy is the easy part - we are targeting Q_{\\pi_\\star}.\nThe behavior policy is the policy that we are following but what is that ?\n\nit is clearly not Q_{\\pi_\\star} as we don’t know it yet.\nwe initialized Q(s,a) arbitrarily - so we may have a uniform random policy.\nbu we actual have any random policy.\n\nany action that is a legit transition from the current state is a valid action.\nso long as their probabilities add up to 1.\n\nlater Martha keeps saying that we need the ergodicity of the MDP to ensure that out policy will visit all states and actions with non-zero probability.\nthis anyhow is one source of confusion.\nhowever an epsilon greedy policy of the induced policy from Q seems like a very good choice. Can we do better ?\nanother point to consider here is that this is a value iteration algorithm.\n\nwhat can we say about the inermediate Q functions that we are learning ?\nare they even a valid action value function ?\nis the policy they induce a coherent probability distribution over actions ?\n\n\n\n\nWhen we select the action A’ what policy are we using ?\n\n\nwe are clearly not using the policy that we are learning \\pi_\\star - we dont know it yet.\nwe are could use an \\epsilon-greedy policy by greedifying Q. this seems the most logical\nbut we could pretty much use any other policy.\nthis is because Q-learning is an off-policy algorithm.\nthe confusion arises because it is not clear what “any policy derived from Q” means in the algorithm.\nq-learning\n\nis a value iteration algorithm\nuses the Bellman optimality equation to update the action-value function.\nselects the action based on greedyfing the current q-values and then\n\nit policy evaluation by a TD updates of Q the action-value function based on the reward and the next action.\n\n\n\n\n\n\n\nwhy can’t q-learning account for the consequences of exploration in its policy ?\n\n\n\nq-learning learns the optimal policy but follows a some other policy. Let suppose the optimal policy is deterministic. And let’s suppose that the behavior policy is epsilon greedy based on that.\nThe alg does not follow the optimal policy - it follows the behavior policy and this will perform much worse because of exploration.\nIf we need to account for the consequences of exploration in the policy we need to use a different algorithm!"
  },
  {
    "objectID": "posts/coursera/c2-w3.html#sec-l2g4",
    "href": "posts/coursera/c2-w3.html#sec-l2g4",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Q-learning in an Example MDP",
    "text": "Q-learning in an Example MDP\n\n\n\n\nwindy gridworld\n\nIn this grid world isn’t a good fit for MC methods as most policies never terminate.\nQ-learning can learn the optimal policy in this environment by abandoning the epsilon greedy policy in mid episode and finding better policies.\nWe can see that early episodes take longer to terminate after the e-greedy policy stops peaks.\nHowever Q-learning does not take seem to factor in the consequences of exploration in its policy.\nThis is because it is learning the optimal policy and not the policy that it follows.\nQ-learning does not need Importance sampling to learn off-policy. This is because it is learning action values."
  },
  {
    "objectID": "posts/coursera/c2-w3.html#sec-l2g5",
    "href": "posts/coursera/c2-w3.html#sec-l2g5",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Comparing Sarsa and Q-learning",
    "text": "Comparing Sarsa and Q-learning\nQ-learning is an off-policy algorithm:\n\nthe target policy is the optimal policy since the update rule approximates the Bellman optimality equation.\nthe behavior policy is initially given updated at each step from the inital get updated a bit towards the optimal policy at each step.\n\nbecause it is learning \\pi_* (the optimal policy) but it samples a different policy.\nThis is in contrast to Sarsa, which is an on-policy algorithm because it learns the policy that it follows."
  },
  {
    "objectID": "posts/coursera/c2-w3.html#lesson-3-expected-sarsa",
    "href": "posts/coursera/c2-w3.html#lesson-3-expected-sarsa",
    "title": "Temporal Difference Learning Methods for Control",
    "section": "Lesson 3: Expected SARSA",
    "text": "Lesson 3: Expected SARSA\n\nLesson Learning Goals\n\nDescribe the Expected SARSA algorithm #\nDescribe Expected SARSA’s behavior in an example MDP #\nUnderstand how Expected SARSA compares to SARSA control #\nUnderstand how Expected SARSA can do off-policy learning without using importance sampling #\nExplain how Expected SARSA generalizes Q-learning #\n\n\n\n\\begin{aligned}\nQ(S_t, A_t) & \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\mathbb{E}[Q(S_{t+1}, A_{t+1})] - Q(S_t, A_t)] \\newline\n& \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\sum_a \\pi(a|S_{t+1}) \\cdot Q(S_{t+1}, a) - Q(S_t, A_t)]\n\\end{aligned}\n\n\nUnderstanding Expected Sarsa\nlets recap the Bellman equation for the action-value function:\n\n\\begin{aligned}\nq_\\pi(s,a) & \\dot = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\qquad \\newline\n& = \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\sum_{a'}\\pi(a' \\mid s') q_\\pi(s', a')] \\qquad\n\\end{aligned}\n\\tag{5}\nWe can now use the same idea of bootstrapping to update the action-value function after each step. This is basis of Sarsa algorithm.\nin the sarsa update rule:\n\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n\\tag{6}\nwe knows the policy \\pi so we can make a better update by replacing the sampled next action with the expected value of the next action under the policy \\pi.\nThis is the basis of Expected Sarsa.\nwhich uses the update rule:\n\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\sum_a \\pi(a|S_{t+1}) \\cdot Q(S_{t+1}, a) - Q(S_t, A_t)]\n\\tag{7}\nOtherwise the algorithm is the same as Sarsa.\n\nThis target is more stable than the Sarsa target because it is less noisy.\nThis makes it converge faster than Sarsa.\n\nthis has a has a down side - it has more computation than Sarsa due to avaraging over many actions for every step.\n\n\n\n\n\n\nExpected Sarsa\n\n\n\n\n\n\\begin{algorithm} \\caption{expected SARSA}\\begin{algorithmic} \\State Initialize: \\State $\\qquad Q(s,a) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S^+}, a\\in\\mathcal{A}(s)\\ except\\ for\\ Q(terminal,\\cdot)=0$ \\For {each episode e:} \\State Initialize $S$ \\State Choose A from S using policy derived from Q (e.g., $\\epsilon$-greedy) \\For {each step of e} \\State Take action A, observe R, S' \\State Choose A' from S' using policy derived from Q (e.g., $\\epsilon$-greedy) \\State $Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\sum_a \\pi(a|S') Q(S',A') - Q(S, A)]$ \\State $S \\leftarrow S'$; $A \\leftarrow A'$ \\State $S$ is terminal \\EndFor \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\n\nSummary - Connecting the dots\n\n\n\n\nExpected Sarsa is a generalization of Q-learning and Sarsa.\nThere are many RL algorithms in our specialization flowchart.\nWe would like to find a few or even one algorithm that may be widely applicable to many different settings, carrying over the insights we learned from each new algorithm.\nThe first step in this direction was introducing the epsilon parameter to the bandit algorithms, which allowed us to treat the exploration-exploitation trade-off. We have seen additional strategies for exploration but we have been using either an \\epsilon-greedy strategy or a epsilon-soft strategy in most algorithms.\n\nWe also got the powerful idea of using confidence intervals as tie breakers in the case of multiple actions with the same expected reward.\n\nAnother step in this direction was to introduce discounting of rewards which let us parameter discounting with \\gamma and thus treat episodic and continuing tasks in the same way.\nThe MC algorithms showed that this is not enough to fully generalize to episodic and continuing tasks. However we got a powerful new ideas of inverse sampling and doubly robust estimators. For use with off-policy learning.\nNext we introduced GPI in which we combined policy evaluation and policy improvement algorithms to iteratively approximate the optimal policy in a single algorithm.\nAnother lesson was to using the TD error to bootstrap the value function. This let us update value functions after each step, rather than waiting until the end of the episode, increasing the data efficiency of the algorithms.\n\nWe also saw that we can use this idea with action-value functions, which is more fine grained than the value function and can lead to more efficient learning.\n\nNext we saw that Expected Sarsa is one such algorithm that can be used in many different settings.\n\nIt can be used in episodic and continuing tasks,\nIt can be used for on-policy and off-policy learning.\nIt is a GPI algorithm that uses the TD error to update the action-value function. And it the \\epsilon-greedy strategy implicit in its action-value function."
  },
  {
    "objectID": "posts/coursera/c3-w3.html",
    "href": "posts/coursera/c3-w3.html",
    "title": "Control with Approximation",
    "section": "",
    "text": "RL logo\n\n\n\n\n\n\n\nRL algorithm decision tree\n\n\n\n\nFigure 1: The algorithms we will be discussing in this lesson are function approximation versions of SARSA, Expected SARSA, and Q-learning. These are all sample-based algorithms that solve the Bellman equation for action-values. They differ in how they estimate the action-values and how they update them."
  },
  {
    "objectID": "posts/coursera/c3-w3.html#episodic-sarsa-with-function-approximation-video",
    "href": "posts/coursera/c3-w3.html#episodic-sarsa-with-function-approximation-video",
    "title": "Control with Approximation",
    "section": "Episodic SARSA with function approximation (Video)",
    "text": "Episodic SARSA with function approximation (Video)\nIn this video, Adam White, discusses the algorithm for “Episodic SARSA with function approximation”. He explains how it can be used to solve reinforcement learning problems with large or continuous state-spaces. He also delineates the importance of feature choices in this algorithm and how they can impact the performance of the system."
  },
  {
    "objectID": "posts/coursera/c3-w3.html#sec-l1g2",
    "href": "posts/coursera/c3-w3.html#sec-l1g2",
    "title": "Control with Approximation",
    "section": "Two ways to construct action dependent features?",
    "text": "Two ways to construct action dependent features?\n\n\n\n\n\n\n\nStacking\n\n\n\n\nFigure 2: Stacking involves concatenating the state features with the action features\n\n\n\n\n\n\n\n\nPassing\n\n\n\n\nFigure 3: Passing Actions to Features involves passing the state features through a neural network that also takes the action as input\n\n\n\nWe see two techniques for constructing action dependent features. It is worthwhile noting that these two techniques are quite generally used in Machine Learning. For example Concatenating is used in CNN and with multi-head attention in Transformers.\n\nStacking involves concatenating the state features with the action features. This is a simple and effective way to construct action dependent features.\n\nStacking can used both for linear function approximation and for neural networks in much the same way. Stacking is very simple but it has a problem, it keeps the same state features used for different actions separate. This seems to be both an over-parameterization (i.e. overfitting) and a an impediment to learning a good representation of the state.\n\nPassing Actions to Features attempts to remedy this issue. In this technique, the state features are passed through a neural network that also takes the action as input. This allows the network to learn a better representation of the state that is dependent on both the state and the action. This technique is more complex but can lead to better performance."
  },
  {
    "objectID": "posts/coursera/c3-w3.html#how-to-use-sarsa-in-episodic-tasks-with-function-approximation",
    "href": "posts/coursera/c3-w3.html#how-to-use-sarsa-in-episodic-tasks-with-function-approximation",
    "title": "Control with Approximation",
    "section": "How to use SARSA in episodic tasks with function approximation",
    "text": "How to use SARSA in episodic tasks with function approximation\nNext we will see how to use SARSA in episodic tasks with function approximation. The main idea is to use a similar update rule as before, but with the action value function approximated by a function approximator, i.e. it will be parametrized by weights w. Also we will need to add a the gradient of the to the update rule. As we will want to move the weights in the direction of the gradient of the action value function.\n\n\n\n\n\n\nSARSA\n\n\n\n\n\n\\begin{algorithm} \\caption{SARSA($\\alpha,\\epsilon$)}\\begin{algorithmic} \\State Initialize: \\State $\\qquad Q(s,a) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S^+}, a\\in\\mathcal{A}(s)\\ except\\ for\\ Q(terminal,\\cdot)=0$ \\For {each episode e:} \\State Initialize $S$ \\State Choose A from S using policy derived from Q (e.g., $\\epsilon$-greedy) \\For {each step of e} \\State Take action A, observe R, S' \\State Choose A' from S' using policy derived from Q (e.g., $\\epsilon$-greedy) \\State $Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma Q(S',A') - Q(S, A)]$ \\State $S \\leftarrow S'$; $A \\leftarrow A'$ \\EndFor \\State until $S$ is terminal \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\nSARSA is a sample-based algorithm to solve the Bellman equation for action-values.\n\nIt picks an action based on the current policy and then\nIt policy evaluation by a TD updates of Q the action-value function based on the reward and the next action.\nThen it does a policy improvement.\n\n\n\n\n\n\n\n\nEpisodic SARSA with function approximation\n\n\n\n\nFigure 4: This figure shows the algorithm for Episodic semi-gradient SARSA which uses function approximation. The update rule is modified from tabular case, with an approximate action value function \\hat{q}(s,a,\\mathbf{w}) as well as the inclusion of the gradient of the action value function \\nabla \\hat{q} (S_t, A_t, \\mathbf{w}) . These two modification allows us to update the weights of the function approximator The rest of the algorithm remains unchanged.\n\n\n[@Rummery1994OnlineQU] introduced SARSA, but the name is due to Rich Sutton.\n\n\n\n\n\n\nEpisodic Semi-gradient SARSA\n\n\n\n\n\n\\begin{algorithm} \\caption{Episodic Semi-gradient SARSA for estimating ($\\hat{q} \\approx q_*$)}\\begin{algorithmic} \\State $\\textbf{Input:}$ \\State $\\qquad \\text{a differentiable action-value fn parameterization } \\hat{q}: \\mathcal{S} \\times A \\times \\mathbb{R}^d \\to \\mathbb{R}$ \\State $\\textbf{Initialize:}$ \\State $\\qquad$ value function weights $\\mathbf{w} \\in \\mathbb{R}^d$ arbitrarily, (e.g., $\\mathbf{w} = 0$) \\For {each episode e:} \\State Initialize $S$ \\State Choose A from S using policy derived from $\\color{red}\\hat{q}(S'A,\\mathbf{w})$ (e.g., $\\epsilon$-greedy) \\For {each step of e} \\State Take action A, observe R, S' \\If {$S'$ is terminal} \\State $\\color{red}\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ R - \\hat{q}(S, A, \\mathbf{w}) \\right] \\nabla \\hat{q}(S, A, \\mathbf{w})$ \\State Go to next episode \\EndIf \\State Choose A' as a function of $\\color{red}\\hat{q}(S'A,\\mathbf{w})$ (e.g., $\\epsilon$-greedy) \\State $\\color{red}\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ R + \\gamma \\hat{q}(S', A', \\mathbf{w}) - \\hat{q}(S, A, \\mathbf{w}) \\right] \\nabla \\hat{q}(S, A, \\mathbf{w})$ \\State $S \\leftarrow S'$; \\State $A \\leftarrow A'$ \\EndFor \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\nThis allows us to learn a good policy for the task. We will also need a step to update the weights.\n\n\n\n\n\n\n\nFigure 5: Talk titled ‘Advances in Value Estimation in Reinforcement Learning’ at London Machine Learning Meetu by Martha White on Two Pieces of Research on Exploration in Reinforcement Learning. On work from [patterson2024generalizedprojectedbellmanerror]\n\n\n\n\n\n\n\n\nOver Thinking Semi-gradient SARSA {.unnumbered}:\n\n\n\nSo you think you understand SARSA with function approximation?\n\nWhy is this a semi-gradient method?\nHints:\n\nwhat is the MSE td error of the Q-value the next state?\n\n\nIf we define the MSE of the Q-value of the next state as the projected Bellman error, then we can see that the update rule is a projected gradient descent on the projected Bellman error. This is a semi-gradient method because we are not using the true value of the next state in the update rule.\n\n\\overline{{QE}}(w) \\doteq \\sum_{s\\in \\mathcal{S},a\\in \\mathcal{A}} \\mu(s)\\left[  q^*(s,a) - \\hat{q}(s,a,\\mathbf{w})  \\right]^2\n\nsince we dont know the action value function we can’t compute the true value of the next state. Instead we use the value of the next state.\n\n\\begin{align*}\n  \\overline{{QE}}(w) &\\approx \\sum_{s\\in \\mathcal{S},a\\in \\mathcal{A}} \\mu(s)\\left[ r+\\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w})  \\right]^2 \\\\\n  &= \\mathbb{E}_\\mu \\left[ (r+\\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}))^2 \\right]\n\\end{align*}\n\nthen we have the following objective function:\n J(w) = \\mathbb{E}_\\mu \\left[ \\sum_{t=0}^\\infty \\gamma^t  \\left( r_t + \\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}) \\right)^2 \\right]\n\n\nwhere:\n\n\\mu is the state visitation distribution\n\\hat{q}(s,a,\\mathbf{w}) is the approximated action value function\n\\mathbf{w} is the weight vector\nr_t is the reward at time step t\n\\gamma is the discount factor\n\n\n\n\\begin{align*}\n\\nabla_\\mathbf{w} J(w) &= \\mathbb{E}_\\mu \\left[ \\sum_{t=0}^\\infty \\gamma^t \\nabla_\\mathbf{w} \\left( r_t + \\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}) \\right)^2 \\right] \\\\\n&= \\mathbb{E}_\\mu \\left[ \\sum_{t=0}^\\infty \\gamma^t 2 \\left( r_t + \\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}) \\right) \\nabla_\\mathbf{w} \\left( r_t + \\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}) \\right) \\right] \\newline\n&= \\mathbb{E}_\\mu \\left[ \\sum_{t=0}^\\infty \\gamma^t 2 \\delta  \\left( \\gamma \\nabla_\\mathbf{w} \\hat{q}(s',a',\\mathbf{w}) - \\nabla_\\mathbf{w} \\hat{q}(s,a,\\mathbf{w}) \\right) \\right] \\\\\n\\end{align*}\n\nwhere \\delta = r_t + \\gamma \\hat{q}(s',a',\\mathbf{w}) - \\hat{q}(s,a,\\mathbf{w}) is the TD error.\nthis is the projected gradient of the projected Bellman error.\nwhich would give us the following update rule:\n\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\delta \\left( \\gamma \\nabla_\\mathbf{w} \\hat{q}(s',a',\\mathbf{w}) - \\nabla_\\mathbf{w} \\hat{q}(s,a,\\mathbf{w}) \\right)\n\nrather than the update rule we have in the algorithm:\n\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\delta \\nabla_\\mathbf{w} \\hat{q}(s,a,\\mathbf{w})\n\n\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R_{t+1} + \\gamma \\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}) - \\hat{q}(S_t,A_t,\\mathbf{w})] \\nabla \\hat{q}(S_t,A_t,\\mathbf{w})\n\n\nCan we do better than this?\n\nHint: using a projected Bellman error, we may get a better algorithm than SARSA with function approximation.\n\nDo we have any convergence guarantees for SARSA with function approximation?\n\nIn the text book [@sutton2018reinforcement sec. 10.1] the authors state that for a constant policy 1, this method converges in the same way that TD(0) does, with the same kind of error bound. Then they go on to say that for a non-constant policy, the convergence is a matter of ongoing research.\nFor the tabular setting, [@singh2000convergence] the authors show that the algorithm has asymptotic converges to the optimal policy provided that the policies from the policy improvement operator is “greedy in the limit with infinite exploration”\nFor the function approximation setting, Empirical results show that the linear SARSA can chatter, i.e. the weight vector does not go to infinity (i.e., it does not diverge) but oscillates in a bounded region.\n\nSince it is semi-gradient, we are not using the true value of the next state in the update rule.\nAre we biased?\nDoes this algorithm have lower variance than the TD(0) algorithm?\nWhen we choose the next action, are we on policy or off policy?\nIn the non terminal rule we use two samples, but they are highly correlated. Why is this a problem\nHow can we reduce this correlation?\nWhat is the policy improvement operator?\nWhat is the Lipschitz constant of this operator and why don’t we see this in the SARSA algorithm?"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#sec-l1g1",
    "href": "posts/coursera/c3-w3.html#sec-l1g1",
    "title": "Control with Approximation",
    "section": "The update for Episodic SARSA with function approximation",
    "text": "The update for Episodic SARSA with function approximation\nSo far we have only been using function approximation to parametrize state value function,\n\nV_\\pi(s) ≈ \\hat{v}(s,w) \\doteq \\mathbf{w}^T \\cdot \\mathbf{x}(S) \\qquad\n\\tag{1}\nFor SARSA we need a parametrized approximation \\hat{q} for the the action value function q_pi,\n\nq_\\pi(s,a) ≈ \\hat{q}(s,a,\\mathbf{w}) \\doteq \\mathbf{w}^T \\cdot \\mathbf{x}(s,a) \\qquad\n\\tag{2}"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#episodic-sarsa-in-mountain-car-video",
    "href": "posts/coursera/c3-w3.html#episodic-sarsa-in-mountain-car-video",
    "title": "Control with Approximation",
    "section": "Episodic SARSA in Mountain Car (Video)",
    "text": "Episodic SARSA in Mountain Car (Video)\n\n\n\n\n\n\n\nFigure 6: The mountain car environment"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#feature-choices-in-episodic-sarsa-with-function-approximation",
    "href": "posts/coursera/c3-w3.html#feature-choices-in-episodic-sarsa-with-function-approximation",
    "title": "Control with Approximation",
    "section": "Feature Choices in Episodic SARSA with Function Approximation",
    "text": "Feature Choices in Episodic SARSA with Function Approximation\n\n\n\n\n\n\n\nFigure 7: The feature representations for the mountain car problem\n\n\nWhat features do we use for the mountain car problem?\nfor the state:\n\nposition\nvelocity\n\nfor the action:\n\naccelerate left\naccelerate right\ndo nothing"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#sec-l1g3",
    "href": "posts/coursera/c3-w3.html#sec-l1g3",
    "title": "Control with Approximation",
    "section": "Visualizing Value Function and Learning Curves",
    "text": "Visualizing Value Function and Learning Curves\n\n\n\n\n\n\n\nFigure 8: The value function for the mountain car problem\n\n\n\n\n\n\n\n\nFigure 9: The trajectory through the state space for the mountain car problem\n\n\n\n\n\n\n\n\nFigure 10: The learning curve for the mountain car problem\n\n\n\n\nThe first two figures show the learned value function for the mountain car problem. The first figure shows the value function for each state. The second shows a possible trajectory through the state space.\nThen we look at the learning curve for the mountain car problem. This shows how the value function improves over time as the agent learns the optimal policy. We see the familiar exponential decay in the learning curves.\nIt worth noting that this is a very simple environment and that many more sophisticated deep learning techniques don’t do a very good job on this problem."
  },
  {
    "objectID": "posts/coursera/c3-w3.html#expected-sarsa-with-function-approximation-video",
    "href": "posts/coursera/c3-w3.html#expected-sarsa-with-function-approximation-video",
    "title": "Control with Approximation",
    "section": "Expected SARSA with Function Approximation (Video)",
    "text": "Expected SARSA with Function Approximation (Video)\nNow we extend SARSA with under function approximation into Expected SARSA .\nFirst, recall the update rule for Tabular SARSA:\n\nQ(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha (R_{t+1} + \\gamma Q( \\textcolor{red}{ S_{t+1}, A_{t+1} }) - Q(S_t,A_t)) \\qquad\n\\tag{3}\nSARSA’s update target includes the action value for the next state in action.\nNext, recall how Tabular Expected SARSA uses the expectation over its target policy instead.\n\nQ(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha (R_{t+1} + \\gamma \\textcolor{red}{ \\sum_{a'} \\pi (a' \\mid S_{t+1}) Q(S_{t+1},a')} - Q(S_t,A_t)) \\qquad\n\\tag{4}\nWe can compute the same expectation using function approximation.\nFirst, recall the update for SARSA with function approximation. It looks similar to the tabular setting except the action value estimates are parameterized by the weight factor, \\mathbf{w}. i.e. q_\\pi (s, a) \\approx \\hat{q} (s, a, \\mathbf{w})\n\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R_{t+1} + \\gamma \\hat{q}(\\textcolor{red} { S_{t+1} } , \\textcolor{red} { A_{t+1}}, \\mathbf{w}) - \\hat{q}(S_t, A_t, \\mathbf{w})] \\nabla \\hat{q} (S_t, A_t, \\mathbf{w}) \\qquad\n\\tag{5}\nExpected Sarsa with function approximation follows a similar structure.\n\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R_{t+1} + \\gamma   \\textcolor{red}{ \\sum_{a'} \\pi( a' \\mid S_{t+1}) \\hat{q} (S_{t+1}, a' , \\mathbf{w} )} - \\hat{q}(S_t , A_t , \\mathbf{w})] \\nabla \\hat{q}(S_t , A_t , \\mathbf{w}) \\qquad\n\\tag{6}"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#sec-l1g4",
    "href": "posts/coursera/c3-w3.html#sec-l1g4",
    "title": "Control with Approximation",
    "section": "How this extends to Q-learning easily, since it is a subset of Expected SARSA",
    "text": "How this extends to Q-learning easily, since it is a subset of Expected SARSA\nFinally, for Q-learning with function approximation as Q-learning is a special case of expected SARSA in which we take the greedy. So next we replace the expectation over the target policy by argmax action to derive the Q-learning update rule.\nThe Q-learning update rule is:\n\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R_{t+1} + \\gamma  \\textcolor{red}{ \\max_{a'} \\hat{q}(S_{t+1},a',\\mathbf{w})} −\\hat{q}(S_t,A_t,\\mathbf{w})] \\nabla \\hat{q}(S_t,A_t,\\mathbf{w}) \\qquad\n\\tag{7}"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#exploration-under-function-approximation-video",
    "href": "posts/coursera/c3-w3.html#exploration-under-function-approximation-video",
    "title": "Control with Approximation",
    "section": "Exploration under Function Approximation (Video)",
    "text": "Exploration under Function Approximation (Video)"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#sec-l2g1",
    "href": "posts/coursera/c3-w3.html#sec-l2g1",
    "title": "Control with Approximation",
    "section": "Optimistic Initialization as a Form of Exploration",
    "text": "Optimistic Initialization as a Form of Exploration\nIn the tabular setting, we saw that we could use optimistic initialization as a form of early exploration. The way this works is that we initialize the value function to be very high, this way the agent will be encouraged to try to exploit one of the states. It will discover that the value of the state is not as high as it thought and will try to exploit another and so on until it have visited all the states and learned more realistic values for them. Over time the effect of the initial values will diminish. This however assumes two things:\n\nthe number of states is finite\nthe values are independent of each other\n\nIn the function approximation setting, we can try to do the same thing. This time we will want to initialize the weights so as to make the value function high. We face a number of issues in this setting:\n\nthe values are not independent of each other so each weight may loose its optimistic value long before the agent has explored many of the states within the features neighborhood in the state space.\nUnlike values we can’t be certain that high weights will lead to high values. While this may work for linear function approximation, for a non-linear function approximation using Neural Networks with tanh activation functions, positive weights may lead to negative values.\n\nepsilon-greedy exploration is easy to implement even with non-linear function approximation. However it is not as effective in the function approximation setting. Because it relies on randomness to explore states near those followed by the current policy. This is not as systematic as the exploration due to optimistic initialization in the tabular setting.\nImproving exploration with function approximation is an open research question\nIn this course we will stick to epsilon-greedy exploration.\nQ. Why is epsilon-greedy exploration ineffective in the function approximation setting?\n\nlike in the bandit setting, the agent keeps exploring even after it has found the optimal policy.\nlike in environments with a changing maze multiple epsilon-greedy exploration steps may be required to explore states required by the optimal policy that are not near the current policy. The chance for such an exploration is \\mu(s)\\epsilon^n where mu(s) is importance of the nearest state and n is the number of steps required to reach the target state. This can be vanishingly small for large state spaces. Which means it can take too long to find the optimal policy using epsilon-greedy exploration. We need to think of better ways to organize exploration in the function approximation setting.\nPrioritizing using count based on a coarse coding may be more effective. Even better if we track the uncertainty in the value function for each if these features. This is a form of intrinsic motivation.\n\nHowever the last video by Satinder Singh discusses using intrinsic motivation to improve exploration in reinforcement learning systems. And in it he shows a different paradigm of exploration. Rather than getting agents to explore systematic one wants to explore in a way that is interesting to the agent."
  },
  {
    "objectID": "posts/coursera/c3-w3.html#average-reward-a-new-way-of-formulating-control-problems-video",
    "href": "posts/coursera/c3-w3.html#average-reward-a-new-way-of-formulating-control-problems-video",
    "title": "Control with Approximation",
    "section": "Average Reward: A New Way of Formulating Control Problems (Video)",
    "text": "Average Reward: A New Way of Formulating Control Problems (Video)\nYou probably never thought about it, since discounting is familiar like a geometric series, but it can really skew the value function.\n\n\n\n\n\n\n\nThe near sighted MDP\n\n\n\n\nFigure 11: The near sighted MDP. The lower discounting causes the agent to prefer the smaller immediate reward over the larger delayed reward.\n\n\nIn most states, there’s only one action, so there are no decisions to be made. There’s only one state were a decision can be made. In this state, the agent can decide which ring to traverse.\nThis means there are two deterministic policies, traversing the left ring or traversing the right ring. The reward is zero everywhere except for in one transition in each ring. In the left ring, the reward is +1 immediately after state S. In the right ring, the reward is +2 immediately before state S. Intuitively, you would pick the right action because you know you will get +2 reward. But what would the value function tell us to do?\nIf we use discounting, what are the values of state S under these two different policies?\nThe policy that chooses the left action has a value of v_l(S) = \\frac{1}{1-\\gamma^5}. How do we figure this out? If you write out the infinite discounted return, you will see this is a fairly straightforward geometric series with a closed form solution.\nThe policy that chooses the right action has a value of v_r(S) = \\frac{2 \\gamma^4}{1-\\gamma^5}. Let’s think of the value of state S under these two part policies for particular values of \\gamma.\n\\gamma= 0.5 \\implies V_L \\approx 1 \\qquad V_R \\approx 0.1\nThis means the policy that takes the left action is preferable under this more myopic discount.\nLet’s try\n\\gamma= 0.9 \\implies V_L \\approx 2.4 \\qquad V_R \\approx 3.2\nSo now we prefer the other policy.\nIn fact, we can figure out the minimum value of \\gamma so that the agent prefers the policy that goes right. \\gamma needs to be at least 0.841. So the problem here is that the discount magnitude depends on the problem.\nFor this example, 0.85 is sufficiently large. But if the rings had 100 states each, this discount factor would need to be over 0.99.\nIn general, the only way to ensure that the agents actions maximize reward over time is to keep increasing the discount factor towards 1.\nDepending on the problem, we might need \\gamma to be quite large. And we can’t set it to 1 in a continuing setting as the return might be infinite.\nNow, what’s wrong with having larger \\gamma? Larger values of \\gamma can also result in larger and more variables sums, which might be difficult to learn. So is there an alternative?"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#sec-l3g1",
    "href": "posts/coursera/c3-w3.html#sec-l3g1",
    "title": "Control with Approximation",
    "section": "The Average Reward Setting",
    "text": "The Average Reward Setting\n\nr(\\pi) \\doteq \\lim _{h \\to \\infty} \\frac{1}{h} \\sum_{t=1}^{h} \\mathbb{E}[R_t S_0,A_{0:t−1} \\sim \\pi]  \\qquad\n\\tag{8}\nLet’s discuss a new objective called the average reward. Imagine the agent has interacted with the world for H steps. This is the reward it has received on average across those H steps. In other words, it’s rate of reward. If the agents goal is to maximize this average reward, then it cares equally about nearby and distant rewards. We denote the average reward of a policy with R_\\pi.\n\nr(\\pi) = \\sum_{s} \\mu_\\pi (s) \\sum_{a} \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a) r\n\\tag{9}\nMore generally, we can write the average reward using the state visitation, \\mu. This inner term is the expected reward in a state under policy \\pi. The outer sum takes the expectation over how frequently the policy is in that state. Together, we get the expected reward across states. In other words, the average reward for a policy.\nIn the nearsighted example, the two deterministic possible policies visit either the left loop or the right loop indefinitely. In both cases, the five states in each loop are visited equally many times. In the left loop, the immediate expected reward is +0 for all states except one, which gets +1. This results in an average reward of 1 every 5 steps or 0.2.\nr(\\pi_L)=1/5=0.2 \\qquad r(\\pi_R)=2/5=0.4\nMost states in the right loop also have +0 \\mu to expected reward. But this time, the last state gets +2. This gives an average reward of 2 every 5 steps or 0.4.\n\n\n\n\n\n\n\nNear sighted MDP return for Average rewards\n\n\n\n\nFigure 12: Return for the nearsighted MDP for average rewards. The average reward for the left policy is 0.4 and for the right policy is 1.4.\n\n\n\n\n\n\n\n\nNear sighted MDP return for Average rewards\n\n\n\n\nFigure 13: Return for the nearsighted MDP for average rewards. The average reward for the left policy is G_t=-1.8 and for the right policy is G_t=0.8.\n\n\n\nWe can see the average reward puts preference on the policy that receives more reward in total without having to consider larger and larger discounts.\nThe average reward definition is intuitive for saying if one policy is better than another, but how can we decide which actions from a state are better?\nWhat we need are action values for this new setting. The first step is to figure out what the return is. In the average reward setting, returns are defined in terms of differences between rewards and the average reward R_\\pi. This is called the differential return.\n\nG_t = R_{t+1} −r_\\pi + R_{t+2} −r_\\pi + R_{t+2} −r_\\pi \\ldots \\qquad\n\\tag{10}\nLet’s look at what the differential returns are in our nearsighted MDP.\nThe differential return represents how much more reward the agent will receive from the current state in action compared to the average reward of the policy. Let’s look at the differential return starting in state s, first choosing action L and then following \\pi L afterwards.\nThe average reward for this policy is 0.2.\nThe differential return is the sum of rewards into the future with the average reward subtracted from each one. This sum starts in state S with the action L. We can compute it by summing to some finite horizon H. Then taking the limit as H goes to infinity. We are simplifying things slightly with this limit notation. While notation provider works in many cases, we need to use a different technique when the environment is periodic. In this case, we compute the return using a more general limit called the Cesàro sum, but this technical detail is not critical. The main point here is the intuition. We find that the differential return is 0.4. Now, let’s look at the other action. This time, we can break the differential return into two parts. First the sum for a single trajectory through the right loop. We can write the sum explicitly and it’s equal to 1. Then the sum corresponding to taking the left action indefinitely. This sum is the same as the differential return we just computed, 0.4. Adding the two parts together, we find that the differential return is 1.4.\nWe can write the average reward using the state visitation, \\pi. This inner term is the expected reward in a state under policy \\pi. The outer sum takes the expectation over how frequently the policy is in that state. Together, we get the expected reward across states. In other words, the average reward for a policy\nIn the average reward setting, returns are defined in terms of differences between rewards and the average reward r_\\pi, which is called the differential return. The differential return represents how much more reward the agent will receive from the current state in action compared to the average reward of the policy.\nThe differential return represents how much better it is to take an action in a state then on average under a certain policy. The differential return can only be used to compare actions if the same policy is followed on subsequent time steps. To compare policies, their average reward should be used instead\nThis quantity captures how much more reward the agent will get by starting in a particular state than it would get on average over all states if it followed a fixed policy.\n\nq_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s,A_t = a] \\qquad\n\\tag{11}\nLike in the discounted setting, differential value functions can be written as Bellman equations. They only differ in that they subtract r() from the immediate reward and there is no discounting. \nq_\\pi(s,a) = \\sum_{s',r} p(s',r \\mid s,a)(r −r(\\pi)) + \\sum_{a'} \\pi(a' \\mid s')q_\\pi(s',a') \\qquad\n\\tag{12}\nMany algorithms from the discounted settings can be rewritten to apply to the average reward case."
  },
  {
    "objectID": "posts/coursera/c3-w3.html#sec-l3g2",
    "href": "posts/coursera/c3-w3.html#sec-l3g2",
    "title": "Control with Approximation",
    "section": "When Average Reward Optimal Policies are Different from Discounted Solutions",
    "text": "When Average Reward Optimal Policies are Different from Discounted Solutions"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#sec-l3g3",
    "href": "posts/coursera/c3-w3.html#sec-l3g3",
    "title": "Control with Approximation",
    "section": "Differential Value Functions v.s. Discounted Value Functions",
    "text": "Differential Value Functions v.s. Discounted Value Functions\n\n\n\n\n\n\nEpisodic Semi-gradient SARSA\n\n\n\n\n\n\\begin{algorithm} \\caption{Differential Semi-gradient SARSA for estimating ($\\hat{q} \\approx q_*$)}\\begin{algorithmic} \\State $\\textbf{Input:}$ \\State $\\qquad \\text{a differentiable action-value fn parameterization } \\hat{q}: \\mathcal{S} \\times A \\times \\mathbb{R}^d \\to \\mathbb{R}$ \\State $\\textbf{Algorithm parameters:}$ \\State $\\qquad \\alpha, \\beta &gt; 0$ \\State $\\textbf{Initialize:}$ \\State $\\qquad$ value function weights $\\mathbf{w} \\in \\mathbb{R}^d$ arbitrarily, (e.g., $\\mathbf{w} = 0$) \\State $\\bar{R} source \\in \\mathbb{R}$ avg reward estimate \\State Initialize $S$ and action $A$ \\For {each step} \\State Take action A, observe R, S' \\State Choose A' as a function of $\\hat{q}(S',\\cdot,\\mathbf{w})$ (e.g., $\\epsilon$-greedy) \\State $\\delta \\leftarrow R - \\bar{R} + \\hat{q}(S', A', \\mathbf{w}) - \\hat{q}(S, A, \\mathbf{w}) $ \\State $ \\bar{R}\\leftarrow \\bar{R} + \\beta \\delta$ \\State $\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\delta \\nabla \\hat{q}(S, A, \\mathbf{w})$ \\State $S \\leftarrow S'$; $A \\leftarrow A'$ \\EndFor \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#satinder-singh-on-intrinsic-rewards-video",
    "href": "posts/coursera/c3-w3.html#satinder-singh-on-intrinsic-rewards-video",
    "title": "Control with Approximation",
    "section": "Satinder Singh on Intrinsic Rewards (Video)",
    "text": "Satinder Singh on Intrinsic Rewards (Video)\n\n\n\n\n\n\n\nFigure 14: This is a high level talk by Satinder Singh on AI and RL\n\n\n\n\n\n\n\n\nFigure 15: This talk titled ‘Steps Towards Continual Learning’ by Satinder Singh on Reinforcement Learning at DLSS & RLSS 2017 - Montreal\n\n\n\n\n\n\n\n\nFigure 16: Talk titled ‘Discovery in Reinforcement Learning’ at Beijing Academy of Artificial Intelligence by Satinder Singh on Two Pieces of Research on Exploration in Reinforcement Learning.\n\n\n\n\nSatinder Singh is a professor at the University of Michigan. He is a leading researcher in reinforcement learning and has made significant contributions to the field. In this video, he discusses intrinsic rewards and how they can be used to improve learning in reinforcement learning systems. It’s worth noting that he is one of the researchers who has worked on options with Doina Precup.\nNow Satinder Singh is a good speaker and he has lots of interesting research results to share. Unfortunately, this video is not his finest hour. I would definitely recommend watching some of his other talks linked above.\n\n\n\n\n\n\nDiscussion prompt\n\n\n\n\nWhat are the issues with extending some of the exploration methods we learned about bandits and Dyna to the full RL problem? How can we do visitation counts or UCB with function approximation?\n\n\nA control agent with function approximation has to explore to find the best policy, learn a good state representation, and try to get a lot of reward, all at the same time. How might an agent balance these potentially conflicting goals?"
  },
  {
    "objectID": "posts/coursera/c3-w3.html#footnotes",
    "href": "posts/coursera/c3-w3.html#footnotes",
    "title": "Control with Approximation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ne.g. epsilon greedy policy without decay↩︎"
  },
  {
    "objectID": "posts/coursera/c3-w1.html",
    "href": "posts/coursera/c3-w1.html",
    "title": "On-Policy Prediction with Approximation",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms\n\n\n\nWe now start the third course in the reinforcement learning specialization.\nIn terms of the ?@fig-rl-chart we are on the left branch of the tree.\nThis course is about prediction and control with function approximation.\nThe main difference in this course is that will start consider continuous state spaces and action spaces which cannot be represented as tables.\n\nHowever many of the methods we will develop will be useful in handling large scale tabular problems as well.\nWe will use methods from supervised learning but only online methods that can handle non-stationary data.\nThe main differences are the use of weights to parameterize the value functions.\nThe use of function approximation to estimate value functions and policies in reinforcement learning.\n\nWeights lead to using a loss function to estimate the value function.\nMinimizing the continuous loss function leads to Gradient descent and\nUsing sampling leads to Stochastic gradient descent.\n\nThe idea of learning weights rather than values is a key idea in this course.\nThe tradeoff between discrimination and generalization is also a key idea in this course.\n\nWe will learn how to use function approximation to estimate value functions and policies in reinforcement learning.\nWe will also learn how to use function approximation to solve large-scale reinforcement learning problems.\nWe will see some simple linear function approximation methods and later\nWe will see modern nonlinear approximation methods using deep neural networks.\n\nI did not find the derivation of the SGD alg particularly enlightening and I have seen it several times. However the online setting is the best motivation for the use of SGD and makes perfect sense in the context of reinforcement learning. Minibatches are then a natural extension of this idea.\n\n\n\n\n\n\nReadings\n\n\n\n\n\n\n[@sutton2018reinforcement§9.1-9.4, pp. 194-209] book"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#moving-to-parameterized-policies-video",
    "href": "posts/coursera/c3-w1.html#moving-to-parameterized-policies-video",
    "title": "On-Policy Prediction with Approximation",
    "section": "Moving to Parameterized Policies (video)",
    "text": "Moving to Parameterized Policies (video)\nThis video covers the first four learning objectives.\nThis video covers parameterized policies and how they can be used to approximate value functions. The idea is that using a table has some limitations/ The first is that the tables can be very large. For continuous states they can become infinite.\nThe second is that the tables can be very sparse and in a table we don’t generalize between states.\nWe see that we don’t really want functions that directly approximate the value function. We want functions that have some structure that we can learn.\nThis is called a parameterized function. The ideas is to use a weighted sum of the features of the state. This allows us to learn the weights and use them to approximate the value function. This is called linear function approximation. The way to get around this which is two fold. We first represent the salient properties of a states into features.\nThen we use weights to combine these features to approximate the value function. This is called linear function approximation.\nMore generally we can use non linear parameterized functions to approximate value functions.\nAdam shows that is the features are not picked wisely we may not be able to discriminate between states - out function for one state will be the same as for another dissimilar state. Learning about one will make us forget what we learned about the other. This is called bias. On the other hand if we have too many features we may not be able to generalize between states. This is called variance. The goal is to balance between bias and variance."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l1g1",
    "href": "posts/coursera/c3-w1.html#sec-l1g1",
    "title": "On-Policy Prediction with Approximation",
    "section": "Understanding parameterized functions",
    "text": "Understanding parameterized functions\n\nIn the previous courses we represented value functions as tables or arrays:\n\nFor V(s) we had an array of size |S|,\nFor Q(s,a) we had an array of size |S| \\times |A|. This becomes impractical as |S| \\rightarrow \\infty. We can use parameterized functions to approximate value functions. This is called function approximation.\n\nLinear value function approximation is a simple and popular method.\n\nWe represent the value function as a linear combination of features:\n\n\n\n\\hat{v}(s, \\mathbb{w}) \\approx v_\\pi(s) \\qquad\n\\tag{1}\n\nwhere:\n\n\\hat{v}() is the approximate value function\n\\mathbf{w} is a weight vector\n\nfor example:"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l1g2",
    "href": "posts/coursera/c3-w1.html#sec-l1g2",
    "title": "On-Policy Prediction with Approximation",
    "section": "Linear value function approximation",
    "text": "Linear value function approximation\n\nWe can write the approximate value function as a linear combination of features:\n\n\n\\hat{v}(s, \\mathbb{w}) \\dot = w_1 X + w_2 + Y \\qquad\n\\tag{2}\n\nwhere:\n\nX and Y are features of the state s\nw_1 and w_2 are the weights of the features\n\nnow learning becomes finding better weights that parameterize the value function.\n\nfinding the weights that minimize the error between the approximate value function and the true value function:"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l1g3",
    "href": "posts/coursera/c3-w1.html#sec-l1g3",
    "title": "On-Policy Prediction with Approximation",
    "section": "Tabular case is a special case of linear value function approximation",
    "text": "Tabular case is a special case of linear value function approximation\n \n\\begin{align*}\n\\hat{v}(s, \\mathbb{w}) & \\dot = \\sum w_i x_i(s) \\newline\n                       & = &lt;\\mathbf{w}, \\mathbf{x}(s)&gt; \\qquad\n\\end{align*}\n\n\nhere:\n\n\\mathbf{w} is a weight vector\n\\mathbf{x}(s) is a feature vector that is 1 in the i-th position and 0 elsewhere.\n\nlinear value function approximation is a generalization of the tabular case.\nlimitations of linear value function approximation:\n\nthe choice of features limits the expressiveness of the value function.\nit can only represent linear relationships between the features and the value function.\nit can only represent a limited number of features.\n\nso how are tabular functions a special case of linear value function approximation?\n\nwe can see from the figure that all we need is use one hot encoding for the features. Then the weighted vector will be the same as the value function in the table.\n\n\n\n\n\n\n\n\n\nFigure 1: Linear value function approximation failure"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l1g4",
    "href": "posts/coursera/c3-w1.html#sec-l1g4",
    "title": "On-Policy Prediction with Approximation",
    "section": "There are many ways to parameterize an approximate value function",
    "text": "There are many ways to parameterize an approximate value function\n\n\n\n\n\n\n\nFigure 2: neural networks are non-linear fn approximators\n\n\n\nWe can use different types of functions to approximate the value function:\n\none hot encoding\nlinear functions\ntile coding\nneural networks"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#generalization-and-discrimination-video",
    "href": "posts/coursera/c3-w1.html#generalization-and-discrimination-video",
    "title": "On-Policy Prediction with Approximation",
    "section": "Generalization and Discrimination (video)",
    "text": "Generalization and Discrimination (video)\nIn this video Martha covers the next three learning objectives. The video is about:\n\nGeneralization - using knowledge (V,Q,Pi) about similar states.\nDiscrimination - being able to distinguish between different states."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l1g5",
    "href": "posts/coursera/c3-w1.html#sec-l1g5",
    "title": "On-Policy Prediction with Approximation",
    "section": "Understanding generalization and discrimination",
    "text": "Understanding generalization and discrimination\n\n\n\n\n\n\n\nFigure 3: generalization and discrimination\n\n\n\nGeneralization:\n\nthe ability to estimate the value of states that were not seen during training.\nin the case of policy evaluation, generalization is the ability of updates of value functions in one state to affect the value of other states.\nin the tabular case, generalization is not possible because we only update the value of the state we are in.\nin the case of function approximation, we can think of generalization as corresponding to an embedding of the state space into a lower-dimensional space.\n\nDiscrimination: the ability to distinguish between different states."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l1g6",
    "href": "posts/coursera/c3-w1.html#sec-l1g6",
    "title": "On-Policy Prediction with Approximation",
    "section": "How generalization can be beneficial",
    "text": "How generalization can be beneficial\n\nGeneralization can be beneficial because:\n\nIt allows us to estimate the value of states that are similar to states seen during training.\nThis includes states that were not seen during training.\nIt allows us to estimate the value of states that are far from states seen during training. (So long as they are similar in terms of the features we are using to approximate the value function)"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l1g7",
    "href": "posts/coursera/c3-w1.html#sec-l1g7",
    "title": "On-Policy Prediction with Approximation",
    "section": "Why we want both generalization and discrimination from our function approximation",
    "text": "Why we want both generalization and discrimination from our function approximation\n\nWe want both generalization and discrimination from our function approximation because:\n\ngeneralization allows us to estimate the value of states that were not seen during training.\ndiscrimination allows us to distinguish between different states.\ngeneralization allows us to estimate the value of states that are similar to states seen during training.\ndiscrimination allows us to estimate the value of states that are far from states seen during training.\n\n\nWe hear a lot about function approximation and gradient methods having a bias or high variance. I tracked this from wikipedia and statistical learning. While it makes sense for a Bayesian regression I’m not sure that it is quite correct for RL. Unfortunately I don’t have a better explanation, though reviewing this policy gradient lecture might be helpful\n\n\n\n\n\n\nBias-variance tradeoff\n\n\n\n\nan important result called the bias-variance tradeoff:\n\nBias is the error introduced by approximating a real-world problem, which may be extremely complicated, by a much simpler model. This means that since we cannot discriminate between different states that share weights for the same feature vector we have errors we characterize as bias.\nHigh bias corresponds to underfitting in our model.\nVariance is the opposite issue arising from having more features than we need to discriminate between states. This means that updating certain weights will affect only some of these related states and not others. This type of error is called variance and is also undesirable.\nHigh variance corresponds to overfitting in our model which can be due to our model fitting the noise in the data rather than the underlying signal.\nIn general for a model there is some optimal point where the bias and variance are balanced. Going forward from that point we observe a trade off between bias and variance so we need to choose one or the other.\nThis choice is usually governed by business realities and the nature of the data or the problem we are trying to solve."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#framing-value-estimation-as-supervised-learning-video",
    "href": "posts/coursera/c3-w1.html#framing-value-estimation-as-supervised-learning-video",
    "title": "On-Policy Prediction with Approximation",
    "section": "Framing Value Estimation as Supervised Learning (video)",
    "text": "Framing Value Estimation as Supervised Learning (video)\nIn this video we cover the next two learning objectives. Martha has a good background in supervised learning and she explains how many parts of RL can be framed as supervised learning problems.\nThings we like to learn in RL in this course:\n\nValue fn approximation (V)\nAction fn values (Q)\nPolicies (Pi)\n\nIn reality we may want to learn other things as well which can be framed as supervised learning problems:\n\nState representations - i.e. better features (CNNs, RNNs, etc)\nModels of Dynamics i.e. Transition probabilities (P)\nReward precesses (R) What is a good reward function? How do we learn it? This is an inverse reinforcement learning problem. It is ill posed because there are many reward functions that can explain the data. We need to find the simplest one that explains the data. This is intertwined with learning internal motivations and goals. c.f. Satinder Singh’s work on intrinsic motivation.\nGeneralized Value functions (Gvfs) c.f. Martha and Adam White’s work on GVFs.\nOptions (Spatial or Temporal Aggregations of actions) c.f. Doina Precup’s work on options and semi-markov decision processes.\n\nEven more things that we might want to learn in RL that might be framed as supervised learning problems:\n\nApproximate/Compressed policies for sub goals AKA heuristics\n\nFully Pooled policies (all states are the same) - Think random uniform policy\nPartial pooled policies, (some states are the same)\nUnpooled policies (all states are different - tabular setting)\nPriors for Policies\nHierarchies of options - Think Nethack\n\nBeliefs about policies\nBeliefs about other agents (theory of mind)\nBeliefs about the environment.\nCausal models of the environment\n\nwhat can we influence and what can’t we influence.\n\nCoordination and communication with other agents c.f. work by Jakob Foerster, Natasha Jacques, and Marco Baroni on emergent communication.\n\nWhat part of communication is cheap talk\nWhat part of communication is credible"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l1g8",
    "href": "posts/coursera/c3-w1.html#sec-l1g8",
    "title": "On-Policy Prediction with Approximation",
    "section": "How value estimation can be framed as a supervised learning problem",
    "text": "How value estimation can be framed as a supervised learning problem\n\nThe problem of policy evaluation in reinforcement learning can be framed as supervised learning problem\n\nIn the case of Monte Carlo methods,\n\nthe inputs are the states and\nthe outputs are the returns G.\n\nIn the case of TD methods,\n\nthe inputs are the states and\nthe outputs are the one step bootstrapped returns. U_t \\dot=R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l1g9",
    "href": "posts/coursera/c3-w1.html#sec-l1g9",
    "title": "On-Policy Prediction with Approximation",
    "section": "Not all function approximation methods are well suited for reinforcement learning",
    "text": "Not all function approximation methods are well suited for reinforcement learning\n\nIn principle, any function approximation technique from supervised learning can be applied to the policy evaluation task. However, not all are equally well-suited. – Martha White\n\n\nin RL the agent interacts with the environment and generates data, which corresponds to the online setting in supervised learning.\nWhen we want to use supervised learning we need to choose a method that is well suited for the online setting which can handle\n\nnon-stationary data.\nnon-stationary and correlated data (which is the case in RL).\n\n\nIn fact much of the learning in RL is about learning such correlations and quickly adapting to non-stationary in the environment.\nIn TD learning the target depends on w but in supervised learning the target is fixed and given."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#the-value-error-objective-video",
    "href": "posts/coursera/c3-w1.html#the-value-error-objective-video",
    "title": "On-Policy Prediction with Approximation",
    "section": "The Value Error Objective (Video)",
    "text": "The Value Error Objective (Video)\nIn this video Adam White covers the first two learning objectives of the unit.\nThe main subject about using the mean squared error as a loss for the approximate value function.\nwe get a sequence of (S_1,v_{\\pi}(S_1) ),(S_2,v_{\\pi}(S_2) ),(S_3,v_{\\pi}(S_3) ), \\ldots and we want to approximate the value function . We can track how well we are approximating v_{\\pi}(s) by using \\hat{v}(s,\\mathbb{w}). The difference can be positive or negative so if we average it the sum will tend to cancel out. If we square the error we get a positive number we have a much better estimate of the error. And if normalize it by taking the mean we can get use it to compare runs of different lengths. This is called the mean squared error.\nIt turns out that this is not enough for RL and we need to take a weighted average using the state distribution \\mu(s). This is because we care more about some states than others. The state distribution is the long run probability of visiting the state s under the policy \\pi. This weighted average is called the mean squared value error.\n\n\n\n\n\n\nYour Objective is My Loss\n\n\n\n\n\nIn the optimization literature the loss function is called the objective function. This is because we are trying to optimize the weights of the function to minimize the loss. So we will often hear the term objective function or just objective. Life is simpler if we recall that this is just a loss function for a supervised learning problem.\nA related point is that if we want to optimize our approximate value we can swap the with a different loss function or with a different approximation function and the outcome should remain the same, at least under certain conditions. This is how we can switch from the mean squared value error objective to the Monte Carlo objective and then to the TD learning objective."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l2g1",
    "href": "posts/coursera/c3-w1.html#sec-l2g1",
    "title": "On-Policy Prediction with Approximation",
    "section": "Understanding the mean-squared value error objective for policy evaluation",
    "text": "Understanding the mean-squared value error objective for policy evaluation\n\n\n\n\n\n\n\nFigure 4: mean-squared value error objective\n\n\n\nAn idealized Scenario:\n\ninput: \\{(S_1, v_\\pi(S_1)), (S_2, v_\\pi(S_2)), \\ldots, (S_n, v_\\pi(S_n))\\}\noutput: \\hat v(s,w) \\approx v_\\pi(s)\nhowever in reality we may get some some error in the approximation.\n\nthis could be due to our choice of the approximation.\nbut initially we just don’t have good weights - to fit the data.\n\nWhat we need is a way to measure the error in the approximation.\nAlso we may care more about some states than others and we can encode this using the state distribution \\mu(s).\n\nThe mean-squared value error objective for policy evaluation is to minimize the mean-squared error between the true value function and the approximate value function:\n\n\n\\overline{VE} = \\sum_{s\\in S}\\mu(s)[v_\\pi(S) - \\hat{v}(S, \\mathbf{w})]^2\n\\tag{3}\n\nwhere:\n\n\\overline{VE} is the mean-squared value error\n\\mu(s) is the state distribution\nv_\\pi(s) is the true value of state s\n\\hat{v}(s, \\mathbf{w}) is the approximate value of state s with weights \\mathbf{w}\n\nthe goal is to find the weights that minimize the mean-squared value error."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l2g2",
    "href": "posts/coursera/c3-w1.html#sec-l2g2",
    "title": "On-Policy Prediction with Approximation",
    "section": "Explaining the role of the state distribution in the objective",
    "text": "Explaining the role of the state distribution in the objective\n\nThe state distribution \\mu(s) is the long run probability of visiting the state s under the policy \\pi.\nThis makes more sense if our markov chain is ergodic - i.e. we can reach any state from any other state by following some transition trajectory.\nThe state distribution is important because it determines how much we care about the error in each state.\nThe state distribution is usually unknown, and hard to estimate as it has complex dependencies on the policy and the environment.\nWe will later see a result that shows how we can avoid the need to know the state distribution.\nIn the diagram we see that the state distribution is a probability distribution over the states of the MDP and that there is little probability mass of visiting states at the edges of the state space.\nThe mean square error has less impact in these low probability states."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l2g3",
    "href": "posts/coursera/c3-w1.html#sec-l2g3",
    "title": "On-Policy Prediction with Approximation",
    "section": "The idea behind gradient descent and stochastic gradient descent",
    "text": "The idea behind gradient descent and stochastic gradient descent\n\nGradient descent is an optimization algorithm that uses the gradient to find a local minimum of a function.\nThe gradients points in the direction of the steepest ascent of the function and our objective is to minimize the mean squared error we move in the opposite direction.\nHence the name gradient descent.\nThe gradient of the mean-squared value error with respect to the weights \\mathbf{w} is given by:\n\n\n    w \\dot = \\left [ \\begin{matrix} w_1 \\\\ \\vdots \\\\ w_d  \\end{matrix} \\right ] \\qquad \\nabla f = \\left [ \\begin{matrix} \\frac{\\partial f}{\\partial w_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial w_d}  \\end{matrix} \\right ] \\qquad\n\n\nfor a linear function:\n\n\n\\hat{v}(s, \\mathbf{w}) = \\sum \\mathbf{w}^T \\mathbf{x}(s) \\\\\n\\frac{\\partial \\hat{v}(s, \\mathbf{w})}{\\partial w_i} = \\mathbf{x_i}(s) \\\\\n\\nabla \\hat{v}(s, \\mathbf{w}) = \\mathbf{x}(s) \\qquad\n\\tag{4}\n\nwe can write the update rule for the weights as:\n\n\nw_{t+1} \\dot= w_t - \\alpha \\nabla J(\\mathbf{w_t}) \\qquad\n\\tag{5}\n\nStochastic gradient descent is a variant of gradient descent that uses a random sample of the data to estimate the gradient.\nStochastic gradient descent uses mini-batches of data to estimate the gradient, which makes it computationally efficient and reduces the variance of the gradient estimate.\nIn practice we will use variants like:\n\nAdam - which adapts the learning rate based on the gradient.\nRMSProp - which uses a moving average of the squared gradient.\nAdagrad - which uses a different learning rate for each parameter.\nSGD - which uses a fixed learning rate."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l2g7",
    "href": "posts/coursera/c3-w1.html#sec-l2g7",
    "title": "On-Policy Prediction with Approximation",
    "section": "Gradient descent converges to stationary points",
    "text": "Gradient descent converges to stationary points\n\n\n\n\n\n\n\nFigure 5: gradient descent\n\n\n\nGradient descent converges to stationary points because the gradient of the mean-squared value error is zero at the minimum.\nGradient descent can get stuck in a local minima, so it is important to use a good initialization and learning rate.\nStochastic gradient descent can escape a local minima because it uses a random sample of the data to estimate the gradient.\nIn general the optimizer is not guaranteed to find the global minimum of the function - just a local minima"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l2g8",
    "href": "posts/coursera/c3-w1.html#sec-l2g8",
    "title": "On-Policy Prediction with Approximation",
    "section": "How to use Gradient descent and Stochastic gradient descent to minimize the value error",
    "text": "How to use Gradient descent and Stochastic gradient descent to minimize the value error\n\n\\begin{align*}\n\\nabla J(\\mathbf{w}) & = \\nabla \\sum_{s\\in S} \\mu(s)[v_\\pi(s) - \\hat{v}(s, \\mathbf{w})]^2 \\qquad  \\qquad  \\hat{v}(s, \\mathbf{w}) = &lt;\\mathbf{w},\\mathbf{x}(s)&gt;\\newline\n                    & =  \\sum_{s\\in S}  \\mu(s) \\nabla [v_\\pi(s) - \\hat{v}(s, \\mathbf{w})]^2 \\qquad  \\qquad \\nabla   \\hat{v}(s, \\mathbf{w}) = \\mathbf{x}(s)\\newline\n                    & =  \\sum_{s\\in S} \\mu(s) 2 [v_\\pi(s) - \\hat{v}(s, \\mathbf{w})]\\nabla \\hat{v}(s, \\mathbf{w})\n\\end{align*} \\qquad\n\n\nStochastic Gradient Descent\nIf we have a sample of states s_1, s_2, \\ldots, s_n observed by following pi\nwe can write the update rule for a pair of weights as:\n\nw_{t+1} \\dot= w_t + \\alpha [v_\\pi(S_1) - \\hat{v}(s_1, \\mathbf{w_1})]\\nabla \\hat{v}(s, \\mathbf{w}) \\qquad\n\\tag{6}\nThis allows us to decrease the error in the value function by making updates for one state at a time and moving the weights in the direction of the negative gradient. By making this type of update we might increase the error occasionally but in the long run we will decrease the error.\n\nThis updating approach is called stochastic gradient descent, because it only uses a stochastic estimate of the gradient. In fact, the expectation of each stochastic gradient equals the gradient of the objective. You can think of this stochastic gradient as a noisy approximation to the gradient that is much cheaper to compute, but can nonetheless make steady progress to a minimum – Martha White\n\nwe have here one issue - we don’t know the true value of the policy v_pi(s_1), how do we get around this?\none option is to replace the true value with an estimate, one option is to use the return from the state s_1.\nrecall that\n\nv_\\pi(s) = \\mathbb{E}[G_t \\mid S_t = s] \\qquad\n\nso we can substitute the true value with the return from the state s_1.\n\n\\begin{align*}\nw_{t+1} & \\dot= w_t + \\alpha [v_\\pi(S_1) - \\hat{v}(s_1, \\mathbf{w_1})]\\nabla \\hat{v}(s, \\mathbf{w}) \\qquad \\\\\n        & \\dot = w_t + \\alpha [G_1 - \\hat{v}(s_1, \\mathbf{w_1})]\\nabla \\hat{v}(s, \\mathbf{w}) \\qquad\n\\end{align*}\n\\tag{7}"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l2g4",
    "href": "posts/coursera/c3-w1.html#sec-l2g4",
    "title": "On-Policy Prediction with Approximation",
    "section": "The gradient Monte Carlo algorithm for value estimation",
    "text": "The gradient Monte Carlo algorithm for value estimation\nWe now have a way to update the weights of the value function using the gradient of the mean-squared value error. Which allows us to present the gradient Monte Carlo algorithm for value estimation.\n\n\n\n\n\n\nNote 1: MC prediction fist visit for estimating V \\approx v_\\pi\n\n\n\n\n\n\\begin{algorithm} \\caption{GradientMC($\\pi$)}\\begin{algorithmic} \\State Input \\State $\\qquad \\pi$ to be evaluated \\State $\\qquad \\text{a differentiable function } \\hat{v}: \\mathcal{S} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ \\State Algorithm parameters: \\State $\\qquad \\alpha \\in (0, 1]$ step size \\State Initialize: \\State $\\qquad \\mathbf{w} \\leftarrow x \\in \\mathbb{R^d} \\text{ arbiterly}$ (e.g. w=0) \\For {each episode:} \\State Generate an episode by following $\\pi: S_0, A_0, R_1, S_1, A_1, R_2,\\ldots, S_{T-1}, A_{T-1}, R_T$ \\State $G \\leftarrow 0$ \\For {each step of episode, $t = 0, 1, \\ldots, T-1$:} \\State $\\mathbf{w} \\leftarrow w_t + \\alpha [G_t - \\hat{v}(S_t , \\mathbf{w_1})] \\nabla \\hat{v}(S_t , \\mathbf{w})$ \\EndFor \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\nThe Gradient Monte Carlo algorithm is a policy evaluation algorithm that uses stochastic gradient descent to minimize the mean-squared value error."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l2g5",
    "href": "posts/coursera/c3-w1.html#sec-l2g5",
    "title": "On-Policy Prediction with Approximation",
    "section": "How state aggregation can be used to approximate the value function",
    "text": "How state aggregation can be used to approximate the value function\n\n\n\n\n\n\n\nFigure 6: gradient mc with state aggregation\n\n\n\nState aggregation\nis a method for reducing the dimensionality of the state space by grouping similar states together.\ncan be used to approximate the value function by representing each group of states as a single state.\ncan be used to reduce the number of parameters in the value function and improve generalization.\nthe example used a 1000 state MDP with 10 groups of 100 states each.\nleft and right jump left 1-100 states and right 1-100 states.\nif they pass the terminal state they get to the terminal state.\nstate aggregation is a way to reduce the number of parameters in the value function by grouping similar states together.\nit is an example of linear function approximation.\nthere is one feature for each group of states.\nthe weights are updated using the gradient of the mean-squared value error.\n\n\nw \\leftarrow w + \\alpha [G_t - \\hat{v}(S_t, \\mathbf{w})] \\nabla \\hat{v}(S_t, \\mathbf{w})\n\nand the gradient of the approximate value function is given by:\n\n\\nabla \\hat{v}(S_t, \\mathbf{w}) = \\mathbf{x}(S_t)\n\nwhich is either 1 or 0 depending on the group of states."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l2g6",
    "href": "posts/coursera/c3-w1.html#sec-l2g6",
    "title": "On-Policy Prediction with Approximation",
    "section": "Applying Gradient Monte-Carlo with state aggregation",
    "text": "Applying Gradient Monte-Carlo with state aggregation\n\nGradient Monte Carlo with state aggregation is a policy evaluation algorithm that uses state aggregation to approximate the value function.\nThe algorithm works as follows:"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l3g1",
    "href": "posts/coursera/c3-w1.html#sec-l3g1",
    "title": "On-Policy Prediction with Approximation",
    "section": "The TD-update for function approximation",
    "text": "The TD-update for function approximation\n\nrecall the Monte Carlo update rule:\n\n\nw \\leftarrow w + \\alpha [G_t - \\hat{v}(S_t, \\mathbf{w})] \\nabla \\hat{v}(S_t, \\mathbf{w})\n\n\nwe can use other targets for the update rule than the return G_t.\nwe can replace the return with any estimate of the value of the next state.\nwe can call this target U_t and if it is unbiased it converge to a local minimum of the mean squared value error.\nwe can use the one step bootstrapped return:\n\n\nU_t \\dot=R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})\n\\tag{8}\n\nbut this is not unbiased because it essential approximating the expected return using the current value of the state.\nthere is no guarantee that the the update will converge to a local minimum of the mean squared value error.\nhowever the update has many advantages over the Monte Carlo update:\n\nit has lower variance because it uses a single sample.\nit can update the value function after every step.\nit can learn online.\nit can learn from incomplete episodes.\nit can learn from non-episodic tasks.\n\nthe TD-update is nor a true gradient update because the target is not the true value of the state. we call it a semi-gradient update.\n\nlet’s estimate the gradient of the mean squared value error with respect to the weights \\mathbf{w}:\n\n\n\n\\begin{align*}\n\\nabla J(\\mathbf{w}) & = \\nabla \\frac{1}{2}[U_t - \\hat{v}(S_t, \\mathbf{w})]^2 \\newline\n                     & = (U_t - \\hat{v}(S_t, \\mathbf{w})) (\\nabla U_t - \\nabla \\hat{v}(S_t, \\mathbf{w})) \\\\\n                     & \\ne - (U_t - \\hat{v}(S_t, \\mathbf{w})) \\nabla \\hat{v}(S_t, \\mathbf{w}) \\quad \\text{unless} \\quad \\nabla  U_t = 0\n\\end{align*}\n\\tag{9}\n\nbut for TD we have:\n\n\n\\begin{align*}\n\\nabla U_t = & = \\nabla (R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})) \\newline\n            & = \\gamma \\nabla \\hat{v}(S_{t+1}, \\mathbf{w}) \\newline\n            & \\ne 0\n\\end{align*}\n\\tag{10}\n\nSo the TD-update isn’t a true gradient update. However TD often converge in many cases we care updates."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l3g2",
    "href": "posts/coursera/c3-w1.html#sec-l3g2",
    "title": "On-Policy Prediction with Approximation",
    "section": "Advantages of TD compared to Monte-Carlo",
    "text": "Advantages of TD compared to Monte-Carlo\n\nAdam point out that in Gradient Monte Carlo we need to run the alg for a long time and decay the step size to get convergence. But that in practice we don’t decay the step size and we use a fixed step size.1\nTD has several advantages over Monte-Carlo:\n\nTD can update the value function after every step, while Monte-Carlo can only update the value function after the episode is complete.\nTD can learn online, while Monte-Carlo can only learn offline.\nTD can learn from incomplete episodes, while Monte-Carlo requires complete episodes.\nTD can learn from non-episodic tasks, while Monte-Carlo can only learn from episodic tasks."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l3g3",
    "href": "posts/coursera/c3-w1.html#sec-l3g3",
    "title": "On-Policy Prediction with Approximation",
    "section": "The Semi-gradient TD(0) algorithm for value estimation",
    "text": "The Semi-gradient TD(0) algorithm for value estimation\n\nThe Semi-gradient TD(0) algorithm is a policy evaluation algorithm that uses the TD-update for function approximation.\n\n\n\n\n\n\n\nThe Semi-gradient TD(0) algorithm for estimating v_\\pi\n\n\n\n\n\n\\begin{algorithm} \\caption{Semi-gradient TD(0) for estimating $v_\\pi$}\\begin{algorithmic} \\State Input: \\State $\\qquad \\pi \\leftarrow \\text{policy to be evaluated}$ \\State $\\qquad \\text{a differentiable function } \\hat{v}: \\mathcal{S} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ \\State Algorithm parameters: \\State $\\qquad \\alpha \\in (0, 1]$ step size \\State $\\qquad \\gamma \\in [0, 1]$ discount factor \\State Initialize: \\State $\\qquad value function weights w \\leftarrow x \\in \\mathbb{R}^d \\quad \\forall s \\in \\mathcal{S}$ (e.g. w=0) \\FORALL {episode $e$:} \\State $Initialize S$ \\FORALL {step $S \\in e$:} \\State $\\text{Choose } A \\sim \\pi(\\cdot \\mid S)$ \\State Take action $A$, observe $R, S'$ \\State $w \\leftarrow w + \\alpha [R + \\gamma \\hat{v}(S', \\mathbf{w}) - \\hat{v}(S, \\mathbf{w})] \\nabla \\hat{v}(S, \\mathbf{w})$ \\State $S \\leftarrow S'$ \\State until $S$ is terminal \\ENDFOR \\ENDFOR \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l3g4",
    "href": "posts/coursera/c3-w1.html#sec-l3g4",
    "title": "On-Policy Prediction with Approximation",
    "section": "TD converges to a biased value estimate",
    "text": "TD converges to a biased value estimate\n\nTD converges to a biased value estimate because it updates the value function using an estimate of the next state.\nThe bias of TD can be reduced by using a smaller step size or by using a more accurate estimate of the next state."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l3g5",
    "href": "posts/coursera/c3-w1.html#sec-l3g5",
    "title": "On-Policy Prediction with Approximation",
    "section": "TD converges much faster than Gradient Monte Carlo",
    "text": "TD converges much faster than Gradient Monte Carlo\n\n\n\n\n\n\n\nFigure 7: early learning experiment\n\n\n\nWe run the same random walk experiment for the 1000 episodes 1000 step random walk and we see that TD has a worse fit than MC on most of the range.\nWe run a second experiments with only 30 episodes to see early learning performance and we see that TD has a better fit than MC on most of the range. In this case we used the best alpha for each method. MC needed a much smaller alpha to get a good fit.\nTD converges much faster than Gradient Monte Carlo because it updates the value function after every step.\nGradient Monte Carlo can only update the value function after the episode is complete, which can be slow for long episodes.\nTD can learn online, while Gradient Monte Carlo can only learn offline.\nTD can learn from incomplete episodes, while Gradient Monte Carlo requires complete episodes.\nTD can learn from non-episodic tasks, while Gradient Monte Carlo can only learn from episodic tasks."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l3g6",
    "href": "posts/coursera/c3-w1.html#sec-l3g6",
    "title": "On-Policy Prediction with Approximation",
    "section": "Doina Precup’s talk on Building Knowledge for AI Agents with Reinforcement Learning",
    "text": "Doina Precup’s talk on Building Knowledge for AI Agents with Reinforcement Learning\n\n\n\n\n\n\n\nFigure 8: Dorina Precup\n\n\n\n\n\n\n\n\nFigure 9: Options are a temporal generalization\n\n\n\nIn this talk, Dorina Precup discusses the challenges of building knowledge for AI agents using reinforcement learning.\n\nDorina Precup is a professor at McGill University and a research team lead at DeepMind.\nShe is an expert in reinforcement learning and machine learning.\nHer interests are in the areas of abstractions.\nWhen I think about generalization in RL I think about:\n\nLearning a parameterized value function that can be used to estimate the value of any state.\nLearning a parameterized policy that can be used to select actions in any state.\nBeing able to transfer this policy to a similar task\nBeing able to learn using less interaction with the environment and more from replaying past experiences.\nBeing able to learn from a small number of examples.\n\nDorina talks about two other aspects of generalization:\n\nAction duration are one time step in an MDP, yet in reality some actions like traveling from one city to another require sticking to the action over an extended period of time.\n\nThis might be happen through planning but idealy, agents should be able to learn skills which are sequences of actions that are executed over an extended period of time.\nThis has been formalized in the literature as options.\nShe references two sources\n\n[@Sutton1999BetweenMA] a paper from 1999 on options in reinforcement learning.\n[@precup2000temporal] her doctoral thesis from 2000 on temporal abstraction in reinforcement learning.\n\nOptions consists of\n\nan initiation set \\iota_\\omega(s) the precondition which is a probability of starting the option in state s.\na policy \\pi_\\omega(a\\mid s) that is executed in the option\na termination condition \\beta_\\omega(s). the termination condition is a probability of terminating the option in state s.\n\nOptions are “chunks of behavior” that can be executed over an extended period of time.\nThe model will need to learn options and work with them.\nIT needs expected reward over the option.\nA transition model over the option.\nThese models are predictive models about outcomes conditioned on the model being executed.\nAdding options to the model weakens the MDP assumption, because the option duration is not fixed so state now have a longer dependence is a sequence of actions that are not Markovian 2.\nPrecup’s point out that combining temporal and spatial abstraction is an ongoing research challenge.\nShe also points out that the model needs to learn the options and the value function at the same time.\nAccording to her profile Precup has a number of students working on this problem. Some additional references are:\n\n[@Bacon2016TheOA] a paper from 2016 on option-critic architecture which extends actor-critic algorithms to work with options.\n\nEarlier work uses the term macro-actions to refer to options.\n\n[@bradtke1994reinforcement] a paper from 1994 on reinforcement learning with hierarchies of machines.\n\n\n\n\n\n\n\n\nOptions & CI\n\n\n\n\nThis type of formulation seems very similar to that used by Judea Pearl in his structureal graphical model of Causality. If we can express options as a graph of states we can use his algorithms to infer the best options to take in a given state.\noptions are like do operations (interventions)\nchoosing between options is like conterfactual reasoning."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l4g1",
    "href": "posts/coursera/c3-w1.html#sec-l4g1",
    "title": "On-Policy Prediction with Approximation",
    "section": "Deriving the TD-update with linear function approximation",
    "text": "Deriving the TD-update with linear function approximation\n\nLinear function is both:\nsimple enough to be understood, yet\npowerful enough that with TD to be useful to create agents that are stornger than human Atari games.\nThe TD-update with linear function approximation is a way to update the weights of the value function using the TD-error.\nThe TD-update with linear function approximation works as follows:\n\nCompute the TD-error \\delta as the difference between the one-step bootstrapped return and the approximate value of the next state.\nUpdate the weights \\mathbf{w} in the direction of the TD-error.\n\n\nrecall the TD-update rule:\n\n\\delta \\dot= R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}) \\\\\nw \\leftarrow w + \\alpha \\delta_t \\nabla \\hat{v}(S_t, \\mathbf{w})\n in the linear case we can write the value function as:\n\n\\hat{v}(S_t, \\mathbf{w}) \\dot = \\sum \\mathbf{w}^T \\mathbf{x}(S_t) \\\\\n\\nabla \\hat{v}(S_t, \\mathbf{w}) = \\mathbf{x}(S_t)\n\n\nw \\leftarrow w + \\alpha \\delta_t \\mathbf{x}(S_t)"
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l4g2",
    "href": "posts/coursera/c3-w1.html#sec-l4g2",
    "title": "On-Policy Prediction with Approximation",
    "section": "Tabular TD(0) is a special case of linear semi-gradient TD(0)",
    "text": "Tabular TD(0) is a special case of linear semi-gradient TD(0)\n\nTabular TD(0) is a special case of linear semi-gradient TD(0) where the features are one-hot encoded.\nIn the tabular case, the weights are the same as the value function in the table.\nIn the linear case, the weights are the parameters of the value function.\nTabular TD(0) can be seen as a special case of linear semi-gradient TD(0) where the features are one-hot encoded."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l4g4",
    "href": "posts/coursera/c3-w1.html#sec-l4g4",
    "title": "On-Policy Prediction with Approximation",
    "section": "Advantages of linear value function approximation over nonlinear",
    "text": "Advantages of linear value function approximation over nonlinear\n\nLinear value function approximation has several advantages over nonlinear value function approximation:\n\nLinear value function approximation is computationally efficient and easy to implement.\nLinear value function approximation is easy to interpret and understand.\nLinear value function approximation is less prone to overfitting than nonlinear value function approximation.\nLinear value function approximation can be used to approximate any function, while nonlinear value function approximation is limited by the choice of features.\n\nIf we have access to expert knowledge we can use it to define good features and use linear value function approximation to learn the value function quickly.\nMost of the theory of function approximation in reinforcement learning is based on linear value function approximation."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l4g5",
    "href": "posts/coursera/c3-w1.html#sec-l4g5",
    "title": "On-Policy Prediction with Approximation",
    "section": "The fixed point of linear TD learning",
    "text": "The fixed point of linear TD learning\n\nw_{t+1} \\dot= w_t + \\alpha [R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w})] \\nabla \\hat{v}(S_t, \\mathbf{w})\n recall that in the linear case we defined the approximate value function as:\n\n\\hat{v}(S_{t+1}, \\mathbf{w}) \\cdot=  \\mathbf{w}^T \\mathbf{x}(S_{t+1})\n with :\n\n\\mathbf{w} the weights of the value function\n\\mathbf{x}(S_{t+1}) the features of the next state\n\nusing this definition we can write the update rule as:\n\n\\begin{align*}\nw_{t+1} & = w_t + \\alpha [R_{t+1} + \\gamma \\mathbf{w}^T \\mathbf{x}_{t+1} - \\mathbf{w}^T \\mathbf{x}] \\mathbf{x}_t \\newline\n        &=  w_t + \\alpha [R_{t+1} \\mathbf{x}_t -  \\mathbf{x}_t( \\mathbf{x}_t - \\gamma \\mathbf{x_{t+1}})^T \\mathbf{w_t}] \\newline\n\\end{align*}\n\nlet us now consider what this update looks like in expectation:\nwe can think about it as an expected update plus a noise term but the noise term is dominated by the behaviour of the expected update.\n\n\\mathbb{E}[\\Delta w_{t}]  = \\alpha(b-Aw_t)\n where:\n\nb = \\mathbb{E}[R_{t+1} \\mathbf{x}_t] - expectation over the features and the rewards\nA = \\mathbb{E}[\\mathbf{x}_t( \\mathbf{x}_t - \\gamma \\mathbf{x_{t+1}})^T] - an expectation over the rewards\n\nnote: this is a linear system of equations that looks like a linear regression problem.\nwhen the weights do not change we have a fixed point:\n\n\\begin{align*}\n\\mathbb{E}[\\Delta w_{TD}] & = \\alpha(b-Aw_{TD}) = 0 \\newline\n\\implies & w_{TD} = A^{-1}b\n\\end{align*}\n\\tag{11} more generally w_{TD} is the solution to this equation and we could show that it minimises\n\n(b-Aw)^T(b-Aw)\n\\tag{12}\nthis is related to Bellman equations via the projected Bellman error."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l4g6",
    "href": "posts/coursera/c3-w1.html#sec-l4g6",
    "title": "On-Policy Prediction with Approximation",
    "section": "Theoretical guarantee on the mean squared value error at the TD fixed point",
    "text": "Theoretical guarantee on the mean squared value error at the TD fixed point\n\n\\overline{VE}(w_{TD}) \\leq \\frac{1}{1-\\gamma} \\min_{w} \\overline{VE}(w)\n\\tag{13}\n\nif \\gamma \\approx 1 then the mean squared value error at the TD fixed point can be large\nif \\gamma \\approx 0 then the mean squared value error at the TD fixed point can be small\nif the features representation is good then the two will be equal regardless of \\gamma. since both will be almost zero."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#sec-l4g7",
    "href": "posts/coursera/c3-w1.html#sec-l4g7",
    "title": "On-Policy Prediction with Approximation",
    "section": "Semi-gradient TD(0) algorithm",
    "text": "Semi-gradient TD(0) algorithm\nIn the assignment I implemented the Semi-gradient TD(0) algorithm for value estimation."
  },
  {
    "objectID": "posts/coursera/c3-w1.html#footnotes",
    "href": "posts/coursera/c3-w1.html#footnotes",
    "title": "On-Policy Prediction with Approximation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwhy not?↩︎\nthe property that the future is independent of the past given the present↩︎"
  },
  {
    "objectID": "posts/papers/Evolution-of-language/index.html",
    "href": "posts/papers/Evolution-of-language/index.html",
    "title": "The Evolution of Language",
    "section": "",
    "text": "litrature review\nI came accross this paper after reading (Skyrms 2010) and (Lewis 1969) and I was looking at models that give some ideas on how languages might evolve but in particular how parctical constraints might shape the evolution of languages. This paper is one such stepping stone towards a more principled approach to engineering language for RL agents.\nThis is an early paper on emergence of languages. It quite a bit of a challenge to read. The authors are experts on evolutionary game theory and this is another aspect which is less familiar to me. However there is lots of clever thinking here and the more I ponder this paper the more I get to develop my own ideas and intuitions."
  },
  {
    "objectID": "posts/papers/Evolution-of-language/index.html#the-review",
    "href": "posts/papers/Evolution-of-language/index.html#the-review",
    "title": "The Evolution of Language",
    "section": "The Review",
    "text": "The Review\n\nThe Evolution of Signal–Object Associations.\nThis authors describes a setup that is very similar to Lewis signaling game. Each agent is both a sender and a receiver. They payoffs are also made symmetric as they get the averaged expected payoffs for each individual. I.e. each might have a different expected payoffs (one might be a poor sender but a good reciever) The other might be great at both. But since the payoffs are averaged, the game becomes symmetric and therefore cooperative!\n\nEach agents A_i is initially assigned a randomized language \\mathcal{L}_i=&lt;P,Q&gt; . There are two agents per interaction but many agents in the population. Payoffs are symmetric but with can communicate better get a higher payoffs. So this game also combines cooperation and competition.\nI think that instead of working with binary permutations matrices the agents use probability matrices. These are continuous and may be easier to optimize using gradient descent.\nFor RL - we can have a richer reward signal. (Though we could also get it for the lewis signaling) It is the mean of each agent’s expected communication success with the other agents.\nIn terms of modeling it demonstrates that we can combine\n\na cooperative communication paradigm with (symmetric payoff)\na competitive evolutionary paradigm (zero sum game)\n\nThe formulation is probabilistic.\n\nThe output matrix can be interpreted as a probabilistic mapping and analyzed by by expanding the terms as nested sums of the products of the input matrices.\n\nThe notion of using a phonemic uncertainty matrix is very interesting. It is a simple way to model the uncertainty in the mapping between signals and states. And is mapped back to the similarity of the phonemes.\nIt could be extended to add salience, signaling risk levels, if they are formulated in terms of probabilities. By multiplying additional matrices.\nWe could also use a diagonal block matrix to model normal subgroup to model subspace of the state space. Which would maker the\nWe might be able to add Welch-Hadamard blocks in the diagonal to create entangled error correcting subspace. This would allow us to model bound states in the state space. These are embedding subspace.\nFinally I think that we might also use Self-Organizing maps to learn the subspace structure of the state space and use it to find a matching compositional communication structures.\n\n\nWhat remains to be seen is if it leads to more desireable set of equilibria that are easier to learn, more robust to perturbation and other desiderata that enhance a signaling system into a language.\n\n\n\n\n\n\nSome challenges in the approach\n\n\n\nThe agents are not learning through communications but rather being selected in proportion to their fitness (Expected communication ability). Which means the agent with the best ability to communicate with most other agents will dominate the population, rather than the agent with the best signaling system.\nIf we dig a bit deeper. If there are two types of agents - SS_i agents which a maximally incompatible signaling system1 - CP_i which uses a completely pooling system (one that ignores the input and randomizes the output)\nthen we have:\n0=F(SS_i,SS_j) &lt; F(CP,SS_i) = 1/n \\qquad \\forall i.\ni.e. guessing is better then perfect miss-coordination.\nOf course this kind of assignment is unlikely to come up in a random assignment. But it indicates that perfect signaling systems are less likely to evolve if they must compete with many more imperfect ones with mutual that can partially coordinate with more agents.\nI.e. agents with poor multilingual capabilities will have a comparative advantage over agents with monolingual abilities if the monolingual ones are sufficiently sparse.\nif we swap out the CP agent with a PP agent (partial pooling) the\nI guess this issue could be mitigated by having very many more agents then equilibria. But as signaling systems are grow as N! and they are the sparse in the possible equilibria, this is not feasible for large signaling systems.\nTo explore these notions we need an efficient learning algorithm that can learn the equilibria that are also robust to errors. My Bayesian Adaptive RL algorithm can handle errors, so it might be a good start. However I have yet to consider it complexity in the number of agents and signals.\nA new algorithm in the works that is also aware of actions of normal subgroups to structure state into subspaces is likely to be more efficient to learn and to scale better.\n\n\n\n\n\n\n\n\nbasic Evolutionary language game vs Lewis signaling game\n\n\n\nThe paper makes no mention of the Lewis signaling game and only cites sources on evolutionary game theory, population dynamics and biological signaling. But the basic evolutionary language game is very similar to the Lewis signaling game. I have not analysed it but from the text in figure 1, I think it has the same types of equilibria.\nI also simulated some algorithms in which agents had (P,Q) and (P’,Q’) belief or urn matracies when learning the Lewis signaling game. But my algorithms were RL and Bayesian RL.\nWhat seems differrnt from the lewis is that the agents have different Languages L=(P,Q) and L’=(P’,Q’) and they evolve through population dynamics. But the agent’s intial linguistic endowment is not necessarily optimal. I.e. we are not told P*Q = 1 rather that \\sum_i p_{ij} =1 they have random values. In such a setup it unclear if evolution will lead to a separating equilibrium or just reinforce the initial bias.\n\n\nIn the basic evolutionary language game with\n\nm sounds (signals)\nn objects\nThe active matrix P with entries p_{ij}, denoting the probability that for a speaker that an object i is associated with sound j.\nA passive matrix Q contains the entries q_{ji}, that denote the probability that for a listener that a sound j is associated with object i.\n\nIt is not clear how an agent uses the active matrix to produce a signal given a some object j.\n\nDo they pick a signal i with probability p_{ij} - this is a Bayesian interpretation of the active matrix witch each agent having a subjective Language \\mathcal{L}.\nDo they use a MAP estimate, i.e. pick the most likely signal, - this approach is closest to the Lewis signaling game. This is a deterministic interpretation of the active matrix.\nDo they they send all signals at once weighted by the p_{ij}?\n\nI prefer 1 as it is how I tend to simulate it.\n\nSuppose A sees object i and signals, then B will infer object i with probability \\sum_{j=1}^m p_{ij} q_{ji}\n\n\nThe overall payoff is symmetric for communication between A and B is taken as the average of A’s ability to convey information to B, and B’s ability to convey information to A.\n\n\n\\begin{align*}\n\\mathbb{E}[\\text{Payoffs}\\mid L,L'] &= \\frac{1}{2}  \\overbrace{\\sum_{i=1}^n \\sum_{j=1}^m p_{ij} q'_{ij}}^{\\text{A's ability to send interpretable messages}}  + \\frac{1}{2}  \\overbrace{\\sum_{i=1}^n \\sum_{j=1}^m p'_{ji} q_{ji}}^{\\text{B's ability to interpret messages}} \\\\&=\n\\frac{1}{2}  \\sum_{i=1}^n \\sum_{j=1}^m ( p_{ij} q'_{ij} +p'_{ji} q_{ji} ) =\n\\end{align*}\n\nThe original formula is rather cryptic. So this version breaks it into parts and annotates it.\n\nwhere:\n\nL = &lt;P,Q&gt; is the language of speaker and\nL'= &lt;P',Q'&gt; is the language of the listener.\n\n\nThis seems like an expectation of the joint probability of the speaker and listener.\nso far this seems to be very much aligned with the Lewis signaling game.\n\n\n\n\n\n\n\nFigure 1: emergence of a language by population dynamics\n\n\nFigure 1 shows how a signaling system emerges through population dynamics.\nNotes:\n\nthe agents don’t actually learn during the evolution, rather generate offsprings in proportion to their fitness. So with a bit of luck one agent will eventually dominate the population and all the other types will die out.\nAgents with lower fitness are replaced by agents with higher fitness. This means that we are just reinforcing the initial bias towards the most central agent in the cluster as it will have the highest fitness.\nDue to the layouts of the figures, I always think that the agents were on a grid talking with their neighbors, but I believe they are on a simplex and all talk to each other.\n\nThe agents evolve a language that\nThe paper again lacks some important details. Are the agents the details\n\n\nWord Formation\n\n\n\n\n\n\n\nFigure 2: figure 2\n\n\nIf we increase our basic signals (think phonemes) we can handle more states. However phonemes exist in a restricted space and as more are added it becomes harder to distinguish between them. This is exacerbated by the fact that we add an explicit chance of communications error based on the phonemic similarity.\n\n\\begin{align*}\n\\mathbb{E}[\\text{Payoffs}\\mid L,L'] &= \\frac{1}{2}  \\sum_{i=1}^n \\sum_{j=1}^m \\left [  p_{ij} \\left( \\sum_{j=1}^m u'_{jk}q'_{ki} \\right) + p'_{ij} \\left( \\sum_{j=1}^m u_{jk}q_{ki} \\right) \\right] =\n\\end{align*}\n\nwhere:\n\nU are matrices with u_{ij}=s_{ij}\\sum_{k=1}^m s_{ik} and\ns_ij is the similarity between sounds (signals) i and j.\n\n\n\nThe Evolution of Basic Grammatical Rules.\n\n\n\n\n\n\n\nFigure 3: figure 3\n\n\n\n\n\n\n\n\nFigure 4: figure 4"
  },
  {
    "objectID": "posts/papers/Evolution-of-language/index.html#the-paper",
    "href": "posts/papers/Evolution-of-language/index.html#the-paper",
    "title": "The Evolution of Language",
    "section": "The paper",
    "text": "The paper\nthe source\n\n\n\nembeded paper"
  },
  {
    "objectID": "posts/papers/Evolution-of-language/index.html#some-ideas",
    "href": "posts/papers/Evolution-of-language/index.html#some-ideas",
    "title": "The Evolution of Language",
    "section": "Some ideas",
    "text": "Some ideas\nI spent a large amount of time to see how the game is related to the Lewis signaling game.\nWhat I slowly came to realize is that the formulation in this paper is potentially more general then the Lewis signaling game.\nWhat I understood is that in the Lewis signaling agents are trying to learning invertible mapping from an to signal space.\nIn this as far as I can tell, the agents are ‘modeling’ such mappings using mixtures (of states or signals). If the Lewis signaling uses binary matrices to model connections between signals and states, this formulation is uses continuous random variables to model the connections. We can go to a binary matrix by using the MAP estimate of the mixture.\nI found this formulation is rather annoying at first glance but I stated to see its potential in the second variant. In this extension agents also apply a mapping that corresponds to phonemic uncertainty. This can also be viewed as a noisy filter. Anyhow they end up with a game with more complex mappings in which similar signals are more likely to lead to the wrong state.\nIt is a very simple way to model the uncertainty in the mapping between signals and states. One of the advantages of this approach is that it can be expanded to add saliency, risk levels that can allow us to understand how such constraints shape natural language.\nBased on the similarity of the phonemes, the agents can make errors in communication. This is a very interesting idea that I have not seen before. It is a very simple way to model the uncertainty in the mapping between signals and states.\nIn this the agents compose a mapping corresponding to phonemic uncertainty. Based on the similarity of the phonemes, the agents can make errors in communication. This is a very interesting idea that I have not seen before. It is a very simple way to model the uncertainty in the mapping between signals and states. . This is a very interesting idea that I have not seen before. It is a very simple way to model the uncertainty in the mapping between signals and states."
  },
  {
    "objectID": "posts/papers/Evolution-of-language/index.html#footnotes",
    "href": "posts/papers/Evolution-of-language/index.html#footnotes",
    "title": "The Evolution of Language",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthis is a new idea. IF we look at the n! permutations matrices we will generally find a shared signal state mapping. But if we restrict to one permutation matrix say I and shit it down once we get no two mappings with a match. e.g. [1,2,3] and [2,3,1] and [3,2,1] are a maximally incompatible set for n = 3.↩︎"
  },
  {
    "objectID": "posts/papers/BabbleLabble/index.html",
    "href": "posts/papers/BabbleLabble/index.html",
    "title": "Training Classifiers with Natural Language Explanations",
    "section": "",
    "text": "litrature review\n\n\n\n\n\n\n\nFigure 1: Babble Labble: Learning From Natural Language Explanations (NIPS 2017 Demo)\n\n\n\n\n\n\n\n\nFigure 2: Braden Hancock - Training Classifiers with Natural Language Explanations\nIn (Hancock et al. 2018) the authors consider how to learn labeling functions from natural language explanations. Such explanations can come from a data labeler on Amazon mechnical Turk, a domain expert and perhaps from an LLM. Labeling function capture a heuristic for labeling data. The author ties it up with some work on data programming which lead to snorkel another data labeling tool. This paper isn’t about RL at all. It is interesting for a number if reasons.\nThe hook for me was its approach to aggregation. Since Scott E. Page pointed out the challenges of aggregation in his book The Difference I have been considering how it manifests in many forms - particularly in language emergence.\nThe chart I saw for the presentation was extremely similar to another chart I had seen in a paper on emergent languages. It looked like this paper was solving an aggregation problem I had been considering in emergent languages. It turned out to be a coincidence. These are different problems, and they aggregation is for different things. And yet there still seems to be an underlying similarity."
  },
  {
    "objectID": "posts/papers/BabbleLabble/index.html#abstract",
    "href": "posts/papers/BabbleLabble/index.html#abstract",
    "title": "Training Classifiers with Natural Language Explanations",
    "section": "Abstract",
    "text": "Abstract\n\nTraining accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100× faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices. — (Hancock et al. 2018)"
  },
  {
    "objectID": "posts/papers/BabbleLabble/index.html#outline",
    "href": "posts/papers/BabbleLabble/index.html#outline",
    "title": "Training Classifiers with Natural Language Explanations",
    "section": "Outline",
    "text": "Outline\n\n\n\n\n\n\n\nSystem Overview\n\n\n\n\nFigure 4: Natural language explanations are parsed into candidate labeling functions (LFs). Many incorrect LFs are filtered out automatically by the filter bank. The remaining functions provide heuristic labels over the unlabeled dataset, which are aggregated into one noisy label per example, yielding a large, noisily-labeled training set for a classifier.\n\n\n\n\n\n\n\n\nSeamntic Parse of An Explanation\n\n\n\n\nFigure 5: Valid parses are found by iterating over increasingly large sub-spans of the input looking for matches among the right hand sides of the rules in the grammar. Rules are either lexical (converting tokens into symbols), unary (converting one symbol into another symbol), or compositional (combining many symbols into a single higher-order symbol). A rule may optionally ignore unrecognized tokens in a span (denoted here with a dashed line).\n\n\n\n\nIntroduction\n\nDescribes the standard protocol for collecting labeled data for training classifiers.\nHighlights limitations of labeling: each label only provides a single bit of information.\nMentions previous works’ approaches to improve information gain from examples.\nPresents BabbleLabble, a framework where annotators provide natural language explanations for each labeling decision.\n\n\n\nThe BabbleLabble Framework\n\nDescribes how BabbleLabble converts explanations and unlabeled data into a noisy training set.\nPresents the three main components of BabbleLabble: semantic parser, filter bank, and label aggregator.\nMentions how explanations provide high-level information about patterns in the data.\nNotes that the semantic parser converts explanations into a set of logical forms representing labeling functions.\nExplains how the filter bank removes incorrect labeling functions based on semantic and pragmatic criteria.\nDescribes how the label aggregator combines labels from the labeling functions into probabilistic labels for each example.\nExplains the benefits of training a discriminative model using the noisy labels instead of classifying directly with the label aggregator.\n\n\n\nExplanations\n\nDiscusses the format and content of user-provided explanations.\nHighlights that explanations should refer to specific aspects of the example.\n\n\n\nSemantic Parser\n\nDescribes the goal of the semantic parser: generate a set of candidate labeling functions (LFs).\nPresents the rule-based semantic parser used in BabbleLabble and its key features.\nDiscusses the parser’s grammar and the included predicates.\nNotes that the parser is domain-independent, allowing transferability to new tasks.\n\n\n\nFilter Bank\n\nExplains the role of the filter bank: remove incorrect labeling functions without requiring ground truth labels.\nDiscusses the two types of filters: semantic and pragmatic.\nDescribes the purpose and operation of semantic and pragmatic filters.\nHighlights the effectiveness of the filter bank in removing incorrect labeling functions.\n\n\n\nLabel Aggregator\n\nExplains the function of the label aggregator: combine potentially conflicting labels from multiple labeling functions into a single probabilistic label.\nDiscusses the limitations of a simple majority vote approach.\nPresents the data programming approach used in BabbleLabble, which models the relationship between true labels and labeling function outputs as a factor graph.\n\n\n\nDiscriminative Model\n\nDiscusses the advantages of using a discriminative model trained with noisy labels.\nExplains how a discriminative model can leverage features not explicitly mentioned in explanations.\nNotes that the noisy labels provide a form of weak supervision that promotes generalization.\n\n\n\nExperimental Setup\n\nDescribes the three relation extraction tasks used for evaluation: Spouse, Disease, and Protein.\nPresents details about each dataset: source, task description, and size.\nDiscusses the experimental settings: text preprocessing, semantic parser implementation, label aggregator, and discriminative model.\nMentions hyperparameter tuning and evaluation metrics.\n\n\n\nExperimental Results\n\nPresents the F1 scores achieved by BabbleLabble compared to traditional supervision.\nHighlights the rate of improvement in F1 score with the number of user inputs.\nDiscusses the effectiveness of the filter bank in removing incorrect labeling functions.\nPresents an analysis of the utility of incorrectly parsed labeling functions.\nCompares using labeling functions as functions versus features.\nDiscusses the impact of unlabeled data on performance.\n\n\n\nRelated Work and Discussion\n\nDiscusses previous work on learning from natural language explanations.\nMentions research on learning from weak supervision, particularly in the context of relation extraction.\nHighlights the potential of natural language as a high-bandwidth communication channel for machine learning.\nDiscusses future research directions, including applying the framework to other tasks and exploring more interactive settings.\n\n\n\n\n\n\n\nMy Thoughts\n\n\n\n\nBig ideas\n\nAlways be on the lookout for tricks on how to convert1 a supervised learning task into an unsupervised one. While this paper fails to do so, it does provide a step in the right direction and more so one that yields a 100x speed up in labeling tasks. In reality this claim is a marketing shtick for their paper, however the real point is that if you have a labeling function, its utility grows in proportion to the amount of unlabeled data you can bring to bear.\nBranden Hancock keeps reiterating that the bulk of the time spent in Labeling is understanding the text/data not the annotation. The authors point that time to elicit explanation is only double the time of labeling. Highlighting evidence or even writing an explanation is thus a small burden in comparison to just annotating the most significant part of the text? Perhaps not yet if we include the benefits of the labeling functions that result in 6-10x more labels the math works out.\nIt is easy to miss the most significant aspect of the paper - the importance of the Filter Bank in creating value. However there is an extensive literature on PBE (Program By Example) that they do not seem to be aware of which can convert such examples into programs.\nAnother point made by Branden in support of Explanation is that you can’t highlight when the evidence is negative - i.e. when the classification is grounded in some context not being in the text.\nA third reason to like explanations is that data-centric tasks are forever changing. There are new requirements (e.g. a new class) or drifts in the classifiers’s distributions. If you have collected labels you need to rethink the project but if you collected explanations it is easier to retain on more data then relabel. If there is a better semantic parser, you can swap it and re-train.\n\n\n\nDevils and Details\n\nThe most interesting aspects of the paper for RL are how an agent interacting with people can elicit explanation that can then be used to create labeling functions.\nThe idea of how the labeling function are aggregated is also instructive. That said, I can’t say that this is a big idea - it looks very much like ranking, weighting and even less powerful than TD-IDF. So what I mean is that when we learn a language probably want to learn features from functions not labels as labels do not generalize. If we also have a framing game that is driving language learning then we may have further uses for a sensible form of aggregation in our classifier. More abstractly, the RL agent needs to pick an action - that is usually like a classification of a state into an action space. For Life long learning we could\nQ & A are a good often considered in the Semantic Parsing literature.\nLLM may well be useful for closing the weak supervision loop. I.e. one can use a general purpose query to elicit explanations from an LLM. With a range of prompts one should be able to get different explanations.\n\n\n\nAction Items\n\nTry babble labble, snorkel, prodigy, and SippyCup.\nMake a MVP of the filter bank.\nmake a cheat-sheet on how to convert a supervised learning tasks into an unsupervised ones. These are ideas that disrupted the field of ML.\nclose the loop on weak supervision with LLM for a RL agent\ncompare this approach to other PBE\n\n\n\n\nThe people behind BabbleLabble are also behind Snorkel. Snorkel is a framework for building weakly supervised models. It is a more general framework that can be used for a wide range of tasks. BabbleLabble is a specific application of Snorkel to the task of training classifiers with natural language explanations. The key difference is that BabbleLabble focuses on the use of natural language explanations to generate labeling functions, while Snorkel is a more general framework for building weakly supervised models.\nA point made on the Snorkel Site is that the people who worked on the tool moved on to building a platform. Is this just to monetize their work? My guess is not. In reality there are many other tools that do much the same thing. Labeling data is not very glamorous but building a tool is just a ui and a classifier. You can’t lock clients in and it is likely that each project required expensive customization. At some point it becomes simpler to do this in a form that is more general and can be used by future clients. This is harder in open source as you need consensus and a community. On the other hand in a platform each time you do a project you have added value to the platform.\nFrom other talks by Snorkel people it seems that many companies invest massively in labeling datasets (i.e. teams with sizes of hundreds). Thus every small improvement in the process can have a big impact. And as a business model it makes a lot of sense to try to bring as many of those improvements in house.\n\n\n\n\n\n\nAggregation your’e No good for me\n\n\n\nAggregation can take simple underlying components and combine them into arbitrarily complex structures. This is a powerful idea that is used in many areas of science and engineering. Here are a few examples:\n\nThe Statistics the Central limit theorem allow us to aggregate many random variables into a Normal distributed one under fairly broad conditions.\n\n“Aggregate statistics can sometimes mask important information”. – Ben Bernanke\n\nIn Physics we can aggregate the behavior of many particles into that of an ensemble. We have a number of examples, the most famous being the ideal gas, statistical mechanics, the Ising model and the Potts model. Though the idea of aggregation is very much endemic to many areas of physics.\n\n“Imagine how difficult physics would be if electrons could think.” – Murray Gell-Mann\n\nIn the case of a classifier we can aggregate many weak classifiers into a strong one. This is the idea behind boosting and bagging. This idea is behind well known algorithms like Random Forest and Gradient Boosting and even Stable Diffusion\nIn the case of a language we can aggregate the meaning or semantics of many smaller units of meaning into larger ones. This is the idea behind semantics. This idea is behind well known algorithms like Word2Vec and BERT and even GPT-3\nPreferences can be aggregated into utility functions. This is the idea behind utility theory. This idea is behind demand theory.\nInformation aggregation is the notion that the wisdom of the crowd is better than the wisdom of the individual. The Wisdom of the Crowds points out that diverse opinions can play a big role here. There are other notions here like the Delphi Method.\nSensor Fusion via Kalman Filters, SLAM, and Particle Filters are ways to aggregate information from sensors. The Kalman Filter is a way to aggregate information from a sensor. SLAM is a way to aggregate information from a sensor. Particle Filters is a way to aggregate information from a sensor.\nSocial Networks have a number of ways to aggregate information. PageRank is a way to aggregate the importance of a node in a network. The Small World Phenomenon is a way to aggregate the number of hops between two nodes. The Strength of Weak Ties is a way to aggregate the number of connections between two nodes. The Friendship Paradox is a way to aggregate the number of friends a person has.\nelections and voting have the notion of aggregation. Arrow’s Impossibility Theorem points out that there is no perfect way to aggregate preferences. The Condorcet Paradox points out that there is no perfect way to aggregate opinions. The Gibbard-Satterthwaite Theorem points out that there is no perfect way to aggregate votes.\nIn chaos theory many chaotic systems can become synchronized."
  },
  {
    "objectID": "posts/papers/BabbleLabble/index.html#the-paper-annotated",
    "href": "posts/papers/BabbleLabble/index.html#the-paper-annotated",
    "title": "Training Classifiers with Natural Language Explanations",
    "section": "The Paper Annotated",
    "text": "The Paper Annotated\n\n\n\npaper"
  },
  {
    "objectID": "posts/papers/BabbleLabble/index.html#footnotes",
    "href": "posts/papers/BabbleLabble/index.html#footnotes",
    "title": "Training Classifiers with Natural Language Explanations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTODO: make a cheat-sheet on this topic!?↩︎"
  },
  {
    "objectID": "posts/papers/DCTS/index.html",
    "href": "posts/papers/DCTS/index.html",
    "title": "Dynamic collaborative filtering Thompson Sampling for cross-domain advertisements recommendation",
    "section": "",
    "text": "litrature review\nSo I don’t have much time for this today so here is a quick note on: (Ishikawa, Chung, and Hirate 2022)\nOne line on Thompson sampling, one of the oldest technique in the RL playbook which uses the following rule: pick an action at random from the posterior distribution of the action values and then use the outcome to update the posterior distribution for the next step."
  },
  {
    "objectID": "posts/papers/DCTS/index.html#the-paper",
    "href": "posts/papers/DCTS/index.html#the-paper",
    "title": "Dynamic collaborative filtering Thompson Sampling for cross-domain advertisements recommendation",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This are my notes on Reinforcement learning. I got started with David Silver’s lectures on YouTube Then took the four course specilization By Martha nad Adam White from Alberta Coursera. That coverd most of Sutton and Barto’s book. This is a nice course and I got a perfect grade. But I can’t put up the code or quizzes online due to the honor code. - The Deep Reinforcement Learning course by Hugging Face does not have this restrictions. So I’m now working my way theough this.\n\nRichard S. Sutton has some suggestions like keeping a research notebook. I kept having questions during study and I found that by keeping a notebook I could get some of them out of my head. I could even solve some or make progress.\nThe specilization also featured a large number of resaerchers in the field.\nI stated looking them up and seeing if they had interesting talks online.\n\nI started doing that too. The next step is to consolidate the many notes into this blog.\nthey answers to questions no botherd to ask\nThis is a space to share my insights about my interests.\n\nOpen.AI has a RL blog. They list a bunch of papers on deep RL. I plan to read them and write about them here time premmiting.\nHowever there is lots of intersting material I found online\nTalk on double robust tomphson sampling\nAdKDD 2022 Dynamic collaborative filtering Thompson Sampling for cross-domain ad recommendation\npaper"
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "This are my notes on Reinforcement learning. I got started with David Silver’s lectures on YouTube Then took the four course specilization By Martha nad Adam White from Alberta Coursera. That coverd most of Sutton and Barto’s book. This is a nice course and I got a perfect grade. But I can’t put up the code or quizzes online due to the honor code. - The Deep Reinforcement Learning course by Hugging Face does not have this restrictions. So I’m now working my way theough this.\n\nRichard S. Sutton has some suggestions like keeping a research notebook. I kept having questions during study and I found that by keeping a notebook I could get some of them out of my head. I could even solve some or make progress.\nThe specilization also featured a large number of resaerchers in the field.\nI stated looking them up and seeing if they had interesting talks online.\n\nI started doing that too. The next step is to consolidate the many notes into this blog.\nthey answers to questions no botherd to ask\nThis is a space to share my insights about my interests.\n\nOpen.AI has a RL blog. They list a bunch of papers on deep RL. I plan to read them and write about them here time premmiting.\nHowever there is lots of intersting material I found online\nTalk on double robust tomphson sampling\nAdKDD 2022 Dynamic collaborative filtering Thompson Sampling for cross-domain ad recommendation\npaper"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rl-notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nTraining Classifiers with Natural Language Explanations\n\n\nPaper Review\n\n\n\nPaper\n\n\nReview\n\n\nNLP\n\n\nClassification\n\n\nNamed Entity Recognition\n\n\n\nA review of the paper ‘Training Classifiers with Natural Language Explanations’ by Hancock et al.\n\n\n\n\n\nTuesday, January 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOff-Policy Learning-to-Bid with AuctionGym\n\n\nPaper Review\n\n\n\nPaper\n\n\nReview\n\n\nAdvertising\n\n\n\nAdditional work with AuctionGym\n\n\n\n\n\nSaturday, January 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLearning to Bid with AuctionGym\n\n\nPaper Review\n\n\n\nPaper\n\n\nReview\n\n\nAdvertising\n\n\n\nIntroducing (AuctionGym)\n\n\n\n\n\nSaturday, January 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic collaborative filtering Thompson Sampling for cross-domain advertisements recommendation\n\n\nPaper Review\n\n\n\nPaper\n\n\nReview\n\n\nBandit\n\n\nAdvertising\n\n\nCollaborative Filtering\n\n\n\nIntroducing (DCTS) The Dynamic Collaborative Filtering Thompson Sampling algorithm for cross-domain advertisements recommendation.\n\n\n\n\n\nSaturday, January 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOn Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain\n\n\nPaper Review\n\n\n\nReinforcement Learning\n\n\nPaper\n\n\nReview\n\n\n\nA comparison of five mathematical models that predict student behavior in repeated decision-making tasks involving gains and losses.\n\n\n\n\n\nThursday, January 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal Abstraction in Reinforcement Learning with the Successor Representation\n\n\nPaper Review\n\n\n\nReinforcement Learning\n\n\nPaper\n\n\nReview\n\n\nTemporal Abstraction\n\n\nOptions\n\n\n\nThis paper review is an extended introduction to temporal abstraction using options. It covers lots of advanced concepts in reinforcement learning that were introduced in Doina’s Precup’s talk in the Coursera Specialization on Reinforcement Learning by Martha White and Adam White. The paper is a deep dive into the topic of options and the successor representation. It is a long paper with lots of advanced concepts and algorithms. The paper is a great resource for anyone interested in reinforcement learning and temporal abstraction.\n\n\n\n\n\nSunday, November 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEmergent Communication of Generalizations\n\n\npaper review\n\n\n\nsignaling games\n\n\nstub\n\n\n\n\n\n\n\n\n\nWednesday, October 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Evolution of Language\n\n\nPaper Review\n\n\n\nPaper\n\n\nReview\n\n\nSignaling Games\n\n\nEmergent Languages\n\n\nEvolutionary game theory\n\n\n\nIn this paper the authors model the evolutionof Languages through its adaptation to resist errors in communication.\n\n\n\n\n\nThursday, April 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPolicy Gradient\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nPrediction and Control with Function Approximation\n\n\nReinforcement Learning\n\n\n\n\n\n\n\n\n\nThursday, April 4, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nControl with Approximation\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nPrediction and Control with Function Approximation\n\n\nReinforcement Learning\n\n\n\n\n\n\n\n\n\nWednesday, April 3, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nConstructing Features for Prediction\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nPrediction and Control with Function Approximation\n\n\nReinforcement Learning\n\n\n\n\n\n\n\n\n\nTuesday, April 2, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nConstructing Features for Prediction\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nPrediction and Control with Function Approximation\n\n\nReinforcement Learning\n\n\n\n\n\n\n\n\n\nTuesday, April 2, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nOn-Policy Prediction with Approximation\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nPrediction and Control with Function Approximation\n\n\nReinforcement Learning\n\n\n\n\n\n\n\n\n\nMonday, April 1, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nOn-Policy Prediction with Approximation\n\n\nPrediction and Control with Function Approximation\n\n\n\nCoursera\n\n\nPrediction and Control with Function Approximation\n\n\nReinforcement Learning\n\n\n\n\n\n\n\n\n\nMonday, April 1, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSample-based Learning Methods\n\n\nPlanning, Learning & Acting\n\n\n\nCoursera\n\n\nSample-based Learning Methods\n\n\nReinforcement Learning\n\n\n\nIn these module we define cover model based RL sampling. We start with the Dyna architecture. Then we consider tabular Q-planning algorithm, the Tabular Dyna-Q and Dyna-Q+ algorithms\n\n\n\n\n\nMonday, March 4, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal Difference Learning Methods for Control\n\n\nSample-based Learning Methods\n\n\n\nCoursera\n\n\nSample-based Learning Methods\n\n\nReinforcement Learning\n\n\n\nThis week, we will learn to using TD learning for control, as a generalized policy iteration strategy. We will see three different algorithms based on bootstrapping and Bellman equations for control: Sarsa, Q-learning and Expected Sarsa. We will see some of the differences between the methods for on-policy and off-policy control, and that Expected Sarsa is a unified algorithm for both.\n\n\n\n\n\nSunday, March 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal Difference Learning Methods for Prediction\n\n\nSample-based Learning Methods\n\n\n\nCoursera\n\n\nSample-based Learning Methods\n\n\nReinforcement Learning\n\n\n\nIn these unit we define some key terms like rewards, states, action, value functions, action values functions. Then we consider at the the multi-armed bandit problem leading to exploration explotation dillema, the epsilon greedy algorithm.\n\n\n\n\n\nSaturday, March 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMonte-Carlo Methods for Prediction & Control\n\n\nSample-based Learning Methods\n\n\n\nCoursera\n\n\nSample-based Learning Methods\n\n\nReinforcement Learning\n\n\n\nIn this module we learn about Sample based MC methods that allow learning from sampled episodes. We revise our initial algorithm to better handle exploration. In off policy RL we see methods to learn a policy using samples from another policy, corrected using importance sampleing.\n\n\n\n\n\nFriday, March 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Programming\n\n\nRL Fundamentals\n\n\n\nCoursera\n\n\nFundamentals\n\n\nReinforcement Learning\n\n\n\nIn week 4 we learn how to compute value functions and optimal policies, assuming you have the MDP model. You will implement dynamic programming to compute value functions and optimal policies and understand the utility of dynamic programming.\n\n\n\n\n\nThursday, May 5, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nValue Functions & Bellman Equations\n\n\nRL Fundamentals\n\n\n\nCoursera\n\n\nFundamentals\n\n\nReinforcement Learning\n\n\n\nIn week 3 we learn about Value Functions and Bellman Equations, which are the key technology behind all the algorithms we will learn. We learn the definition of policies and value functions, as well as Bellman equations.\n\n\n\n\n\nWednesday, May 4, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Decision Processes\n\n\nRL Fundamentals\n\n\n\nCoursera\n\n\nFundamentals\n\n\nReinforcement Learning\n\n\n\nIn week 2 we learn about Markov Decision Processes (MDP) and how to compute value functions and optimal policies, assuming you have the MDP model. We implement dynamic programming to compute value functions and optimal policies.\n\n\n\n\n\nTuesday, May 3, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nThe K-Armed Bandit Problem\n\n\nRL Fundamentals\n\n\n\nCoursera\n\n\nFundamentals\n\n\nReinforcement Learning\n\n\n\nIn week 1 we define some key concepts like rewards, states, action, value functions, action values functions. We consider the the multi-armed bandit problem, leading to exploration explotation dillema, and the epsilon greedy algorithm.\n\n\n\n\n\nMonday, May 2, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Introduction\n\n\nRL Fundamentals\n\n\n\nCoursera\n\n\nFundamentals\n\n\nReinforcement Learning\n\n\n\nIn week 1 we define some key concepts like rewards, states, action, value functions, action values functions. We consider the the multi-armed bandit problem, leading to exploration explotation dillema, and the epsilon greedy algorithm.\n\n\n\n\n\nSunday, May 1, 2022\n\n\nOren Bochman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/papers/Becoming-a-successful-loser/index.html",
    "href": "posts/papers/Becoming-a-successful-loser/index.html",
    "title": "On Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain",
    "section": "",
    "text": "I tracked this paper due to it being highlighted in (Skyrms 2010) as the source of a model that learns a signaling systems faster. I got me started with the loss domain. I was eventually able to find how to speed up learning by in the Lewis signaling game by considering the much more likely mistakes. Once I got on this algorithm I was thinking that using this idea for Bayesian updating of beliefs. This eventually led me to a second algorithm that was able to rapidly adjust a belief regarding the the state of the world in lewis signaling game with changing distributions.\nI’ts worth noteing that although this paper is about reinforment learning methods, the approach taken is more in line with how this is considered in the field of economics and pholosophy and less from machine learning or continous control. This is a good example of how the same problem can be approached from different fields. I would hazzard to say that the approches also align with the bandit settings."
  },
  {
    "objectID": "posts/papers/Becoming-a-successful-loser/index.html#abstract",
    "href": "posts/papers/Becoming-a-successful-loser/index.html#abstract",
    "title": "On Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain",
    "section": "Abstract",
    "text": "Abstract\n\nOne of the main difficulties in the development of descriptive models of learning in repeated choice tasks involves the abstraction of the effect of losses. The present paper explains this difficulty, summarizes its common solutions, and presents an experiment that was designed to compare the descriptive power of the specific quantifications of these solutions proposed in recent research. The experiment utilized a probability learning task. In each of the experiment’s 500 trials participants were asked to predict the appearance of one of two colors. The probabilities of appearance of the colors were different but fixed during the entire experiment. The experimental manipulation involved an addition of a constant to the payoffs. The results demonstrate that learning in the loss domain can be faster than learning in the gain domain; adding a constant to the payoff matrix can affect the learning process. These results are consistent with by (Roth and Erev 1995) adjustable reference point abstraction of the effect of losses, and violate all other models\n— (Bereby-Meyer and Erev 1998)"
  },
  {
    "objectID": "posts/papers/Becoming-a-successful-loser/index.html#outline",
    "href": "posts/papers/Becoming-a-successful-loser/index.html#outline",
    "title": "On Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain",
    "section": "Outline:",
    "text": "Outline:\n\nIntroduction\n\nHighlights the difficulty in developing descriptive models of learning in repeated choice tasks that involve potential losses.\nPresents the main goal of the paper: to compare the descriptive power of five distinct solutions to this difficulty and to identify a robust approximation of learning in simple decision tasks.\n\n\n\nThe Challenge and Alternative Solutions\n\n\n\n\nModels and Solutions\n\n\nDiscusses the difficulty in abstracting the effect of losses given the approximately linear relationship between choice probabilities and the ratio of accumulated payoffs observed in the gain domain.\nIntroduces probability learning tasks used to derive and compare the predictions of different models.\nPresents five alternative solutions to the problem:\n\nLow Reference Point (LRP):\n\nTransforms objective payoffs into non-negative rewards by subtracting the worst possible outcome\n\nAdjustable Reference Point and Truncation (ARP)\n\nUses an evolving reference point to distinguish gains and losses and truncates negative values to ensure positive propensities\n\nExponential Response Rule (EDS, EFP, EWA)\n\nApplies an exponential function to propensities, eliminating the need for handling negative values directly. Examples include Exponential Discounted Sum (EDS), Exponential Fictitious Play (EFP), and Experience Weighted Attractions (EWA) models\n\nCumulative Normal Response Rule (CNFP)\n\nUses a cumulative normal distribution to model the relationship between payoffs and propensities- Employs the cumulative normal distribution function to map propensities (which can be negative) to choice probabilities (which are always between 0 and 1). The CNFP model exemplifies this.\n\nRelative Reinforcement solutions (CLO)\n\nUses outcome-specific parameters to determine the impact of different outcomes on choice probabilities. The Cardinal Linear Operator (CLO) model demonstrates this.\n\n\nDescribes the specific implementations of these solutions through different models, including their assumptions and parameterizations.\n\n\n\nExperiment\n\nDescribes the experimental method, including the participants, the procedure, and the payoff structure of the probability learning task across three conditions.\n\n\n\nResults\n\n\n\n\nModels and Solutions\n\n\nPresents the aggregated experimental results, showing a significant effect of the reward condition on the proportion of optimal choices.\nCompares the quantitative predictive and descriptive power of the models using correlation and mean squared deviation (MSD) measures.\nDiscusses the between-subject variability observed in the data and the limitations of the models in capturing this variability.\nConducts a model-based analysis to evaluate the robustness of the condition effect.\nPerforms a sensitivity analysis to assess the robustness of the ARP model’s predictions to changes in parameter values.\n\n\n\nDiscussion\n\nDiscusses the main finding that the addition of constants to payoffs affects the speed of learning, highlighting the role of the distinction between gains and losses.\nNotes the advantage of the ARP model in capturing the observed results and acknowledges the potential validity of other solutions under specific assumptions or parameterizations.\nAddresses the generality of the findings by discussing:\n\nSettings where the ARP model’s predictions are consistent with previous research (probability learning, signal detection).\nSettings where the model might fail (learning among only positive outcomes, influence of other players’ payoffs).\n\n\n\n\nConclusions\n\nConcludes that human learning is affected by the distinction between gains and losses.\nEmphasizes that modeling this distinction, particularly through the adjustable reference point approach, improves the descriptive power of adaptive learning models.\nAcknowledges the need for further research to refine the quantification of the reference point for a more accurate and generalizable model."
  },
  {
    "objectID": "posts/papers/Becoming-a-successful-loser/index.html#key-takeaways",
    "href": "posts/papers/Becoming-a-successful-loser/index.html#key-takeaways",
    "title": "On Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nIn most RL settings rewards are sparse. One way to speed up learning is to try and increase our reward signal.\nThis is the basis for seeling out to decompose the reward signal into an internal motivation for the agent and an external motivation for the problem designer.\nAnother approach though is to consider the loss domain. If we can get signals out of losses we can speed up learning and RL agents are slow learners - especially deep RL agents.\nA third approach that I was able to make use of is to use both the loss domain and the gain domain to update beliefs about the possible states of the world. This was allowed me to speed up Bayesian learning algorithm for a coordination task.\n\nBesides this the paper has a lot of possible options for potential update rules to get this potential speed up.\n\nHow does adding a constant to all payoffs in a decision task affect learning, and which model best explains this effect?\n\nOne of the results I learned is that adding a constant to the payoff matrix doesn’t change it. In fact linear transformations of the payoff matrix don’t change the outcomes. In policy gradient methods this we call this trick learning with baselines. What we see is that it doesn’t bias the estimator but can drastically reduce the variance of the estimator. And this variance is the noise that slows down learning by the agent. So adding a constant can surprisingly impact learning speed. The ARP model uniquely predicts this: subtracting a constant to introduce losses speeds learning compared to a purely gain-based scenario. This highlights the psychological impact of the gain-loss framing.\nAnother insight I had about this is while trying to abstract the RL algorithms. Was that under some conditions we can convert the reward function into a distance metric. Having a metric makes navigation states space much simpler. I really can’t think of a better feature.\n\nWhat are the limitations of the ARP model?\n\n\nThe ARP model, with its current parameters, assumes an initial reference point of zero and a slow adjustment process. This might not hold when:\n\nAll options are positive: The model would predict slow learning even when clear differences exist.\nSocial comparison exists: People may adjust their reference point based on other players’ payoffs, a factor not currently incorporated in the model.\n\n\n\nHow would you define the loss domain and the gain domain ?\n\n\nThe gain domain is when choice probabilities are approximately linearly related to the ratio of accumulated reinforcement.\nThe loss domain is when negative payoffs are possible.\nIn the gain domain, the probabilities of choosing an alternative match the ratio of accumulated reinforcement, meaning that individuals are more likely to choose options that have yielded higher rewards in the past.\nDescriptive models have to assume that choice probabilities are determined by a function of the accumulated reinforcements, which must have strictly positive values. However, this presents a problem when losses are possible because negative payoffs can result in negative values for the function.\n\n\nIn the paper the autors mention the value function from prospect theory c.f. (Kahneman 1979). How does this relate to the ARP model?\n\nThe authors state that models that use solutions other than the adjustable reference point can account for the results of the study under the assumption that the model’s parameters can be affected by the payoffs. One way to account for this is to use reinforcement functions with the characteristics of Prospect Theory’s value function. Prospect theory, developed by Kahneman and Tversky, suggests that individuals make decisions based on the potential value of losses and gains rather than the final outcome, and that losses have a greater impact on individuals than gains do. This relates to the ARP model because it also assumes that reinforcements are evaluated relative to a reference point, meaning outcomes above the reference point are perceived as gains (reinforcements) and outcomes below the reference point are perceived as losses (punishments).\n\nIs there a formal definition of this prospect theoretic value function?\n\n\n\n\n\na value function\n\nA key element of this theory is the value function, which exhibits these characteristics: - It’s defined on deviations from a reference point. - It’s generally concave for gains and convex for losses. - It’s steeper for losses than for gains, meaning an equivalent loss has a greater psychological impact than the corresponding gain."
  },
  {
    "objectID": "posts/papers/Becoming-a-successful-loser/index.html#the-paper",
    "href": "posts/papers/Becoming-a-successful-loser/index.html#the-paper",
    "title": "On Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "posts/papers/AuctionGym/index.html",
    "href": "posts/papers/AuctionGym/index.html",
    "title": "Learning to Bid with AuctionGym",
    "section": "",
    "text": "litrature review\nSo I don’t have much time for this today so here is a quick note on: (Jeunen, Murphy, and Allison 2022)"
  },
  {
    "objectID": "posts/papers/AuctionGym/index.html#abstract",
    "href": "posts/papers/AuctionGym/index.html#abstract",
    "title": "Learning to Bid with AuctionGym",
    "section": "ABSTRACT",
    "text": "ABSTRACT\n\nOnline advertising opportunities are sold through auctions, billions of times every day across the web. Advertisers who participate in those auctions need to decide on a bidding strategy: how much they are willing to bid for a given impression opportunity. Deciding on such a strategy is not a straightforward task, because of the interactive and reactive nature of the repeated auction mechanism. Indeed, an advertiser does not observe counterfactual outcomes of bid amounts that were not submitted, and successful advertisers will adapt their own strategies based on bids placed by competitors. These characteristics complicate effective learning and evaluation of bidding strategies based on logged data alone.\n\n\nThe interactive and reactive nature of the bidding problem lends itself to a bandit or reinforcement learning formulation, where a bidding strategy can be optimised to maximise cumulative rewards. Several design choices then need to be made regarding parameterisation, model-based or model-free approaches, and the formulation of the objective function. This work provides a unified framework for such “learning to bid” methods, showing how many existing approaches fall under the value-based paradigm. We then introduce novel policy-based and doubly robust formulations of the bidding problem. To allow for reliable and reproducible offline validation of such methods without relying on sensitive proprietary data, we introduce AuctionGym: a simulation environment that enables the use of bandit learning for bidding strategies in online advertising auctions. We present results from a suite of experiments under varying environmental conditions, unveiling insights that can guide practitioners who need to decide on a model class. Empirical observations highlight the effectiveness of our newly proposed methods. AuctionGym is released under an open-source license, and we expect the research community to benefit from this tool."
  },
  {
    "objectID": "posts/papers/AuctionGym/index.html#the-bidding-objective",
    "href": "posts/papers/AuctionGym/index.html#the-bidding-objective",
    "title": "Learning to Bid with AuctionGym",
    "section": "The bidding Objective",
    "text": "The bidding Objective\n\n\n\nHigh-level overview of a real-time-bidding flow in computational advertising\n\n\nHigh-level overview of a real-time-bidding flow in computational advertising\n\nU = W(V − P)\n\\tag{1}\nwhere U is the utility, W is the weight, V is the value, and P is the price. The value V is the expected value of the impression, and the price P is the bid amount.\nThe utility U is the loss function. The goal is to maximize the utility U according to some contextual policy \\pi(B\\mid A; X).\nChoosing a Counterfactual Estimator\n\nValue-based Estimation (The “Direct Method”) High Bias model P(win|bid)\nPolicy-based Estimation (IPS) High Variance\nDoubly Robust Estimation Unbiased, lower variance\n\nHow do you evaluate this?\n\nOffline: use counterfactual estimators . . . &gt; “When a measure becomes a target, it ceases to be a good measure” (Goodhart’s Law)\nOnline: A/B-tests span weeks, require production-level prototypes, …\nSimulate\n\n\n\n\n\n\n\nMy ideas\n\n\n\n\nWhat do they mean that “auctions are not incentive compatible”?\nMarketing are the worst POMDPs. Testing real stuff is very hard so a good environment might help.\nSimulation is very powerfull as it allows to know the ground truth.\nHowever its not easy to simulate the real world and any discrepency may lead to unrealistic results."
  },
  {
    "objectID": "posts/papers/AuctionGym/index.html#the-paper",
    "href": "posts/papers/AuctionGym/index.html#the-paper",
    "title": "Learning to Bid with AuctionGym",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper\n\n\n\n\n\nslides\n\n\n\n\n\nauctiongym code"
  },
  {
    "objectID": "posts/papers/Offpolicy-AuctionGym/index.html",
    "href": "posts/papers/Offpolicy-AuctionGym/index.html",
    "title": "Off-Policy Learning-to-Bid with AuctionGym",
    "section": "",
    "text": "litrature review\nSo I don’t have much time for this today so here is a quick note on: (Jeunen, Murphy, and Allison 2023)"
  },
  {
    "objectID": "posts/papers/Offpolicy-AuctionGym/index.html#abstract",
    "href": "posts/papers/Offpolicy-AuctionGym/index.html#abstract",
    "title": "Off-Policy Learning-to-Bid with AuctionGym",
    "section": "Abstract",
    "text": "Abstract\n\nOnline advertising opportunities are sold through auctions, billions of times every day across the web. Advertisers who participate in those auctions need to decide on a bidding strategy: how much they are willing to bid for a given impression opportunity. Deciding on such a strategy is not a straightforward task, because of the interactive and reactive nature of the repeated auction mechanism. Indeed, an advertiser does not observe counterfactual outcomes of bid amounts that were not submitted, and successful advertisers will adapt their own strategies based on bids placed by competitors. These characteristics complicate effective learning and evaluation of bidding strategies based on logged data alone. The interactive and reactive nature of the bidding problem lends itself to a bandit or reinforcement learning formulation, where a bidding strategy can be optimised to maximise cumulative rewards. Several design choices then need to be made regarding parameterisation, model-based or model-free approaches, and the formulation of the objective function. This work provides a unified framework for such “learning to bid” methods, showing how many existing approaches fall under the value-based paradigm. We then introduce novel policy-based and doubly robust formulations of the bidding problem. To allow for reliable and reproducible offline validation of such methods without relying on sensitive proprietary data, we introduce AuctionGym: a simulation environment that enables the use of bandit learning for bidding strategies in online advertising auctions. We present results from a suite of experiments under varying environmental conditions, unveiling insights that can guide practitioners who need to decide on a model class. Empirical observations highlight the effectiveness of our newly proposed methods. AuctionGym is released under an open-source license, and we expect the research community to benefit from this tool.\n\n\n\n\n\n\n\nMy ideas\n\n\n\n\nFind what data set was used.\nIs this dataset available?\nCan we make a minimal version to quickly test this kind of agent?\nFigure out a framework that extends tompson sampling to other RL problems.\n\nneed to add P(action|state) i.e. add conditioning of the bernulli on the state.\nprehaps do simple counts of steps since starts or last reward.\nprehaps using a succeror representation can help\n\nMarketing are the worst POMDPs. Testing real stuff is very hard so a good environment might help.\nI want to make an petting zoo env to support single & multiagent:\n\nauctions / non autions\nadvertising (rec sys) with costs\npricing with policies.\n\n\nIt should also allow incorperating real data from a dataset. Diretly or via sampling\nIt would be even neater to do this using a heirarchiacal model.\nIt would be even better if we can also incorportate the product, user hierecies."
  },
  {
    "objectID": "posts/papers/Offpolicy-AuctionGym/index.html#the-paper",
    "href": "posts/papers/Offpolicy-AuctionGym/index.html#the-paper",
    "title": "Off-Policy Learning-to-Bid with AuctionGym",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper\n\n\n\n\n\nslides\n\n\n\n\n\nauctiongym code"
  },
  {
    "objectID": "posts/papers/Emergent-communication/index.html",
    "href": "posts/papers/Emergent-communication/index.html",
    "title": "Emergent Communication of Generalizations",
    "section": "",
    "text": "I think this is an amazing paper. I read it critically and made copious notes to see what I could learn from it. The paper point out some limitations of Lewis referential games^[In this game the sender sees images, it needs to classify them into some representation and sends a message. The reciever gets the same or similar images + distractors, it needs to run a classifier and needs to select the correct one. Learning an image classifier per agent is expensive and requires access to both an an image classification is likely shared. This however presents a problem…. It allows the agents? ] and suggest a couple of extentions that can over come these limitations. There is\n(Mu and Goodman 2021)\n\nMu, Jesse, and Noah D. Goodman. 2021. “Emergent Communication of Generalizations.” CoRR abs/2106.02668. https://arxiv.org/abs/2106.02668."
  },
  {
    "objectID": "posts/papers/Emergent-communication/index.html#tldr",
    "href": "posts/papers/Emergent-communication/index.html#tldr",
    "title": "Emergent Communication of Generalizations",
    "section": "",
    "text": "I think this is an amazing paper. I read it critically and made copious notes to see what I could learn from it. The paper point out some limitations of Lewis referential games^[In this game the sender sees images, it needs to classify them into some representation and sends a message. The reciever gets the same or similar images + distractors, it needs to run a classifier and needs to select the correct one. Learning an image classifier per agent is expensive and requires access to both an an image classification is likely shared. This however presents a problem…. It allows the agents? ] and suggest a couple of extentions that can over come these limitations. There is\n(Mu and Goodman 2021)\n\nMu, Jesse, and Noah D. Goodman. 2021. “Emergent Communication of Generalizations.” CoRR abs/2106.02668. https://arxiv.org/abs/2106.02668."
  },
  {
    "objectID": "posts/papers/Emergent-communication/index.html#abstract",
    "href": "posts/papers/Emergent-communication/index.html#abstract",
    "title": "Emergent Communication of Generalizations",
    "section": "Abstract",
    "text": "Abstract\nTo build agents that can collaborate effectively with others, recent research has trained artificial agents to communicate with each other in Lewis-style referential games. However, this often leads to successful but uninterpretable communication. We argue that this is due to the game objective: communicating about a single object in a shared visual context is prone to overfitting and does not encourage language useful beyond concrete reference. In contrast, human language conveys a rich variety of abstract ideas. To promote such skills, we propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. We find that these games greatly improve systematicity and interpretability of the learned languages, according to several metrics in the literature. Finally, we propose a method for identifying logical operations embedded in the emergent languages by learning an approximate compositional reconstruction of the language."
  },
  {
    "objectID": "posts/papers/Emergent-communication/index.html#the-video",
    "href": "posts/papers/Emergent-communication/index.html#the-video",
    "title": "Emergent Communication of Generalizations",
    "section": "The Video",
    "text": "The Video"
  },
  {
    "objectID": "posts/papers/Emergent-communication/index.html#the-paper",
    "href": "posts/papers/Emergent-communication/index.html#the-paper",
    "title": "Emergent Communication of Generalizations",
    "section": "The Paper",
    "text": "The Paper\nHere is the paper with my annotation and highlights."
  },
  {
    "objectID": "posts/papers/Emergent-communication/index.html#annotations",
    "href": "posts/papers/Emergent-communication/index.html#annotations",
    "title": "Emergent Communication of Generalizations",
    "section": "Annotations",
    "text": "Annotations\n\n\n\nTo promote such skills, propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. 1\n1 Significant modification the game: tweak payoffs, assign categories to symbols and allow sending of categories.\n\nWe argue that the reference games typically used in these studies are ill-suited to drive linguistic systematicity for two reasons 2\n2 The best the original Lewis signaling game can do is establish a one to one convention between a sender’s siganal of states and reciever action per states. This is just a coordination part of communication.\n\nThese tasks are more difficult 3\n3 adding categories can result in a combinatorial increase the total messages. So that the agents need to coordinate on one of many more equilibria. Also you now want the agents to learn a much narrower subset of those possible equilibrium i.e. those that are are faithfull to certain structures in of the states. This is essentially a new problem which could be embodies as a second step after the Lewis game. There is no guarantee in general that such a structure exists. And as the authors suggest other structures are not considered\n\nIn the set reference (setref) game, a teacher must communicate to a student not just a single object, but rather a group of objects belonging to a concept 4\n4 this is an interesting game - and also similar to add reference one modifies the game to refer to set of states by adding and, or and not operators giving an agent an basic reasoning ability. The new signaling systems allows specifying many more states. This can be useful in many applications. Of course learning to send additional operators becomes trivial conceptually if not in the practical sense\n\nThese tasks are more difficult than traditional reference games 5\n5 a “concept” like a red triangle is a specific type of a set. so this should be a easier task than the set reference. The difficulty seems to be not in the language or the categories but in the added classification of varied visual representation of seagulls\n\n\n\npaper\n\n\n\n\n\n\npaper"
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html",
    "href": "posts/papers/Temp-Abstraction/index.html",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "",
    "text": "This paper review is an extended introduction to temporal abstraction using options. It covers lots of advanced concepts in reinforcement learning that were introduced by Guest Speaker Doina Precup from Coursera Specialization on Reinforcement Learning by Martha White and Adam White. The paper is a deep dive into the topic of options and the successor representation. It is a long paper with lots of advanced concepts and algorithms. The paper is a great resource for anyone interested in reinforcement learning and temporal abstraction."
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#introduction",
    "href": "posts/papers/Temp-Abstraction/index.html#introduction",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "Introduction",
    "text": "Introduction\nEver since I saw (White 2022) the video lecture on subtasks by Martha White about learning tasks in parallel. However the video does not address the elephant in the room - how to discover the options.\n\n\n\n\n\n\n\nFigure 1: Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.\n\n\n\n\n\n\n\n\nFigure 2: Talk at Cohere.AI by Marlos C. Machado on Representation-driven Option Discovery in Reinforcement Learning. He discusses the Representation-driven Option Discovery (ROD) cycle and how it can be used to discover options in reinforcement learning. The talk covers much of the material in the paper as well as some more recent follow up work.\n\n\n\nThis is a hefty paper 70 pages with 8 algorithms many figures and citations from research spanning thirty years. It is filled to the brim with fascinating concepts that are developed by the authors but builds on lots of work by earlier researchers. It may seem to cover a niche topic but (c.f. Machado 2024, time 773) makes an eloquent argument that this paper deals with a fundamental question of where options come and if we put aside the jargon for a second we are trying to capture a form of intelingence that includes elements of generalization, planning, problem solving, learning at a level much closer what we are familiar with. And these familiar forms of mental abstractions much harder to consider in the context of Supervised or Unsupervised learning which lack the ineraction with the environment that is the hallmark of reinforcement learning.\nI came about this paper by accident. I a quick summary before I realized how long it was and I put out my first pass, and I hope to flesh it including perhaps a bit of code.\nI’ve been developing my own ideas regarding the creation and aggregation of options in reinforcement learning. My thinking to date has been different. I am exploring a Bayesian based tasks. I’ve considered creating shared semantics via emergent symbolic semantics and looking at a number of composability mechanisms for state, language and of options including using hierarchial bayesian models. While working on coding environments for this subjects a search led to this amazing paper!\nIn (Machado 2024) Marlos C. Machado, has given a talk that explains many of the complex ideas within this paper. This talk is available on YouTube.\nMarlos C. Machado is a good speaker and going over that paper and the video certainly helps to understand the challenges of temporal abstractions as well as the solutions that the paper proposes.\n\n\n\n\n\n\nTL;DR - Option Discovery with Successor Representations\n\n\n\n\n\n\nOption Discovery in a nutshell\n\n\nThis paper posits that successor representations, which encode states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstraction like options if these are not known.\nOptions are a powerful form of temporal abstraction that allows agents to make predictions and to operate at different levels of abstraction within an environment in ways idiosyncratic of human approach to tackle many problems. One of the key questions has been how to discover good options. The paper presents a rather simple yet powerful answer to this.\nThis paper is a quite challangeing. You might listen to this lighthearted deep dive courtesy by notebooklm to ease you into the topics.\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract\n\n\n\n\n\n\nReasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation, which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the successor representation can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these results as instantiations of a general framework for option discovery in which the agent’s representation is used to identify useful options, which are then used to further improve its representation. This results in a virtuous, never-ending, cycle in which both the representation and the options are constantly refined based on each other. Beyond option discovery itself, we also discuss how the successor representation allows us to augment a set of options into a combinatorially large counterpart without additional learning. This is achieved through the combination of previously learned options. Our empirical evaluation focuses on options discovered for temporally-extended exploration and on the use of the successor representation to combine them. Our results shed light on important design decisions involved in the definition of options and demonstrate the synergy of different methods based on the successor representation, such as eigenoptions and the option keyboard.\n— (Machado et al. 2023)"
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#the-review",
    "href": "posts/papers/Temp-Abstraction/index.html#the-review",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "The Review",
    "text": "The Review\n\nIntroduction\n\n\n\n\n\n\n\nFigure 3: Doina Precup’s Talk at DeepHack.RL on Temporal abstraction in reinforcement learning covers both the intro and the background material on options.\n\n\nIn this section, the authors introduce the reinforcement learning problem and the options framework. Next they discuss the benefits of using options and highlight the option discovery problem. Next they present the successor representation (SR) as a representation learning method that is conducive to option discovery, summarizing its use cases and connecting it to neuroscience They go on to describe the paper’s focus on temporally-extended exploration and the use of eigenoptions and covering options. The finnish the introduction by highlight the paper’s evaluation methodology and the use of toy domains and navigation tasks for clarity and intuition.\nIn (Precup 2017) Doina precup gives a talk on temporal abstraction in reinforcement learning. This talk covers both the introduction and the background material on options and is on YouTube."
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#background",
    "href": "posts/papers/Temp-Abstraction/index.html#background",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "Background",
    "text": "Background\n\nDefines the reinforcement learning problem, covering Markov Decision Processes, policies, value functions, and common algorithms such as Q-learning.\nIntroduces the options framework (Richard S. Sutton, Precup, and Singh 1999), (Precup and Sutton 2000), defining its components (initiation set, policy, termination condition), execution models, and potential benefits.\n\nAn option \\omega \\in \\Omega is a 3-tuple\n\n\\omega = &lt;I_\\omega , \\pi_\\omega , \\beta_\\omega &gt; \\qquad\n\\tag{1}\n\nwhere\n\nI_\\omega ⊆ S the options’s initiation set,\n\\pi_\\omega : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1] the option’s policy, such that \\sum_a \\pi_\\omega (·, a) = 1, and\n\\beta_\\omega : \\mathcal{S} \\rightarrow [0, 1] the option’s termination condition 1"
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#a-framework-for-option-discovery-from-representation-learning",
    "href": "posts/papers/Temp-Abstraction/index.html#a-framework-for-option-discovery-from-representation-learning",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "A Framework for Option Discovery from Representation Learning",
    "text": "A Framework for Option Discovery from Representation Learning\n\n\n\n\n\n\n\nFigure 4: Representation-driven Option Discovery (ROD) cycle (Machado 2019). The option discovery algorithms discussed in this paper can be seen as instantiating this cycle. The incoming arrow to Collect samples depicts the start of the process. The arrow from Define option to Option set highlights the output generated by the ROD cycle. Note that other generated artifacts can also be used by the agent outside the ROD cycle, such as the learned representation.\n\n\n\nIntroduces a general framework for option discovery driven by representation learning, named the Representation-driven Option Discovery (ROD) cycle.\n\nCollect samples\nLearn a representation\nDerive an intrinsic reward function from the representation\nLearn to maximize intrinsic reward\nDefine option\n\nPresents a step-by-step explanation of the ROD cycle, outlining its iterative and constructivist nature, as depicted in the figure."
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#the-successor-representation",
    "href": "posts/papers/Temp-Abstraction/index.html#the-successor-representation",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "The Successor Representation",
    "text": "The Successor Representation\n\n\n\n\n\n\n\nFigure 5: Example similar to Dayans 1993 of the SR, w.r.t. the uniform random policy, of state A (left). Consider a navigation task where the agent has access to its (x, y) coordinates. It is tempting to use some distance metric such as the Euclidean distance to define distance between states. However, if one considers the gray tiles to be walls, an agent in point A can reach point B much quicker than point C. The SR captures this distinction, ensuring that, in this representation, point A is closer to point B than it is to point C. The plots of the SR were generated using a discretization of the grid, where each tile is a state. Red represents larger values while blue represents smaller values (states that are temporally further away). Recall the SR of a state, in the tabular case, is an |S|-dimensional representation, thus allowing us to depict it as a heatmap over the state space.\n\n\n\nPresents the successor representation (SR) as a method to extract representations from observations.\nIn (Machado et al. 2023, sec. 4.1) Defines the SR in the tabular setting, explaining its ability to capture environment dynamics by encoding expected future state visitation, as shown in Equation 7 and the figure.\n\n\n\\Psi_{\\pi}(s,s') = \\mathbb{E}_{\\pi,p} \\sum_{t=0}^\\infty  γ^t \\mathbb{1}_{\\{S_t=s'}\\} | {S_0 = s } \\qquad\n\\tag{2}\n\n\n\n\n\n\n\nFigure 6: First three PVFs in the (a) four-room domain. Gray squares represent walls and white squares represent accessible states. Four actions are available: up, down, right, and left. The transitions are deterministic and the agent is not allowed to move into a wall. (b-d) These plots depict the first, second, and third eigenvectors associated with each state. The axes are rotated for clarity. The bottom left corner of the four-room domain is the state closer to the reade\n\n\n\nDiscusses the estimation of the SR with temporal-difference learning, its connection to general value functions 2, and its relationship to the transition probability matrix Equation 9.\n\n\n\\Psi(S_t,j) \\leftarrow \\hat{\\Psi}(S_t, j) + \\eta [\\mathbb{1}_{\\{S_t=j\\}} + \\gamma \\hat{\\Psi}(S_{t+1}, j) − \\hat{\\Psi}(S_t, j)]\n\\tag{3}\n\n\\Psi_\\pi =  \\sum_{t=0}^\\infty (\\gamma P_\\pi)^t = (I-\\gamma P_\\pi)^{-1}\\qquad\n\\tag{4}\n\nIntroduces successor features (SFs) as a generalization of the SR to the function approximation setting, extending the definition of the SR to arbitrary features, as shown in Equation 11.\nHighlights the relationship between the SR and PVFs."
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#temporally-extended-exploration",
    "href": "posts/papers/Temp-Abstraction/index.html#temporally-extended-exploration",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "Temporally-Extended Exploration",
    "text": "Temporally-Extended Exploration\n\n(Machado et al. 2023, sec. 5) discusses temporally-extended exploration with options and its potential to enhance exploration in RL.\n(Machado et al. 2023, sec. 5.1) introduces eigenoptions, which are options defined by the eigenvectors of the SR. &gt; “Eigenoptions are options defined by the eigenvectors of the SR.2 Each eigenvector assigns an intrinsic reward to every state in the environment.”\n\nExplains the concept of eigenoptions using the four-room domain as an example (Figure 5).\nDescribes how to learn eigenoptions’ policies using an intrinsic reward function derived from the eigenvectors of the SR.\nDefines the initiation set and termination condition of eigenoptions, as shown in Equation 16.\nPresents Theorem 1, which guarantees the existence of at least one terminal state for each eigenoption.\n\nIntroduces covering options, which are point options defined by the bottom eigenvector of the graph Laplacian and aim to minimize the environment’s cover time.\n\nExplains the concept of covering options using the four-room domain (Figure 7).\nDescribes how to learn covering options’ policies using a simplified intrinsic reward function.\nDefines the initiation set and termination condition of covering options.\nHighlights the iterative nature of covering option discovery, where options are added one by one at each iteration."
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#evaluation-of-temporally-extended-exploration-with-options",
    "href": "posts/papers/Temp-Abstraction/index.html#evaluation-of-temporally-extended-exploration-with-options",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "Evaluation of Temporally-Extended Exploration with Options",
    "text": "Evaluation of Temporally-Extended Exploration with Options\n\n(Machado et al. 2023, sec. 6)Evaluates eigenoptions and covering options in the context of temporally-extended exploration.\nUses the diffusion time, a task-agnostic metric, to quantify exploration effectiveness by measuring the expected number of decisions required to navigate between states.\nPresents results comparing eigenoptions and covering options:\n\nShows that both approaches can reduce diffusion time in the four-room domain when computed in closed form (Figure 8).\nDiscusses the impact of different initiation set sizes, highlighting the trade-off between avoiding sink states and ensuring option availability.\n\nInvestigates the effectiveness of eigenoptions and covering options in an online setting:\n\nDemonstrates the robustness of eigenoptions to online SR estimation (Figure 11).\nReveals the challenges of using covering options online, particularly due to their restrictive initiation set and reliance on a single eigenvector (Figure 12).\n\nExplores the impact of using options on reward maximization in a fixed task:\n\nShows that eigenoptions can accelerate reward accumulation when used for temporally-extended exploration in Q-learning (Figure 9).\nObserves that covering options do not consistently improve reward maximization in this setting, likely due to their sparse initiation set.\n\n\n\n\n\n\n\n\n\nFigure 7: figure 9"
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#iterative-option-discovery-with-the-rod-cycle",
    "href": "posts/papers/Temp-Abstraction/index.html#iterative-option-discovery-with-the-rod-cycle",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "Iterative Option Discovery with the ROD Cycle",
    "text": "Iterative Option Discovery with the ROD Cycle\n\n(Machado et al. 2023, sec. 7) introduces Covering Eigenoptions (CEO), a new algorithm that performs multiple iterations of the ROD cycle for option discovery.\nDescribes the steps of CEO, emphasizing its use of eigenoptions and online SR estimation, as outlined in Algorithm 2.\nDemonstrates the benefits of multiple ROD cycle iterations with CEO, showing a significant reduction in the number of steps needed to visit all states in the four-room domain (Figure 14).\nIllustrates the behavior of CEO over multiple iterations, highlighting its ability to progressively discover more complex options (Figure 14).\nCombining Options with the Option Keyboard\nDiscusses the option keyboard as a way to combine existing options to create new options without additional learning, potentially expanding the agent’s behavioral repertoire.\nIntroduces Generalized Policy Evaluation (GPE) and Generalized Policy Improvement (GPI), generalizations of standard policy evaluation and improvement.\nExplains how to use GPE and GPI to synthesize options from linear combinations of rewards induced by eigenvectors of the SR, as outlined in Algorithm 3.\nCombining Eigenoptions with the Option Keyboard\nDemonstrates the synergy of eigenoptions and the option keyboard.\nPresents a qualitative analysis of options generated by combining eigenoptions with the option keyboard (Figures 16 and 17).\nShows that the option keyboard leads to a combinatorial explosion of new options, as evidenced by the number of unique options generated (Figure 18).\nDemonstrates the diversity of options generated by the option keyboard through heatmaps showing the frequency of termination in different states (Figures 19 and 20).\nPresents a quantitative analysis of the diffusion time induced by eigenoptions combined with the option keyboard, highlighting the improvement in exploration effectiveness (Figures 21 and 22)."
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#related-work",
    "href": "posts/papers/Temp-Abstraction/index.html#related-work",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "Related Work",
    "text": "Related Work\n\nDiscusses option discovery methods for planning and bottleneck options, including those based on spectral clustering and normalized cuts.\nMentions other option discovery methods for temporally-extended exploration, such as diffusion options.\nOutlines extensions of the SR and option discovery methods to function approximation, including linear and non-linear function approximation techniques.\nDiscusses the connection of the SR to other reinforcement learning concepts, such as proto-value functions, slow-feature analysis, and dual representations.\nHighlights the relationship of the SR to neuroscience, including its potential to model hippocampal place fields and grid cell activations.\nMentions the SR’s application to explaining human behavior and decision-making."
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#conclusion",
    "href": "posts/papers/Temp-Abstraction/index.html#conclusion",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "Conclusion",
    "text": "Conclusion\n\nHighlights the potential of using the SR as the main substrate for temporal abstraction, pointing out promising directions for future work.\nEmphasizes the importance of iterative option discovery and its role in building intelligent agents capable of continual learning and complex skill acquisition.\n\nHere are the successor representations algorithms from the paper:\n\n\n\n\n\n\n\nFigure 8: successor representations algorithms\n\n\nNext is the covering eigenoptions algorithm:\n\n\n\n\n\n\n\nFigure 9: Covering Eigenoptions algorithm"
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#study-guide-for-the-paper",
    "href": "posts/papers/Temp-Abstraction/index.html#study-guide-for-the-paper",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "Study guide for the paper",
    "text": "Study guide for the paper\n\nWhat is an option in reinforcement learning?\n\nWe actually took the definition from the paper. But here is another from the video. This is perhaps a more elegant definition. It comes from []\nIn reinforcement learning, an option is a temporally extended course of actions that allows an agent to operate at different levels of abstraction within an environment. Options are a form of temporal abstraction that enables agents to make predictions and execute actions over extended time horizons, providing a way to structure and organize the agent’s behavior. \nv_{\\pi,\\beta}^{c,z}(s) = \\mathbb{E}_{\\pi,\\beta} \\left[ \\sum_{j=1}^K c(S_j) + \\gamma^{K-1} z(S_k) | S_0 = s \\right] \\qquad \\text{for all } s \\in S\n\n\nwhere\n\nv_{\\pi,\\beta}^{c,z}(s) is the value function of the option,\n\\pi is the policy,\n\\beta is the termination condition,\nc is the extrinsic reward function, z is the intrinsic reward function,\nS_j is the state at time j,\nK is the duration of the option, and\n\\gamma is the discount factor.\n\n\n\nHow can options be used ?\n\n\nFor planning: you can use eigenvectors of the SR to identify bottleneck, states that are difficult to reach under a random walk, and then use options to guide the agent to those states. c.f. (Solway et al. 2014)\nFor exploration: you can use eigenoptions to encourage exploration by driving the agent toward states that are difficult to reach under a random walk.\n\n\nExplain the successor representation\n\nThe successor representation (SR) is a method in reinforcement learning that represents states based on their expected future visits under a given policy. It captures the environment’s dynamics by encoding how likely an agent is to visit each state in the future, starting from a particular state.\n\nThe SR is denoted as Ψ_π, where π represents the agent’s policy.\nIt can be estimated online using temporal difference learning and generalized to function approximation using successor features.\n\nThe SR allows for Generalized Policy Evaluation (GPE): once the SR is learned, an agent can immediately evaluate its performance under any reward function that can be expressed as a linear combination of the features used to define the SR.\nThe SR offers a powerful tool for discovering and using temporal abstractions in reinforcement learning, enabling the development of more intelligent and efficient agents. It is used in option discovery methods like eigenoptions and covering options, providing a natural framework for identifying and leveraging temporally extended courses of actions.\nHere is a breakdown of the mathematical definition of the SR:\n\nΨ_\\pi (s, s') =  \\mathbb{E}_{π,p} [\\sum^\\infty_{t=0} γ^t\\mathbb{1}_{S_t = s'} | S_0 = s ] \\qquad\n\\tag{5}\n\nWhere:\n\ns, s' are states in the environment.\n\\gamma is the discount factor, determining the weight of future rewards.\nThe expectation (E) is taken over the policy \\pi and the transition probability kernel p.\n\\mathbb{1}_{S_t = s'} is an indicator function that equals 1 if the agent is in state s’ at time t, and 0 otherwise.\n\n\nThis equation calculates the expected discounted number of times the agent will visit state s’ in the future, given that it starts in state s and follows policy π. The SR matrix stores these expected visitations for all state pairs.\n\nExplain what is an eigenoption a covering option and the difference\n\nEigenoptions and covering options are two methods for option discovery in RL that use the successor representation (SR). Options represent temporally extended courses of actions.\nEigenoptions are options defined by the eigenvectors of the SR.\n\nEach eigenvector of the SR assigns an intrinsic reward to every state in the environment.\nAn eigenoption aims to reach the state with the highest (or lowest) value in the corresponding eigenvector.\nThey encourage exploration by driving the agent toward states that are difficult to reach under a random walk.\nEigenoptions have a broad initiation set, meaning they can be initiated from many states.\nThey terminate when the agent reaches a state with a (locally) maximum value in the eigenvector, meaning the agent can’t accumulate more positive intrinsic reward.\nEigenoptions tend to have different durations based on the eigenvalue they are derived from, allowing the agent to operate at different timescales.\n\nCovering options are defined by the bottom eigenvector of the graph Laplacian, which is equivalent to the top eigenvector of the SR under certain conditions.\n\nThey aim to minimize the environment’s expected cover time, which is the number of steps needed for a random walk to visit every state.\nEach covering option connects two specific states: one with the lowest value and one with the highest value in the corresponding eigenvector.\nThey are discovered iteratively. After each option is discovered, the environment’s graph is updated, and the process repeats.\nCovering options have a restrictive initiation set, containing only the single state with the lowest value in the eigenvector.\nThey terminate when they reach the state with the highest value in the eigenvector.\n\nHere’s a table summarizing the key differences between eigenoptions and covering options:\n\n\n\n\n\n\n\n\nFeature\nEigenoption\nCovering Option\n\n\n\n\nDefinition\nBased on any eigenvector of the SR\nBased on the bottom eigenvector of the graph Laplacian (equivalent to the top eigenvector of the SR under certain conditions)\n\n\nGoal\nReach states with high/low values in the corresponding eigenvector\nMinimize environment’s cover time\n\n\nInitiation Set\nBroad (many states)\nRestrictive (single state)\n\n\nTermination Condition\nReaching a (local) maximum in the eigenvector\nReaching the state with the highest value in the eigenvector\n\n\nDiscovery Process\nCan be discovered in parallel, in a single iteration\nDiscovered iteratively, one option at a time\n\n\nTimescale\nDifferent eigenoptions can have different durations\nGenerally have similar durations\n\n\n\nBoth eigenoptions and covering options can be effective for exploration, but they have different strengths and weaknesses. Eigenoptions can learn more diverse behaviors and capture different timescales, while covering options may be simpler to implement and can guarantee improvement in the environment’s cover time."
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#the-paper",
    "href": "posts/papers/Temp-Abstraction/index.html#the-paper",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "The paper",
    "text": "The paper\n\n\n\npaper"
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#resources",
    "href": "posts/papers/Temp-Abstraction/index.html#resources",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "Resources",
    "text": "Resources\nThere are a few blog posts that dive deeper into some of the concepts in the paper.\n\nThe Representation-driven Option Discovery"
  },
  {
    "objectID": "posts/papers/Temp-Abstraction/index.html#footnotes",
    "href": "posts/papers/Temp-Abstraction/index.html#footnotes",
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthe probability that option ω will terminate at a given state.↩︎\n“the SR can be estimated from samples with temporal-difference learning methods (Richard S. Sutton 1988), where the reward function is replaced by the state occupancy”↩︎"
  },
  {
    "objectID": "posts/coursera/c3-w2.html",
    "href": "posts/coursera/c3-w2.html",
    "title": "Constructing Features for Prediction",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/coursera/c3-w2.html#coarse-coding-video",
    "href": "posts/coursera/c3-w2.html#coarse-coding-video",
    "title": "Constructing Features for Prediction",
    "section": "Coarse Coding (Video)",
    "text": "Coarse Coding (Video)\nIn this video, Adam White introduces the concept of coarse coding, covering the first learning objective of this lesson.\nCoarse coding are a way to represent states in a more general way than tabular representations. This allows for generalization across states. The trade-off is that the representation is less discriminative."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l1g1",
    "href": "posts/coursera/c3-w2.html#sec-l1g1",
    "title": "Constructing Features for Prediction",
    "section": "The difference between coarse coding and tabular representations",
    "text": "The difference between coarse coding and tabular representations\n\n\n\n\napproximation\n\nRecall that linear function approximation are paramertized by a weight vector \\mathbf{w} and a feature vector \\mathbf{x}(s).\nAs we saw in the previous unit tabular representations associates one feature per state, this is called a one-hot encoding of the state space.\n\n\n\n\none hot coding\n\nWe associate one hot encoding with an indicator function \\delta_{ij}(s). This is a very discriminative representation but it does generalize.\n\n\n\n\nstate aggregation\n\nWe also discussed using state aggregation for the 1000 state random walk example. In state aggregation we break the continuous state space into discrete regions and associate a feature with each region. This is a more general representation than tabular representations but less discriminative.\n\n\n\n\ncoarse coding\n\nCoarse coding uses multiple overlapping shapes to represent states. This is a more General representation than state aggregation but less discriminative. Features are the circles they are in. If the circles overlap, we can have items that are in multiple circles. I.e. they are characterized by multiple features. In the example shown there can be from one to three active features.\nSo the difference is that tabular representations are one hot encodings while coarse coding uses membership in multiple overlapping shapes to represent states.\nHow does coarse coding relates to state aggregation?\nCoarse coding is also a generalization of state aggregation. In state aggregation we break the state space into discrete regions and associate a feature with each region. But we don’t let these regions overlap. In coarse coding we allow the regions to overlap which can give greater generalization as regions can share features.\nIn this video the term Reception Field is used to describe the region of the state space that a feature is associated with. This is an idea that comes from CNNs."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#generalization-properties-of-coarse-coding-video",
    "href": "posts/coursera/c3-w2.html#generalization-properties-of-coarse-coding-video",
    "title": "Constructing Features for Prediction",
    "section": "Generalization Properties of Coarse Coding (Video)",
    "text": "Generalization Properties of Coarse Coding (Video)\nIn this video Martha White discusses the generalization properties of coarse coding.\nShe looks at using small overlapping 1-d intervals to represent a 1-d function.\nWe see that changing shape size and number of effects the generalization properties of the representation.\n\n\n\n\nscale\n\n\n\n\nshape\n\n\n\n\ndiscrimination\n\n\n\nNext we looked at using short interval vs longer intervals to approximate a 1-d function. We see that the longer intervals give a smoother approximation."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l1g2",
    "href": "posts/coursera/c3-w2.html#sec-l1g2",
    "title": "Constructing Features for Prediction",
    "section": "The trade-off between discrimination and generalization",
    "text": "The trade-off between discrimination and generalization"
  },
  {
    "objectID": "posts/coursera/c3-w2.html#tile-coding-video",
    "href": "posts/coursera/c3-w2.html#tile-coding-video",
    "title": "Constructing Features for Prediction",
    "section": "Tile Coding (Video)",
    "text": "Tile Coding (Video)\nIn this video, Martha White introduces the concept of tile coding. This is simply a implementation of coarse coding using multiple overlapping grids."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l1g4",
    "href": "posts/coursera/c3-w2.html#sec-l1g4",
    "title": "Constructing Features for Prediction",
    "section": "Explain how tile coding is a (computationally?) convenient case of coarse coding",
    "text": "Explain how tile coding is a (computationally?) convenient case of coarse coding\nTile coding is a computationally efficient implementation of coarse coding. We use multiple overlapping tilings to represent states. Each tiling is a grid of tiles. Each tile is a feature.\nIf we use one tiling we get state aggregation. If we use multiple tilings we get coarse coding. One tiling means we don’t discriminate between states that are in the same tile. Multiple tilings means we can discriminate between states that are in the same tile in one tiling but not in another."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l1g5",
    "href": "posts/coursera/c3-w2.html#sec-l1g5",
    "title": "Constructing Features for Prediction",
    "section": "Describe how designing the tilings affects the resultant representation",
    "text": "Describe how designing the tilings affects the resultant representation\nThe textbook goes into some more details about how we can generalize using tile coding - using regular tilings generates in a diagonal pattern. Using random tilings generates more spherical regions.\nHowever we also saw that the number size and shape of the tiles affects the generalization properties of the representation. And that increasing the overlap between the tiles an increase the discrimination properties of the representation."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l1g6",
    "href": "posts/coursera/c3-w2.html#sec-l1g6",
    "title": "Constructing Features for Prediction",
    "section": "Understand that tile coding is a computationally efficient implementation of coarse coding",
    "text": "Understand that tile coding is a computationally efficient implementation of coarse coding\nTile coding is a computationally efficient implementation of coarse coding. Since grids are uniform it is easy to compute which cells a state is in. A second reason is that end up with a sparse representations thus the dot product is just the sum of the weights of the active features for each state.\nOne caveat is that in high dimensional spaces we end up an exponential number of features. This is called the curse of dimensionality."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#using-tile-coding-in-td-video",
    "href": "posts/coursera/c3-w2.html#using-tile-coding-in-td-video",
    "title": "Constructing Features for Prediction",
    "section": "Using Tile Coding in TD (Video)",
    "text": "Using Tile Coding in TD (Video)\nIn this video, Adam White shows how to use tile coding in TD learning. He goes back to the 1000 state random walk example and shows how to use tile coding to approximate the value function. We end up needing six tiles.\n\n\n\n\ntile coding v.s. state aggregation"
  },
  {
    "objectID": "posts/coursera/c3-w2.html#feature-construction-for-linear-methods",
    "href": "posts/coursera/c3-w2.html#feature-construction-for-linear-methods",
    "title": "Constructing Features for Prediction",
    "section": "Feature Construction for Linear Methods",
    "text": "Feature Construction for Linear Methods\nIn the textbook we see two forms of features for linear methods that are not covered in the videos.\nThe first are polynomials. We might use polynomials features for the state to represent the state space. This seems to be a good for problems where RL is dealing to a greater extent with interpolation or regression.\nThe following is given as an example of a polynomial feature representation of the state space. It took a bit of time to understand what was going on here.\nThey explain about the different combination of two features s_1 and s_2 doesn’t cover some edge cases but using four (1,s_1,s_2,s_1s_2) covers all the possible combinations of the two features. We might also want to include higher powers of the atoms and that is what the polynomial representation is doing.\n\nx_i(s) = \\prod_{j=1}^k s_j^{c_{ij}}\n\nIt important to point out that we are not using the polynomials as a function approximation basis function. What we are talking about is a formulation of multinomial from a set of fixed numbers s_1 \\lsots s_k I.e. we are talking about all the possible products product from powers of these atoms.\nThe second are Fourier bases.\n\nx_i(s) = \\cos\\left(\\frac{2\\pi s^T a_i}{b}\\right)\n\nThe book mentions that the Fourier basis is particularly useful for periodic functions.\nThere are many other orthogonal bases used as functnio expansions that could be used, as features for linear function approximation.\n\nWalsh functions and Haar wavelets have discrete support and are used in signal processing.\nLegendre polynomials are used in physics.\nChebyshev polynomials are used in numerical analysis."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#other-forms-of-coarse-coding",
    "href": "posts/coursera/c3-w2.html#other-forms-of-coarse-coding",
    "title": "Constructing Features for Prediction",
    "section": "Other Forms of Coarse Coding",
    "text": "Other Forms of Coarse Coding\nIn the textbook we see that there are other forms of coarse coding.\nFor example in section 9.5.5 we see using radial basis functions.\n\nAn RBF\n\nis a real-valued function whose value depends only on the distance between the input and a fixed point (called the center).  \n\n\nVisualizing - Imagine a hill or bump centered at a specific point. The height of the hill at any other point depends solely on its distance from the center. The hill gets flatter as you move away from the cente\n \nx_i(s) = \\exp\\left(-\\frac{\\|s-c_i\\|^2}{2\\sigma_i^2}\\right)\n\n\nWhere\nc_i is the center of the radial basis function and\n\\sigma_i is the width.\n\nThis is a form of coarse coding where the features are the distance from a set of centers. This is a more general representation than tile coding but less discriminative. The advantage of RBFs over tiles is that they are approximate functions that vary smoothly and are differentiable. However it appears there is both a computational cost and no real advantage in having continuous/differential features according to the book.\n\n\n\n\n\n\n\n\n\nFigure 1: One-dimensional radial basis functions with centers at -2, 0, and 2.\n\n\n\n\nI find this a bit disappointing as it seems like a nice intermediate step between linear function approximation with its convergence guarantees and neural networks which have no such guarantees."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#what-is-a-neural-network-video",
    "href": "posts/coursera/c3-w2.html#what-is-a-neural-network-video",
    "title": "Constructing Features for Prediction",
    "section": "What is a Neural Network? (Video)",
    "text": "What is a Neural Network? (Video)\nIn this video, Martha White introduces the concept of a neural network. We look at a simple one layer feed forward neural network. Where the output=f(sW) is a non-linear function of the input."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l2g1",
    "href": "posts/coursera/c3-w2.html#sec-l2g1",
    "title": "Constructing Features for Prediction",
    "section": "Define a neural network",
    "text": "Define a neural network\n\nA Neural network consists of a network of nodes which process and pass on information.\n\nThe circles are the noes\nThe lines are the connections\nThe nodes are organized in layers\n\nData starts at the input layer. It is passed through the connections to the hidden layer. The hidden layer is preforms some computation on the data and passes it to the output layer. This process repeats until the last layer produces the output of the network."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#deep-neural-networks-video",
    "href": "posts/coursera/c3-w2.html#deep-neural-networks-video",
    "title": "Constructing Features for Prediction",
    "section": "Deep Neural Networks (Video)",
    "text": "Deep Neural Networks (Video)\nIn this video, Martha White introduces the concept of neural networks with multiple hidden layers and activation functions."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#neural-networks-mechanics",
    "href": "posts/coursera/c3-w2.html#neural-networks-mechanics",
    "title": "Constructing Features for Prediction",
    "section": "Neural Networks Mechanics",
    "text": "Neural Networks Mechanics\nA node in the network is a function\n\noutput = f[(w_1 \\times input_1) + (w_2 \\times input_2) + \\ldots + (w_n \\times input_n) + b]\n\n\nwhere:\n\nw_i are the weights,\ninput_i are the inputs, and\nb is the bias.\nf is the activation function.\n\n\nThe sum of the product of the weights and inputs is a linear operation. The activation function f is where a non-linearity is introduced into the network."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l2g2",
    "href": "posts/coursera/c3-w2.html#sec-l2g2",
    "title": "Constructing Features for Prediction",
    "section": "Define activation functions",
    "text": "Define activation functions\nActivation functions are non-linear functions that are applied to the output of a node. They introduce non-linearity into the network.\n\n\n\n\ntanh activation\n\n\n\n\nrectified linear activation function\n\n\nMartha White also mentions threshold activation functions. However these are not used in practice as they are not differentiable. There is some work since this course came out on compressing neural networks to use threshold activation functions which are easy to compute on a CPU as matrix multiplication becomes a series of comparisons. However these are trained with a differentiable approximation of the threshold function and then quantized to the threshold function."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#the-neural-network-implementation",
    "href": "posts/coursera/c3-w2.html#the-neural-network-implementation",
    "title": "Constructing Features for Prediction",
    "section": "The Neural Network Implementation",
    "text": "The Neural Network Implementation\n\n\n\n\nNeural Network Implementation\n\nA neural network is a parameterized function that is a composition of linear and non-linear functions. It is a function of the state. The linear functions are the weights and the non-linear functions are the activation functions. The weights are learned from data."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l2g3",
    "href": "posts/coursera/c3-w2.html#sec-l2g3",
    "title": "Constructing Features for Prediction",
    "section": "Define a feed-forward architecture",
    "text": "Define a feed-forward architecture\nA feed forward architecture is a neural network where the connections between nodes do not form a cycle. The data flows from the input layer to the output layer.\nAn example of a non-feed forward architecture is a recurrent neural network where the connections between nodes form cycles."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#non-linear-approximation-with-neural-networks-video",
    "href": "posts/coursera/c3-w2.html#non-linear-approximation-with-neural-networks-video",
    "title": "Constructing Features for Prediction",
    "section": "Non-linear Approximation with Neural Networks (video)",
    "text": "Non-linear Approximation with Neural Networks (video)"
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l2g4",
    "href": "posts/coursera/c3-w2.html#sec-l2g4",
    "title": "Constructing Features for Prediction",
    "section": "How Neural Networks are doing feature construction",
    "text": "How Neural Networks are doing feature construction\n\n\n\n\n\nneural feature 1\n\n\ndarker means greater activation for the feature\n\n\n\n\nneural feature 2\n\n\nthe one generalize differently\n\nWe construct a non-linear function of the state using a neural network.\nrecall A node takes the form\n\noutput = f[(w_1 \\times input_1) + \\ldots + (w_n \\times input_n) + b]\n\nWe call this output of the node a feature! We can see that these features are a non-linear function of the inputs. We repeat this process until we evaluate all the nodes of the final layer. And the output of this final layer is called the representation.\nNote: This is not very different from tile coding where we pass input to a tile coder and get back a new representation of the state.\nIn both cases we are constructing a non-linear mapping of the input of the features. And we take a nonlinear function of the representation to form the output - a nonlinear approximation of the state.\nRecall that in tile coding we had to set some hyper-parameters: size shape of tiles + number of tiling. These are fixed before training. In a neural network we also have hyperparameters for the size of the layers, the number of layers, the activation functions. These too are fixed before training.\nThe difference is that Neural networks have weights that get updated during training. But tile coding does not change during training."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l2g5",
    "href": "posts/coursera/c3-w2.html#sec-l2g5",
    "title": "Constructing Features for Prediction",
    "section": "How neural networks are a non-linear function of state",
    "text": "How neural networks are a non-linear function of state\n\n\n\n\n\nneural feature 3\n\n\nthere are no hard boundaries\n\n this shows how it generelises\n\nNeural networks are non linear functions of the because of the non-linear nature of the activation functions. These are applied recursively as we move to the final layer."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#deep-neural-networks-video-1",
    "href": "posts/coursera/c3-w2.html#deep-neural-networks-video-1",
    "title": "Constructing Features for Prediction",
    "section": "Deep Neural Networks (Video)",
    "text": "Deep Neural Networks (Video)"
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l2g6",
    "href": "posts/coursera/c3-w2.html#sec-l2g6",
    "title": "Constructing Features for Prediction",
    "section": "How deep networks are a composition of layers",
    "text": "How deep networks are a composition of layers\nNeural networks are modular. We can add or remove layers. Each layer is a function of the previous layer. The output of the previous layer is the input to the next layer.\nDepth allows composition of features. Each layer can learn a different representation of the input. The final layer can learn a representation of the input that is a composition of the representations learned by the previous layers\nWe can design the network to remove undesirable features. For example we can design a network with a bottleneck that has less features than the input. This forces the network to learn a compressed representation of the input."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l2g7",
    "href": "posts/coursera/c3-w2.html#sec-l2g7",
    "title": "Constructing Features for Prediction",
    "section": "The tradeoff between learning capacity and challenges presented by deeper networks",
    "text": "The tradeoff between learning capacity and challenges presented by deeper networks\nDepth can increase the learning capacity of the network by allowing the network to learn complex compositions and abstractions. However, deeper networks are harder to train."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#gradient-descent-for-training-neural-networks-video",
    "href": "posts/coursera/c3-w2.html#gradient-descent-for-training-neural-networks-video",
    "title": "Constructing Features for Prediction",
    "section": "Gradient Descent for Training Neural Networks (Video)",
    "text": "Gradient Descent for Training Neural Networks (Video)\n\n\n\nIf we use the square error loss then\n\nL(\\hat y_k,y_k) = (\\hat y_k-y_k)^2 \\qquad\n\\tag{1}\n\nA = A −αδ^As\n\n\nB = B −αδ^Bx\n\nLet’s start at the output of the network and work backwards. Recall: \nx = f_A(sA)\n\n\n\\hat{y} = f_B(xB)\n\nWe start by taking the partial derivative of the loss function with respect to the first set of weights B.\nWe use the chain rule given the derivative of L with respect to \\hat{Y} \\times \\frac{∂\\hat{y}}{∂B}. The next step is again to use the chain rule for this derivative.\n\n\\frac{∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂\\hat{y}_k}{∂B_{jk}}\n\nlet’s introduce a new variable, θ where θ is the output of the hidden layer times the last set of weights.\n\nθ \\dot = xB\n\nThus\n\n\\hat y \\dot = f_B(θ)\n\nRewriting we have:\n\n\\frac{∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂f_B(\\theta_k)}{∂\\theta} \\frac{∂\\theta_k}{∂B_{jk}}\n\nand since\n\n\\frac{∂\\theta_k}{∂B_{jk}} = x_j\n\n\n\\frac{∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂f_B(\\theta_k)}{∂\\theta} x_j\n\nnow that we calculated the gradient for the last layer we can move to the previous layer.\nwe use\n\n\\Psi \\dot =  sA\n\nand\n\nx \\dot = f_A(\\Psi)\n\n\n\\begin{aligned}\n\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}} &= \\delta_k^B \\frac {∂\\theta_k}{∂A_{ij}} \\newline\n& = \\delta_k^B B_{jk} \\frac {∂x_j}{∂A_{ij}} \\newline\n  & = \\delta_k^B B_{jk} \\frac {∂f_A(\\Psi_j)}{∂\\Psi_j} \\frac {∂\\Psi_j}{∂A_{ij}}\n\\end{aligned}\n\nsince\n\n\\frac {∂\\Psi_j}{∂A_{ij}} = s_{ij}\n\nwe have\n\n\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}} = \\delta_k^B B_{jk} \\frac {∂f_A(\\Psi_j)}{∂\\Psi_j} s_{ij}\n\nWe can clean up this derivative by again, defining a term δ_A.\n\nδ^A_j = (B_{jk}δ^B_k ) \\frac{∂f_A(ψ_j)}{∂ψ_j}\n\nThe final result will be:\n\n\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}}= δ^A_j s_i\n\nObtaining as a final result for both gradients the next expressions\n\n\\frac {∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = δ^B_k x_j"
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l3g1",
    "href": "posts/coursera/c3-w2.html#sec-l3g1",
    "title": "Constructing Features for Prediction",
    "section": "Computing the Gradient for a Single Hidden Layer Neural Network",
    "text": "Computing the Gradient for a Single Hidden Layer Neural Network\nLet’s summerize the results:\n\n\\frac {∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = δ^B_k x_j \\qquad\n\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}}= δ^A_j s_i\n\nwhere:\n\nδ^B_k = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂f_B(\\theta_k)}{∂\\theta} \\qquad\nδ^A_j = (B_{jk}δ^B_k ) \\frac{∂f_A(ψ_j)}{∂ψ_j}"
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l3g2",
    "href": "posts/coursera/c3-w2.html#sec-l3g2",
    "title": "Constructing Features for Prediction",
    "section": "Computing the Gradient for Arbitrarily Deep Networks",
    "text": "Computing the Gradient for Arbitrarily Deep Networks\n\n\n\n\n\n\n\nFigure 2: Gradient Descent Pseudo-code\n\n\n\n\n\n\n\n\nFigure 3: Gradient Descent Pseudo-code for RELU\n\n\n\nNow that we have estimated the gradient for a hidden layer neural network. We can use it to learn to optimize the weights of the network by updating the weights to minimize the error in the loss function in the direction of the negative gradient.\nThe pseudocode in the figure outlines how to implementing the backprop algorithm with Stochastic gradient descent.\nFor each data point s, y in our dataset, we first get our prediction \\hat{y} from the network. This is the forward pass. Then we can estimate the loss using the actual value y\nNext we compute the gradients starting from the output. We first compute δ^B and the gradient for B, then we use this gradient to update the parameters B, with the step size α_B for the last layer.\nNext, we update the parameters A. We compute δ^A which reuses δ^B.\nNotice, that by computing the gradients of the end of the network first, we avoid recomputing the same terms for A, that were already computed for δB. We then compute the gradient for A and update A with this gradient using step size α_A.\n\nNext we look at how we adapt the pseudocode to work with the ReLU activation on the hidden layer and a linear unit for the output.\nFirst, we compute the error for the output layer, then we compute the derivative of the ReLU units with respect to \\Psi, and finally, we use the aerial signal from the output layer along with you to compute the air signal for the hidden layer, the rest remains the same"
  },
  {
    "objectID": "posts/coursera/c3-w2.html#optimization-strategies-for-nns-video",
    "href": "posts/coursera/c3-w2.html#optimization-strategies-for-nns-video",
    "title": "Constructing Features for Prediction",
    "section": "Optimization Strategies for NNs (Video)",
    "text": "Optimization Strategies for NNs (Video)"
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l3g3",
    "href": "posts/coursera/c3-w2.html#sec-l3g3",
    "title": "Constructing Features for Prediction",
    "section": "The Importance of Initialization for Neural Networks",
    "text": "The Importance of Initialization for Neural Networks\nOne simple yet effective initialization strategy for the weights, is to randomly sample the initial weights from a normal distribution with small variance Fig. 42. This way, each neuron has a different output from other neurons within its layer. This provides a more diverse set of potential features.\n\n\n\n\nWeights initialization\n\nBy keeping the variants small, we ensure that the output of each neuron is within the same range as its neighbors. One downside to this strategy is that, as we add more inputs to a neuron, the variance of the output grows."
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l3g4",
    "href": "posts/coursera/c3-w2.html#sec-l3g4",
    "title": "Constructing Features for Prediction",
    "section": "Strategies for Initializing Neural Networks",
    "text": "Strategies for Initializing Neural Networks\n\nW_{init} ~ N(0,1)\n\n\nW_{init} ~ \\frac{N(0,1)}{\\sqrt{n_{in}}}"
  },
  {
    "objectID": "posts/coursera/c3-w2.html#sec-l3g5",
    "href": "posts/coursera/c3-w2.html#sec-l3g5",
    "title": "Constructing Features for Prediction",
    "section": "Optimization Techniques for Training Neural Networks",
    "text": "Optimization Techniques for Training Neural Networks\n\nmomentum update AKA heavy ball method \nW_{t+1} ← W_t −α∇_wL(W_t) + λM_t\n\n\n\nM_{t+1} = λM_t −α∇_wL\n\nvector step size adaptation\n\nseparate step size for each weight"
  },
  {
    "objectID": "posts/coursera/c2-w2.html",
    "href": "posts/coursera/c2-w2.html",
    "title": "Temporal Difference Learning Methods for Prediction",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/coursera/c2-w2.html#lesson-1-introduction-to-temporal-difference-learning",
    "href": "posts/coursera/c2-w2.html#lesson-1-introduction-to-temporal-difference-learning",
    "title": "Temporal Difference Learning Methods for Prediction",
    "section": "Lesson 1: Introduction to Temporal Difference Learning",
    "text": "Lesson 1: Introduction to Temporal Difference Learning\n\nLesson Learning Goals\n\nDefine temporal-difference learning #\nDefine the temporal-difference error #\nUnderstand the TD(0) algorithm #\n\n\n\nTemporal Difference Learning definition\nNow we turn to a new class of methods called Temporal Difference (TD) learning. According to Bass, TD learning is one of the key innovations in RL. TD learning is a method that combines the sampling of Monte Carlo methods with the bootstrapping of Dynamic Programming methods. The term temporal in the name references learning from two subsequent time steps and the term difference refers to using the difference between the values of each state.\nLet us now derive the TD update rule for the value function:\nRecall the definition of the value function from the previous course, is the expected return when starting in a particular state and following a particular policy.\nWe write this as:\n\nv_\\pi(s_t) \\dot = \\mathbb{E}_\\pi[G_t | S_t = s] \\qquad\n\\tag{1}\nWe can motivate td update rule by considering the DP and MC update rules.\nIn The MC update rule a sample update based on the return for the entire episode. Which means that we can only update our value function at the end of the episode.\n\nV(S_t) \\leftarrow V(S_t) + \\alpha [\\underbrace{G_t}_{\\text{MC target}} -V(S_t)]  \\qquad\n\\tag{2}\nwhere:\nG_t is the return at time step t and S_t is the state at time step t.\nThis is the actual return at time t.\nIn the DP update rule, the target is a full backup based on the entire distribution of possible next states which we can solve for exactly since we have the full dynamics of the environment.\n\nV(S_t) \\leftarrow V(S_t) + \\sum_a\\pi(a|S_t) ([r(s,a) + \\gamma p(s'\\mid s,a) V(s')]) \\qquad\n\\tag{3}\nwhere:\n\nR_{t+1} is the reward at time step t+1\nV(S_{t+1}) is the approximate value of the state at time step t+1\n\\gamma is the discount factor\n\\alpha is the learning rate\n\nHow can we make updates at each time step?\n\n\\begin{aligned}\nG_t & \\dot= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\qquad \\newline\n         &= R_{t+1} + \\gamma G_{t+1} \\qquad\n\\end{aligned}\n\\tag{4}\n\n\\begin{aligned}\nv_\\pi(s_t)  & = \\mathbb{E}_\\pi [G_t \\mid S_t = s] & \\text{definition}\\newline\n          & = \\mathbb{E}_\\pi [R_{t+1} + \\gamma G_{t+1} \\mid S_t = s] & \\text{(subst. Recursive return)}\\newline\n          & = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi (S_{t+1}) |S_t=s]    & \\text{(subst. value function)}\\newline\n          & = \\mathbb{E}_\\pi[R_{t+1}|S_t=s]  + \\gamma \\mathbb{E}_\\pi[v_\\pi (S_{t+1}) |S_t=s] & \\text{(by linearity of Expectation)} \\newline\n          & = R_{t+1} + \\gamma v_\\pi (S_{t+1}) & \\text{(by Expectation of constant RV)}\n\\end{aligned}\n\nIn this formula we have replaced G_t with R_{t+1} + \\gamma V(S_{t+1}). We now have a recursive formula for the value functions in terms of the next value function. The next value function is a stand in for the value of the Return G_{t+1} which we don’t know.\nwe can use this formula to make an online update to our value function using an MC estimate of the return, without saving the full list of rewards.\nsince G_t, the MC update target is the return for the entire episode, we can only update our value function at the end of the episode. We will soon look at TD learning, which allows us to make updates at each time step.\nthe motivation for TD learning is based on the MC update rule. The MC update rule is a sample update based on the return for the entire episode. This means that we can only update our value function at the end of the episode. We will soon look at TD learning, which allows us to make updates at each time step.\n\nV_\\pi(S_t) \\leftarrow V(S_t) + \\alpha [\n\\underbrace{R_{t+1} + \\gamma V(S_{t+1})}_{\\text{TD target}} - V(S_t)]\n\nWe can use the following to make online TD updates like we were able to update our value function without saving the full list of rewards. here:\n\nV(S_t) is the value of the state at time t\nV(S_{t+1}) is the value of the state at time t+1\nR_{t+1} is the reward at time t+1\n\\alpha is a constant the learning rate\nthe target is an estimate of the return\n\n\n\\begin{aligned}\nG_t & \\dot = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\newline\n&= R_{t+1} + \\gamma G_{t+1} \\newline\n\\end{aligned}\n\n\nThe MC target is an estimate of the expected value (average of sampled return) - The DP target is an estimate because the true value of the state is not known - The TD target is an estimate for both reasons: a sample of the expected value of the reward, and the current estimate of the value of the state rather than the true value.\n\nLike with MC, the target is a sample updates based on a single observed transition. This is in stark contrast to DP, where the target is a full backup based on the entire distribution of possible next states which we can solve for exactly since we have the full dynamics of the environment.\n\n\nTemporal Difference Error\n\nThe TD error is the difference between the estimated value of a state and the value of the state at the next time step.\n\n\n\\delta_t \\dot = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n this is one of the most important equations in reinforcement learning - we will see it again and again.\n\n\nTD(0) Algorithm\n\nThe TD(0) algorithm is a TD learning algorithm that uses a bootstrapping method to estimate the value of a state.\n\n\n\n\n\n\n\nThe TD(0) algorithm for estimating v_\\pi\n\n\n\n\n\n\\begin{algorithm} \\caption{TD(0) for estimating $v_\\pi$}\\begin{algorithmic} \\State Input: \\State $\\qquad \\pi \\leftarrow \\text{policy to be evaluated}$ \\State Initialize: \\State $\\qquad V(s) \\leftarrow x \\in \\mathbb{R} \\quad \\forall s \\in \\mathcal{S}$ \\FORALL {episode $e$:} \\State $S \\leftarrow \\text{initial state of episode}$ \\State $\\text{Choose } A \\sim \\pi(\\cdot \\mid S)$ \\FORALL {step $S \\in e$:} \\State Take action $A$, observe $R, S'$ \\State $V(S) \\leftarrow V(S) + \\alpha [R + \\gamma V(S') - V(S)]$ \\State $S \\leftarrow S'$ \\State $\\text{Choose} A' \\sim \\pi(\\cdot \\mid S)$ \\State $S$ is terminal \\ENDFOR \\ENDFOR \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "posts/coursera/c2-w2.html#lesson-2-advantages-of-td",
    "href": "posts/coursera/c2-w2.html#lesson-2-advantages-of-td",
    "title": "Temporal Difference Learning Methods for Prediction",
    "section": "Lesson 2: Advantages of TD",
    "text": "Lesson 2: Advantages of TD\n\nLesson Learning Goals\n\nUnderstand the benefits of learning online with TD #\nIdentify key advantages of TD methods over Dynamic Programming and Monte Carlo methods #\nIdentify the empirical benefits of TD learning #"
  },
  {
    "objectID": "posts/coursera/c3-w4.html",
    "href": "posts/coursera/c3-w4.html",
    "title": "Policy Gradient",
    "section": "",
    "text": "RL logo\n\n\n\n\n\n\n\nRL algorithm decision tree\n\n\n\n\nFigure 1: The algorithms discussed in this lesson are all part of the policy gradient family. These allow us to consider both discrete and continuous actions in the the average rewards settings. We will consider Softmax Actor-Critic, Gaussian Actor-Critic, and the REINFORCE algorithm. The last is missing from the chart.\nSometimes, the behavior codified in the policy is much simpler than the action value function. Thus, learning the policy directly can be more efficient. Learning policies is an end-to-end solution for solving many real-world RL problems. Coding such end-to-end solutions may be done under the umbrella of policy gradient methods. Once we cover the policy gradient theorem, we will see how we still need to use action value approximations to estimate the gradient of the average reward objective. A second way that we will use value function approximations is in the actor-critic algorithms. Here, the policy is called the actor, and the value function is called the critic. The critic evaluates the policy, and the actor is used to update the policy. The actor-critic algorithms are a hybrid of policy gradient and value function methods. They are more stable than policy gradient methods and can be used in more complex environments.\nAlso, I was disappointed that this course does not cover more modern algorithms, such as TRPO, PPO, or other Deep learning algorithms. I cannot stress this point enough.\nIn the last video in the previous lecture’s notes by Satinder Singh, all of the research on using Meta gradients to learn intrinsic rewards is also built on top of policy gradient methods - where he and his students looked at propagating these gradients through multiple the planing algorithms and later through the learning algorithm to learn a reward function and tackle the issues of exploration."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#learning-policies-directly-video",
    "href": "posts/coursera/c3-w4.html#learning-policies-directly-video",
    "title": "Policy Gradient",
    "section": "Learning policies directly (Video)",
    "text": "Learning policies directly (Video)\n\n\n\n\n\n\n\nEnergy pumping policy\n\n\n\n\nFigure 6: In the mountain car environment, the parameterized value function is complex, but the parameterized policy is simple.\n\n\nIn this lesson course instructor Adam White introduces the idea of learning policies directly. He contrasts this with learning value functions and explains why learning policies directly can be more flexible and powerful.\n\n\n\n\n\n\nRethinking policies\n\n\n\nMoving on we will need to think very clearly about policies.\nTo this end it is worth spending a minute to quickly recap the definition properties and notation of a policy from the previous lessons:\n\nIntuitively a policy \\pi is just decision making rule.\nA deterministic policy is just a function that maps a state to an action. \n\\pi : s\\in \\mathcal{S} \\to a \\in \\mathcal{A} \\qquad \\text{(deterministic policy)}\n\nA stochastic policy is a function that maps a state to a probability distribution over actions. Stochastic policies are more general and include deterministic policies as a special case. So while we may talk of deterministic policies, we will use the mathematical form of a stochastic policy.\n\n\n\\pi : s\\in \\mathcal{S} \\to \\mathbb{P}(\\mathcal{A}) \\qquad \\text{(stochastic policy)}\n\n\nFormally, the policy is defined probabilistically as follows:\n\n\n\\pi(a \\mid s) \\doteq Pr(A_t = a \\mid S_t = s) \\qquad \\text{(policy)}\n\\tag{1}\n\nnote that this is a shorthand for the following:\n\n\n\\pi(a \\mid s) = \\mathbb{E}[A_t \\mid S_t = s] \\qquad \\text{(policy)}\n\nWhere \\pi is a probability distribution over actions given a state."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l1g1",
    "href": "posts/coursera/c3-w4.html#sec-l1g1",
    "title": "Policy Gradient",
    "section": "How to Parametrize a Policies?",
    "text": "How to Parametrize a Policies?\n\n\n\n\n\n\n\npolicy parametrization\n\n\n\n\nFigure 7: When we parametrize a policy we will use the greek letter \\theta \\in \\mathbb{R}^d to denote the parameters of the policy.\n\n\n\n\n\n\n\n\npolicy parametrization constraints\n\n\n\n\nFigure 8: Constraints on the policy parameters can be used to ensure that the policy is valid.\n\n\n\nSo far we have been mostly looking at learning value functions. But when it comes to function approximation, it is often simpler to learn a policy directly.\nIn the mountain car environment we see the power pumping policy which accelerates the car in the direction it is moving. This is a near optimal policy for this environment. The policy is simple and can be learned directly and it makes no use of value functions. This may not always be the case.\nA visual summary of the policy parametrization is shown in the figure. Recall that the policy is a function that takes in a state and outputs a probability distribution over actions. We will use the greek letter \\theta \\in \\mathbb{R}^d to denote the parameters of the policy. This way we can reference the parameters of \\hat{Q}(s,a,w) the action value function are denoted by \\mathbf{w}.\nThe parametrized policy is defined as follows:\n\n\\pi(a \\mid s, \\theta) \\doteq Pr(A_t = a \\mid S_t = s, \\theta) \\qquad \\text{(parametrized policy)}\n\\tag{2}\nis a probability distribution over actions given a state and the policy parameters.\nSince we are dealing with probabilities, the policy parameters must satisfy certain constraints. For example, the probabilities must sum to one. This is shown in the figure. These policy parameters constraints will ensure that the policy is valid.\nPolicy Gradient use gradient ascent:\n\n\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta J(\\theta) \\qquad \\text{(gradient ascent)}\n\\tag{3}\nwhere \\alpha is the step size and \\nabla_\\theta J(\\theta) is the gradient of the objective function J(\\theta) with respect to the policy parameters \\theta.\n\nmethods that follow this update rule are called policy gradient methods.\nmethods that learn both a value function and a policy are called actor-critic methods."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l1g2",
    "href": "posts/coursera/c3-w4.html#sec-l1g2",
    "title": "Policy Gradient",
    "section": "Define one class of parameterized policies based on the softmax function",
    "text": "Define one class of parameterized policies based on the softmax function\n\n\n\n\n\n\n\nsoftmax properties\n\n\n\n\nFigure 9\n\n\nThe Softmax policy based on the Boltzmann distribution is a probability distribution over actions given a state. It is parameterized by a vector of action preferences h(s, a, \\theta).\n\n\\pi(a \\mid s, \\theta) \\doteq \\frac{e^{h(s, a, \\theta)}}{\\sum_{b\\in \\mathcal{A}} e^{h(s, b, \\theta)}} \\text{(softmax policy)} \\qquad\n\\tag{4}\n\nthe numerator is the exponential of the action preference\nthe denominator is the sum of the exponentials of all action preferences\n\nSome properties of the softmax policy are that it can take in a vector of weights for different actions and output a probability distribution over actions. A second property is that the softmax policy generalizes the max function. A third property is that unlike the max function which is discontinuous the softmax policy is differentiable, making it amenable to gradient-based optimization.\n\nnegative values of h lead to positive action probabilities.\nequal values of h lead to equal action probabilities.\nthe softmax policy is a better option over than the \\epsilon-greedy policy over the action-value based methods."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#advantages-of-policy-parameterization-video",
    "href": "posts/coursera/c3-w4.html#advantages-of-policy-parameterization-video",
    "title": "Policy Gradient",
    "section": "Advantages of Policy Parameterization (Video)",
    "text": "Advantages of Policy Parameterization (Video)\nIn this video we consider the advantages of using parameterized policies over action-value based methods. We will see that parameterized policies are more flexible than action-value based methods and can start off stochastic and then become deterministic."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l1g3",
    "href": "posts/coursera/c3-w4.html#sec-l1g3",
    "title": "Policy Gradient",
    "section": "Advantages of using parameterized policies over action-value based methods",
    "text": "Advantages of using parameterized policies over action-value based methods\n\n\n\n\n\n\n\nsoftmax policy v.s. epsilon-greedy\n\n\n\n\nFigure 10: Softmax policy v.s. \\epsilon-greedy\n\n\n\n\n\n\n\n\nShort corridor with switched action\n\n\n\n\nFigure 11: In the Short corridor with switched action environment a deterministic policy fails to reach the goal. The only optimal policy is stochastic.\n\n\n\n\nOne advantage of parameterizing policies according to the softmax in action preferences is that the approximate policy can approach a deterministic policy, whereas with \\epsilon-greedy action selection over action values there is always an \\epsilon probability of selecting a random action.\n\n\nA second advantage of parameterizing policies according to the softmax in action preferences is that it enables the selection of actions with arbitrary probabilities. In problems with significant function approximation, the best approximate policy may be stochastic.\n\nFor example, in card games with imperfect information the optimal play is often a mixed strategy which means you should take two different actions each with a specific probability, such as when bluffing in Poker.\nAction-value methods have no natural way of finding stochastic optimal policies, whereas policy approximating methods can, as shown in The short Corridor environment"
  },
  {
    "objectID": "posts/coursera/c3-w4.html#the-objective-for-learning-policies-video",
    "href": "posts/coursera/c3-w4.html#the-objective-for-learning-policies-video",
    "title": "Policy Gradient",
    "section": "The Objective for Learning Policies (Video)",
    "text": "The Objective for Learning Policies (Video)\nIn this video Martha White dive deep into the objective for policy gradient algorithms. She then contrasts this with the gradient of the value fn objectives and lists some challenges for estimating the gradient of the average reward objective. We will learn that we can use the policy gradient theorem and to overcome these challenges and derive a gradient with an neat update rule ."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l2g1",
    "href": "posts/coursera/c3-w4.html#sec-l2g1",
    "title": "Policy Gradient",
    "section": "The objective for policy gradient algorithms",
    "text": "The objective for policy gradient algorithms\nFormalizing the goal as an Objective\n\n\\begin{align*}\nG_t &= \\sum_{t=0}^{T}  R_{t}  \\quad && \\text{(episodic)} \\newline\nG_t &= \\sum_{t=0}^{\\infty} \\gamma^t R_{t}   \\quad && \\text{(continuing - discounted reward)} \\newline\nG_t &= \\sum_{t=0}^{\\infty} R_{t} - r(\\pi)  \\quad && \\text{(continuing - avg. reward)}\n\\end{align*}\n\\tag{5}\nThe average reward Objective for a policy is as follows: \nr(\\pi) = \\sum_{t=0}^{T} \\mu(s) \\sum_{a} \\pi(a \\mid s, \\theta)  \\sum_{s',r} p(s',r \\mid s,a) r \\quad \\text{(avg. reward objective)}\n\\tag{6}\nWhat does this mean?\n\nthe last sum is the expected reward for a state-action pair. \\mathbb{E}[R_t \\mid S_t = s , A_t=a]\nthe last two sums together are the expected reward for a state under weighted by the policy \\pi. \\mathbb{E}_\\pi[R_t \\mid S_t = s]\nfull sum ads the time we spend in state s under \\pi therefore the expected reward for a state under the policy \\pi and the environment dynamics p. \\mathbb{E}_\\pi[R_t]\n\nto optimize the average reward, we need to estimate the gradient of the avg. objective\n\n\\nabla_\\theta r(\\pi) = \\nabla_\\theta \\sum_{t=0}^{T} \\textcolor{red}{\\underbrace{\\mu(s)}_{\\text{Depends on }\\theta}} \\sum_{a} \\pi(a \\mid s, \\theta) \\sum_{s',r} p(s',r \\mid s,a) r \\qquad\n\\tag{7}\n\nMethods based on this are called policy gradient methods.\nWe are trying to maximize the average reward.\n\nThere are a few challenges with using the gradient in the above equation:\nAccording to the lesson \\mu(s) depends on \\theta. Martha White point out that this state importance though parameterized only by s actually depends on the the policy \\pi which will evolve during its training based on the values of \\theta. Which means out notation here is a bit misleading. She then contrasts it with the value function gradient is being evaluated using a fixed policy.\n\n\\begin{align*}\n\\nabla_w \\bar{VE} &= \\nabla_w \\sum_{s}\\textcolor{red}{\\underbrace{\\mu(s)}_{\\text{Independent of }\\mathbf{w}}}  [V_{\\pi}(s)-\\bar{v}(s,w)]^2 \\newline\n&=\\sum_{s} \\textcolor{red}{\\mu(s)} \\nabla_w [V_{\\pi}(s)-\\bar{v}(s,w)]^2\n\\end{align*} \\text{(value function gradient)} \\qquad\n\\tag{8}\nWe can avg reward as an objective for policy optimization and the its for the stochastic gradient ascent. Next we will consider how the policy gradient theorem can help us estimate the gradient of the average reward objective despite these setbacks."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#the-policy-gradient-theorem-video",
    "href": "posts/coursera/c3-w4.html#the-policy-gradient-theorem-video",
    "title": "Policy Gradient",
    "section": "The Policy Gradient Theorem (Video)",
    "text": "The Policy Gradient Theorem (Video)\n\n\n\n\n\n\n\nUnderstanding the pg theorem up\n\n\n\n\nFigure 13\n\n\n\n\n\n\n\n\nUnderstanding the pg theorem left\n\n\n\n\nFigure 14\n\n\n\n\n\n\n\n\nUnderstanding the pg theorem all\n\n\n\n\nFigure 15\n\n\n\n\nIn this video course instructor Martha White explains the policy gradient theorem, a key result for optimizing policy in reinforcement learning. The goal is to maximize the average reward by adjusting policy parameters \\theta using gradient ascent. The challenge is estimating the gradient of the average reward, which initially involves a complex expression with the gradient of the stationary distribution over states (\\mu(s))."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l2g2",
    "href": "posts/coursera/c3-w4.html#sec-l2g2",
    "title": "Policy Gradient",
    "section": "The results of the policy gradient theorem",
    "text": "The results of the policy gradient theorem\nThe policy gradient theorem simplifies this by providing a new expression for the gradient. This expression involves summing, over all states and actions, the product of the gradient of the policy and the action-value function (Q). The gradient of the policy indicates how to adjust parameters to increase the probability of an action, while the action-value function represents the expected return from taking that action.\nThe video illustrates this with a grid world example, showing how gradients for different actions point in different directions. By weighting these gradients with the corresponding action values, the theorem provides a direction to update the policy parameters that increases the probability of high-value actions and decreases the probability of low-value actions.\nThe product rule \n\\nabla(f(x)g(x)) = \\nabla f(x)g(x) + f(x)\\nabla g(x) \\qquad \\text{(product rule)}\n\\tag{9}\ntherefore:\n\n\\begin{align*}\n\\nabla_\\theta r(\\pi) &= \\sum_{t=0}^{T} \\nabla \\mu(s) \\sum_{a} \\pi(a \\mid s,\\theta) \\sum_{s',r} p(s',r \\mid s,a) r \\newline &+  \\sum_{t=0}^{T} \\mu(s) \\nabla \\sum_{a} \\pi(a \\mid s, \\theta) \\sum_{s',r} p(s',r \\mid s,a) r\n\\end{align*}\n\\tag{10}\nThe first term is the gradient of the stationary distribution and the second term is the gradient of the policy. The policy gradient theorem simplifies this expression by eliminating the need to estimate the gradient of the stationary distribution.\n\n\\begin{align*}\n\\nabla_\\theta r(\\pi) &= \\sum_{s\\in \\mathcal{S}} \\mu(s) \\textcolor{red}{ \\sum_{a\\in{\\mathcal{A}}} \\nabla \\pi(a \\mid s,\\theta)  q_\\pi(s,a) }\n\\end{align*}\n\\tag{11}\nThe policy gradient theorem provides a new expression for the gradient of the average reward objective. This expression involves summing, over all states and actions, the product of the gradient of the policy and the action-value function (Q). The gradient of the policy indicates how to adjust parameters to increase the probability of an action, while the action-value function represents the expected return from taking that action.\nMartha White points out that this expression is much easier to estimate.\nNow let’s try to understand how we use the theorem to estimate the gradient.\nWhat we will use it to approximate the gradient. Computing the sum over states is impractical.\nWhat we will do do is take a stochastic samples. This involves updating the policy parameters based on the gradient observed at the current state.\nTo simplify the update rule, the concept of expectations is introduced. By re-expressing the gradient as an expectation under the stationary distribution of the policy, the update can be further simplified to involve only a single action sampled from the current policy.\nThe final update rule resembles other learning rules seen in the course, where the policy parameters are adjusted proportionally to a stochastic gradient of the objective. The magnitude of the step is controlled by a step-size parameter\nThe actual computation of the stochastic gradient requires two components: the gradient of the policy and an estimate of the action-value function"
  },
  {
    "objectID": "posts/coursera/c3-w4.html#the-policy-gradient-theorem",
    "href": "posts/coursera/c3-w4.html#the-policy-gradient-theorem",
    "title": "Policy Gradient",
    "section": "The policy gradient theorem",
    "text": "The policy gradient theorem\nWe need some preliminary results and definitions.\n\nThe four part dynamics function from the [@sutton2018reinforcement pp. 48] book:\nNext we need the result from Exercise 3.18 in [@sutton2018reinforcement pp. 62]\nNext we need the result from Exercise 3.19 in [@sutton2018reinforcement pp. 62]\n\n\np(s', r \\mid s, a) \\doteq Pr\\{S_t=s', R_t=r \\mid S_{t-1} = s , A_{t-1}= a\\} \\qquad \\text{(S.B. 3.2)}\n\\tag{12}\n\n\n\n\n\n\nExercise 3.18\n\n\n\n\n\n\nThe value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:\n\n\n\nGive the equation corresponding to this intuition and diagram for the value at the root node, v_\\pi(s), in terms of the value at the expected leaf node, q_\\pi(s, a), given S_t = s. This equation should include an expectation conditioned on following the policy, \\pi. Then give a second equation in which the expected value is written out explicitly in terms of \\pi(a \\mid s) such that no expected value notation appears in the equation.\n\n\n\n\n\n\n\n\n\n\nbackup diagram from v() to q()\n\n\n\n\nFigure 16: backup diagram from v_\\pi(s) to q_\\pi(s,a)\n\n\n\nSolution\n\n\\begin{align}\nV_\\pi(s) &= \\mathbb{E_\\pi}[q(s_t,a)  \\mid s_t = s, a_t=a ] && \\text{(def. of Value)} \\newline\n&= \\sum_a Pr(a \\mid s) q_\\pi(s,a) && \\text{(def. of Expectation)} \\newline\n&= \\textcolor{red}{\\sum_a \\pi(a \\mid s)} \\textcolor{green}{q_\\pi(s,a)} && \\text {(def. of policy)}\n\\end{align}\n\\tag{13}\n\n\n\n\n\n\nExercise 3.19\n\n\n\n\n\n\nThe value of an action, q_\\pi(s, a), depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state—action pair) and branching to the possible next states:\n\n\n\nGive the equation corresponding to this intuition and diagram for the action value, q_\\pi(s, a), in terms of the expected next reward, R_{t+1}, and the expected next state value, v_\\pi(S_{t+1}), given that S_t = s and A_t = a. This equation should include an expectation but not one conditioned on following the policy. Then give a second equation, writing out the expected value explicitly in terms of p(s_0, r \\mid s, a) defined by eq 3.2, such that no expected value notation appears in the equation.\n\n\n\n\n\n\n\n\n\n\nbackup diagram from q() to v()\n\n\n\n\nFigure 17: backup diagram from q_\\pi(s,a) to v_\\pi(s')\n\n\n\n\nSolution\n\n\\begin{align*}\nq_\\pi(s, a) &= \\mathbb{E}[R_{t+1} v_\\pi (s_{t+1}) \\mid s_t = s, a_t = a] \\newline\n&= \\textcolor{blue}{\\sum_{s', r} p(s', r \\mid s, a)} \\textcolor{pink}{[r + v_\\pi(s')]}\n\\end{align*} \\qquad\n {#ex-319-solution}\n\n\\begin{align*}\nq_\\pi(s, a) &= \\sum_{s'} \\mathbb{E}_\\pi[G_{t} \\mid S_{t+1}=s'] Pr\\{S_{t+1} = s' \\mid S_t = s, A_t = a\\}\n\\newline &= \\sum_{s'} \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s, A_t = a, S_{t+1} = s'] Pr\\{S_{t+1} = s' \\mid S_t = s, A_t = a\\}\n\\newline &= \\sum_{s',r} \\left( r + \\gamma \\underbrace{\\mathbb{E}[G_{t+1} \\mid S_{t+1} = s']}_{v_\\pi|(s')} \\right) p(s', r \\mid s, a)\n\\newline &= \\sum_{s',r} [ r + \\gamma v_\\pi(s')] p(s', r \\mid s, a)\n\\end{align*}\n\nHere is my version of the proof:\n\n\\begin{align*}\n\\textcolor{cyan}{\\nabla_\\theta V_\\pi(s)} &= \\nabla_\\theta \\sum_a \\textcolor{red}{\\pi(a \\mid s)} \\textcolor{green}{ q_\\pi(s,a) } && \\text{backup } v_\\pi \\to q_\\pi \\text{ (Ex 3.18)}\n\\newline &= \\sum_a \\nabla_\\theta \\pi(a \\mid s) q_\\pi(s,a) + \\pi(a \\mid s) \\nabla_\\theta q_\\pi(s,a) && \\text{product rule}\n\\newline &= \\sum_a \\nabla_\\theta \\pi(a \\mid s) q_\\pi(s,a) + \\pi(a \\mid s) \\nabla_\\theta \\sum_{s'} \\textcolor{blue}{P(s',r \\mid s, a)} \\textcolor{pink}{[r + V_\\pi(s')]} && \\text{backup } q_\\pi \\to v_\\pi \\text{ (Ex 3.19)}\n\\newline &= \\sum_a \\nabla_\\theta \\pi(a \\mid s) q_\\pi(s,a) + \\pi(a \\mid s) \\sum_{s'} P(s',r \\mid s, a) \\nabla_\\theta V_\\pi(s') && P, r \\text{ are const w.r.t. } \\theta \\newline\n=& \\sum_{a \\in \\mathcal{A}} \\Big( \\nabla_\\theta \\pi(a \\mid s)Q_\\pi(s, a) + \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s,a)  \\textcolor{cyan}{\\nabla_\\theta V_\\pi(s')} \\Big) && \\text{total rule of probability on r for P }\n\\newline & \\blacksquare && \\qquad\n\\end{align*}\n\\tag{14}"
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l2g3",
    "href": "posts/coursera/c3-w4.html#sec-l2g3",
    "title": "Policy Gradient",
    "section": "The importance of the policy gradient theorem",
    "text": "The importance of the policy gradient theorem\nCrucially, the policy gradient theorem eliminates the need to estimate the gradient of the stationary distribution (\\mu), making the gradient much easier to estimate from experience. This sets the stage for building incremental policy gradient algorithms, which will be discussed in the next lecture."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l3g1",
    "href": "posts/coursera/c3-w4.html#sec-l3g1",
    "title": "Policy Gradient",
    "section": "Derive a sample-based estimate for the gradient of the average reward objective",
    "text": "Derive a sample-based estimate for the gradient of the average reward objective\n\n\\theta_{t+1} \\doteq \\theta_t + \\alpha \\frac{ \\nabla_ \\pi (a_t \\mid s_t, \\theta)}{\\pi (a_t \\mid s_t, \\theta)} q_\\pi(s_t, a_t) \\qquad \\text{()}\n\n\n\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta ln \\pi(a_t \\mid s_t, \\theta) q_\\pi(s_t, a_t) \\qquad \\text{()}\n\\tag{15}\nwhere \\alpha is the step size and \\nabla_\\theta J(\\theta) is the gradient of the objective function J(\\theta) with respect to the policy parameters \\theta.\n\n\\nabla \\ln (f(x)) = \\frac{\\nabla f(x)}{f(x)} \\qquad \\text{(log derivative)}\n\\tag{16}"
  },
  {
    "objectID": "posts/coursera/c3-w4.html#reinforce-algorithm-extra",
    "href": "posts/coursera/c3-w4.html#reinforce-algorithm-extra",
    "title": "Policy Gradient",
    "section": "Reinforce Algorithm (Extra)",
    "text": "Reinforce Algorithm (Extra)\nThe reinforce algorithm isn’t covered in the course. However, it is in the readings. Also the reinforce algorithm is said to be the most direct implementation of the policy gradient theorem. Finaly the reinforce algorithm is used in one of my research projects and this seems to be a great opportunity to understand it better.\nSo without further ado, let’s dive into the reinforce algorithm.\n\n\n\nReinforce Algorithm\n\n\nReinforce reveals the main issues with the policy gradient theorem. While the policy gradient theorem provides an unbiased estimate of the gradient of the average reward objective, it is a high variance estimator. This means that the gradient is very noisy and can lead to slow learning.\nOne wat to reduce the variance of the policy gradient theorem is to use a baseline. A baseline is a function that is subtracted from the reward to reduce the variance of the policy gradient theorem. Subtracting the baseline does not change the expected value of the gradient3, but it can reduce the variance of the gradient estimate.\n\n\n\nReinforce Algorithm\n\n\nthe change is in the last three lines. The baseline is subtracted from the return G and the gradient is scaled by the baseline."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l3g2",
    "href": "posts/coursera/c3-w4.html#sec-l3g2",
    "title": "Policy Gradient",
    "section": "Describe the actor-critic algorithm for control with function approximation, for continuing tasks",
    "text": "Describe the actor-critic algorithm for control with function approximation, for continuing tasks"
  },
  {
    "objectID": "posts/coursera/c3-w4.html#actor-critic-with-softmax-policies-video",
    "href": "posts/coursera/c3-w4.html#actor-critic-with-softmax-policies-video",
    "title": "Policy Gradient",
    "section": "Actor-Critic with Softmax Policies (video)",
    "text": "Actor-Critic with Softmax Policies (video)\nAdam White discusses one specific implementation of the actor-critic reinforcement learning algorithm using a linear function approximation of the action value with tile coding and a Softmax policy parameterization.\nActor-critic methods combine direct policy optimization (actor) with value estimation (critic) using temporal difference learning.\nThe critic evaluates the policy by updating state value estimates, while the actor updates policy parameters based on feedback from the critic. This implementation is designed for finite action sets and continuous states. It employs a Softmax policy that maps state-dependent action preferences to probabilities, ensuring these probabilities are positive and sum to one. Each state effectively has its own Softmax distribution, and actions are sampled proportionally to these probabilities.\nBoth the value function and action preferences are parameterized linearly. The critic uses a feature vector representing the current state to estimate the value function. For the actor, the action preferences depend on both state and action, necessitating a state-action feature vector. The parameterization requires duplicating state feature vectors for each action, resulting in a policy parameter vector (θ) larger than the critic’s weight vector (W).\nThe algorithm’s update equations include:\nCritic Update: A straightforward semi-gradient TD update using the feature vector scaled by the temporal difference residual (TDR). Actor Update: A more complex gradient that involves two components: State-action features for the selected action. A sum over all actions of state-action features scaled by the policy probabilities."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l4g1",
    "href": "posts/coursera/c3-w4.html#sec-l4g1",
    "title": "Policy Gradient",
    "section": "Derive the actor-critic update for a softmax policy with linear action preferences",
    "text": "Derive the actor-critic update for a softmax policy with linear action preferences\nThe critic’s update rule is:\n\n\\mathbf{w} \\leftarrow \\mathbf{w} + α^\\mathbf{w} \\delta \\nabla \\hat{v}(S,w)\n\nwhich uses semigradient TD(0) to update the value function.\nThe actor uses the tf-error from the critic to update the policy parameters: \nθ \\leftarrow θ + α^θ δ ∇ \\ln \\pi (A \\mid S,\\theta)\n\npolicy update with a softmax policy is:\n\n\\pi(a \\mid s, \\theta) \\doteq \\frac{e^{h(s, a, \\theta)}}{\\sum_{b\\in \\mathcal{A}} e^{h(s, b, \\theta)}}\n\nthis is like having a different softmax for each state\n  \nFeature of the action preferences function\nfor the critic \n\\hat{v}(s,w) \\doteq w^T x(s)\n\nfor the actor\n\nh(s,a,θ) \\doteq θ^T x_h(s,a)\n\nwe can do this by stacking\nSo with the softmax policy the critic’s update is:\n\nw  \\leftarrow w + α^w \\delta x(s)\n\nand the actor’s update to the preferences looks as follows.\n\n\\nabla \\ln \\pi(a \\mid s, \\theta) = x_h(s,a) - \\sum_b \\pi(b \\mid s, \\theta) x_h(s,b)\n\nThe gradient has two parts.\nThe first is the state action features for the selected action xh(s,a).\nThe second part is the state action features multiplied by the policy summed over all actions ∑ b π(b|s,θ)xh(s,b)."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l4g2",
    "href": "posts/coursera/c3-w4.html#sec-l4g2",
    "title": "Policy Gradient",
    "section": "Implement this algorithm",
    "text": "Implement this algorithm"
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l4g3",
    "href": "posts/coursera/c3-w4.html#sec-l4g3",
    "title": "Policy Gradient",
    "section": "Design concrete function approximators for an average reward actor-critic algorithm",
    "text": "Design concrete function approximators for an average reward actor-critic algorithm"
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l4g4",
    "href": "posts/coursera/c3-w4.html#sec-l4g4",
    "title": "Policy Gradient",
    "section": "Analyze the performance of an average reward agent",
    "text": "Analyze the performance of an average reward agent"
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l4g5",
    "href": "posts/coursera/c3-w4.html#sec-l4g5",
    "title": "Policy Gradient",
    "section": "Derive the actor-critic update for a gaussian policy",
    "text": "Derive the actor-critic update for a gaussian policy\n\n\n\nActor-Critic\n\n\n\n\n\nActor-Critic Continuing\n\n\n\n\n\nActor-Critic Episodic"
  },
  {
    "objectID": "posts/coursera/c3-w4.html#sec-l4g6",
    "href": "posts/coursera/c3-w4.html#sec-l4g6",
    "title": "Policy Gradient",
    "section": "Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions",
    "text": "Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions\n\n\n\n\n\n\nDiscussion prompt\n\n\n\n\nAre tasks really ever continuing? Everything eventually breaks or dies. It’s clear that individual people do not learn from death, but we don’t live forever. Why might the continuing problem formulation be a reasonable model for long-lived agents?\n\n\n\n\n\n\n\n\n\nChapter 13\n\n\n\n\nFigure 18: Chapter 13 of [@sutton2018reinforcement] covering policy gradient methods."
  },
  {
    "objectID": "posts/coursera/c3-w4.html#footnotes",
    "href": "posts/coursera/c3-w4.html#footnotes",
    "title": "Policy Gradient",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwhich models a single roll of a die based on its historical performance. It generalizes the Bernoulli and a special case of the Multinomial for a single trial↩︎\nA step that does not logically follow from the previous one↩︎\nthe bias↩︎"
  },
  {
    "objectID": "posts/coursera/c1-w3.html",
    "href": "posts/coursera/c1-w3.html",
    "title": "Value Functions & Bellman Equations",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms\nDecision theory is the branch of Mathematics dealing with the analysis of decisions by a single agent. Game theory is the branch of Mathematics dealing with the analysis of decisions by multiple agents. The introduction of a second agent makes the problem more complex and introduces the notion of strategic behavior. Decision theory is in many ways a simplification of game theory. In [@silver2015], Dave Silver responded to a question that a simple way of viewing MARL is that each agents are an independent decision maker.\nOnce the problem is formulated as an MDP, finding the optimal policy is more efficient when using value functions.\nThis week, we learn the definition of policies and value functions, as well as Bellman equations, which are the key technology behind all the algorithms we will learn.\nFor someone with a background in game theory, the concept of a policy \\pi is not new in game theory, we call this a strategy and it is a mapping from states to actions. i.e. an assignment of some action to each state representing the best action that an agent should take in that state.\nA second familiar concept is the value function. In game theory, we call this the payoff for an action. The payoffs are typically assigned to the terminal states of the game and can be backpropagated to non-terminal states using the laws of probability. Here we are interested in the expected value of the rewards that an agent can expect to receive when following a policy \\pi from a given state s.\nI found the Policy and values functions somewhat families due to some background in game theory and markov processes.\nI found the Bellman equations more of a challenge. I think the main issue is the unfamiliarity with the notation which make the material look like gibberish. However, the more I made myself more familiar with the notation, I came to see that these equations express a rather simple idea.\nWe describe a MDP as a linear process in time. However, it is really a tree of possible actions. What the Bellman equations express is that if we want to estimate the value v_\\pi(s) of a state or more specifically the value of an action q_\\pi(s,a) what we do is consider the immediate rewards and then we have have a copy of pretty much the same tree. As we move forward in time we will end up making ever smaller (discounted) corrections to our best assessment."
  },
  {
    "objectID": "posts/coursera/c1-w3.html#sec-policy-definition",
    "href": "posts/coursera/c1-w3.html#sec-policy-definition",
    "title": "Value Functions & Bellman Equations",
    "section": "Policy Definition",
    "text": "Policy Definition\n\nA policy \\pi is a distribution over actions for each possible state.\nIt is denoted by \\pi(a|s), which is the probability of taking action a in state s under policy \\pi."
  },
  {
    "objectID": "posts/coursera/c1-w3.html#sec-stochastic-vs-deterministic",
    "href": "posts/coursera/c1-w3.html#sec-stochastic-vs-deterministic",
    "title": "Value Functions & Bellman Equations",
    "section": "Stochastic vs Deterministic Policies",
    "text": "Stochastic vs Deterministic Policies\n\nA policy can be deterministic or stochastic.\nA deterministic policy is a policy that selects a single action in each state.\n\nFor example, the greedy policy selects the action with the highest value\n\nA stochastic policy is a policy that selects actions with some probability that can be conditioned on the state.\n\nFor example the uniform policy selects each action with equal probability."
  },
  {
    "objectID": "posts/coursera/c1-w3.html#sec-value-functions",
    "href": "posts/coursera/c1-w3.html#sec-value-functions",
    "title": "Value Functions & Bellman Equations",
    "section": "Value Functions",
    "text": "Value Functions\n\nWe generally want to evaluate the value of each state or better yet the value of each action in each state before we create the policy. To do this we define two types of value functions:\n\n\nState-value functions V_\\pi\nThe state-value function v_{\\pi}(s) is the expected return when starting in state s and following policy \\pi thereafter.\n\nv_\\pi(s) \\dot = \\mathbb{E_\\pi}[G_t|S_t = s] \\quad \\text{for policy} \\quad \\pi \\qquad\n\\tag{1}\n\n\nAction-value functions Q_\\pi\nThe action-value function q_{\\pi}(s,a) is the expected return when starting in state s, taking action a, and following policy \\pi thereafter.\n\nq_\\pi(s,a) \\dot = \\mathbb{E_\\pi}[G_t|S_t = s, A_t = a] \\quad \\text{for policy} \\quad \\pi \\qquad\n\\tag{2}\n\n\nRelationship between Value Functions and Policies\nIn the short term, the value functions are more useful than the return G\n\nThe return G is not immediately available\nThe return G can be non-deterministic.\n\nThe value functions are deterministic and can be computed from the MDP."
  },
  {
    "objectID": "posts/coursera/c1-w3.html#lesson-2-bellman-equations",
    "href": "posts/coursera/c1-w3.html#lesson-2-bellman-equations",
    "title": "Value Functions & Bellman Equations",
    "section": "Lesson 2: Bellman Equations",
    "text": "Lesson 2: Bellman Equations\n\n\n\n\n\n\nGoals\n\n\n\n\nDerive the Bellman equation for state-value functions #\nDerive the Bellman equation for action-value functions #\nUnderstand how Bellman equations relate current and future values #\nUse the Bellman equations to compute value functions the state value function is v(s) #"
  },
  {
    "objectID": "posts/coursera/c1-w3.html#sec-bellman-equation-state-value-functions",
    "href": "posts/coursera/c1-w3.html#sec-bellman-equation-state-value-functions",
    "title": "Value Functions & Bellman Equations",
    "section": "Bellman Equation for State-Value Functions",
    "text": "Bellman Equation for State-Value Functions\n\n\n\n\n\n\n\nFigure 3: backup diagram for v_\\pi\n\n\n\n\n\n\n\n\nBellman Equation intuition\n\n\n\nRichard Bellman was a uniquely gifted mathematician who worked on dynamic programming. The Bellman equations is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state. These equations form the basis of Dynamic Programming which is used in disparate problems including\n\nSchedule optimization\nString algorithms (e.g. sequence alignment)\nGraph algorithms (e.g. the shortest path problem)\nGraphical algorithms (e.g. the Vitrebi algorithm)\nBioinformatics (e.g. lattice models)\n\nAlthough Bellman was one of the greatest problem solvers of the 20th century, he was not a great communicator. He was known for his terse and cryptic writing. The Bellman equations are a case in point. They are simple to understand once you get the hang of them but they are not easy to read for the first time. However the key to understanding the Bellman equations is to understand that they are a recursive equation based on some physical process\n\nThe trick that one learns over time, a basic part of mathematical methodology, is to sidestep the equation and focus instead on the structure of the underlying physical process – Richard Bellman\n\nin the case of RL the recursive physical process is: \n  S \\rightarrow A \\rightarrow R.\n\\tag{3}\nand we can diagram it using a backup diagram as shown in the Figure 3 above.\nThe name backup diagram comes from the idea that we are backing up the value of the state v(s) from the successor state v(s'). I.e. we are going back up the tree of possible effects of some action a starting from the state s.\nWhile the Bellman equation are difficult to read, remember and to derive, the backup diagram are very easy to sketch even if you don’t remember the equations. Once you have sketch the backup diagram, you should be able to easily derive the Bellman equations.\nThis same intuition can be used for working through all the above dynamic programming algorithms!\n\n\nThe Bellman equation for state-value functions is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state.\n\n\\begin{align}\n  v_\\pi(s) &= \\mathbb{E_\\pi}[G_t|S_t=s] \\newline\n           &= \\mathbb{E_\\pi}[R_{t+1} + \\gamma G_{t+1}|S_t=s] \\newline\n           &= \\mathbb{E_\\pi}[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t=s] \\newline\n           &= \\sum_a \\pi(a|s) \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma \\mathbb{E_\\pi}[G_{t+1}|S_{t+1}=s']) \\newline\n           &= \\sum_a \\pi(a|s) \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma v_\\pi(s'))\n\\end{align}\n\\tag{4}"
  },
  {
    "objectID": "posts/coursera/c1-w3.html#sec-bellman-equation-action-value-functions",
    "href": "posts/coursera/c1-w3.html#sec-bellman-equation-action-value-functions",
    "title": "Value Functions & Bellman Equations",
    "section": "Bellman Equation for Action-Value Functions",
    "text": "Bellman Equation for Action-Value Functions\n\n\n\n\nbackup diagram for q(s,a) function\n\nThe Bellman equation for action-value functions is a recursive equation that decomposes the value of a state-action pair into the immediate reward and the discounted value of the successor state-action pair.\n\n\\begin{align}\n  q_\\pi(s,a) & \\dot = \\mathbb{E_\\pi}[G_t|S_t=s, A_t=a] \\newline\n             &= \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma \\mathbb{E_\\pi}[G_{t+1}|S_{t+1}=s']) \\newline\n             & = \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma \\sum_{a'} \\pi(a'|s') \\mathbb{E_\\pi}[G_{t+1}|S_{t+1}=s', A_{t+1}=a']) \\newline\n             &= \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s',a'))\n\\end{align}\n\\tag{5}"
  },
  {
    "objectID": "posts/coursera/c1-w3.html#bellman-equations",
    "href": "posts/coursera/c1-w3.html#bellman-equations",
    "title": "Value Functions & Bellman Equations",
    "section": "Bellman Equations",
    "text": "Bellman Equations\nthe bellman equations capture the relationship between the current value and the future value. The Bellman equations are a set of equations that express the relationship between the value of a state and the value of its successor states. The Bellman equations are used to compute the value functions of a Markov Decision Process (MDP)."
  },
  {
    "objectID": "posts/coursera/c1-w3.html#example-gridworld",
    "href": "posts/coursera/c1-w3.html#example-gridworld",
    "title": "Value Functions & Bellman Equations",
    "section": "Example: Gridworld",
    "text": "Example: Gridworld\n\n\n\nA\nB\n\n\nC\nD\n\n\n\nIn the 2x2 gridworld example, the agent can move up, down, left, or right. The agent receives a reward of 0 for each step taken unless it gets to location B for which it gets +5. The agent receives will return to the current cell if it bumping into the wall.\nWe will use the uniform random policy where the agent selects each action with equal probability.\ngamma = 0.7\nlets calculate the value of each state using the Bellman equation.\n\n\\begin{align}\nv_\\pi(A) &= \\sum_a \\pi(a|A) \\sum_{s'} \\sum_r p(s',r|A,a) (r + \\gamma v(s')) \\newline\n     &= \\sum_a \\pi(a|A) (r + 0.7 v_\\pi(s')) \\newline\n     &= 0.25 \\times 0.7 \\times v(A) + 0.25 \\times (5 + 0.7 \\times v(B)) + 0.25 \\times 0.7 \\times v(C) + 0.25 \\times 0.7 \\times v(A) \\newline\nv_\\pi(B) &= 0.25 \\times 0.7 \\times v(A) + 0.5 \\times (5 + 0.7 \\times v(B)) + 0.25 \\times 0.7 \\times v(D) \\newline\nv_\\pi(C) &= 0.25 \\times 0.7 \\times v(A) + 0.5 \\times (0.7 \\times v(B)) + 0.25 \\times 0.7 \\times v(C) \\newline\nv_\\pi(D) &= 0.25 \\times 0.7 \\times v(B) + 0.5 \\times 0.7 \\times v(C) + 0.25 \\times 0.7 \\times v(D)\n\\end{align}\n we can solve these equations to get the value of each state.\ntheses are\n\n\\begin{align*}\nv_\\pi(A) &= 4.2 \\newline\nv_\\pi(B) &= 6.1 \\newline\nv_\\pi(C) &= 2.2 \\newline\nv_\\pi(D) &= 4.2 \\newline\n\\end{align*}\n\nWe can use the Bellman equation to calculate the value of each state in the Gridworld. The value of each state is the expected return when starting in that state and following the policy \\pi thereafter. The value of each state is calculated by summing the immediate reward and the discounted value of the successor states.\nFor larger MDP the Bellman equations are not practical method to calculate the value of each state. Instead, we will use algorithms based on the Bellman equations to estimate the value of each state.\n\nLesson 3: Optimality (Optimal Policies & Value Functions)\n\n\n\n\n\n\nGoals\n\n\n\n\nDefine an optimal policy #\nUnderstand how a policy can be at least as good as every other policy in every state. #\nIdentify an optimal policy for given MDPs.\nDerive the Bellman optimality equation for state-value functions\nDerive the Bellman optimality equation for action-value functions\nUnderstand how the Bellman optimality equations relate to the previously introduced Bellman equations\nUnderstand the connection between the optimal value function and optimal policies\nVerify the optimal value function for given MDPs"
  },
  {
    "objectID": "posts/coursera/c1-w3.html#sec-optimal-policy",
    "href": "posts/coursera/c1-w3.html#sec-optimal-policy",
    "title": "Value Functions & Bellman Equations",
    "section": "Optimal Policy",
    "text": "Optimal Policy\n\n\n\n\nBellman Optimality Equation\n\n\nA policy pi_1 is better than a policy \\pi_2 if v_{\\pi_1}(s) \\geq v_{\\pi_2}(s) for all states s.\nGiven any two policies \\pi and \\pi', if we pick the action that maximizes the value function, from either at every state, we will get a new policy that is at least as good as both \\pi and \\pi'.\nThere is always at least one deterministic optimal policy for any MDP.\nAn optimal policy \\pi^* is a policy that is at least as good as every other policy in every state.\nThe optimal policy is denoted by \\pi^* and is defined as:\n\n\n\\pi^* \\dot = \\arg \\max_{\\pi} v_{\\pi}(s) \\quad \\forall s\\in S\n\\tag{6}"
  },
  {
    "objectID": "posts/coursera/c1-w3.html#sec-bellman-optimality-state-value-function",
    "href": "posts/coursera/c1-w3.html#sec-bellman-optimality-state-value-function",
    "title": "Value Functions & Bellman Equations",
    "section": "Bellman Optimality Equation for State-Value Functions",
    "text": "Bellman Optimality Equation for State-Value Functions\n\nThe Bellman optimality equation for state-value functions is a recursive equation that decomposes the value of a state into the immediate reward and the discounted value of the successor state under the optimal policy.\n\n\\begin{align}\nv_*(s) & \\dot = \\max_{\\pi} v_{\\pi}(s) \\quad \\forall s \\in S\\newline\n       & = \\max_a \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1})|S_t=s, A_t=a] \\newline\n       & = \\max_a \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma v_*(s'))\n\\end{align}\n\\tag{7}"
  },
  {
    "objectID": "posts/coursera/c1-w3.html#sec-bellman-optimality-action-value-function",
    "href": "posts/coursera/c1-w3.html#sec-bellman-optimality-action-value-function",
    "title": "Value Functions & Bellman Equations",
    "section": "Bellman Optimality Equation for Action-Value Functions",
    "text": "Bellman Optimality Equation for Action-Value Functions\n\nThe Bellman optimality equation for action-value functions is a recursive equation that decomposes the value of a state-action pair into the immediate reward and the discounted value of the successor state-action pair under the optimal policy.\n\n\\begin{align}\nq_*(s,a) & \\dot = \\max_{\\pi} q_{\\pi}(s,a) \\quad \\forall s \\in S, \\forall a \\in A \\newline\n         & = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} q_*(S_{t+1}, a')|S_t=s, A_t=a] \\newline\n         & = \\sum_{s'} \\sum_r p(s',r|s,a) (r + \\gamma \\max_{a'} q_*(s',a'))\n\\end{align}\n\\tag{8}\nMartha White asks the question: “How can \\Pi_3 have better strictly better values than both \\Pi_1 and \\Pi_2 in all states if all we did is take the best action in each state from either \\Pi_1 or \\Pi_2?”\nThis is because if for example we found a fast path through a bottleneck for any state that is before the bottleneck will have a higher value in the other policies which may have had longer paths through the bottleneck."
  },
  {
    "objectID": "posts/coursera/c1-w3.html#sec-optimal-value-functions",
    "href": "posts/coursera/c1-w3.html#sec-optimal-value-functions",
    "title": "Value Functions & Bellman Equations",
    "section": "Optimal Value Functions",
    "text": "Optimal Value Functions\n\nThe optimal value function v_*(s) is the expected return when starting in state s and following the optimal policy thereafter.\nThe optimal action-value function q_*(s,a) is the expected return when starting in state s, taking action a, and following the optimal policy thereafter.\nAn optimal policy can be obtained from the optimal action-value function by selecting the action with the highest value."
  },
  {
    "objectID": "posts/coursera/c1-w4.html",
    "href": "posts/coursera/c1-w4.html",
    "title": "Dynamic Programming",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/coursera/c1-w4.html#sec-policy-evaluation-control",
    "href": "posts/coursera/c1-w4.html#sec-policy-evaluation-control",
    "title": "Dynamic Programming",
    "section": "Policy Evaluation and Control",
    "text": "Policy Evaluation and Control\nThe distinction between policy evaluation and control:\n\npolicy evaluation (prediction)\n\nis the task of evaluating the future, i.e. the value function given some specific policy \\pi.\n\ncontrol\n\nis the task of finding the optimal policy, given some specific value function v.\n\nplanning\n\nis the task of finding the optimal policy \\pi_{\\star} and value function v, given a model of the environment. this is typically done by dynamic programming methods.\n\n\nTypically we need to solve the prediction problem before we can solve the control problem. This is because we need to know the value of the states under the policy to be able to pick the best actions."
  },
  {
    "objectID": "posts/coursera/c1-w4.html#sec-dynamic-programming",
    "href": "posts/coursera/c1-w4.html#sec-dynamic-programming",
    "title": "Dynamic Programming",
    "section": "Dynamic Programming",
    "text": "Dynamic Programming\n\nDynamic programming is a method for solving complex problems by breaking them down into simpler sub-problems.\nIt is a general approach to solving problems that can be formulated as a sequence of decisions.\nDynamic programming can be applied to problems that have the following properties:\n\nOptimal substructure: The optimal solution to a problem can be obtained by combining the optimal solutions to its sub-problems.\nOverlapping sub-problems: The same sub-problems are solved multiple times."
  },
  {
    "objectID": "posts/coursera/c1-w4.html#sec-iterative-policy-evaluation",
    "href": "posts/coursera/c1-w4.html#sec-iterative-policy-evaluation",
    "title": "Dynamic Programming",
    "section": "Iterative Policy Evaluation Algorithm",
    "text": "Iterative Policy Evaluation Algorithm\nContinuing with our goal of finding the optimal policy, we now turn to the an algorithms that will allow us to predict the value all the state starting with even the most naive policy.\nThe iterative policy evaluation algorithm is a simple iterative algorithm that estimates the value function for a given policy \\pi.\nWe start with no knowledge of the value function or the policy. We set all the values to zero and we may even assume all actions are equally likely and all states are equally good. This is the uniform random policy. Alternatively we can start with some other policy.\nThese two assumptions are implemented in the initialization step of the algorithm.\nThe crux of the algorithm is the update step which is based on the recursive bellman equation for the value function under a policy \\pi:\n\nv_{\\pi}(s) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')] \\sum_{a} \\pi(a|s)\n I rearranged the terms to make it clear that we are iterating over the states we use this equation to update the value of each state using\n\nthe four part dynamics function p(s',r|s,a) to get the probability of receiving a reward r at a successor state s' given the current state s and action a.\nthe value of the next state V(s'). which we initially assumed is 0 and may have already updated\nthe policy \\pi(a|s) which we use to weigh the previous term\n\nAl this will give us the expected value of the state under the policy \\pi.\nThe final part of the algorithm is the stopping condition. We stop when the change in the value function is less than a small threshold \\theta.\nThe algorithm is guaranteed to converge to the value function for the policy \\pi.\nHere is the concise statement of the algorithm with just one array in pseudo code:\n\n\n\\begin{algorithm} \\caption{Iterative Policy Evaluation, for estimating $V \\approx v_{\\pi}$}\\begin{algorithmic} \\State Input: $\\pi$, the policy to be evaluated, default to uniform random policy \\State Algorithm parameter: a small threshold $\\theta &gt; 0$ determining accuracy of estimation \\State $V(s) \\leftarrow \\leftarrow \\vec 0 \\forall s \\in S$ \\REPEAT \\STATE $\\Delta \\leftarrow 0$ \\FORALL { $s\\in S$} \\STATE $v \\leftarrow V(s)$ \\STATE $V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]\\quad$ \\comment{ Bellman equation} \\STATE $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$ \\ENDFOR \\UNTIL{$\\Delta &lt; \\theta$} \\State Output: $V \\approx v_{\\pi}$ \\end{algorithmic} \\end{algorithm}\n\n\nnote: the algorithm makes a couple of assumptions that are omitted in the pseudo code.\n\nthat we have access to the dynamics function p(s',r|s,a)\nthat we have access to the reward function r(s,a,s')"
  },
  {
    "objectID": "posts/coursera/c1-w4.html#sec-applying-iterative-policy-evaluation",
    "href": "posts/coursera/c1-w4.html#sec-applying-iterative-policy-evaluation",
    "title": "Dynamic Programming",
    "section": "Applying Iterative Policy Evaluation",
    "text": "Applying Iterative Policy Evaluation\nThe iterative policy evaluation algorithm can be applied to compute the value function for a given policy \\pi."
  },
  {
    "objectID": "posts/coursera/c1-w4.html#sec-l2g5",
    "href": "posts/coursera/c1-w4.html#sec-l2g5",
    "title": "Dynamic Programming",
    "section": "Value Iteration",
    "text": "Value Iteration\nValue iteration is an important example of Generalized Policy Iteration. It is an iterative algorithm that computes the optimal value function and the optimal policy for a given MDP but it does not directly referrence a particular policy.\nIn value iteration, the algorithm starts with an initial estimate of the value function and iteratively runs a single step of greedy polict evaluation per step, using the greedy value to update the state-value function.\nupdates the value function until it converges to the optimal value function.\n\n\n\\begin{algorithm} \\caption{Value Iteration, for estimating $\\pi \\approx \\pi_{\\star}$}\\begin{algorithmic} \\State Input: $\\pi$, the policy to be evaluated \\State Algorithm parameter: a small threshold $\\theta &gt; 0$ determining accuracy of estimation \\State Initialize $V(s) \\leftarrow \\vec{0} \\forall s \\in \\mathbb{R}$ \\REPEAT \\STATE $\\Delta \\leftarrow 0$ \\FORALL { $s\\in S$} \\STATE $v \\leftarrow V(s)$ \\STATE $V(s) \\leftarrow max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$ \\STATE $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$ \\ENDFOR \\UNTIL{$\\Delta &lt; \\theta$} \\State Output: $V \\approx v_{\\pi}$ such that \\State $\\pi(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$ \\end{algorithmic} \\end{algorithm}\n\n\n\nThe Dance of Policy and Value\n\n\n\n\nDance of Policy and Value\n\nThe policy iteration algorithm is called the dance of policy and value because it alternates between policy evaluation and policy improvement. The policy evaluation step computes the value function for the current policy, and the policy improvement step constructs a new better greedyfied policy based on the value function.\nThis is also true for other generalized policy iteration algorithms, such as value iteration, which alternates between policy evaluation and policy."
  },
  {
    "objectID": "posts/coursera/c3-w1.1.html",
    "href": "posts/coursera/c3-w1.1.html",
    "title": "On-Policy Prediction with Approximation",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms\nSome of the notes I made in this course became a bit too long. Rather than break the flow of the lesson I decided to move them to a separate file. This is one of those notes."
  },
  {
    "objectID": "posts/coursera/c3-w1.1.html#how-are-generalization-in-ml-is-closely-related-to-transfer-learning-in-rl.",
    "href": "posts/coursera/c3-w1.1.html#how-are-generalization-in-ml-is-closely-related-to-transfer-learning-in-rl.",
    "title": "On-Policy Prediction with Approximation",
    "section": "How are generalization in ML is closely related to transfer learning in RL.",
    "text": "How are generalization in ML is closely related to transfer learning in RL.\nIn ML we have a rather clear understanding of generalization. We have a training set and a test set. We train on the training set and then test on the test set. The goal is to do well on the test set. The test set is a sample from the same distribution as the training.\nGeometrically, for classification we want to a decision boundary that separates the classes with the least error and with a fewest number of parameters. This is the essence of the bias-variance tradeoff.\nIn RL we tend to think of generalization as the ability of an agent to perform well on a task that is different from the one it was trained on. The algorithms can take decades of CPU compute to solve a simple video game. But change a few pixels in the game and the agent can’t play at all. This suggest these agents are severely overfitting.\nWould we be able to learn much faster if we could avoid overfitting i.e. if we could generalize better?\nOne point worth considering here is that solving a general problem is harder than a specific one.\nE.g. To solve a maze we need a policy matrix. To solve all mazes we need an algorithm.\nSo in one sense it is harder to generalize than to solve a specific problem. However it also should allow us to discard most of the irrelevant information that take up most of the model’s capacity and end up slowing down learning.\n\nAgents learn a policy that is only suitable to a specific task. The policy doesn’t generalize to even small changes in the task, e.g. moving the start and goal in the same maze tasks.\nLearned representation for features are not abstract and thus can’t be mapped to a slightly different task (e.g. changing a few pixels in a game)\nWe definitely can’t map the representation to different tasks.\nIdeally, we would like to deal with challenging problems by reusing knowledge from agents trained on other problems.\nOne direction called options lays in decomposing learning a policy for a goal into reframing it into learning sub-goals, strategies and tactics and basic moves.\nAnother direction I call heuristics concerns finding minimal policies that are just strong enough to get the agent to the goal a high percentage of the time.\nLearning should be aggregational and compositional. However, these terms require reinterpretation for each problem and at many levels of abstraction."
  },
  {
    "objectID": "posts/coursera/c3-w1.1.html#human-like-to-use-heuristic-which-are-are",
    "href": "posts/coursera/c3-w1.1.html#human-like-to-use-heuristic-which-are-are",
    "title": "On-Policy Prediction with Approximation",
    "section": "Human like to use Heuristic, which are are:",
    "text": "Human like to use Heuristic, which are are:\n- A minimal sub-optimal policy that is suffiecnt to get the agent to its goal with high probability.\n- In an MDP with lots of sub-goals, we may have benefit in learning learning heuristic style policy for each sub-goal and then compose them into a policy for the goal. \n- Composing heuristics is vague so let try make it clear.\n    - We want to follow the heuristic policy until we reach a sub-goal.\n    - We then switch to the policy for the next sub-goal. \n    - If we have well established entry and exit points for each heuristic we can have two benefits one is generalization and the other is discrimination.\n        - Generalization is due to using the same heuristic from different starting points.\n        - Discrimination is due to having different heuristics for different sub-goals.\n        - A third advantage is that the heuristic policy is for a smaller state space and can be learned faster.\n        - Third advantage is may be that of mapping different sub-problem to the same heuristic may allow us to discard some of the features of the state space that are not required for the heuristic to work.\n    - Thus composing heuristics in this case is just about switching between heuristics at the right time.\n    - Another direction is to use the heuristics as a form of  priors for the policy we want to learn.  \n    - Simple models are often a good fit for more problems than complex models.\n    - If we are good at learning to decompose problems into simpler sub problems and then we might be able to leverage the power of heuristics.\n\n-   Heuristics don't always work but overall they capture the essence of the solution to the problem.\n-   Heuristics are usually more general than an optimal policy.\n-   A heuristic might be a very good behavior policy for off policy learning the optimal policy.\n-   I don't see RL algorithms for heuristics."
  },
  {
    "objectID": "posts/coursera/c3-w1.1.html#models-in-rl-try-to-approximate-mdp-dynamics-using-its-transition-and-rewards",
    "href": "posts/coursera/c3-w1.1.html#models-in-rl-try-to-approximate-mdp-dynamics-using-its-transition-and-rewards",
    "title": "On-Policy Prediction with Approximation",
    "section": "Models in RL try to approximate MDP dynamics using its transition and rewards",
    "text": "Models in RL try to approximate MDP dynamics using its transition and rewards\n-   In ML we often use boosting and bagging to aggregate very simple models.\n-   In RL we often replace the model by sampling from a replay buffer of the agent's past experiences."
  },
  {
    "objectID": "posts/coursera/c3-w1.1.html#the-problem-for-a-general-ai-is-very-much-the-problem-of-transfer-learning-in-rl.",
    "href": "posts/coursera/c3-w1.1.html#the-problem-for-a-general-ai-is-very-much-the-problem-of-transfer-learning-in-rl.",
    "title": "On-Policy Prediction with Approximation",
    "section": "The problem for a general ai is very much the problem of transfer learning in RL.",
    "text": "The problem for a general ai is very much the problem of transfer learning in RL.\n\nagents learn a very specific policy for a very specific task - the learned representation cannot be mapped to other tasks or even other states in the same task.\nif agents learning was decomposed into\n\nlearning very general policies that solved more abstract problems and then\nlearning a good composition of these policies to solve the specific problem.\nonly after getting to this point would the agent try to optimize the policy for the specific task.\ne.g. chess\n\nlearn the basic moves and average value of pieces\nlearning tactics - short term goals\nlearning about end game\n\nupdate the value of pieces based on the ending\n\nlearning about strategy\n\npositional play\n\nlearn about pawn formations and weak square\n\nvalue of pawn formations\nhow they can be used with learned tactics.\n\nthe center\n\nadd value to pieces based on their position on the board\n\nopen files and diagonals\n\nlong term plans\n\nminority attack, king side attack, central breakthrough\ncreating a passed pawn\nexchanging to win in the end game\nsacrificing material to get a better position\nattacking the king\n\ncastling\npiece development and the center\ntempo\n\nlocalize value of pieces in different positions on the board using the learned tactics and strategy."
  },
  {
    "objectID": "posts/coursera/c3-w1.1.html#bayesian-models-and-hierarchical-model-encode-knowledge-using-priors-which-can-pool-or-bias-beliefs-towards-a-certain-outcome.",
    "href": "posts/coursera/c3-w1.1.html#bayesian-models-and-hierarchical-model-encode-knowledge-using-priors-which-can-pool-or-bias-beliefs-towards-a-certain-outcome.",
    "title": "On-Policy Prediction with Approximation",
    "section": "Bayesian models and hierarchical model encode knowledge using priors which can pool or bias beliefs towards a certain outcome.",
    "text": "Bayesian models and hierarchical model encode knowledge using priors which can pool or bias beliefs towards a certain outcome.\n-   learning in Bayesian models is about updating the initial beliefs based on incoming evidence."
  },
  {
    "objectID": "posts/coursera/c3-w1.1.html#ci-may-be-useful-here",
    "href": "posts/coursera/c3-w1.1.html#ci-may-be-useful-here",
    "title": "On-Policy Prediction with Approximation",
    "section": "CI may be useful here",
    "text": "CI may be useful here\n\nIs in a big way about mapping knowledge into\n\nStatistical joint probabilities,\nCasual concepts that are not in the joint distributions like interventions and Contrafactuals, latent, missing, mediators, confounders, etc.\nHypothesizing a causal structural model, deriving a statistical model and Testing it against the data.\nInterventions in the form of actions and options -\n\nMany key ideas in RL are counterfactual reasoning\n\nOff-policy learning is about learning from data generated by a different policy.\nOptions are like do operations (interventions)\nChoosing between actions and options is like contrafactual reasoning.\n\nUsing and verifying CI models could be the way to unify the spatial and temporal abstraction in RL."
  },
  {
    "objectID": "posts/coursera/c2-w1.html",
    "href": "posts/coursera/c2-w1.html",
    "title": "Monte-Carlo Methods for Prediction & Control",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/coursera/c2-w1.html#sec-l2g1",
    "href": "posts/coursera/c2-w1.html#sec-l2g1",
    "title": "Monte-Carlo Methods for Prediction & Control",
    "section": "MC Action-Value Functions",
    "text": "MC Action-Value Functions\n\n\n\n\naction values\n\n\n\n\nback off\n\n\nThis back off diagram indicates that the value of a state S depends on the values of its actions.\n\nRecall that control is simply improving a policy using our action values estimate.\nPolicy improvement is done by Greedyfying a policy \\pi at a state s by selecting the action a with the highest action value.\nIf we are missing some action values we can make the policy worse!\nWe need to ensure that our RL algorithm engages the different actions of a state. There are two strategies:\n\nExploring starts\n\\epsilon-Soft strategies\n\n\n\n\n\n\nexploring starts\n\nThe following is the MC alg with exploring start for estimation.\n\n\n\n\nexploring starts pseudocode\n\nLet’s recap how GPI looks:\n\nKeeping \\pi_0 fixed we do evaluation of q_\\pi using MC–ES\nWe improve \\pi_0 by picking the actions with the highest values\nWe stop when we don’t improve \\pi\n\nHere, in the evaluation step, we estimate the action-values using MC prediction, with exploration driven by exploring Starts or an \\epsilon-soft policy"
  },
  {
    "objectID": "posts/coursera/c2-w1.html#lesson-4-off-policy-learning-for-prediction",
    "href": "posts/coursera/c2-w1.html#lesson-4-off-policy-learning-for-prediction",
    "title": "Monte-Carlo Methods for Prediction & Control",
    "section": "Lesson 4: Off-policy learning for prediction",
    "text": "Lesson 4: Off-policy learning for prediction\n\n\n\n\n\n\nLesson Learning Goals\n\n\n\n\nUnderstand how off-policy learning can help deal with the exploration problem #\nProduce examples of target policies and examples of behavior policies. #\nUnderstand importance sampling #\nUse importance sampling to estimate the expected value of a target distribution using samples from a different distribution. #\nUnderstand how to use importance sampling to correct returns #\nUnderstand how to modify the Monte Carlo prediction algorithm for off-policy learning. #\n\n\n\n\nOff-policy learning\n\nOff-policy learning is a way to learn a policy \\pi using samples from another policy \\pi'.\nThis is useful when we have a policy that is easier to sample from than the policy we want to learn.\nA key idea is to correct the returns using importance sampling.\n\nFor example suppose we can use a rule based model to generate samples of agent state, action and rewards - but we don’t really have an MDP, value function or policy. We could start with a uniform random policy and then use the samples to learn a better policy. However this would require us to interact with the environment and our agents may not be able to do this. In the case of Sugarscape model the agents are not really making decisions, they are following rules.\nIf we wished to develop agent that learn using RL with different rules on or off and other settings and use those to learn a policy using many samples. One advantage of the Sugarscape model is that it is highly heterogeneous so we get a rich set of samples to work with. A second advantage is that the rule based model can be fast to sample from and we can generate many samples by running it using hyper-parameters optimized test-bed.\nSo if we have lots of samples we may not need to explore as much initially, but rather learn to exploit the samples we have. Once we learn a near optimal policy for the samples we can use our agent to explore new vistas in our environment.\n\n\nTarget and behavior policies\n\nThe target policy is the policy we want to learn.\nThe behavior policy is the policy we sample from.\n\n\n\nImportance sampling\n\nImportance sampling is a technique to estimate the expected value of a target distribution using samples from a different distribution.\nWhy cant we just use the samples from the behavior policy to estimate the target policy?\nThe answer is that the samples from the behavior policy are biased towards the behavior policy.\nIn the target policy we may have states that are never visited by the behavior policy.\nFor example we might want to learn a policy that focuses on trade rather than combat or Vica-versa. This extreme idea of introducing/eliminating some action would significantly change behavioral trajectories. Sample based methods could be able to handle these changes - if we can restrict them to each subset of actions but clearly the expected return of states will be diverge in the long run.\nSo what we want is someway to correct the returns from the behavior policy to the target policy.\nIt is used to correct returns from the behavior policy to the target policy.\n\nThe probability of a trajectory under \\pi is:\n\n\\begin{align*}\n  P(A_t, S_{t+1}, & A_{t+1}, ... ,S_T | S_t, A_{t:T-1} \\sim \\pi) \\newline\n  & = \\pi(A_t|S_t)p(S_{t+1}|S_t, A_t)\\pi(A_{t+1}, S_{t+1}) \\cdot\\cdot\\cdot p(S_T|S_{T-1}, A_{T-1}) \\newline\n  & = \\prod_{k=t}^{T-1} \\pi(A_k|S_k)p(S_{k+1}|S_k, A_k)\n\\end{align*}\n\\tag{2}\n\n\nImportance sampling ratio\nDefinition: The importance sampling ratio (rho, \\rho) is the relative probability of the trajectory under the target vs behavior policy:\n\n\\begin{align}\n\\rho_{t:T-1} & \\doteq \\frac{\\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k) \\cancel{ p(S_{k+1} \\mid S_k, A_k)}}{\\prod_{k=t}^{T-1} b(A_k \\mid S_k) \\cancel{ p(S_{k+1} \\mid S_k, A_k)} } \\newline\n             & = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k \\mid S_k)}{b(A_k \\mid S_k)}\n\\end{align}\n\\tag{3}\n\nv_\\pi(s) = \\mathbb{E}_b[\\rho_{t:T-1} \\cdot G_t \\mid S_t = s] \\qquad\n\\tag{4}\n\nV(s) \\doteq \\frac{\\displaystyle \\sum_{t\\in \\mathscr T(s)}\\rho_{t:T(t) - 1} \\cdot G_t}{|\\mathscr T (s)|} \\qquad\n\\tag{5}\n\nV(s) \\doteq \\frac{\\displaystyle \\sum_{t\\in \\mathscr T(s)} \\Big(\\rho_{t:T(t) - 1} \\cdot G_t\\Big)}{\\displaystyle \\sum_{t\\in \\mathscr T(s)}\\rho_{t:T(t) - 1}} \\qquad\n\\tag{6}\n\n\n\n\nimportance sampling example\n\n\n\n\noff policy trajectories\n\n\n\n\n\n\n\n\nOff-policy every visit MC prediction\n\n\n\n\n\n\\begin{algorithm} \\caption{OffPolicyMonteCarloPrediction()}\\begin{algorithmic} \\State Input: \\State $\\qquad \\pi \\leftarrow \\text{policy to be evaluated}$ \\State Initialize: \\State $\\qquad V(s) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S}$ \\State $\\qquad Returns(s) \\text{ an empty list,} \\quad \\forall s \\in \\mathcal{S}$ \\For {each episode:} \\State Generate an episode by following $\\pi: S_0, A_0, R_1,\\ldots, S_{T-1}, A_{T-1}, R_T$ \\State $G \\leftarrow 0, W \\leftarrow 1$ \\For {each step of episode, $t \\in T-1, T-2,..., 0$:} \\State $G \\leftarrow \\gamma WG + R_{t+1}$ \\State Append $G$ to $Returns(S_t)$ \\State $V(S_t) \\leftarrow average(Returns(S_t))$ \\State $W \\leftarrow W \\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}$ \\EndFor \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\n\nOff-policy every visit MC control\n\n\n\n\n\n\\begin{algorithm} \\caption{OffPolicyMonteCarloPrediction()}\\begin{algorithmic} \\State Input: \\State $\\qquad \\pi \\leftarrow \\text{policy to be evaluated}$ \\State Initialize: \\State $\\qquad V(s) \\leftarrow x \\in \\mathbb{R} \\forall s \\in \\mathcal{S}$ \\State $\\qquad Returns(s) \\text{ an empty list,} \\quad \\forall s \\in \\mathcal{S}$ \\For {each episode:} \\State Generate an episode by following $b: S_0, A_0, R_1,\\ldots, S_{T-1}, A_{T-1}, R_T$ \\State $G \\leftarrow 0, W \\leftarrow 1$ \\For {each step of episode, $t = T-1, T-2,\\ldots, 0$:} \\State $G \\leftarrow \\gamma WG + R_{t+1}$ \\State Append $G$ to $Returns(S_t)$ \\State $V(S_t) \\leftarrow average(Returns(S_t))$ \\State $W \\leftarrow W \\frac{\\pi(A_t \\mid S_t)}{b(A_t \\mid S_t)}$ \\EndFor \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\nEmma Brunskill: Batch Reinforcement Learning\nThese guest talks have a dual purpose:\n\nto let the speakers share their passion for the field and introduce us to their research. this can be a good start for reading more about our own interests or for looking how to solve real problems that we are facing.\nto show us how the concepts we are learning are being used in the real world.\n\n\nEmma Brunskill is a professor at Stanford University.\nBurnskill motivated her approach with an edutainment app in which the goal is to maximize student engagement in game based on historical data.\nIn batch RL we have a fixed dataset of samples and we want to learn a policy from this data.\nThis is useful when we have a fixed dataset of samples and we want to learn a policy from this data.\nThe key idea is to use importance sampling to correct the returns from the behavior policy to the target policy. We learned that the challenge this poses is primarily due to the bias of the behavior policy.\nImportance sampling provides us with an unbiased estimate of the value function yet can have high variance. These may can be exponentially large in the number of steps. So these results in very poor estimates for the value function if there are many steps in the trajectory.\nBrunskill suggest that the real challenge posed by batch RL is a sparsity of trajectories with actions leading to optimal next states under the target policy in the historical data.8\nOne point we learned about this is that we should seek algorithms that are more data efficient. However\nA send idea is to use parametric models which are biased by can learn the transition dynamics and the reward function more efficiently.\nBrunskill points out that since we have few samples we may need a better approach to get robust estimates of the value function.\nThis approach which comes from statistic is called doubly robust stimators and has been used in bandits and RL\nShe presents a chart from a 2019 paper with a comparison of different methods for RL in the cart-pole environment.\n\nOff policy policy gradient with state Distribution Correction - dominates the other methods. And has a significantly narrower confidence interval for the value, if I understand the figure correctly.\n\nShe also presents results from many papers on Generalization Guarantees for RL, which show that we can learn a policy that is close to the optimal policy with a small number of samples from another policy. However I cannot make much sense of the result in the slide.\nAn example of this is the Sugarscape model where we have a fixed dataset of samples from the rule-based model.\nMore generally, we can use batch RL to learn from historical data how to make better decisions in the future.\n\n\nCounterfactual\n\nYou don’t know what your life would be like if you weren’t reading this right now.\n\n\n\nCausal reasoning based on counterfactuals is a key idea to tackling this problem.\n\n\nCounterfactual or Batch Reinforcement Learning\n\nIn batch RL we have a fixed dataset of samples and we want to learn a new policy from this data.\n\n\n\n\n\n\n\n\nDoubly Robust Estimators\n\n\n\nDoubly robust estimators is a technique from statistics that and causal inference that allows us to combine to do importance sampling and model based learning and a propensity score to estimate the value function. combine the best of both worlds - they are robust to errors in the model and the policy.\n\n\\hat V_{DR} =\\frac{1}{N} \\sum_{i=1}^N \\left[ \\rho_i (R_i + \\gamma Q(s_{i+1}, \\pi(s_{i+1}))) - \\rho_i \\hat Q_{\\pi}(s_i, a_i) + \\hat Q_{\\pi}(s_i, a_i) \\right]\n where:\n\n\\rho_i is the importance sampling ratio for the i-th sample\nR_i is the reward - Q(s_{i+1}, \\pi(s_{i+1})) is the value of the next state under the target policy\n\\hat Q_{\\pi}(s_i, a_i) is the model based Q-function estimate\nQ(s_{i+1}, \\pi(s_{i+1})) is the value of the next state under the target policy\n\n\n\n\nand paper\n\nProvably Good Batch Reinforcement Learning Without Great Exploration\nData-Efficient Off-Policy Policy Evaluation for Reinforcement Learning"
  },
  {
    "objectID": "posts/coursera/c2-w1.html#footnotes",
    "href": "posts/coursera/c2-w1.html#footnotes",
    "title": "Monte-Carlo Methods for Prediction & Control",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrediction in the sense that we want to predict for \\pi how well it will preforms i.e. its expected returns for a state↩︎\nworth either 1 or 11↩︎\nface card are worth 10↩︎\nthis is a big simplifying assumption↩︎\nin DP we had to solve n\\times n - simultaneous equations↩︎\nthink of a medical trial↩︎\nthink of a self driving car↩︎\n\nCan we learn form one or two examples by sampling ?\nwhat if the good actions are never sampled by our algorithm?\n\n↩︎"
  },
  {
    "objectID": "posts/coursera/c1-w0.html",
    "href": "posts/coursera/c1-w0.html",
    "title": "Course Introduction",
    "section": "",
    "text": "RL logo\n\n\n\n\nRL algorithms"
  },
  {
    "objectID": "posts/coursera/c1-w0.html#a-long-term-plan-to-learning-deep-rl",
    "href": "posts/coursera/c1-w0.html#a-long-term-plan-to-learning-deep-rl",
    "title": "Course Introduction",
    "section": "A Long term plan to learning Deep RL",
    "text": "A Long term plan to learning Deep RL\n\ndo this specialization\nread the rest of the book - there are a number of important subjects and algorithms that are not covered in the course.\nsolve more problems\nread more papers\n\n\nthere are a zillion algorithms and a gazillion environments out there\nreading a few papers will get you up to speed on the state of the art.\n\n\nimplement more algorithms\n\n\nthis is what people are going to pay you for\nstart with gradient bandit algorithms\nthen Thompson Sampling using different distributions\n\n\ntry and solve a real problem:\n\n\nI created a custom RL algorithm to solve the Lewis signaling game quickly and efficient - this allows agents to learn to communicate.\nI am now working on designing a multi-agent version of the game that should learn even faster.\n\n\ndo the deep rl course from Hugging Face\nfind RL challenges on Kaggle"
  },
  {
    "objectID": "posts/coursera/c1-w0.html#footnotes",
    "href": "posts/coursera/c1-w0.html#footnotes",
    "title": "Course Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe annotated book and flashcards will help here. This material is really logical - if you are surprised/confused you never assimilated some part of the material. Once you do it should become almost intuitive to reason about from scratch.↩︎"
  }
]