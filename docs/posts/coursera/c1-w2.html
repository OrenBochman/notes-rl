<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="keywords" content="Markov decision processes, MDP">
<meta name="description" content="In week 2 we learn about Markov Decision Processes (MDP) and how to compute value functions and optimal policies, assuming you have the MDP model. We implement dynamic programming to compute value functions and optimal policies.">

<title>Markov Decision Processes – Notes on Reinfocement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-d166b450ba5a8e9f7a0ab969bf6592c1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-594d21605a7b9fb32547fedacd8fc358.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-d396125c57f3f0defba792e7b0e7a5dc.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background: /images/banner_black_3.jpg;
      }
</style>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Notes on Reinfocement Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Markov Decision Processes</h1>
            <p class="subtitle lead">RL Fundamentals</p>
                  <div>
        <div class="description">
          In week 2 we learn about Markov Decision Processes (MDP) and how to compute value functions and optimal policies, assuming you have the MDP model. We implement dynamic programming to compute value functions and optimal policies.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Coursera</div>
                <div class="quarto-category">Fundamentals</div>
                <div class="quarto-category">Reinforcement Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Tuesday, May 3, 2022</p>
      </div>
    </div>
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Markov decision processes, MDP</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/logo.png" class="img-fluid figure-img"></p>
<figcaption>RL logo</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/alg_selector.png" class="img-fluid figure-img" data-group="slides"></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div>
<section id="lesson-1-introduction-to-markov-decision-processes" class="level1 page-columns page-full">
<h1>Lesson 1: Introduction to Markov Decision Processes</h1>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><span class="citation" data-cites="sutton2018reinforcement">[@sutton2018reinforcement§3.3, pp. 47-56]</span> <a href="http://incompleteideas.net/book/RLbook2020.pdf#page=47">book</a></label></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand Markov Decision Processes (MDP). <a href="#l1g1">#</a></label></li>
<li><label><input type="checkbox" checked="">Describe how the dynamics of an MDP are defined. <a href="#l1g2">#</a></label></li>
<li><label><input type="checkbox" checked="">Understand the graphical representation of a Markov Decision Process. <a href="#l1g3">#</a></label></li>
<li><label><input type="checkbox" checked="">Explain how many diverse processes can be written in terms of the MDP framework. <a href="#l1g4">#</a></label></li>
</ul>
</div>
</div>
<p>Before I started this course I viewed <span class="citation" data-cites="silver2015">[@silver2015]</span> online course by David Silver who in many ways is the face of RL. He is also the lead of the AlphaGo project, a principal researcher at DeepMind and a lecturer at University College London. Silver is also featured in one of the Lectures in this specialization. In his course he develops the MDP constructively starting with simpler structures as is often done in mathematics. I find this is a good way to learn how to think about how we generalize and specialize from more abstract to more concrete structures.</p>
<p>Many students of probability theory will be familiar with Markov Chains. And Markov Decision Processes are a generalization of Markov Chains.</p>
<p>In <span class="citation" data-cites="silver2015">[@silver2015]</span> he begins with a <strong>Markov Process</strong>, with states and transitions probabilities, by adding <strong>rewards</strong> he constructs a <em>Markov reward process</em>. Then by adding <strong>actions</strong> he constructs a <em>Markov decision process</em>. He explains these and in the notes covers three additional extensions. In the the notes he also add the following MDP extensions:</p>
<ul>
<li>Infinite and continuous MDPs - the case of optimal control</li>
<li>Partially observable MDPs.</li>
<li>Undiscounted, average reward MDPs.</li>
<li>David Silver’s 2015 <a href="https://www.davidsilver.uk/teaching/">UCL Course</a> <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">Video</a> and Slides.</li>
</ul>
<section id="sec-Markov-Process" class="level2">
<h2 class="anchored" data-anchor-id="sec-Markov-Process">Markov Process</h2>
<p>Silver goes into some detail on what we mean by state in RL:</p>
<ul>
<li>In th abstract state can be any function of the history.</li>
<li>The state should summarize the information on the previous actions, and rewards.</li>
<li>He points out that the history in RL can be very long, for Atari games it can include actions plus all the pixels for every screen in many plays of the game. In contrast the state tries to capture the bare essentials of the history for decision making at each time step. For Atari games they used the last 4 screens as the state.</li>
<li>A second point is that there is always a state. The full history is also a state, but not a very useful one. The internal representation of the ram in the Atari game is also a state, much smaller but this is the representation used by the environment and contains more information than is available to the agent. Ideally the agent would want to model this state, but again it contains lots more information than is available would need to male a decision.</li>
<li>Another useful property of the state is that it should have the Markov Property for a state space which is when the future is independent of the past given the present.</li>
</ul>
<p>A state S<span class="math inline">_t</span> is Markov if and only if:</p>
<p><span id="eq-markov-property"><span class="math display">
\mathbb{P}[S_{t+1}  \vert  S_{t}] =  \mathbb{P}[S_{t+1}  \vert  S_1,...,S_t] \qquad \text{(Markov Property)}
\tag{1}</span></span></p>
<p><strong>The state captures all relevant information from the history</strong> Once the state is known, the history may be thrown away i.e.&nbsp;<mark>The state is a sufficient statistic of the future</mark></p>
<p>Recall:</p>
<blockquote class="blockquote">
<p>a statistic satisfies the criterion for sufficency when no other statistic that can be calculated from the same <a href="https://en.wikipedia.org/wiki/Sample_(statistics)" title="Sample (statistics)">sample</a> provides any additional information as to the value of the parameter”. — <span class="citation" data-cites="doi:10.1098/rsta.1922.0009">[@doi:10.1098/rsta.1922.0009]</span></p>
</blockquote>
<p>For a Markov state <span class="math inline">s</span> and successor state <span class="math inline">s'</span>, the state transition probability is defined by:</p>
<p><span class="math display">
\mathbb{P}_{ss'} = \mathbb{P}[S_{t+1}=s'  \vert  S_t=s]
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Markov Process or Chain Definion
</div>
</div>
<div class="callout-body-container callout-body">
<dl>
<dt>A Markov Process is</dt>
<dd>
<p>a tuple <span class="math inline">⟨S,P⟩</span></p>
</dd>
</dl>
<p>where:</p>
<ul>
<li><span class="math inline">S</span> is a (finite) set of states</li>
<li><span class="math inline">P</span> is a state transition probability matrix, <span class="math inline">P_{ss'} = P[S_{t+1} = s'  \vert S_t=s]</span> State transition matrix <span class="math inline">P_{ss'}</span> defines transition probabilities from all states <span class="math inline">s</span> to all successor states <span class="math inline">s'</span>,</li>
</ul>
<p><span id="eq-transition-matrix"><span class="math display">
\begin{align*}
  P=\left( \begin{array}{cc}
      p_{11} &amp; \cdots &amp; p_{1n} \newline
      \vdots &amp; \ddots &amp; \vdots \newline
      p_{n1} &amp; \cdots &amp; p_{nn} \end{array} \right)
\end{align*}
\tag{2}</span></span></p>
</div>
</div>
</section>
<section id="sec-MRP" class="level2">
<h2 class="anchored" data-anchor-id="sec-MRP">Markov Reward Process</h2>
<p>A Markov Reward Process <strong>(MRP)</strong> is a Markov chain with values.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Markov Reward Process Definition
</div>
</div>
<div class="callout-body-container callout-body">
<dl>
<dt>A Markov Reward Process is</dt>
<dd>
<p>a tuple <span class="math inline">⟨S, P, R, \gamma⟩</span></p>
</dd>
</dl>
<p>where:</p>
<ul>
<li><span class="math inline">S</span> is a finite set of states</li>
<li><span class="math inline">P</span> is a state transition probability matrix, where <span class="math inline">P_{ss'} =  \mathbb{P}[S_{t+1} = s' \vert  S_t = s]</span></li>
<li><span class="math inline">R</span> is a reward function, <span class="math inline">R_s = \mathbb{E}[R_{t+1} \vert S_t = s]</span></li>
<li><span class="math inline">\gamma</span> is a discount factor, <span class="math inline">\gamma \in [0, 1]</span></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
the return definition
</div>
</div>
<div class="callout-body-container callout-body">
<dl>
<dt>The return <span class="math inline">G_t</span></dt>
<dd>
<p>is the total discounted reward from time-step t.</p>
</dd>
</dl>
<p><span id="eq-return"><span class="math display">
G_t =R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\tag{3}</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">R_t</span> is the reward at time-step <span class="math inline">t</span></li>
<li><span class="math inline">\gamma</span> the discount factor <span class="math inline">\gamma \in [0, 1]</span> is the present value of future rewards.</li>
<li>The value of receiving reward <span class="math inline">R</span> after <span class="math inline">k+1</span> time-steps is <span class="math inline">\gamma^k R</span></li>
<li>This values immediate reward above delayed reward.
<ul>
<li><span class="math inline">\gamma = 0</span> makes the agent short-sighted.</li>
<li><span class="math inline">\gamma = 1</span> makes the agent far-sighted.</li>
</ul></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The value function
</div>
</div>
<div class="callout-body-container callout-body">
<dl>
<dt>The value function:</dt>
<dd>
<p>The state value function <span class="math inline">v(s)</span> of an MRP is the expected return starting from state <span class="math inline">s</span></p>
</dd>
</dl>
<p><span class="math display">
v(s) =\mathbb{E} [G_t  \vert  S_t = s]
</span></p>
</div>
</div>
<section id="sec-bellman-MRP" class="level3">
<h3 class="anchored" data-anchor-id="sec-bellman-MRP">Bellman equations for MRP</h3>
<p>The value function can be decomposed into two parts:</p>
<ul>
<li>an immediate reward <span class="math inline">R_{t+1}</span> and</li>
<li>a discounted value of successor state <span class="math inline">\gamma v(S_{t+1})</span></li>
</ul>
<p>The Bellman equation for MRPs expresses this relationship:</p>
<p><span id="eq-bellman-mrp"><span class="math display">
\begin{align*}
v(s) &amp;= \mathbb{E}[G_t  \vert  S_t=s] \newline
&amp; = \mathbb{E}[R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3} + ...  \vert S_t = s]   \newline
&amp; = \mathbb{E}[R_{t+1} + \gamma( R_{t+2}+\gamma^2 R_{t+3} + ... )  \vert S_t = s]   \newline
&amp; = \mathbb{E}[R_{t+1} + \gamma G_{t+1}  \vert  S_t = s]   \newline
&amp; = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})  \vert S_t = s]
\end{align*} \qquad \text{(Bellman Equation)}
\tag{4}</span></span></p>
<p>The Bellman equation can also be expressed in terms of the dynamics matrix for state transitions:</p>
<p><span id="eq-bellman-mrp-with-dynamics-matrix"><span class="math display">
v(s) = R_s + γ \sum_{s'\in S} P_{ss'}v(s) \qquad \text{value with dynamics}
\tag{5}</span></span></p>
<p>where we use the dynamics matrix <span class="math inline">P</span> to express the expected value of the successor state.</p>
</section>
</section>
<section id="sec-MDP" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-MDP">Markov Decision Processes</h2>
<ul>
<li>The k-Armed Bandit problem does not account for the fact that different situations call for different actions.</li>
<li>Because it the problem is limited to a single state agents can only make decisions based on immediate reward so they fail to consider the long-term impact of their decisions - this is an inability to make plan.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/rl-agent-env.png" class="img-fluid figure-img"></p>
<figcaption>The agent–environment interaction in a Markov decision process.</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MDP definition
</div>
</div>
<div class="callout-body-container callout-body">
<dl>
<dt>A Markov Decision Process is a Markov Reward Process with decisions.</dt>
<dd>
<p>a tuple <span class="math inline">⟨S, A, P, R, \gamma⟩</span></p>
</dd>
</dl>
<p>where:</p>
<ul>
<li><span class="math inline">S</span> is a finite set of states</li>
<li><span class="math inline">A</span> is a finite set of actions</li>
<li><span class="math inline">P</span> is a state transition probability matrix, <span class="math inline">P_{ss'}^a = \mathbb{P}[S_{t+1} = s' \vert S_t = s, A_t = a]</span></li>
<li><span class="math inline">R</span> is a reward function, <span class="math inline">R_s^a = \mathbb{E}[R_{t+1} \vert S_t = s, A_t = a]</span></li>
<li><span class="math inline">\gamma</span> is a discount factor, <span class="math inline">\gamma \in [0, 1]</span></li>
</ul>
</div>
</div>
<section id="sec-MDP-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="sec-MDP-dynamics">The dynamics of an MDP</h3>
<p>In a finite MDP, the sets of states, actions, and rewards (S, A and R) all have a finite number of elements. In this case, the random variables <span class="math inline">S_t</span> and <span class="math inline">R_t</span> have well defined discrete probability distributions dependent only on the preceding state and action.</p>
<p>The dynamics of an MDP are defined by the four argument dynamics function:</p>
<p><span id="eq-four-part-dynamics"><span class="math display">
p(s',r \vert s,a)\ \dot =\ Pr\{S_t = s', R_t = r \vert S_{t-1} = s, A_{t-1} = a\}\qquad  \forall s',s \in S\ \forall r\in R\ \forall a\in A
\tag{6}</span></span></p>
<p>where the sum of the probabilities over fixed set of s,a is 1:</p>
<p><span id="eq-dynamics-function-sum"><span class="math display">
\sum_{s' \in S} \sum_{r \in R} p(s',r \vert s,a) = 1 \qquad \forall s \in S, \forall a \in A \qquad \text{(Dynamics function)}
\tag{7}</span></span></p>
<p>This is just a regular function that takes a state and action and returns a probability distribution over the next state and reward.</p>
<p>In a tabular setting, we can also express this function as a table. Here is my solution for ex 3.4, a table for the recycling robot</p>
<div id="tbl-dynamics-function" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dynamics-function-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Dynamics function for a recycling robot MDP
</figcaption>
<div aria-describedby="tbl-dynamics-function-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">s</span></th>
<th><span class="math inline">a</span></th>
<th><span class="math inline">s'</span></th>
<th><span class="math inline">r</span></th>
<th><span class="math inline">p(s',r \mid s,a)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>high</td>
<td>search</td>
<td>high</td>
<td><span class="math inline">r_{search}</span></td>
<td><span class="math inline">\alpha</span></td>
</tr>
<tr class="even">
<td>high</td>
<td>search</td>
<td>low</td>
<td><span class="math inline">r_{search}</span></td>
<td><span class="math inline">1-\alpha</span></td>
</tr>
<tr class="odd">
<td>low</td>
<td>search</td>
<td>high</td>
<td>-3</td>
<td><span class="math inline">1-\beta</span></td>
</tr>
<tr class="even">
<td>low</td>
<td>search</td>
<td>low</td>
<td><span class="math inline">r_{search}</span></td>
<td><span class="math inline">\beta</span></td>
</tr>
<tr class="odd">
<td>high</td>
<td>wait</td>
<td>high</td>
<td><span class="math inline">r_{wait}</span></td>
<td>1</td>
</tr>
<tr class="even">
<td>low</td>
<td>wait</td>
<td>low</td>
<td><span class="math inline">r_{wait}</span></td>
<td>1</td>
</tr>
<tr class="odd">
<td>low</td>
<td>recharge</td>
<td>high</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>a couple of takeaways from this exercise are:</p>
<ol type="1">
<li><p>rewards are uniquely assigned for action at s resulting in s’ so we don’t need to make r another factor (i.e.&nbsp;list all possible rewards for (s,a,s’) tuple.</p></li>
<li><p>there are (s,a,s’,r) tuples for which we don’t have a non-zero probabilities - since there are no transition possible.</p>
<p>e.g.&nbsp;the robot wont charge when high, so there isn’t a transition from that state, nor a reward, nor a probability.</p></li>
</ol>
</section>
<section id="sec-MDP-graphical" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-MDP-graphical">Graphical representation of an MDP</h3>
<p>The graphical representation of an MDP is a directed graph where the nodes represent states and the edges represent actions. The graph is labeled with the probabilities of transitioning from one state to another given an action.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/rl-mdp-graph.png" class="img-fluid figure-img"></p>
<figcaption>graphical MDP for cleaning robot</figcaption>
</figure>
</div></div><p>In an MDP, the probabilities given by four part dynamics function completely characterize the environment’s dynamics.</p>
<p>That is, the probability of each possible value for <span class="math inline">S_{t+1}</span> and <span class="math inline">R_{t+1}</span> depends only on the immediately preceding state <span class="math inline">S_t</span> and action <span class="math inline">A_t</span>.</p>
<p>This is best viewed a restriction not on the decision process, but on the state.</p>
<p>The state must include information about all aspects of the past agent–environment interaction that make a difference for the future.</p>
<p>If it does, then the state is said to have the Markov property.</p>
<p>We can use the four-part dynamics function to compute the <strong>state transition probabilities</strong> for a given state and action:</p>
<p><span id="eq-transition-probabilities"><span class="math display">
\begin{align}
  p(s' \vert s,a) &amp;= \mathbb{P}[S_t = s'\vert S_{t-1} = s, A_{t-1} = a] \newline
  &amp;= \sum_{r \in R} p(s',r \vert s,a) \qquad \text{(state transition p)}
\end{align}
\tag{8}</span></span></p>
<p>where we summed over all possible rewards to get the state transition probability.</p>
<p>We can use the four-part dynamics function to compute the <strong>expected rewards</strong> for a given state and action:</p>
<p><span id="eq-expected-reward-for-state-action"><span class="math display">
\begin{align}
  r(s,a) &amp;= \mathbb{E}[R_t \vert S_{t-1} = s, A_{t-1} = a] \newline
  &amp;= \sum_{r \in R} r  \times \sum_{s' \in S}  p(s', r, \vert s, a)
\end{align}
\tag{9}</span></span></p>
<p>where we summed over all possible rewards and all successor state to get the expected reward.</p>
<p>We can also get the expected reward for a given state, action, and successor state using the four-part dynamics function:</p>
<p><span id="eq-expected-reward-for-state-action-successor-state"><span class="math display">
\begin{align}
  r(s,a,s') &amp;= \mathbb{E}[R_t \vert S_{t-1} = s, A_{t-1} = a, S_t = s'] \newline
  &amp;= \sum_{r \in R} r \times
  \frac {  p(s', r \vert s, a) } {  p(s' \vert s, a) }
\end{align}
\tag{10}</span></span></p>
<p>where we summed over all possible rewards to get the expected reward.</p>
<p>Note: perhaps implementing these in python might be further clarify the math for a given state transition graph.</p>
</section>
</section>
</section>
<section id="lesson-2-goal-of-reinforcement-learning" class="level1">
<h1>Lesson 2: Goal of Reinforcement Learning</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Describe how rewards relate to the goal of an agent. <a href="#l2g1">#</a></label></li>
<li><label><input type="checkbox" checked="">Understand episodes and identify episodic tasks. <a href="#l2g2">#</a></label></li>
</ul>
</div>
</div>
<section id="l2g1" class="level2">
<h2 class="anchored" data-anchor-id="l2g1">Rewards and the Goal of an Agent</h2>
<p>The agent interacts with the environment by taking actions and receiving rewards.</p>
<p>In the bandit setting, it is enough to maximize immediate rewards. In the MDP setting, the agent must consider the long-term consequences of its actions.</p>
<p>return <span class="math inline">G_t</span> is the total future reward from time-step <span class="math inline">t</span>.</p>
<p><span class="math display">
G_t \dot= R_{t+1} + R_{t+2} +  R_{t+3} + \ldots
</span></p>
<ul>
<li><span class="math inline">G_t</span> is a random variable because both transition and the rewards can be stochastic.</li>
</ul>
<p>The goal of an agent is to maximize the expected cumulative reward.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The reward hypothesis
</div>
</div>
<div class="callout-body-container callout-body">
<p>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p>
<p>see <span class="citation" data-cites="SILVER2021103535">[@SILVER2021103535]</span></p>
</div>
</div>
<p>some basic formulations of the goal of an agent:</p>
<ol type="1">
<li>Maze runner: -1 for each time step until the goal is reached, then 0.</li>
<li>Recycling robot: +1 per can, 0 otherwise.</li>
<li>Chess: 1 for a win, 0 for a draw, -1 for a loss.</li>
</ol>
<p>Note there is more material on this subject in the guest lecture by Michael Littman.</p>
</section>
<section id="l2g2" class="level2">
<h2 class="anchored" data-anchor-id="l2g2">Episodes and Episodic Tasks</h2>
<p>An episode is a sequence of states, actions, and rewards that ends in a terminal state. An episodic task is a task with a well-defined terminal state.</p>
<p>An example of an episodic task is a game of chess, where the game ends when one player wins or the game is a draw. The opposite setting of an episodic task is a continuing task, where the agent interacts with the environment indefinitely.</p>
</section>
<section id="guest-lecture-with-michael-littman-on-the-reward-hypothesis" class="level2">
<h2 class="anchored" data-anchor-id="guest-lecture-with-michael-littman-on-the-reward-hypothesis">Guest Lecture with Michael Littman on The Reward Hypothesis</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Michael Littman
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://www.littmania.com/media#h.p_pIcJ-rzB3Cp6">His website</a></li>
<li></li>
</ul>
<p>Littman is a professor at Brown University and a leading researcher in reinforcement learning. He is known for his work on the reward hypothesis and the exploration-exploitation trade-off. He motivates the reward hypothesis with a humorous take on the old adage:</p>
<ul>
<li>Give a man a fish and he’ll eat for a day - traditional programming</li>
<li>Teach a man to fish and he’ll eat for a lifetime - supervised learning</li>
<li>Give a man a need for fish and he’ll figure it out - reinforcement learning</li>
</ul>
<p>I felt that the guest lecture was a bit of a let down. I was expecting more from a leading researcher in the field. The reward hypothesis is a fundamental concept and the lecture seemed all over the place. It raised many questions but didn’t answer them.</p>
<p>If we accept the hypothesis, then there are two areas need to be addressed:</p>
<ol type="1">
<li>What rewards should agents optimize?</li>
<li>Designing algorithms to maximize them.</li>
</ol>
<p>Some rewards are easy to define, like winning a game, but others are more complex, like driving a car. His example was of air conditioning in a car, where the reward is not just the temperature but also the comfort of the passengers. Running the air conditioning has a cost in terms of fuel, but the comfort of the passengers is much harder to quantify, particularly since each passenger may have different preferences.</p>
<p>Next he covered the two main approaches to setting up rewards in RL:</p>
<ul>
<li>Rewards can be expressed as a final goal, or no goal yet:
<ul>
<li>The goal based representation: Goal achieved = +1, and everything else is 0. This has a downside of not signaling, to the agent, the urgency of getting to the goal.</li>
<li>The action-penalty representation: a -1 could be awarded every step that the goal is not yet achieved. This can cause problems if there is a small probability of getting stuck and never reaching the goal.</li>
<li>It seems that there are many ways to set up rewards. If we take a lesson from game theory, we can see that the value of rewards might be important or the relative value or order of rewards might be important. In rl values are often more important than the ‘order’ of rewards. However it might be interesting to consider if we can encode preferences into the rewards and if this formulation would still make sense in the context of the reward hypothesis and the bellman equations.</li>
</ul></li>
</ul>
<p>Littleman then asked “Where do rewards come from?” and answered that they can come from - Programming - Human feedback - Examples - Mimic the rewards a human would give - Inverse reinforcement learning - learn the reward function from examples - Optimization - Evolutionary optimization like population dynamics. - The reward is the objective function - The reward is the gradient of the objective function</p>
<p>Next he discusses some challenges to the reward hypothesis:</p>
<ul>
<li>Target is something other than cumulative reward:
<ul>
<li>cannot capture risk averse behavior</li>
<li>cannot capture diversity of behavior</li>
</ul></li>
<li>is it a good match for a high level human behavior?
<ul>
<li>single minded pursuit of a goal isn’t characteristic of good people.</li>
<li>The goals we “should” be pursuing may not be immediately evident to us - we might need time to understand making good decisions.</li>
</ul></li>
</ul>
<p>The big elephant in the room is that we can reject the reward hypothesis and have agents that peruse multiple goals. The main challenge is that it becomes harder to decide when there is a conflict between goals. However, the field of multi-objective optimization has been around for a long time and there are many ways to deal with this problem. Some are more similar to the reward hypothesis but others can lead to more complex behavior based on preferences and pareto optimality.</p>
<section id="lesson-3-continuing-tasks" class="level1">
<h1>Lesson 3: Continuing Tasks</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Formulate returns for continuing tasks using discounting. <a href="#l3g1">#</a></label></li>
<li><label><input type="checkbox" checked="">Describe how returns at successive time steps are related to each other. <a href="#l3g2">#</a></label></li>
<li><label><input type="checkbox" checked="">Understand when to formalize a task as episodic or continuing. <a href="#l3g3">#</a></label></li>
</ul>
</div>
</div>
<section id="l3g1" class="level2">
<h2 class="anchored" data-anchor-id="l3g1">Returns for Continuing Tasks</h2>
<ul>
<li>In continuing tasks, the agent interacts with the environment indefinitely.</li>
<li>The return at time <span class="math inline">t</span> is the sum of the rewards from time <span class="math inline">t</span> to the end of the episode.</li>
<li>The return can be formulated using discounting, where the rewards are discounted by a factor <span class="math inline">\gamma</span>.</li>
</ul>
<p><span id="eq-discounted-return-continuing"><span class="math display">
\begin{align}
G_t&amp;=R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \newline
&amp;= \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{align}
\tag{11}</span></span></p>
<ul>
<li>returns have a recursive structure, where the return at time <span class="math inline">t</span> is related to the return at time <span class="math inline">t+1</span> by the discount factor <span class="math inline">\gamma</span>.</li>
</ul>
<p><span id="eq-return-recursive"><span class="math display">  
\begin{align}
G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \newline
&amp;= R_{t+1} + \gamma ( R_{t+2} + \gamma R_{t+3} + \ldots ) \newline
&amp;= R_{t+1} + \gamma G_{t+1}
\end{align}
\tag{12}</span></span></p>
<p>this form of the return is called the recursive form of the return and is usefull in developing algorithms for reinforcement learning.</p>
</section>
<section id="l3g2" class="level2">
<h2 class="anchored" data-anchor-id="l3g2">Returns at Successive Time Steps</h2>
<ul>
<li>The return at time <span class="math inline">t</span> is related to the return at time <span class="math inline">t+1</span> by the discount factor <span class="math inline">\gamma</span>.</li>
<li>The return at time <span class="math inline">t</span> is the sum of the reward at time <span class="math inline">t</span> and the discounted return at time $t+1.</li>
</ul>
</section>
<section id="l3g3" class="level2">
<h2 class="anchored" data-anchor-id="l3g3">Episodic vs.&nbsp;Continuing Tasks</h2>
<ul>
<li>An episodic task has a well-defined terminal state, and the episode ends when the terminal state is reached.</li>
<li>A continuing task does not have a terminal state, and the agent interacts with the environment indefinitely.</li>
<li>To avoid infinite returns in continuing tasks, we use discounting to ensure that the return is finite.</li>
<li>The discount factor <span class="math inline">\gamma\in(0,1)</span> is the present value of future rewards.</li>
</ul>
<p><span class="citation" data-cites="sutton2018reinforcement">@sutton2018reinforcement</span> emphasizes that we can use the discount factor of [0,1] to unify both episodic and continuing tasks Here = 0 corresponds to myopic view of optimizing immediate rewards like in the k-armed bandit problem. The discount factor = 1 corresponds to the long-term view of optimizing undiscounted expected cumulative reward. into a single framework. This is a powerful idea that allows us to use the same algorithms for both types of tasks.</p>
<p>I think it is a good place to consider a couple of ideas raised by Littman in the guest lecture:</p>
<p>The first is hyperbolic discounting and the second risk aversion.</p>
<p>Behavioral economics has considered a notion of hyperbolic discounting where the discount factor is not constant but changes over time. This is a more realistic model of human behavior but is harder to work with mathematically. This idea is not covered in the course perhaps because it is a departure from the more rational exponential discounting model which we use.</p>
<p>two forms of hyperbolic discounting are:</p>
<p><span class="math display">
G(D) = \frac{1}{1 + \gamma D}
</span></p>
<p><span id="eq-hyperbolic-discounting"><span class="math display">
φh(τ) = (1+ ατ)−γ/α
\tag{13}</span></span> where:</p>
<ul>
<li><span class="math inline">φh(τ)</span> is the hyperbolic discount factor at time <span class="math inline">τ</span></li>
<li><span class="math inline">α</span> is the rate of discounting</li>
<li><span class="math inline">γ</span> is the delay parameter</li>
</ul>
<p>there is also quasi-hyperbolic discounting which is a combination of exponential and hyperbolic discounting.</p>
<p><span class="math display">
G(D) = \begin{cases}
  1 &amp; \text{if } t = 0  \newline
  \beta^k \delta^{D} &amp; \text{if } t &gt; 0
  \end{cases}
</span></p>
<p>The notation for both terminal and non-terminal states is <span class="math inline">S^+</span></p>
<p>Exercise 2.10 proof of Equation 3.10</p>
<p><span class="math display">
\begin{align*}
G_t &amp;= \sum_{k=0}^\infty \gamma^k = lim_{n \rightarrow \infty} (1 + \gamma + \gamma^2 + ... + \gamma^n) \newline
&amp;= lim_{n \rightarrow \infty} \frac{(1 + \gamma + \gamma^2 + ... + \gamma^n) (1 - \gamma)}{(1 - \gamma)} \newline
&amp;= lim_{n \rightarrow \infty} \frac{1 - \gamma^{n+1}}{1 - \gamma} \newline
&amp;= \frac{1}{1 - \gamma}
\end{align*}
</span></p>


</section>
</section>
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/orenbochman\.github\.io\/notes-rl\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>