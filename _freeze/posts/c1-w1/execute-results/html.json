{
  "hash": "940654bd750d0cdd34c3233785f71f85",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2022-05-02\nlastmod: 2024-05-07\ntitle: The K-Armed Bandit Problem\nsubtitle: RL Fundamentals\ndescription: In week 1 we define some key concepts like rewards, states, action, value functions, action values functions. We consider the the multi-armed bandit problem, leading to exploration explotation dillema, and the epsilon greedy algorithm.\nauthor: Oren Bochman\ncategories:\n  - Coursera\n  - Fundamentals\n  - Reinforcement Learning\nkeywords:\n  - the k-armed bandit problem\n  - bandit algorithms\n  - exploration \n  - explotation\n  - epsilon greedy algorithm\n  - sample avarage method\njupyter: python3\nimage: img/nlp-brain-wordcloud.jpg\ntitle-block-banner: /images/banner_black_3.jpg\n---\n\n\n\n\n![RL logo](img/logo.png){.column-margin} \n\n![RL algorithms](img/alg_selector.png){.column-margin group=\"slides\"}\n\n# Lesson 1: The K-Armed Bandit {#sec-lesson-k-armed-bandit}\n\n::: {.callout-tip collapse=\"true\"}\n### Read {.unnumbered}\n\n-   [x] [\\@sutton2018reinforcement¬ß2.1-7, pp. 25-36](http://incompleteideas.net/book/RLbook2020.pdf#page=47)\n-   [x] [\\@sutton2018reinforcement¬ß2.8, pp. 42-43](http://incompleteideas.net/book/RLbook2020.pdf#page=47)\n:::\n\n::: callout-note\n### Goals {.unnumbered}\n\n-   [x] Understand the temporal nature of the bandit problem [\\#](#sec-k-armed-bandit)\n-   [x] Define k-armed bandit problem [\\#](#l1g2)\n-   [x] Define action-values and the greedy action selection method [\\#](#sec-l1g3)\n-   [x] Define reward, time steps, and value functions [\\#](#l1g4)\n:::\n\n> In reinforcement learning, the agent generates its own training data by interacting with the world. The agent must learn the consequences of his own actions through trial and error, rather than being told the correct action -- [@white2020fundamental]\n\n## K-armed bandits üêô {#sec-k-armed-bandit}\n\nIn the **k-armed bandit** problem there is an **agent** who is assigned a **state** $s$ by the environment and must learn which action $a$ from the possible set of **actions** $A$ leads to the goal state through a signal based on the greatest **expected reward**.\n\nOne way this can be achieved is using a Bayesian updating scheme starting from a uniform prior.\n\n## Temporal nature of the bandit problem {#sec-l1g1}\n\nThe **bandit problem** cam be static problem with a fixed reward distribution. However, more generally it is a **temporal** problem when the rewards distribution changes over time and agent must learn to adapt to these changes.\n\n::: callout-note\n## Difference between bandits and RL\n\nIn the typical **bandit setting** there is only one state. So after we pull the arm nothing in the problem changes.\n\nBandits problems where agents can discriminate between states are called *contextual bandits.*\n\nHowever, bandits embody one of the main themes of RL - that of estimating an expected reward for different actions.\n\nIn the more general **RL setting** we will be interested in more general problems where actions will lead the agent to new states and the goal is some specific state we need to reach.\n:::\n\n![bandit](img/multi_armed_bandit.webm){.column-margin}\n\n::: {#exm-clinical-trials}\n## Using Multi-armed bandit to randomize a medical trial\n\n-   agent is the doctor\n-   actions {blue, yellow, red} treatment\n-   k = 3\n-   the rewards are the health of the patients' blood pressure.\n-   a random trial in which a doctor need to pick one of three treatments.\n-   q(a) is the mean of the blood pressure for the patient.\n:::\n\n![clinical trial](img/rl-clinical-trial.png){.column-margin}\n\n#### Action Values and Greedy Action Selection {#sec-l1g3}\n\nThe **value** of an action is its **expected reward** which can be expressed mathematically as:\n\n$$\n\\begin{align}\nq_{\\star}(a) & \\doteq \\mathbb{E}[R_t  \\vert  A_t=a] \\space \\forall a \\in \\{a_1 ... a_k\\} \\newline \n             & = \\sum_r p(r|a)r \\qquad \\text{(action value)}\n\\end{align}\n$$ {#eq-action-value}\n\nwhere:\n\n-   $\\doteq$ means definition\n-   $\\mathbb{E}[r \\vert a]$ means expectation of a reward given some action a Since agents want to maximize rewards, recalling the definition of expectations we can write this as:\n\nThe goal of the agent is to maximize the expected reward which we can express mathematically as:\n\n$$\n \\arg\\max_a q(a)=\\sum_r p(r \\vert a) \\times r \\qquad \\text{(Greedification)}\n$$ {#eq-greedification}\n\nwhere:\n\n-   $\\arg \\max_a$ means the argument $a$ maximizes - so the agent is looking for the action that maximizes the expected reward and the outcome is an action.\n\n#### Reward, Return, and Value Functions {#l1g4}\n\nThe **reward** $r$ is the immediate feedback from the environment after the agent takes an action.\n\nThe **return** $G_t$ is the total discounted reward from time-step $t$.\n\nThe **value function** $v(s)$ of an MRP is the expected return starting from state $s$.\n\n![decisions](img/rl-descion-problems.png){.column-margin}\n\nexample of decisions under uncertainty:\n\n-   movie recommendation.\n-   clinical trials.\n-   music recommendation.\n-   food ordering at a restaurant.\n\n![why discuss bandits](img/rl-why-bandits.png){.column-margin}\n\nIt best to consider issues and algorithms design choices in the simplest setting first. The bandit problem is the simplest setting for RL. More advanced algorithms will incorporate parts we use to solve this simple settings.\n\n-   maximizing rewards.\n-   balancing exploration and exploitation.\n-   estimating expected rewards for different actions.\n\nare all problems we will encounter in both the bandit and the more general RL setting.\n\n# Lesson 2: What to learn: understanding Action Values {#sec-lesson-action-values}\n\n::: callout-note\n### Goals\n\n1.  [x] Define action-value estimation methods. [\\#](#L2G1)\n2.  [x] Define exploration and exploitation [\\#](#L2G2)\n3.  [x] Select actions greedily using an action-value function [\\#](#L2G3)\n4.  [x] Define online learning [\\#](#L2G4)\n5.  [x] Understand a simple online sample-average action-value estimation method [\\#](#L2G5)\n6.  [x] Define the general online update equation [\\#](#L2G6)\n:::\n\n### What are action-value estimation methods? {#L2G1}\n\n![estimating action values](img/rl-clinical-trial-q(a).png){.column-margin}\n\nIn Tabular RL settings The action value function $q$ is nothing more than a table with one {state, action} pair per row and its value. More generally, like when we will consider function approximation in course 3, it is a mapping from {state, action} pair to a expected reward.\n\n| State s | Action a         | Action value q(s,a) |\n|---------|------------------|---------------------|\n| 0       | red treatment    | 0.25                |\n| 0       | yellow treatment | 0.75                |\n| 0       | blue treatment   | 0.5                 |\n\nThe higher the action value $q(a)$ of an action **a**, the more likely it is to lead us to a better state which is closer to the objective. We can choose for each state the best or one of the best choices giving us a **plan** for navigating the state space to the goal state.\n\n$$\nQ_t(a) \\doteq \\frac{\\text{sum of rewards for action a taken time } t}{\\text{number of times action a was taken prior to } t} = \\frac{\\sum_{i=1}^{t-1} R_i}{t-1} \\qquad\n$$ {#eq-sample-average}\n\nThe main idea of RL is that we can propagate values from an one adjacent state to another. We can start with the uniform stochastic policy and use it to estimate/learn the action values. Action values will decrease for actions leads to a dead end. And it will increase in the direction of the goal but only once the influence of the goal has propagated. A continuing theme in RL is trying to increase the efficiency for propagation of rewards across the action values.\n\nKnowing the minimum number of action needed to reach a goal can be an approximate indicator of the action value.\n\nA second idea is that once we have let the influence of dead end and the goals spread enough we may have enough information to improve the initial action value to a point where each action is the one of the best choices. [We call picking the one of the best action greedy selection and it leads to a deterministic policy.]{.mark} This is the optimal policy, it might not be unique since some actions might be tied in terms of their rewards. However for all of these we cannot do any better.\n\n### Exploration and Exploitation definition and dilemma {#L2G2}\n\nIn the bandit setting we can define:\n\nExploration\n\n:   Testing any action that might be better than our best.\n\nExploitation\n\n:   Using the best action.\n\nShould the doctor explore new treatments that might harm his patients or exploit the current treatment. In real life bacteria gain immunity to antibiotics so there is merit to exploring new treatments. However, a new treatment can be harmful to some patients. Ideally we want to enjoy the benefits of the best treatment but to be open to new and better alternatives but we can only do one at a time.\n\n[Since exploitation is by definition mutually exclusive with exploration we must choose one and give up the benefits of the other. This is the **dilemma of Exploration and Exploitation**.]{.marked} How an agent resolves this dilemma in practice depends on the agent's preferences and the type of state space it inhabits, if it has just started or encounters a **changing landscape,** it should make an effort to explore, on the other hand if it has explored enough to be certain of a global maximum it would prefer to exploit.\n\n### Defining Online learning ? {#L2G4}\n\nOnline learning\n\n:   learning by updating the agent's value function or the action value function step by step as an agent transverses the states seeking the goal. Online learning is important to handle MDP which can change.\n\nOne simple way an agent can use online learning is to try actions by random and keep track of the subsequent states. Eventually we should reach the goal state. If we repeat this many times we can estimate the expected rewards for each action.\n\n### Sample Average Method for estimating Action Values Incrementally {#L2G5}\n\nAction values help us make decision. Let's try and make estimate action values more formal using the following method:\n\n$$\n q_t(a)=\\frac{\\text{sum or rewards when a taken prior to t}}{\\text{number of times a taken prior to t}}\n       =\\frac{\\sum_{t=1}^{t-1} R_i \\mathbb{I}_{A_i=a}}{\\sum_{t=1}^{t-1}\\mathbb{I}_{A_i=a} } \\qquad\n$$\n\n![example](img/rl-sample-avarage-method.png){.column-margin}\n\n$$\n\\begin{align}\nQ_{n+1} &= \\frac{1}{n} \\sum_{i=1}^n R_i \\newline\n  & = \\frac{1}{n} \\Bigg(R_n + \\sum_i^{n-1} R_i\\Bigg) \\newline\n  & = \\frac{1}{n} \\Bigg(R_n + (n-1) \\frac{1}{(n-1)}\\sum_i^{n-1} R_i\\Bigg) \\newline\n  &= \\frac{1}{n} \\Big(R_n + (n-1) Q_{n}\\Big) \\newline\n  &= \\frac{1}{n} \\Big(R_n + nQ_{n} -Q_{n} \\Big) \\newline\n  &= Q_n + \\frac{1}{n} \\Big[R_n - Q_{n}\\Big]\n\\end{align}\n$$ {#eq-sample-average-incremental-update-rule}\n\n### What are action-value estimation methods? {#L2G6}\n\nWe can now state this in English as:\n\n$$\n\\text{New Estimate} \\leftarrow \\text{Old Estimate} + \\text{Step Size } \\times [\\text{Target} - \\text{Old Estimate}] \\qquad\n$$\n\nhere:\n\n-   step size can be adaptive - changing over time. but typically it is constant and in the range (0,1) to avoid divergence.\n-   for the sample average method the step size is $\\frac{1}{n}$ where n is the number of times the action has been taken.\n-   (Target - OldEstimate) is called the *error*.\n\nMore generally we will use the update rule as:\n\n$$\nQ_{n+1} = Q_n + \\alpha \\Big[R_n - Q_{n}\\Big] \\qquad a\\in (0,1)\n$$ {#eq-general-incremental-update-rule}\n\n``` pseudocode\n#| label: simple-epsilon-greedy-bandit-algorithm\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{Simple Bandit($\\epsilon$)}\n\\begin{algorithmic}[1]\n\\State $Q(a) \\leftarrow 0\\ \\forall a\\ $ \\Comment{ $\\textcolor{blue}{initialize\\ action\\ values}$}\n\\State $N(a) \\leftarrow 0\\ \\forall a\\ $ \\Comment{ $\\textcolor{blue}{initialize\\ counter\\ for\\ actions\\ taken}$}\n\\For{$t = 1, 2, \\ldots \\infty$}\n  \\State  $A_t \\leftarrow \\begin{cases}\n    \\arg\\max_a Q(a) & \\text{with probability } 1 - \\epsilon \\\\\n    \\text{a random action} & \\text{with probability } \\epsilon\n    \\end{cases}$\n  \\State $R_t \\leftarrow \\text{Bandit}(A_t)$\n  \\State $N(A_t) \\leftarrow N(A_t) + 1$\n  \\State $Q(A_t) \\leftarrow Q(A_t) + \\frac{1}{N(A_t)}[R_t - Q(A_t)]$\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n# Lesson 3: Exploration vs Exploitation {#sec-lesson-exploration-exploitation}\n\n::: callout-note\n### Goals\n\n-   Define $\\epsilon$-greedy [\\#](#sec-epsilon-greedy-policies)\n-   Compare the short-term benefits of exploitation and the long-term benefits of exploration [\\#](#sec-benefits-of-exploitation-and-exploration)\n-   Understand optimistic initial values [\\#](#sec-optimistic-initial-values)\n-   Describe the benefits of optimistic initial values for early exploration [\\#](#sec-benefits-of-optimistic-initial-values-for-early-exploration)\n-   Explain the criticisms of optimistic initial values [\\#](#sec-criticisms-of-optimistic-initial-values)\n-   Describe the upper confidence bound action selection method [\\#](#L3G6)\n-   Define optimism in the face of uncertainty [\\#](#L3G7)\n:::\n\nthe following is a Bernoulli greedy algorithm\n\n``` pseudocode\n#| label: alg-greedy-bandit\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{BernGreedy(K, Œ±, Œ≤)}\n\\begin{algorithmic}[1]\n\\For{$t = 1, 2, . . .$}\n\\State\n\\State \\Comment{ estimate model}\n\\For{$k = 1, . . . , K$}\n\\State $\\hat\\theta_k \\leftarrow  a_k / (Œ±_k + Œ≤_k)$\n\\EndFor\n\\State \\Comment{ select and apply action:}\n\\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$\n\\State Apply $x_t$ and observe $r_t$\n\\State \\Comment{ update distribution:}\n\\State $(Œ±_{x_t}, Œ≤_{x_t}) \\leftarrow (Œ±_{x_t} + r_t, Œ≤_{x_t} + 1 ‚àí r_t)$\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n## ∆ê-Greedy Policies {#sec-epsilon-greedy-policies}\n\nThe ∆ê-greedy policy uses a simple heuristic to balance exploration with exploitation. The idea is to choose the best action with probability $1-\\epsilon$ and to choose a random action with probability $\\epsilon$.\n\n``` pseudocode\n#| label: alg-epsilon-greedy\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{EpsilonGreedy(K, Œ±, Œ≤)}\n\\begin{algorithmic}[1]\n\\For{$t = 1, 2, \\ldots $}\n\\State p = random()\n  \\If {$p < \\epsilon$}\n    \\State select radom action $x_t \\qquad$ \\Comment{explore}\n  \\Else\n    \\State select $x_t = \\arg\\max_k \\hat{\\theta}_k \\qquad$  \\Comment{exploit}\n  \\EndIf\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n::: callout-caution\n### The problem with ∆ê-greedy policies\n\n-   A problem with ∆ê-greedy is that it is not optimal in the long run.\n-   Even after it has found the best course of action it will continue to explore with probability $\\epsilon$.\n-   This is because the policy is not adaptive.\n-   One method is too reduce $\\epsilon$ over time. However unless there is a feedback from the environment this will likely stop exploring too soon or too late thus providing sub-optimal returns.\n:::\n\nThe following is a simple implementation of the ∆ê-greedy algorithm in Python from [geeksforgeeks.org](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/?ref=ml_lbp)\n\n::: {#22df15a2 .cell execution_count=1}\n``` {.python .cell-code}\n# Import required libraries \nimport numpy as np \nimport matplotlib.pyplot as plt \n  \n# Define Action class \nclass Actions: \n  def __init__(self, m): \n    self.m = m \n    self.mean = 0\n    self.N = 0\n  \n  # Choose a random action \n  def choose(self):  \n    return np.random.randn() + self.m \n  \n  # Update the action-value estimate \n  def update(self, x): \n    self.N += 1\n    self.mean = (1 - 1.0 / self.N)*self.mean + 1.0 / self.N * x \n  \n  \ndef run_experiment(m1, m2, m3, eps, N): \n      \n  actions = [Actions(m1), Actions(m2), Actions(m3)] \n  \n  data = np.empty(N) \n    \n  for i in range(N): \n    # epsilon greedy \n    p = np.random.random() \n    if p < eps: \n      j = np.random.choice(3) \n    else: \n      j = np.argmax([a.mean for a in actions]) \n    x = actions[j].choose() \n    actions[j].update(x) \n  \n    # for the plot \n    data[i] = x \n  cumulative_average = np.cumsum(data) / (np.arange(N) + 1) \n  \n  # plot moving average ctr \n  plt.plot(cumulative_average) \n  plt.plot(np.ones(N)*m1) \n  plt.plot(np.ones(N)*m2) \n  plt.plot(np.ones(N)*m3) \n  plt.xscale('log') \n  plt.show() \n  \n  for a in actions: \n    print(a.mean) \n  \n  return cumulative_average \n```\n:::\n\n\n::: {#1a5ed31a .cell execution_count=2}\n``` {.python .cell-code}\nc_1 = run_experiment(1.0, 2.0, 3.0, 0.1, 100000) \n#print(c_1)\n```\n\n::: {.cell-output .cell-output-display}\n![](c1-w1_files/figure-html/cell-3-output-1.png){width=571 height=412}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9986145710058402\n1.9658660479954855\n3.0003123552514848\n```\n:::\n:::\n\n\n::: {#a79852b6 .cell execution_count=3}\n``` {.python .cell-code}\nc_05 = run_experiment(1.0, 2.0, 3.0, 0.05, 100000) \n#print(c_05)\n```\n\n::: {.cell-output .cell-output-display}\n![](c1-w1_files/figure-html/cell-4-output-1.png){width=569 height=412}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9953708104175287\n2.0145256629031327\n3.001903056693574\n```\n:::\n:::\n\n\n::: {#3c6823a4 .cell execution_count=4}\n``` {.python .cell-code}\nc_01 = run_experiment(1.0, 2.0, 3.0, 0.01, 100000) \n#print(c_01)\n```\n\n::: {.cell-output .cell-output-display}\n![](c1-w1_files/figure-html/cell-5-output-1.png){width=579 height=412}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1.036959235212011\n1.9936684441473502\n2.996446749853475\n```\n:::\n:::\n\n\n::: {#70983abc .cell execution_count=5}\n``` {.python .cell-code}\n# log scale plot \nplt.plot(c_1, label ='eps = 0.1') \nplt.plot(c_05, label ='eps = 0.05') \nplt.plot(c_01, label ='eps = 0.01') \nplt.legend() \nplt.xscale('log') \nplt.show() \n```\n\n::: {.cell-output .cell-output-display}\n![](c1-w1_files/figure-html/cell-6-output-1.png){width=569 height=412}\n:::\n:::\n\n\n## Benefits of Exploitation & Exploration {#sec-benefits-of-exploitation-and-exploration}\n\n-   In the short term we may maximize rewards following the best known course of action. However this may represent a local maximum.\n-   In the long term agents that explore different options and keep uncovering better options until they find the best course of action corresponding to the global maximum.\n\nTo get the best of both worlds we need to balance exploration and exploitation ideally using a policy that uses feedback to adapt to its environment.\n\n## Optimistic initial values {#sec-optimistic-initial-values}\n\nOptimistic initial values\n\n:   Setting all initially action values greater than the algorithmically available values in \\[0,1\\]\n\nThe methods we have discussed are dependent on the initial action-value estimates, $Q_1(a)$. In the language of statistics, we call these methods biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once. For methods with constant $\\alpha$, the bias is permanent, though decreasing over time.\n\n``` pseudocode\n#| label: alg-optimitc-greedy-bandit\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{OptimisticBernGreedy(K, Œ±, Œ≤)}\n\\begin{algorithmic}[1]\n\\For{$t = 1, 2, . . .$}\n\\State\n\\State \\Comment{ estimate model}\n\\For{$k = 1, . . . , K$}\n\\State $\\hat\\theta_k \\leftarrow  1 \\qquad$ \\Comment{optimistic initial value}\n\\EndFor\n\\State \\Comment{ select and apply action:}\n\\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$\n\\State Apply $x_t$ and observe $r_t$\n\\State \\Comment{ update distribution:}\n\\State $(Œ±_{x_t}, Œ≤_{x_t}) \\leftarrow (Œ±_{x_t} + r_t, Œ≤_{x_t} + 1 ‚àí r_t)$\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n## Benefits of optimistic initial values for early exploration {#sec-benefits-of-optimistic-initial-values-for-early-exploration}\n\nSetting the initial action values to be higher than the true values has the effect of causing various bandit algorithm to try to exploit them - only to find out that most values are not as rewarding as it was led to expect.\n\nWhat happens is that the algorithm will initially explore more than it would have otherwise. Possibly even trying all the actions at least once.\n\nIn the short-term it will perform worse than ∆ê- greedy which tend to exploit. But as more of the state space is explored at least once the algorithm will beat an ∆ê-greedy policy which can take far longer to explore the space and find the optimal options.\n\n![The effect of optimistic initial action-value estimates](img/rl-optimistic-initial-conditions.png)\n\n::: callout-caution\n### Criticisms of optimistic initial values {#sec-criticisms-of-optimistic-initial-values}\n\n-   Optimistic initial values only drive early exploration. The agent will stop exploring once this is done.\n-   For a non-stationary problems - this is inadequate.\n-   In a real world problems the maximum reward is an unknown quantity.\n:::\n\n## The UCB action selection method {#sec-the-ucb-action-selection-method}\n\nUCB is an acronym for Upper Confidence Bound. The idea behind it is to select the action that has the highest upper confidence bound. This has the advantage over epsilon greedy that it will explore more in the beginning and then exploit more as the algorithm progresses.\n\nthe upper confidence bound is defined as:\n\n$$\nA_t = \\arg\\max\\_a \\Bigg[\n  \\underbrace{Q_t(a)}_{exploitation} + \n  \\underbrace{c \\sqrt{\\frac{\\ln t}{N_t(a)} }}_{exploration}\n\\Bigg] \\qquad\n$$ {#eq-ucb}\n\nwhere:\n\n-   $Q_t(a)$ is the action value\n-   $c$ is a constant that determines the degree of exploration\n-   $N_t(a)$ is the number of times action $a$ has been selected prior to time $t$\n\n![UCB intuition](img/rl_wk1_ucb.png){.column-margin}\n\nThe idea is we the action for which the action value plus the highest possible uncertainty give the highest sum. We are being optimistic in assuming this choice will give the highest reward. In reality any value in the confidence interval could be the true value. Each time we select an action we reduce the uncertainty in the exploration term and we also temper our optimism of the upper confidence bound by the number of times we have selected the action. This means that we will prefer to visit the actions that have not been visited as often.\n\nThe main advantage of UCB is that it is more efficient than epsilon greedy in the long run. If we measure the cost of learning in terms of the regret - the difference between the expected reward of the optimal action and the expected reward of the action we choose. UCB has a lower regret than epsilon greedy. The downside is that it is more complex and requires more computation.\n\n``` pseudocode\n#| label: alg-brn-UCB\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{UCB(K, Œ±, Œ≤)}\n\\begin{algorithmic}[1]\n\\For{$t = 1, 2, . . .$}\n  \\For { $k = 1, . . . , K$ }\n    \\State \\Comment{ $\\textcolor{blue}{compute\\ UCBs}$}\n    \\State $U_k = \\hat\\theta_k + c \\sqrt{\\frac{\\ln t}{N_k}}$\n  \\EndFor\n\\State \\Comment{ $\\textcolor{blue}{select\\ and\\ apply\\ action}$}\n\\State $x_t \\leftarrow \\arg\\max_k h(x,U_x)$\n\\State Apply xt and observe $y_t$ and $r_t$\n\\State \\Comment{ $\\textcolor{blue}{estimate\\ model}$}\n\\For{$k = 1, . . . , K$}\n\\State $\\hat\\theta_k \\leftarrow  a_k / (Œ±_k + Œ≤_k)$\n\\EndFor\n\\State \\Comment{ select and apply action:}\n\\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$\n\\State Apply $x_t$ and observe $r_t$\n\\State \\Comment{ update distribution:}\n\\State $(Œ±_{x_t}, Œ≤_{x_t}) \\leftarrow (Œ±_{x_t} + r_t, Œ≤_{x_t} + 1 ‚àí r_t)$\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n```\n\nNote we can model UCB using an urn model.\n\n## Thompson Sampling {#Sec-Thompson-Sampling}\n\nThompson sampling is basically like UCB but taking the Bayesian approach to the bandit problem. We start with a prior distribution over the action values and then update this distribution as we take actions. The action we choose is then sampled from the posterior distribution. This has the advantage that it is more robust to non-stationary problems than UCB. The downside is that it is more computationally expensive.\n\n### Thompson Sampling Algorithm {#Sec-Thompson-Sampling-Algorithm}\n\nThe algorithm is as follows:\n\n``` pseudocode\n#| label: alg-bernoulli-thompson-sampling\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{BernTS(K, Œ±, Œ≤)}\n\\begin{algorithmic}[1]\n\\For{$t = 1, 2, . . .$}\n\\State\n\\State \\Comment{ sample model}\n\\For{$k = 1, . . . , K$}\n\\State Sample $\\hat\\theta_k \\sim beta(Œ±_k, Œ≤_k)$\n\\EndFor\n\\State \\Comment{ select and apply action:}\n\\State $x_t \\leftarrow \\arg\\max_k \\hat{\\theta}_k$\n\\State Apply $x_t$ and observe $r_t$\n\\State \\Comment{ update distribution:}\n\\State $(Œ±_{x_t}, Œ≤_{x_t}) \\leftarrow (Œ±_{x_t} + r_t, Œ≤_{x_t} + 1 ‚àí r_t)$\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n-   [this is a tutorial on Thompson Sampling](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf)\n\n## Optimism in the face of uncertainty {#L3G7}\n\nOptimism in the face of uncertainty\n\n:   This is a heuristic to ensure initial exploration of all actions by assuming that untried actions have a high expected reward. We then try to exploit them but end up successively downgrading their expected reward when they do not match our initial optimistic assessment.\n\nThe downside to this approach is when the space of action is continuous so we can never get to the benefits of exploration.\n\n# Awesome RL resources\n\nLet's list some useful RL resources.\n\n**Books**\n\n-   Richard S. Sutton & Andrew G. Barto [RL An Introduction](http://incompleteideas.net/book/RLbook2020.pdf)\n-   [Tor Latimore's](https://tor-lattimore.com/) [Book](https://tor-lattimore.com/downloads/book/book.pdf) and [Blog](https://banditalgs.com/) on Bandit Algorithms.\n-   [Csaba Szepesvari](https://sites.ualberta.ca/~szepesva/)'s [Book](https://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)\n\n**Courses & Tutorials**\n\n-   [David Silver's](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html) 2015 [UCL Course on RL](https://www.davidsilver.uk/teaching/) [Video](https://www.youtube.com/watch?v=2pWv7GOvuf0) and Slides.\n-   [Charles Isbell](https://faculty.cc.gatech.edu/~isbell/pubs/) and [Michael Littman](https://www.littmania.com/) A free Udacity course on RL, with some emphasis on game theory proofs, and some novel algorithms like [Coco-Q: Learning in Stochastic Games with Side Payments](http://proceedings.mlr.press/v28/sodomka13.pdf).\n-   **Contextual Bandits** [tutorial](https://hunch.net/~rwil/) [video](https://vimeo.com/240429210) + papers from MS research videos on contextual bandit algorithms.\n-   Interesting papers:\n    -   We discussed how Dynamic Programming can't handle games like chess. Here are some RL methods that can.\n        -   [Muzero](https://www.nature.com/articles/s41586-020-03051-4.epdf?sharing_token=kTk-xTZpQOF8Ym8nTQK6EdRgN0jAjWel9jnR3ZoTv0PMSWGj38iNIyNOw_ooNp2BvzZ4nIcedo7GEXD7UmLqb0M_V_fop31mMY9VBBLNmGbm0K9jETKkZnJ9SgJ8Rwhp3ySvLuTcUr888puIYbngQ0fiMf45ZGDAQ7fUI66-u7Y%3D)\n        -   [MuZero](https://arxiv.org/abs/2202.06626) and\n        -   [EfficentZero](https://arxiv.org/abs/2111.00210) [code](https://github.com/YeWR/EfficientZero)\n\n## Coding Bandits with MESA\n\n::: {#6135b06a .cell execution_count=6}\n``` {.python .cell-code}\nfrom tqdm import tqdm\nfrom mesa import Model, Agent\nfrom mesa.time import RandomActivation\nimport numpy as np\n\n\n\nclass EpsilonGreedyAgent(Agent):\n    \"\"\"\n    This agent implements the epsilon-greedy \n    \"\"\"\n\n    def __init__(self, unique_id, model, num_arms, epsilon=0.1):\n        super().__init__(unique_id,model)\n        self.num_arms = num_arms\n        self.epsilon = epsilon\n        self.q_values = np.zeros(num_arms)  # Initialize Q-value estimates\n        self.action_counts = np.zeros(num_arms)  # Track action counts\n\n    def choose_action(self):\n        if np.random.rand() < self.epsilon:\n            # Exploration: Choose random arm\n            return np.random.randint(0, self.num_arms)\n        else:\n            # Exploitation: Choose arm with highest Q-value\n            return np.argmax(self.q_values)\n\n    def step(self, model):\n        chosen_arm = self.choose_action()\n        reward = model.get_reward(chosen_arm)\n        assert reward is not None, \"Reward is not provided by the model\"\n        self.action_counts[chosen_arm] += 1\n        self.q_values[chosen_arm] = (self.q_values[chosen_arm] * self.action_counts[chosen_arm] + reward) / (self.action_counts[chosen_arm] + 1)\n\n\nclass TestbedModel(Model):\n    \"\"\"\n    This model represents the 10-armed bandit testbed environment.\n    \"\"\"\n\n    def __init__(self, num_arms, mean_reward, std_dev,num_agents=1):\n        super().__init__()\n        self.num_agents = num_agents\n        self.num_arms = num_arms\n        self.mean_reward = mean_reward\n        self.std_dev = std_dev\n        self.env_init()\n        #self.arms = [None] * num_arms  # List to store arm rewards\n        self.schedule = RandomActivation(self)\n        for i in range(self.num_agents):\n          self.create_agent(EpsilonGreedyAgent, i, 0.1) \n\n    def env_init(self,env_info={}):\n        self.arms = np.random.randn(self.num_arms)  # Initialize arm rewards\n\n    def create_agent(self, agent_class, agent_id, epsilon):\n        \"\"\"\n        Create an RL agent instance with the specified class and parameters.\n        \"\"\"\n        agent = agent_class(agent_id, self, self.num_arms, epsilon)\n        self.schedule.add(agent)\n        return agent\n\n    def step(self):\n        for agent in self.schedule.agents:\n            chosen_arm = agent.choose_action()\n            reward = np.random.normal(self.mean_reward, self.std_dev)\n            self.arms[chosen_arm] = reward  # Update arm reward in the model\n            agent.step(self)  # Pass the model instance to the agent for reward access\n\n    def get_reward(self, arm_id):\n        # Access reward from the stored list\n        return self.arms[arm_id]\n\n\n# Example usage\nmodel = TestbedModel(10, 0, 1)  # Create model with 10 arms\nnum_runs = 200                  # The number of times we run the experiment\nnum_steps = 1000                # The number of pulls of each arm the agent takes\n\n\n# Run simulation for multiple steps\nfor _ in tqdm(range(num_runs)):\n    for _ in range(num_steps):\n        model.step()\n    model.step()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/oren/.local/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:\n\nThe AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.\nWe would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919\n\n\r  0%|          | 0/200 [00:00<?, ?it/s]\r  4%|‚ñç         | 8/200 [00:00<00:02, 78.08it/s]\r  8%|‚ñä         | 16/200 [00:00<00:02, 77.85it/s]\r 12%|‚ñà‚ñè        | 24/200 [00:00<00:02, 78.08it/s]\r 16%|‚ñà‚ñå        | 32/200 [00:00<00:02, 78.05it/s]\r 20%|‚ñà‚ñà        | 40/200 [00:00<00:02, 78.39it/s]\r 24%|‚ñà‚ñà‚ñç       | 48/200 [00:00<00:01, 78.24it/s]\r 28%|‚ñà‚ñà‚ñä       | 56/200 [00:00<00:01, 78.25it/s]\r 32%|‚ñà‚ñà‚ñà‚ñè      | 64/200 [00:00<00:01, 78.19it/s]\r 36%|‚ñà‚ñà‚ñà‚ñå      | 72/200 [00:00<00:01, 78.01it/s]\r 40%|‚ñà‚ñà‚ñà‚ñà      | 80/200 [00:01<00:01, 77.99it/s]\r 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 88/200 [00:01<00:01, 77.66it/s]\r 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 96/200 [00:01<00:01, 77.58it/s]\r 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 104/200 [00:01<00:01, 77.93it/s]\r 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 112/200 [00:01<00:01, 78.03it/s]\r 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [00:01<00:01, 78.24it/s]\r 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 128/200 [00:01<00:00, 78.20it/s]\r 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 136/200 [00:01<00:00, 77.77it/s]\r 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 144/200 [00:01<00:00, 77.96it/s]\r 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 152/200 [00:01<00:00, 77.79it/s]\r 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 160/200 [00:02<00:00, 78.08it/s]\r 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 168/200 [00:02<00:00, 78.29it/s]\r 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 176/200 [00:02<00:00, 78.33it/s]\r 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 184/200 [00:02<00:00, 78.30it/s]\r 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 192/200 [00:02<00:00, 78.10it/s]\r100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:02<00:00, 78.25it/s]\r100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:02<00:00, 78.06it/s]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "c1-w1_files"
    ],
    "filters": [],
    "includes": {}
  }
}