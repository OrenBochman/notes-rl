{
  "hash": "3e8d8db845a68141272e0a833d28f650",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2022-05-01\nlastmod: 2024-05-06\ntitle: Course Introduction\nsubtitle: RL Fundamentals\ndescription: In week 1 we define some key concepts like rewards, states, action, value functions, action values functions. We consider the the multi-armed bandit problem, leading to exploration explotation dillema, and the epsilon greedy algorithm.\nauthor: Oren Bochman\ncategories: \n  - Coursera\n  - Fundamentals\n  - Reinforcement Learning\nkeywords:\n  - The Bandit problem\n  - Bandit algorithms\n  - Exploration \n  - Explotation\n  - Epsilon greedy algorithm\n  - Sample avarage method\njupyter: python3\nimage: img/nlp-brain-wordcloud.jpg\ntitle-block-banner: /images/banner_black_3.jpg\n---\n\n\n\n\n![RL logo](img/logo.png){.column-margin} \n\n![RL algorithms](img/alg_selector.png){.column-margin group=\"slides\"}\n\n\n# Introduction\n\n- The course instructors,  Adam and Martha White, are world class researches researchers.\n- The University of Alberta is by far the strongest university for RL.\n- RL builds on the foundations of Markov Decision Processes, Dynamic Programming, and Monte Carlo methods as well as Machine Learning and Deep Neural Networks.\n  - I found that learning these topics is better motivated and there fore easier to grasp when presented in the context of RL.\n- I read reviews by a number of people have taken this specialization. A few have had some background in RL and breezed through the material. The rest  suggested that the material can be challenging particularly the last two courses.\n- Some of the assignments appear in RL textbooks, but instead of a few lines of code for the main assignment, in the capstone we get to implement both Parts of the environment, the RL algorithm, the replay buffer, the neural net and the optimizer from scratch. This is a blessing in disguise as it really gives one more opportunity to get to grips with these complex subjects. \n- A big chunk of the programming assignments have to handle the environment rather than the RL algorithm - many students complain about this. \n  - My insight has repeatedly been that in RL the algorithms are often very general and that the key to success for solving a particular problem with RL is all about thinking the environment through. \n  - Many environments have different challenges and can reveal the weaknesses of many time tested algorithms.\n  - Real world problems are much more challenging but the more environments you work with, the better you will be able to find good features that can make solving them with RL very different.\n  - Another aspect of RL is called reward shaping which is defining a reward structure that can provide the algorithm with good signal so as to support better credit assignment to actions in the correct state. This is an integral part of environment design.\n  - Finally I found that when you try to implement RL algorithms for real world problems you will want to modify the algorithms to take advantage of some structure in the environment.\n  - Luckily there are today more environments available then I could list, it is easy to create new ones.\n  The bottom line is that the challenges and algorithms of RL only make sense when considered in the context of environments.\n- In my opinion many of the assignments are not very interesting - the coding problems often border on the trivial.\n- If you want to work with RL professionally you will need to do a lot more coding.\n  - You should consider some of the extra coding assignments that ware included in the text book.\n  - You should also consider implementing additional algorithms on the environments from the assignments.\n  - There are tons of other more interesting environments to work with consider taking you implementation to the next level in these environments.\n    - In reality even many of the environments we use in the assignments have surprising challenges.\n    - The moon lander in the capstone for example tends to learn to hover for a long time to avoid crashing\n      - The solution comes from a paper which suggests adding a time limit to the episode. However the assignment uses the time limit without mentioning this.\n      - I thought that the time limit was there to speed up grading until I tried to dive deeper.\n- We spend lots of time tinkering with RL-Glue a library that is no longer being maintained.\n- Although my background is in Mathematics I found the videos rather terse. \n  - They often list goals at the start and at the end again, so all in all the instructional part can be very brief.\n  - It would be easier to recap not just reference equations, update rules, and algorithms that have been covered a while back or in a previous course.\n  - Many They frequently reference old material which you may not have *fully digested* yet. \n- I discovered that I had worked though all the videos, quizzes and programming assignments scoring 100% but I hadn't really digested the material.\n  - These notes are my attempt to do so. \n  - I try to make the material more accessible and to connect the dots.\n- I don't include the programming assignments in these notes. \n  - I may add simple agents with from different environments and using libraries other than RL-Glue to avoid any issues raised by the Coursera honor code.\n- I don't include the quizzes either \n  - I found that most can be aced by just reviewing the videos before taking them and then reviewing the answers.\n  - I'd like to build a question/answer bank \n  - But I think that coding is much better time investment.\n\n\n- There are lots of good notes on the web on RL.\n  - Summaries of the text book\n  - Slides for the courses offered by Richard S. Sutton and Andrew G. Barto\n  - Slides for the course offered by David Silver\n  - Notes from more advanced courses\n  - Books & Notes from some of the Guest lectures in this course.\n- So I was reluctant to add to the pile. I found out after completing the first two courses that I had not really digested the material. \n  - I had to go back and review the material and then write these notes.\n  - I found that the act of writing the notes helped me to understand some of the trickier bits from the book.\n  - I also wanted to see how I could connect the dots between the different parts of the course.\n  - I hope that you find these notes helpful.\n\n\n::: callout-caution\n## Deceptively simple :bulb: {.unnumbered}\n\n[This course is **deceptively simple**]{.mark} - the chart in the margin provides a great summary of the material for the whole specialization. Only a handful of concepts are needed to master RL.\n\n-   This specialization is all about connecting the dots.\n-   We revisit the same ideas over and over improving them in small but significant ways by relaxing the assumptions. e.g. from bandits with one state we move to MDP with many states and get the ability to formulate plans. From Dynamic programming with a fully specified model we move to model free settings where we might not be able to efficiently learn a model. From tabular methods where we treat each state as a separate entity we we move to function approximation and deep learning where we can generalize from one state to many others.\n-   In this course and the more connections you make the better you will understand and remember material.\n-   And the greater you facility to apply RL to new problems.\n:::\n\nThe following are my tips for getting the most from this specialization\n\n::: callout-tip\n## Connecting The Dot to see the Forest For the Trees üéÑ \n\nTo connect the dots I :heart: recommend:\n\n1.  **Annotate** üñäÔ∏è you e-copy of the book üìñ\n2.  **Flash cards** üóÇÔ∏è are your üßë‚Äçü§ù‚Äçüßë friends.\n  - We don't need too many but they can help you keep the essentials (algorithms, definitions, some formulas, a few diagrams) fresh in your mind.\n  - Keep reviewing these until you impress the folks at DeepMind in your interview. :wink:\n3.  **Review** üëÅÔ∏è the videos/quizzes until nothing seems surprising/confusing [^1].\n4.  **Review** üëÅÔ∏è your notes every time you complete a part of the specialization. Also a great idea if have an RL interview üíº\n5.  **Coding**: If you have time do extra RL coding\n    1.  Start with developing more environments, simple and complex ‚õ©Ô∏è\n    2.  Implement more algorithms - from the course, the books, papers.‚õ©Ô∏è\n    3.  The notebooks also try to teach you experiments and analysis comparing algorithms performance. If you assimilate this part you are really going to shine. ‚õ©Ô∏è\n:::\n\n[^1]: The annotated book and flashcards will help here. This material is really logical - if you are surprised/confused you never assimilated some part of the material. Once you do it should become almost intuitive to reason about from scratch.\n\n::: callout-tip\n## Mathematical Mnemonics üòç\n\nAs a Mathematics major I can attest that Mathematics becomes 10x easier so long as you can recall üß† the basic definitions and their notation.\n\nI have extracted the essentials from the text book below. Best to memorize these or at least keep a copy handy and you are well on your way to grokking this course\n\n-   $G_t$ **return** at time t, for a $(s_t, a_t, r_t...)$ sequence discounted by $\\gamma\\in(0,1)$.\n-   $r(s,a)$ - **expected immediate rewards** for action $a$ in state $s$\n-   $\\pi$ **policy** - a decision making rule for every state.\n-   $\\pi_*$ **optimal policy** - which returns the maximum rewards.\n-   $p(s',r \\vert s,a)$ - **transition probability** to state $s'$ with reward $r$ from state $s$ via action $a$ AKA **four valued dynamics** function.\n-   $p(s' \\vert s,a)$ - **transition probability** to state $s'$ from state $s$ via action $a$ AKA **Markov process transition matrix**\n-   $v_\\pi(s)$ - state's **value** under policy $\\pi$ which is its expected return.\n-   $q_\\pi(s,a)$ - the **action value** in state $s$ under policy $\\pi$.\n:::\n\n\n::: callout-note\n### Guest Lectures {.unnumbered}\n\nThere are a number of guest lectures in this specialization - I think that about half are in the last course.\n\n- These often provide a teaser into an area of research that extends the material in the course. \n- Many are boring but\n  - almost all present papers and results that are worth some review.\n  - a few present too many papers, which suggests a triage approach\n  - sooner of later you might have a real problem that is similar to the one they are addressing.\n  - so I suggest you try and spend some time \n- RL was more or less invented 4 times in different fields - Control Theory, Operations Research, Machine Learning and Psychology.\n- The guest lectures often present the material from one of these fields.\n\n:::\n\n## A Long term plan to learning Deep RL\n\n1. do this specialization\n2. read the rest of the book - there are a number of important subjects and algorithms that are not covered in the course.\n3. solve more problems\n4. read more papers \n  - there are a zillion algorithms and a gazillion environments out there \n  - reading a few papers will get you up to speed on the state of the art.\n5. implement more algorithms\n  - this is what people are going to pay you for\n  - start with gradient bandit algorithms \n  - then Thompson Sampling using different distributions\n6. try and solve a real problem:\n  - I created a custom RL algorithm to solve the Lewis signaling game quickly and efficient - this allows agents to learn to communicate.\n  - I am now working on designing a multi-agent version of the game that should learn even faster.\n7. do the deep rl course from Hugging Face\n8. find RL challenges on Kaggle\n\n",
    "supporting": [
      "c1-w0_files"
    ],
    "filters": [],
    "includes": {}
  }
}