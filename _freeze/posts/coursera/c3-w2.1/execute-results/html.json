{
  "hash": "c0e9edf545bcb255b760974e27b892ce",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-04-02\nlastmod: 2024-04-02\ntitle: Constructing Features for Prediction\nsubtitle: Prediction and Control with Function Approximation\nauthor: Oren Bochman\ndraft: false\ncategories:\n  - Coursera\n  - Prediction and Control with Function Approximation\n  - Reinforcement Learning\nkeywords:\n  - reinforcement learning\n  - neural networks\n  - feature construction\n  - tile coding\n  - coarse coding\n  - feed-forward architecture\n  - fourier basis functions\n  - radial basis functions\n  - gradient rl algorithms  \n  - 1000 step Random Walk\n  - Gymnasium Environment\n  - Einstein Tiling\nimage: img/nlp-brain-wordcloud.jpg\ntitle-block-banner: /images/banner_black_3.jpg\n---\n\n\n\n\n![RL logo](img/logo.png){.column-margin} \n\n![RL algorithms](img/alg_selector.png){.column-margin group=\"slides\"}\n\n\n# Introduction\n\n::: {.callout-tip collapse=\"true\"}\n### Readings {.unnumbered}\n\n-   [x] [@sutton2018reinforcement§9.4-9.5.0, pp. 204-210] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n-   [x] [@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n-   [x] [@sutton2018reinforcement§9.7, pp. 223-228] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n\n:::\n\nThis is not a video lecture or notes for a learning goal. This is however my attempts to cover some material from the readings from chapter 9 of [@sutton2018reinforcement] mentioned above.\n\nI've added this material about a year after completing the specialization as I have been taking a course on deep reinforcement learning. I had felt that the material in this course had been both  challenging on occasion and rather basic on others. \n\nI don't think I would have would have gone ahead and  created this unit without looking at the material by Shangtong Zhang at https://github.com/ShangtongZhang/reinforcement-learning-an-introduction In which he reproduced some of the figures from the book. However this is my own implementation and though similar to some of the course material - I follow the Coursera Honor code and this is not likely to be much help to anyone working on the programming assignments which is a different environment, uses RL-glue and an implementation of tile coding. \n\n\nWhat I felt was that I was not happy about the basics in this chapter. This includes the parameterization of the value function, the convergence results regarding linear function approximation. The ideas about why TD is a semi-gradient method etc. \n\nI am also having many idea about both creating algorithms for creating features for RL environments. As I get familiar with gridwolds, atari games, sokoban etc I had many good ideas for making progress in both these environments and for improving algorithms for more general cases. \n\nFor example it appears that in many papers it turns out the agents are not learning very basic abilities. My DQN agent for space invaders was very poor at shooting bullets. I had an few ideas that should make a big difference. Like adding features for bullets,  the invaders and so on. This are kind of challenging to implement in the current gymnasium environments. However I soon had a much more interesting idea that seems to be good for many of the atari environments and quite possibly even more broadly to most cnn based agents. \n\n- In brief this would combine \n    - a multiframe YOLO, \n    - a generelised value function to replace YOLO's supervision \n    - a 'robust' causal attention mechanism to decide which pixels are more or less important \n        - dropping them would not impact performance. e.g. bullets\n        - which affect survival e.g. certain bullets\n        - for scoring e.g. mother ship \n        - which ones we can influence e.g. standing under an invader gets it to shoot at us.\n    \nNote this is not the causal attention mechanism from NLP where one censored the future inputs but rather a mechanism that decides which pixels represent features that are potentialy the cause of the future states.\n\nClearly this Algorithm and its parts need to be worked out in a very simple environment. The YOLO part is all about modeling features using bounding boxes via a single pass. The GVFs are to replace yolo supervision loss with a RL compatible loss and the causal attention to reduce the overfitting and speed up learning.\n\nI decided that converting some of these simple environments to gymnasium environments would be a good way to kick start some of these ideas, more so as reviewing papers and talks by Adam and Martha White shows that most experiments in RL environments turn out to be too complicated and access to simple environments turns out to be the way to get the experiments started.\n\n\nIn this chapter we use a simple environment called the 1000 state Random walk. I implemented this independently in Python. \n\nWe also learned  the MC prediction algorithm and the TD(0) algorithms for function approximation.\n\nWe will use these algorithms to learn the value function of the Random Walk 1000 environment. We will also use tile coding and coarse coding to create features for the Random Walk 1000 environment.\n\n# Feature Construction & linear Function approximation\n\n::: callout-note\n### Learning Objectives {.unnumbered}\n\n\n\nIn this module we consider environments that allow to consider simple state aggregationI'd like to cover the following topics:\n\n1. [x] create gymnasium compatible environment for the 1000 step Random Walk environment. \n    - This will allow to use it with many RL agents built using many libraries. \n    - [x] I'd also want to extend this environment to include a neighborhood size parameter - this will set the bound how far a left or right step will move us. \n    - [x] A dimension parameter to control dimensions of the state space. This can allow us to consider state aggregation in one two and three dimensions.\n2. [x] Plot the trajectory of the random walk.\n3. [ ] Implement the function approximation environment for gymnasium.\n    - [ ] extend to looking at interpolation and regression problems in the context of RL. (How - using a different dataset loaded by pandas?) Time series, Tabular data, Clustering, Classification, Regression, Pricing with elasticity. Multi-dimensional data.\n    - This environment can be a basis for looking at how Temporal Abstraction aggregation play with function approximation in a highly simplified form. \n    - This is a fundamental issue that [Doina Precup](https://www.youtube.com/watch?v=GntIVgNKkCI) has raised in her talks as an ongoing area of research. \n    - So such an environment might be useful in testing how different approaches can handle these issues in an environment that is very close to supervised learning. \n2. implement Gradient MC agent\n3. implement Semi Gradient TD(0) agent\n4. use these agents with and without state aggregation to learn the value function of the Random Walk 1000 environment.\n5. implement coarse coding via tile coding to create features for the Random Walk 1000 environment.\n6. implement use of **polynomial features** to create features for the Random Walk 1000 environment.\n7. implement use of **radial basis functions** to create features for the Random Walk 1000 environment.\n8. implement use of **Fourier basis functions** to create features for the Random Walk 1000 environment.\n\n:::\n\n## The 1000 Step Random Walk Environment \n\nIn this lesson we implement the 1000 Random Walk example as an environment. This is good to demonstrate how to construct features for linear methods. We will use tile coding and coarse coding to create features for the Random Walk 1000 environment.\n\n::: {#2ee026e5 .cell execution_count=1}\n``` {.python .cell-code}\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass RandomWalk1000(gym.Env):\n    def __init__(self, num_states=1000, neighborhood_size=100, seed=None):\n        super().__init__()\n        self.num_states = num_states\n        self.neighborhood_size = neighborhood_size\n        self.observation_space = spaces.Discrete(num_states + 2) # add two states 0 and num_states + 1 as terminal states\n        self.action_space = spaces.Discrete(2)  # 0 for left, 1 for right\n        self.current_state = 500 # start in the middle\n        self.np_random, seed = gym.utils.seeding.np_random(seed)\n        self.trajectory = [500]\n\n    def reset(self, *, seed=None, options=None):\n        super().reset(seed=seed)\n        self.current_state = 500\n        self.trajectory = [500]\n        return self.current_state, {}\n\n    def step(self, action):\n\n        if action == 0: # move left\n             # left neighbours\n            left_start = max(1, self.current_state - self.neighborhood_size)\n            left_end = self.current_state\n            num_left = left_end - left_start\n\n            if left_start == 1:\n                prob_terminate_left = (self.neighborhood_size - num_left) / self.neighborhood_size\n            else:\n                prob_terminate_left = 0\n            \n            if self.np_random.random() < prob_terminate_left:\n               \n                return 0, -1, True, False, {} # terminate left\n\n            next_state = self.np_random.integers(low=left_start, high=left_end)\n\n\n        elif action == 1: # move right\n             # right neighbours\n            right_start = self.current_state + 1\n            right_end = min(self.num_states + 1, self.current_state + self.neighborhood_size + 1)\n            num_right = right_end - right_start\n            if right_end == self.num_states + 1:\n                 prob_terminate_right = (self.neighborhood_size - num_right) / self.neighborhood_size\n            else:\n                prob_terminate_right = 0\n            \n            if self.np_random.random() < prob_terminate_right:\n\n                return self.num_states + 1, 1, True, False, {} # terminate right\n\n            next_state = self.np_random.integers(low=right_start, high=right_end)\n        else:\n            raise ValueError(\"Invalid action\")\n\n        self.current_state = next_state\n\n        self.trajectory.append(self.current_state)\n        return self.current_state, 0, False, False, {} # not terminated or truncated\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_trajectory(trajectory, num_states):\n    \"\"\"Plots the trajectory of the random walk.\"\"\"\n    x = np.arange(len(trajectory))\n    y = np.array(trajectory)\n    \n    plt.figure(figsize=(12, 4))\n    plt.plot(x, y, marker='o', linestyle='-', markersize=3)\n    plt.xlabel('Time Step')\n    plt.ylabel('State')\n    plt.title('Random Walk Trajectory')\n    plt.yticks(np.arange(0, num_states+2, 100))\n    plt.grid(axis='y')\n\n    plt.tight_layout()\n    plt.show()\n```\n:::\n\n\n::: {#eef7b364 .cell execution_count=2}\n``` {.python .cell-code}\n#import gymnasium as gym\n#from random_walk_gym import RandomWalk1000\n\nenv = RandomWalk1000()\n\n# Reset the env\nobs, info = env.reset()\nterminated = False\n\nwhile not terminated:\n    # For this environment, an action is not needed.\n    # Here we pass in a dummy value\n    obs, reward, terminated, truncated, info = env.step(0)\n    print(f\"State: {obs + 1}, Reward: {reward}, Terminated: {terminated}\")\n\nenv.close()\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nState: 456, Reward: 0, Terminated: False\nState: 364, Reward: 0, Terminated: False\nState: 309, Reward: 0, Terminated: False\nState: 261, Reward: 0, Terminated: False\nState: 198, Reward: 0, Terminated: False\nState: 174, Reward: 0, Terminated: False\nState: 115, Reward: 0, Terminated: False\nState: 109, Reward: 0, Terminated: False\nState: 21, Reward: 0, Terminated: False\nState: 12, Reward: 0, Terminated: False\nState: 1, Reward: -1, Terminated: True\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](c3-w2.1_files/figure-html/cell-3-output-2.png){width=1142 height=374}\n:::\n:::\n\n\n::: {#3b5f44d1 .cell execution_count=3}\n``` {.python .cell-code}\nenv = RandomWalk1000(num_states=1000, neighborhood_size=100)\nobs, info = env.reset()\nterminated = False\ntruncated = False\ntrajectory = []\nwhile not terminated and not truncated:\n    action = env.action_space.sample()  # Replace with your policy\n    obs, reward, terminated, truncated, info = env.step(action)\n    print(f'{obs=}, {action=}, {reward=}, {terminated=}')\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nobs=457, action=0, reward=0, terminated=False\nobs=414, action=0, reward=0, terminated=False\nobs=456, action=1, reward=0, terminated=False\nobs=531, action=1, reward=0, terminated=False\nobs=589, action=1, reward=0, terminated=False\nobs=635, action=1, reward=0, terminated=False\nobs=543, action=0, reward=0, terminated=False\nobs=576, action=1, reward=0, terminated=False\nobs=647, action=1, reward=0, terminated=False\nobs=680, action=1, reward=0, terminated=False\nobs=772, action=1, reward=0, terminated=False\nobs=744, action=0, reward=0, terminated=False\nobs=676, action=0, reward=0, terminated=False\nobs=730, action=1, reward=0, terminated=False\nobs=654, action=0, reward=0, terminated=False\nobs=618, action=0, reward=0, terminated=False\nobs=543, action=0, reward=0, terminated=False\nobs=469, action=0, reward=0, terminated=False\nobs=569, action=1, reward=0, terminated=False\nobs=482, action=0, reward=0, terminated=False\nobs=492, action=1, reward=0, terminated=False\nobs=401, action=0, reward=0, terminated=False\nobs=336, action=0, reward=0, terminated=False\nobs=316, action=0, reward=0, terminated=False\nobs=286, action=0, reward=0, terminated=False\nobs=340, action=1, reward=0, terminated=False\nobs=350, action=1, reward=0, terminated=False\nobs=288, action=0, reward=0, terminated=False\nobs=319, action=1, reward=0, terminated=False\nobs=259, action=0, reward=0, terminated=False\nobs=290, action=1, reward=0, terminated=False\nobs=335, action=1, reward=0, terminated=False\nobs=272, action=0, reward=0, terminated=False\nobs=313, action=1, reward=0, terminated=False\nobs=355, action=1, reward=0, terminated=False\nobs=278, action=0, reward=0, terminated=False\nobs=203, action=0, reward=0, terminated=False\nobs=150, action=0, reward=0, terminated=False\nobs=240, action=1, reward=0, terminated=False\nobs=152, action=0, reward=0, terminated=False\nobs=66, action=0, reward=0, terminated=False\nobs=111, action=1, reward=0, terminated=False\nobs=157, action=1, reward=0, terminated=False\nobs=58, action=0, reward=0, terminated=False\nobs=0, action=0, reward=-1, terminated=True\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](c3-w2.1_files/figure-html/cell-4-output-2.png){width=1142 height=374}\n:::\n:::\n\n\nLets simulate the random walk till success and plot its trajectory.\n\n::: {#3ad5d6ea .cell execution_count=4}\n``` {.python .cell-code}\nenv = RandomWalk1000(num_states=1000, neighborhood_size=100)\nobs, info = env.reset()\nterminated = False\ntruncated = False\nwhile not terminated and not truncated:\n    action = env.action_space.sample()  # Replace with your policy\n    obs, reward, terminated, truncated, info = env.step(action)\n\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n```\n\n::: {.cell-output .cell-output-display}\n![](c3-w2.1_files/figure-html/cell-5-output-1.png){width=1142 height=374}\n:::\n:::\n\n\n## A short digression on Einstein Tiling for RL\n\nOne (awful) Idea I keep returning to is to use Einstein Tiling for RL. I mentino that Einstein in this context is not the physicist but rather a pun on the word 'Einstein' which means 'one stone' in German.\n\nLet's quickly review why it is a bad idea, and then why it is also a fascinating idea.\n\n1. Unlike A square tiling this is an aperiodic tiling so we need to generate it efficiently. Depending on the space it will may take some time to generate the tiling. We need to store the tiling in memory.\nFor a square tiling we can generate the tiling in a few lines of code. We can access the tiling or tile using a simple formula.\n\n2. We need a quick way to find which tile a point is in. This is not to hard for one tile. But as the number of tiles increases this becomes more difficult. It is trivial for a square tiling where again we have a formula to efficiently determine the tile a point belongs to.\n\nSome reasons why it is a fascinating idea.\n\n1. We only need one tiling. If we have one we can map the first tile ton any other location it is in the same orientation and we will get a new tiling! This is due to the aperiodic nature of the tiling.\n2. The hat tile is constructed by glueing together eight smaller kite tiles that are sixth of a hexagon. We can easily use larger kites that so we can use two such grids as coarse and coarser tilings. \n3. Different can be similar locally but will tend to diverge. This suggest that we will get a good generalization.\n4. There may be variant einsteins that are easier to generate and use\n5. In https://www.ams.org/journals/proc/1995-123-11/S0002-9939-1995-1277129-X/ the authors show that for d>=3 aperiodic tilings can naturally avoid more symmetries than just translations. I.e. we can have a periodic tilings in higher dimensions. \n\nI may be wrong but It may be possible to generate the tiling using a simple formula. Ok so far tiling generation is insanely complicated.\nThough this is not a judgment on the complexity of the tiling but rather the complexity of the code and mathematics to generate the tiling.\n\nThe hat tile allows one to create many different tilings of the state space in two dimensions. Each tiling is going to have a different set of features. \n\nAs the hat tile is constructed by glueing together eight smaller tiles. Tilings are created in a hierarchical manner. This suggests that we can will get fine and course feature in this process and that \nwe can just keep going to increase discrimination.\n\n\nSome issues - it is possible to get two tilings that are the same but for a 'conway worm' this is a curve in the tiling that is different.\nThe problem here is that the features will be the same for every where except the worm. Not good for generalization.\n\n",
    "supporting": [
      "c3-w2.1_files"
    ],
    "filters": [],
    "includes": {}
  }
}